<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[红杉资本刘星：新零售未来5年的“五化”趋势]]></title>
    <url>%2F2018%2F4f8923b2%2F</url>
    <content type="text"><![CDATA[《零售老板内参》联合36氪举办的新零售创新创投峰会在深圳金蝶软件园隆重举行。专注消费零售领域的顶级投资机构、投资人，深耕新零售一线实践创业精英。 文 | 红杉资本中国基金合伙人刘星 零售老板内参独家专稿 未经许可不得转载 4月24日，《零售老板内参》联合36氪举办的新零售创新创投峰会在深圳金蝶软件园隆重举行。专注消费零售领域的顶级投资机构、投资人，深耕新零售一线实践创业精英，新消费新零售领域备受追捧的现象级产品和项目，零售行业的权威大咖、意见领袖，以及500多名零售行业精英齐聚，围绕“新消费、新零售、新机遇”展开丰富多样的主题演讲、圆桌对话，深入探讨消费零售行业2018年的新趋势以及新消费、新零售赛道的新机遇。 在峰会上，红杉资本中国基金合伙人刘星发表了精彩演讲。以下为演讲速记整理： 感谢36氪和《零售老板内参》的邀请，让我有机会跟大家分享。我是一个不合格的学生，我刚才知道老师给我出的题是无人零售，我自作主张给自己出了一个新题“探索消费新时代”。 之所以用“探索”这个词，大家说2017年是新零售的元年，2017年在新零售领域风风火火，主持人提到2018年会更加活跃。我同意以上的说法，但我们应该把眼光和思考放在更长的时间维度。所以客观来说我们还在探索阶段，很多新模式、新业态，我们看到的很好的市场反映和特别好的业内媒体的报道，相信他们探索的创新处于早期发展阶段，包括无人零售、无人货架、自助售货机、无人便利店。 在休息室有两位创业者跟我讨论“无人”是否重要。我们应该有探索的心态，既然是探索，我们不能太着急，没什么东西可以一口吃成大胖子。零售消费行业需要一段时间的沉淀，既然是探索，给自己犯错的空间。在创业探索的过程中可能会遇到挫折、交一点学费，这都是正常的，所以我特别想谈谈“探索”。 消费新时代，在接下来5-10年展现在我们面前的，是一个很壮观的画面。这是我们心目中的消费新时代，我们今天刚好走上探索之路。 这是我去年分享过的PPT，当中有什么迭代，在此基础上有什么新想法。2017年4月，我在一个小型会议上做分享——《新零售究竟新在哪里》。去年我第一个讲的是新业态，今天我把新业态放在最右边，这是我认知的改变。我们开始接触新零售时，发现由于新业态的出现，让我们感觉有新东西出现。实际上新业态是一个结果，因为人们在前三个维度上展开探索，最后像一盘菜似的端到别人面前。最后是新人群、新品牌、新技术。收入能达到中产水平的人显著增加，这些人群背后是我们消费新时代的主力，这些人更个性、更识货。他通过海淘、旅行更会辨别好商品，“更会花”意味着他更有钱、更懂得怎么花钱，他知道如何选择好的商品，如何为好的商品买单。这些人群的特征推动新零售、新消费的发展。 前几天一个朋友在群里提出一个很好的问题，“技术为新零售提供什么价值”，有成功的案例让我们看到应用后，便有明显的业绩提升吗？大家能不能脱口而出告诉大家某一个特别成功的案例，在我们跟很多创业者、消费零售企业的接触过程中，可以清晰的看到潮流的涌动。 在新技术的问题上要有一定的信仰，看看快手、拼多多的崛起，这是纯线上的企业，这两家是非常好的代表，两个创始人都是理工男，都是工程师，他们对技术有天生的信仰，事实上结果证明其信仰是正确的。快手内容的推荐不通过编辑，决定哪些内容在前面出现，哪些内容在后面出现。拼多多的老总并没有商品团队，由算法驱动什么商品卖得好。如果有机会看到这两家公司的图表，如拼多多，拼多多和阿里、淘宝、唯品会相比，他达到2亿用户的时间比前几家公司短非常多，证明数据和算法驱动是有效的。行业和公司要有信仰，相信数据和技术的力量，并不是你今天使用工具，明天便能看到效果，这需要一定的时间沉淀和积累。 新零售的目标是什么，带给客户美好的新的体验。我希望大家在用户体验上花更多的时间打磨和思考，我是否带给客户美好的体验，这个体验是否让客户感受到舒服、顺畅、贴心，这个体验是否有新鲜感。作为企业的领导者，我是否考虑降低成本，提高效率。作为零售行业，最终不能达到效率和成本，无法形成经济模型。虽然你给客户新的体验，但生意可能做不下去。我认为这是从业者需要考虑的点，凡是都有一个最根本的立足点，最根本的立足点在于给客户提供美好的新的体验。如果能为客户提供美好的新的体验，你的经济模型理论上是成立的。我们不能为了新零售而新零售，技术很重要，对数据要有信仰，非得为了应用和数据做技术，有点离开初心了。 新零售也离不开新品牌，在未来5-8年会有大批的新兴品牌出现，这些品牌从何而来，除了从品类创新入手外，更多的会源于品质提升，同时也会出现一些在品味上带来新体验的。有多少人看过《品类杀手》这本书，从新品类入手创建一个新品牌，这是被历史和世界各国验证过的创立品牌的方式。中国经过过去二三十年经济大发展，很多品类被品牌占据了，是否有新的品类作为基础构建新品牌，我认为有。 前两天买了一双鞋，这个品牌说你穿这个鞋子到野外跑步，可以在野外比较软的地上跑步。中国人特别多，哪怕你做的是小众产品，事实上小众不小，因为中国人口基数非常大。前5-10年中国品牌从品类诞生，我认为未来新品牌诞生的基础是更高的品质。 有几条简单而朴素的原理，你是不是用更好的材料，可能由于技术的创新导致你的品质提升。你的确用了更多的匠心精神在产品品牌上，品味有调性，就像诚品说“我们不是卖书的，我们是推广阅读的”。无印良品推崇的是“我们购买商品不看品牌，不看包装”。可品味，让用户一方面感觉到有品味，另一方面是可以跟你互动、把玩，我可以细细的咀嚼感受，这是可品味。我认为新品牌未来会有大量的机会，我认为未来5-10年更大的机会是以品质为根基创建品牌，最后以品味为根基创建品牌。 品牌零售化和零售品牌化。原来品牌商在传统的央视广告、门店，对消费者每个个体，对商品每个SKU在什么场景下被谁购买，被谁使用，零售终端并不了解。在现在新技术时代下，有新零售互联网的传播，通过营销传达给客户，通过直营门店了解市场的脉搏，品牌商把自己变得更加零售化，让自己更懂零售，让自己更有机会直接面对消费者。零售在品牌化，一些新兴零售企业非常注重本身的品牌打造，不像以前的零售企业只是提供一个场所，在合适的时间把合适的商品卖给合适的用户。现在新零售企业非常重视本身的品牌塑造，同时很多开始做自有品牌，自有品牌的比例在拉升。零售品牌化是非常明显的趋势。 在新技术上，大家应该更勇敢、更积极的拥抱创业公司提供的解决方案。国际解决方案的企业和软件企业不能很好的了解中国市场的变化，中国本土企业和创业企业更了解市场结构，更接地气，更能为消费零售企业提供合适的解决方案。 展望未来5年，我总结为“五化”：体验化、智能化、品牌化、细分化和集中化。在增量市场里，出现更多细分化的特色零售业态。在存量市场上，商超、便利店、水果零售等现有业态里出现整合，形成更集中化。祝愿大家在接下来的会议过程中有更多的收获和分享，谢谢！ 转载来源：红杉资本刘星：新零售未来5年的“五化”趋势]]></content>
      <categories>
        <category>财经</category>
      </categories>
      <tags>
        <tag>投资</tag>
        <tag>经济</tag>
        <tag>创业</tag>
        <tag>风投</tag>
        <tag>红杉资本</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[F8大动作：PyTorch 1.0现身（Logo也换了），围棋AI开源]]></title>
    <url>%2F2018%2Fcb052d87%2F</url>
    <content type="text"><![CDATA[Caffe2和PyTorch合体事件有了清晰的发展方向，同样服务于深度学习开发者的ONNX也宣布支持更多的框架。 夏乙 若朴 发自 凹非寺量子位 出品 | 公众号 QbitAI F8开发者大会第二天，Facebook亮出了一系列AI大动作。 Caffe2和PyTorch合体事件有了清晰的发展方向，同样服务于深度学习开发者的ONNX也宣布支持更多的框架。 另外，Facebook开源了视频理解、自然语言处理的模型，开源了围棋AI ELF OpenGo，还展示了一个打星际的AI。 PyTorchv0.4.0正式版发布没几天，Facebook在F8开发者大会第二天宣布将发布PyTorch 1.0，还提前展示了这款新框架的特性。 这个框架，还换了LOGO： 深度学习框架Caffe2的作者贾扬清，在知乎上将这一版本的发布总结为Caffe2 + PyTorch = PyTorch 1.0。 也就是将原本两款框架面向研究的和面向生产的特性结合了起来。 Facebook介绍说，PyTorch 1.0结合了Caffe2和ONNX模块化、面向生产的特性，和PyTorch自身灵活、面向研究的特性结合起来，为广泛的AI项目提供了一个从科研原型到生产部署的快速、无缝途径，让用户可以快速实验，通过一个能在强制执行模式和声明执行模式之间无缝切花的混合前端优化性能。 除了将研究和生产特性结合起来，PyTorch 1.0还将ONNX（开放神经网络交换）包含进来。ONNX是Facebook去年联合多家软硬件公司发布的神经网络模型转换协议，现在，它新增了对苹果的Core ML、百度PaddlePaddle、高通SNPE的支持，再加上原本支持的MXNet、Caffe2、PyTorch、TensorFlow、CNTK等框架，实现了神经网络模型在各种主流框架之间的转换。 PyTorch 1.0 beta版将在今年夏天和用户见面。 不过，Facebook内部已经用上了。官方称，Facebook多款产品和服务都在大规模应用这个新框架，它每天要处理60亿次文本翻译任务。 PyTorch最初亮相于1年多以前，Facebook的另一款深度学习框架Caffe2，则在去年的F8大会上正式发布。 不过今年4月，Caffe2已经宣布全部代码并入PyTorch。接下来的几个月里，两款框架原本的组件将深度结合，成为一个单独的软件包。 就在上周，PyTorch发布了v0.4.0版本，将Tensors（张量）和Variables（变量）合并，新增了零维张量，还开始了对Windows系统的官方支持。 展示PyTorch 1.0的同时，Facebook还开源了一部分研究成果。比如用于视频理解的ResNext3D模型将于6月发布，视频行为识别模型Res 2+1今天就已经开源，PyTorch中的自然语言理解库Translate也将开源。 发布了这么多资源和工具，去哪找呢？Facebook还为旗下所有的AI资源推出了一个网站： https&#58;//facebook.ai/developers 围棋AI开源下载在F8大会上，还开源了一个围棋AI：ELF OpenGo。 这个AI是Facebook团队对DeepMind技术的一个重现，最近他们选择与四名排名世界前30的人类高手对战，取得了14-0的胜利。 和AlphaGo一样，这个AI的重点也并不只是下围棋，而是想要更好的解决问题。现在ELF OpenGo已经可以开源下载。 对此，田渊栋在知乎上有更详细的解答： 我们最近改进了ELF框架，并且在上面实现了DeepMind的AlphaGoZero及AlphaZero的算法。用两千块GPU训练约两到三周后得到的围棋AI，基本上超过了强职业的水平。我们和韩国棋院合作进行了一次测试，给这个AI单卡每步50秒搜索时间（每步搜索8万个局面），给人类棋手任意长时间思考，结果AI以14比0完胜。参与测试的棋手包括金志锡，申真谞，朴永训及崔哲瀚，在这里我们非常感谢他们的合作，大家都很尽力，一些棋局下了三四个小时极其精彩。应棋手们的要求，这14局棋谱中的12局不久将公开。另外我们也和现在著名的LeelaZero比较了下。我们采用了LeelaZero除ponder外的缺省配置（约一分钟一步），及4月25日的公开权重(192x15, 158603eb)，结果我们的AI以200比0获胜。在此我们非常感谢Leela团队的工作，对于他们的开源精神，我们表示由衷的敬意。这次我们将训练代码，测试代码及训练出来的模型（224x20）全部公开，首要目的是贯彻我们一直以来坚持的开源方针，让AI为全世界服务。其次是对于AlphaGoZero及AlphaZero这样非常优秀的算法，我们想要提供一个可重复的参考实现，让全球的研究者们能在这上面继续改进，充分发挥自己的创造力。最后是借此机会推广一下我们的ELF平台和PyTorch深度学习框架，希望更多的人能使用和完善它。感谢大家的支持！田渊栋，龚渠成&amp;马子嫯（Jerry Ma）, Shubho Sengupta, 陈卓远，Larry Zitnick ELF OpenGo代码及模型的地址： https&#58;//github.com/pytorch/ELF 其他在F8大会上，还展示了一个可以打《星际争霸》的AI，Facebook也计划随后开源这一项目。星际争霸和围棋一直也都是Facebook团队研究的方向。 还有一项突破研究。基于35亿张用户已打标签（17000个）的公开图像，Facebook成功训练了一个图像识别系统，这比之前只能用手动打标签的5000万张图片训练相比，提高了系统的识别能力，在ImageNet上获得了创纪录的高分（准确率85.4%）。 更多信息，可以参考这个页面： https&#58;//code.facebook.com/posts/1700437286678763/ 此外，F8大会上还展示了AR和VR方面的进步。 Facebook已经创建了一个原型系统，可以生成效果惊人的三围重建画面。下面这个链接最后的视频，展示了正常的视频与3D重建画面的比较，几乎难以分辨左右哪个画面为真。（友情提示：左边露出操作员脚部的是真实世界） https&#58;//mp.weixin.qq.com/s/iFe6y5rzM2EqV02aNVqZfA — 完 — 诚挚招聘 量子位正在招募编辑/记者，工作地点在北京中关村。期待有才气、有热情的同学加入我们！相关细节，请在量子位公众号(QbitAI)对话界面，回复“招聘”两个字。 量子位 QbitAI · 头条号签约作者 վ’ᴗ’ ի 追踪AI技术和产品新动态 转载来源：F8大动作：PyTorch 1.0现身（Logo也换了），围棋AI开源]]></content>
      <categories>
        <category>科技</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>机器学习</tag>
        <tag>人工智能</tag>
        <tag>Facebook</tag>
        <tag>围棋</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[小米申请在香港上市，将成2014年以来全球最大IPO]]></title>
    <url>%2F2018%2F4f09df33%2F</url>
    <content type="text"><![CDATA[5月3日，港交所官网显示，小米已正式提交IPO申请文件，小米有望成为港交所“同股不同权”第一股。 5月3日，港交所官网显示，小米已正式提交IPO申请文件，小米有望成为港交所“同股不同权”第一股，将成2014年以来全球最大IPO。 5月3日早间，小米向港交所递交招股书，2017年收入1146亿元，经营利润122.16亿元。2015年、2016年及2017年，小米集团分别产生亏损人民币76亿元、利润人民币49.16亿元及亏损人民币439亿元。 招股书中小米董事长雷军发布公开信。雷军称，小米不是单纯的硬件公司，而是创新驱动的互联网公司。小米的商业模式经历了考验，得到了充分验证。今天，小米走到了历史性的重要节点。面向未来，小米建立的全球化商业生态有著 极具想像力的远大前景。 以下为雷军公开信原文： 您好！感谢您对小米的关注和支持。当您打开这份文件时，看到的不仅仅是一家风华正茂、勃勃向上的公司，更是一份由勇气和信任所支撑的新商业蓝图。 在此，我想向您说明，小米是谁，小米为什麽而奋斗。 小米不是单纯的硬件公司，而是创新驱动的互联网公司 具体而言，小米是一家以手机、智能硬件和IoT平台为核心的互联网公司。我们的使命是，始终坚持做“感动人心、价格厚道”的好产品，让全球每个人都能享受科技带来的美 好生活。 8年来的每一天里，“和用户交朋友，做用户心中最酷的公司”的愿景都在驱动著我们努力创新，不断追求极致的产品和效率，成就了一个不断缔造成长奇蹟的小米。 2010年4月成立小米时，我和我的合伙人们只有一个简单的想法：做一款让我们自己 喜欢、觉得够酷的智能手机。我们8个联合创始人中，6人是工程师，另外2人是设计师，都是消费电子设备狂热的“发烧友”。 “感动人心、价格厚道”这八个字是一体两面、密不可分的整体，远超用户预期的极致产品，还能做到“价格厚道”，才能真正“感动人心”。创新科技和顶尖设计是小米基因中的追求，我们的工程师们醉心于探究前人从未尝试的技术与产品，在每一处细节都反覆雕琢，立志拿出的每一款产品都远超用户预期。我们相信打破陈规的勇气和精益求精的信念 才是我们能一直赢得用户欣赏、拥戴的关键。 不止于技术，我们推崇大胆创新的文化。从手机工艺、屏幕和芯片等技术的前沿探索，到数年赢得的200多项全球设计大奖；从“铁人三项”商业模式，到通过“生态链”公司集群；从“用户参与的互联网开发模式”，到小米线上线下一体的高效新零售创新精神，在小 米蓬勃发展并渗透到每个角落，推动我们不断加快探索的步伐。 目前，我们是全球第四大智能手机製造商，并且创造出众多智能硬件产品，其中多个品类销量第一。我们还建成了全球最大消费类IoT平台，连接超过1亿台智能设备。与此同时，我们还拥有1.9亿MIUI月活跃用户，并为他们提供一系列创新的互联网服务。 真正让我们更加自豪的并非是这些数字，而是中国智能手机和智能设备等一系列行业的面貌因为我们的出现而彻底改变。 我们推动了智能手机在中国的快速普及和品质提升，这为中国移动互联网的快速爆发打下坚实基础。移动支付、电商、社交网络、短视频等行业在中国的蓬勃发展都有赖于移动互联网涌入了数以亿计的庞大人口。中国这一全球最大市场中，移动互联网行业的跨越式发展、成熟的背后，我们也被公认作出了不少贡献。 优秀的公司赚的是利润，卓越的公司赢的是人心。更让我们自豪的是，我们是一家少见的拥有“粉丝文化”的高科技公司。被称为“米粉”的热情的用户不但遍及全球、数量巨 大，而且非常忠诚于我们的品牌、并积极参与我们产品的开发和改进。 浴火重生，小米商业模式被充分验证 作为一家年轻的互联网公司，小米的发展并非一路坦途。2016年，我们的市场占有率曾有过下滑。我们清醒地认识到早先几年过于迅猛的发展背后还有很多基础没有夯实， 因此我们主动减速、积极补课。2017年，小米顺利完成“创新+质量+交付”的三大补课任务， 迅速重回世界前列。 据我们了解，除了小米，还没有任何一家手机公司，销量下滑之后能够成功逆转。 浴火重生，小米经历了一家能够长期稳定发展的公司所必需的修炼。我们的管理更加有序，我们的人才储备更加充实，我们的技术积累更加深厚，我们的供应链能力和产能 管理能力更加强大。 更重要的是，我们的商业模式经历了考验，得到了充分验证。 小米不是单纯的硬件公司，而是创新驱动的互联网公司。尽管硬件是我们重要的用户入口，但我们并不期望它成为我们利润的主要来源。我们把设计精良、性能品质出众的产品紧贴硬件成本定价，通过自有或直供的高效线上线下新零售渠道直接交付到用户手中， 然后持续为用户提供丰富的互联网服务。 这就是我们独创的“铁人三项”商业模式：硬件+新零售+互联网服务。小米至今的成就说明了这一模式强大的生命力。创业仅7年时间，我们年收入就突破了千亿元人民币，这 一成长速度是许多传统公司无法企及的。 效率的提升来自于运营成本，尤其是交付产品给用户时的交易成本的极大降低。小米独特的商业模式使得商品既好又便宜得以实现，造就了用户信任的基础。 （文 | AI财经社 鲁智高） 转载来源：小米申请在香港上市，将成2014年以来全球最大IPO]]></content>
      <categories>
        <category>财经</category>
      </categories>
      <tags>
        <tag>IPO</tag>
        <tag>小米科技</tag>
        <tag>移动互联网</tag>
        <tag>智能手机</tag>
        <tag>智能硬件</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python调用百度接口，实现ocr识别]]></title>
    <url>%2F2018%2F50f14f9e%2F</url>
    <content type="text"><![CDATA[1.文本识别：2.身份证识别：3.银行卡识别：4.行驶证识别：5.营业执照识别： 1.文本识别： 2.身份证识别： 3.银行卡识别： 4.行驶证识别： 5.营业执照识别： 转载来源：Python调用百度接口，实现ocr识别]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>编程语言</tag>
        <tag>Python</tag>
        <tag>技术</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Facebook两大深度学习框架正式合体，开源了击败14名高手的围棋AI]]></title>
    <url>%2F2018%2F46956228%2F</url>
    <content type="text"><![CDATA[Caffe2和PyTorch合体事件有了清晰的发展方向，同样服务于深度学习开发者的ONNX也宣布支持更多的框架。 夏乙 若朴 发自 凹非寺量子位 出品 | 公众号 QbitAI F8开发者大会第二天，Facebook亮出了一系列AI大动作。 Caffe2和PyTorch合体事件有了清晰的发展方向，同样服务于深度学习开发者的ONNX也宣布支持更多的框架。 另外，Facebook开源了视频理解、自然语言处理的模型，开源了围棋AI ELF OpenGo，还展示了一个打星际的AI。 PyTorchv0.4.0正式版发布没几天，Facebook在F8开发者大会第二天宣布将发布PyTorch 1.0，还提前展示了这款新框架的特性。 这个框架，还换了LOGO： 深度学习框架Caffe2的作者贾扬清，在知乎上将这一版本的发布总结为Caffe2 + PyTorch = PyTorch 1.0。 也就是将原本两款框架面向研究的和面向生产的特性结合了起来。 Facebook介绍说，PyTorch 1.0结合了Caffe2和ONNX模块化、面向生产的特性，和PyTorch自身灵活、面向研究的特性结合起来，为广泛的AI项目提供了一个从科研原型到生产部署的快速、无缝途径，让用户可以快速实验，通过一个能在强制执行模式和声明执行模式之间无缝切花的混合前端优化性能。 除了将研究和生产特性结合起来，PyTorch 1.0还将ONNX（开放神经网络交换）包含进来。ONNX是Facebook去年联合多家软硬件公司发布的神经网络模型转换协议，现在，它新增了对苹果的Core ML、百度PaddlePaddle、高通SNPE的支持，再加上原本支持的MXNet、Caffe2、PyTorch、TensorFlow、CNTK等框架，实现了神经网络模型在各种主流框架之间的转换。 PyTorch 1.0 beta版将在今年夏天和用户见面。 不过，Facebook内部已经用上了。官方称，Facebook多款产品和服务都在大规模应用这个新框架，它每天要处理60亿次文本翻译任务。 PyTorch最初亮相于1年多以前，Facebook的另一款深度学习框架Caffe2，则在去年的F8大会上正式发布。 不过今年4月，Caffe2已经宣布全部代码并入PyTorch。接下来的几个月里，两款框架原本的组件将深度结合，成为一个单独的软件包。 就在上周，PyTorch发布了v0.4.0版本，将Tensors（张量）和Variables（变量）合并，新增了零维张量，还开始了对Windows系统的官方支持。 展示PyTorch 1.0的同时，Facebook还开源了一部分研究成果。比如用于视频理解的ResNext3D模型将于6月发布，视频行为识别模型Res 2+1今天就已经开源，PyTorch中的自然语言理解库Translate也将开源。 发布了这么多资源和工具，去哪找呢？Facebook还为旗下所有的AI资源推出了一个网站： https&#58;//facebook.ai/developers 围棋AI开源下载在F8大会上，还开源了一个围棋AI：ELF OpenGo。 这个AI是Facebook团队对DeepMind技术的一个重现，最近他们选择与四名排名世界前30的人类高手对战，取得了14-0的胜利。 和AlphaGo一样，这个AI的重点也并不只是下围棋，而是想要更好的解决问题。现在ELF OpenGo已经可以开源下载。 对此，田渊栋在知乎上有更详细的解答： 我们最近改进了ELF框架，并且在上面实现了DeepMind的AlphaGoZero及AlphaZero的算法。用两千块GPU训练约两到三周后得到的围棋AI，基本上超过了强职业的水平。我们和韩国棋院合作进行了一次测试，给这个AI单卡每步50秒搜索时间（每步搜索8万个局面），给人类棋手任意长时间思考，结果AI以14比0完胜。参与测试的棋手包括金志锡，申真谞，朴永训及崔哲瀚，在这里我们非常感谢他们的合作，大家都很尽力，一些棋局下了三四个小时极其精彩。应棋手们的要求，这14局棋谱中的12局不久将公开。另外我们也和现在著名的LeelaZero比较了下。我们采用了LeelaZero除ponder外的缺省配置（约一分钟一步），及4月25日的公开权重(192x15, 158603eb)，结果我们的AI以200比0获胜。在此我们非常感谢Leela团队的工作，对于他们的开源精神，我们表示由衷的敬意。这次我们将训练代码，测试代码及训练出来的模型（224x20）全部公开，首要目的是贯彻我们一直以来坚持的开源方针，让AI为全世界服务。其次是对于AlphaGoZero及AlphaZero这样非常优秀的算法，我们想要提供一个可重复的参考实现，让全球的研究者们能在这上面继续改进，充分发挥自己的创造力。最后是借此机会推广一下我们的ELF平台和PyTorch深度学习框架，希望更多的人能使用和完善它。感谢大家的支持！田渊栋，龚渠成&amp;马子嫯（Jerry Ma）, Shubho Sengupta, 陈卓远，Larry Zitnick ELF OpenGo代码及模型的地址： https&#58;//github.com/pytorch/ELF 其他在F8大会上，还展示了一个可以打《星际争霸》的AI，Facebook也计划随后开源这一项目。星际争霸和围棋一直也都是Facebook团队研究的方向。 还有一项突破研究。基于35亿张用户已打标签（17000个）的公开图像，Facebook成功训练了一个图像识别系统，这比之前只能用手动打标签的5000万张图片训练相比，提高了系统的识别能力，在ImageNet上获得了创纪录的高分（准确率85.4%）。 更多信息，可以参考这个页面： https&#58;//code.facebook.com/posts/1700437286678763/ 此外，F8大会上还展示了AR和VR方面的进步。 Facebook已经创建了一个原型系统，可以生成效果惊人的三围重建画面。下面这个链接最后的视频，展示了正常的视频与3D重建画面的比较，几乎难以分辨左右哪个画面为真。（友情提示：左边露出操作员脚部的是真实世界） https&#58;//mp.weixin.qq.com/s/iFe6y5rzM2EqV02aNVqZfA — 完 — 诚挚招聘 量子位正在招募编辑/记者，工作地点在北京中关村。期待有才气、有热情的同学加入我们！相关细节，请在量子位公众号(QbitAI)对话界面，回复“招聘”两个字。 量子位 QbitAI · 头条号签约作者 վ’ᴗ’ ի 追踪AI技术和产品新动态 转载来源：Facebook两大深度学习框架正式合体，开源了击败14名高手的围棋AI]]></content>
      <categories>
        <category>科技</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>机器学习</tag>
        <tag>人工智能</tag>
        <tag>Facebook</tag>
        <tag>围棋</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[36氪领读 | luckin coffee 侵占你朋友圈的秘密，裂变拉新]]></title>
    <url>%2F2018%2F5029d5f0%2F</url>
    <content type="text"><![CDATA[36氪专门为读书设立了栏目，筛选一些值得读的书，并提供一些书摘。内容简介移动互联网时代，信息日益冗余，新闻速朽；整体流量增长速度放缓，而竞争者数量高速增加；流量呈现变少、变贵、欺诈频繁的现状。 36氪专门为读书设立了【36氪领读】栏目，筛选一些值得读的书，并提供一些书摘。希望你手边有一本称心的书，让读书这场运动继续下去。 内容简介移动互联网时代，信息日益冗余，新闻速朽；整体流量增长速度放缓，而竞争者数量高速增加；流量呈现变少、变贵、欺诈频繁的现状。品效合一的营销策略成为共识，而实现路径成为痛点。 多次开创各营销渠道效果之最的营销人、各种刷屏级营销事件操盘手、神州专车CMO杨飞，这一次倾囊相授，诚恳讲述如何实现流量获取、营销转化以及流量的运营和再挖掘。 作者简介杨飞，2017年广告门年度CMO，“流量池营销”理论提出者，传播学硕士；曾创办移动营销机构氢互动，获得国内外各类营销奖项近200次；2015年出任神州优车集团CMO，操盘神州专车、神州买买车等多次营销战役。现任luckin coffee营销操盘者。 书籍摘录裂变营销，最低成本的获客之道先讲一个刚刚发生的故事。我所操盘的luckin coﬀee 营销第一 仗最近启动了。虽然这是一杯典型的网络新零售咖啡（App 下单， 可自提可外卖，高品质咖啡），但由于获客第一步是App 下载，推 广难度还是不小的。你想想：谁会为了喝一杯咖啡，愿意下一个10 多兆的 App ？ 之前内部讨论时，luckin coﬀee 的 CEO 问我：“你认为最重要 的 App 获客方式会是什么？”我毫不犹豫地回答：“裂变拉新。” 是的，相比于传统广告的品牌曝光、饱和式投放、内容营销、 公关事件等手段，我心里清楚，咖啡作为一种典型的社交饮品，将大部分广告费用拿来作为用户补贴，激发老用户分享好友拉新，将是最核心的获客手段。 事实也证明如此。2018 年 1 月 5 日，我们正式上线拉新赠杯活 动，当天新增用户注册量环比翻番，订单环比增长了40%，而且相 比于之前精准的微信LBS 商圈定投，该形式获客成本大幅度降低。 神奇的裂变！ 不夸张地说，今天一个企业如果没有太多预算做广告并投放到 媒体，我不会特别在意，但如果它的App 或者微信中没有裂变营销，那是不可接受的。 社交流量：移动互联网上最重要的免费流量 移动互联网时代最贵的是什么？是流量吗？是，也不是。流量只是结果，移动互联网时代最贵的是用户关系和关系链。 人是社会中的个体，离不开各种人际交往，而移动互联网让这种人际关系变得更紧密、更具交互性。在复杂的人际交往中，信息的流动构成了源源不断的流量，这些流量对企业而言就是巨大的、可发掘的金矿。 我们知道，腾讯之所以能够稳坐互联网三巨头之一的位置，靠 的不是工具应用的垄断，而是通过QQ、微信等社交产品，打通和绑定用户关系链。这种绑定带来的最大的商业价值，就是不需要通过传统的广告和营销模式去告知用户，只需要通过充分的“社交挑逗”（我喜欢用这个词）就能让用户追随朋友的喜好，比如“你的朋友正在干吗，你要不要跟他一起来？”从而去接受一个新鲜产品（想想很多人是怎么开始玩《王者荣耀》的）。 关系链成本是锁定用户行为和忠诚度的一个指标，如果没有社交关系的绑定，很多功能强大的产品就很容易被用户放弃，而注入了社交因素的产品，使用频次会明显增多，口碑推荐会提高用户信任，消费购买完毕朋友间还可以相互比较。而当用户要放弃产品时，也要慎重考虑脱离圈子的影响。你可以很轻易地离开一个书店或者商场，却很难轻易地离开一个朋友和一个朋友圈。 社交关系链是任何企业、任何产品在移动互联网上最强大的护城河。低成本社交流量的获取关键就在于社交关系链的打通。企业要想办法持续输出内容来刺激用户，使其从用户转为“粉丝”，再主动将企业的品牌或产品信息传播出去，成为企业在移动互联网时网状用户结构中的重要连接点。 以上就是裂变的理论基础。 今天的企业，一定要善于借助社交平台（微信公众号、微信群、朋友圈）的力量，在内容和福利的驱动下，触发用户身边的连接点，进而将用户的整个关系网络打通。当企业自有用户流量达到一定量级时，裂变的效果也就喷薄而出。 AARRR：从拉新到裂变 AARRR 是近几年兴起的增长黑客中提到的App 运营增长 模型。AARRR 分别是指：获取用户（acquisition）、提高活跃度 （activation）、提高留存率（retention）、收入获取变现（revenue）、 自传播（refer）。 AARRR 模型不仅适用于App，企业在营销的过程中也可以按 照这 5 步来检验营销效果。 第一步，获取用户：获取用户是运营一款应用的第一步，所有企业建立品牌、推广、营销的目的都是获客拉新。 第二步，提高活跃度：很多用户第一次使用产品的场景其实很被动。有些品牌，用户可能只用一次就离开了，那么这个用户就没有成为产品真正的用户。究其原因，有可能是注册流程太烦琐，或者产品功能同质化，或者产品没有达到用户的期望值而且不能满足其需求，抑或第一次使用完全是利益驱动。 种种原因都能影响到用户后续的体验和消费。但显而易见的是，一个用户在App 中的活跃频次，决定了该用户是否是此产品的真正用户。所以企业要通过运营或者有趣的营销手段，快速提高用户的消费频次，将初次用户转化成忠实用户。 第三步，提高留存率：“用户来得快，走得也快”是企业产品面临的另一大难题。在 当下，一个产品获客100 人能够留存10% 就已经很厉害了，如果能留存二三十人，那就是爆品。用户来了之后，用完你的产品就走了，这是一个很不好的现象，证明你的产品用户体验不太好。更糟糕的情况是，你的产品教育了市场，说明用户知道了市场还有你这样的产品，一旦他们发现更好的竞争对手的产品，就会投奔到竞争对手那儿，等于你帮竞争对手打了广告。 第四步，收入获取变现：变现是产品最核心的部分，也是企业最关心的部分。有些互联网产品前期采用补贴策略，获取收入很少甚至无收入（比如共享单车）。产品本身就能获取一些收入，让企业盈利为正，这是企业希望达到的理想状态。而收入直接或间接来自用户，所以前三个步骤是应用获取收入的基础，只有付费用户多了，或者补贴减少，收入才可能实现规模化正向盈利。 第五步，自传播：自传播这一环节在社交网络兴起的当下至关重要。如果用户觉得好玩儿、有趣，或者有利益驱动，就会自发性地将产品社交媒体中。然后，通过老用户找新用户，产品获得更大的扩散。自传播也就是产品的流量裂变。 自传播的核心是，产品本身是否真正满足了用户的需求且产生了价值。从自传播到获取新用户，产品形成了一个螺旋式的上升轨道，用户群体可能会产生爆发式的增长。 可以看出，在AARRR 模型中，获取用户就是流量入口，提高活跃度就是惊喜时刻，提高留存率就是产品价值，收入获取变现是单位价值，而自传播就是放大传播效应。 从以上营销的角度来解读AARRR 模型，我认为有三点尤其重要。 第一，获得第一批种子用户。只有有了第一批用户，才可能完成后续其他行为。尤其是本章推荐的裂变营销，其实质是用老用户带新用户，所以第一批用户非常关键，是营销的基础。 第二，提高留存率。想要提高留存率，在网络营销中可以不断试验，这是增长黑客和传统市场营销的本质区别。增长黑客提出的 A/B 测试、MVT （最小化测试）都是为了提高留存转化率。当然，社交关系链也是提高留存率的重要手段之一。 例如，Facebook 早期发现用户流失非常严重，为了避免用户 流失进一步扩大，Facebook 在注销流程后面新增了一个页面。当用 户要离开的时候，系统会读出好友列表中互动最亲密的5 个人，询问：“你真的确定要离开吗？”很多本来要注销的用户担心再也见不到这些朋友，看不到他们的状态，心一软就留下了。这个页面上线后，在没花一分钱的情况下，一年之内为Facebook 减少了2% 的 损失，留下了 300 万用户。 第三，裂变，也就是老用户如何通过技术手段，将应用产品病毒式推荐给新用户。 增长黑客会取代市场总监吗？ 由于裂变型增长更多地采用技术和数据来驱动，也让增长黑客的概念在近两年很流行。有必要在这里做一个知识补充。 增长黑客的概念最早起源于美国硅谷。2010 年，肖恩·埃利斯在自己的博客上首次提出了“增长黑客”的概念，他也被称为“增长黑客之父”。 肖恩对增长黑客有一个有趣的定义：增长黑客的唯一使命就是增长，因为公司的估值是与增长息息相关的，增长是所有公司的核心指针。在“技术控”眼里，品牌、创意、媒介、公关等这些传统市场手段是效率并不高的增长方式，甚至需要被增长黑客所取代。 近几年，增长黑客这个概念从美国延伸到中国，并且在国内十分火热，很大一个原因在于，现今国内的公司获取流量的压力太大，同时市场遇冷，导致竞争增强，传统营销方式收效甚微。每个企业都希望在各个环节提升效率，而不论是工程效率、金钱效率还是用户获取效率，增长黑客都能带来低成本、快速的提高。 同时，越来越多的企业不仅仅关注获客，也开始关注用户的整个生命周期，开始通过数据驱动的方法，不断地对产品进行迭代，这些都是导致增长可能成为新一代营销命题的重要原因。 2017 年 3 月，可口可乐宣布取消设立24 年之久的CMO 一职， 取而代之的是一个新角色——首席增长官（Chief Growth Oﬃcer）。 可口可乐的整体战略也向“以增长为导向，以消费者为中心”持续转型。增设首席增长官并非可口可乐一家公司的特例。高露洁、亿滋等快消品巨头都聘请了首席增长官，以实现品牌的快速增长，提升增长在品牌战略中的地位。 这一现象的背后，带来的一个明显趋势是AdTech 和 MarTech的对决。 AdTech 从字面上理解就是把广告和品牌内容送达消费者的技术 和手段。在AdTech 中，付费媒介、网页广告、SEM 付费搜索、原生广告、程序化购买、DSP 等都是经常使用的方式。 MarTech 主要是指利用即时服务、优化消费者体验流程、优化顾客转化技术等技术手段，借助大数据标签、客户关系管理、营销自动化等管理系统而实现的技术化营销。 AdTech 比较像营销人员的“外功”，有预算和出街创意就能实 现；而 MarTech 更像“内功”，可以为企业数字化转型和商业转型提供整体解决方案。 现实情况是，MarTech 在增长驱动和获客成本上明显要优于 AdTech，也越来越成为企业的核心增长手段。 我曾参加国内的GrowingIO（一家数据分析公司）增长大会，作为一个营销人，在场下听到一帮程序员在台上的用词，居然也有“创意”“热点”“事件营销”“自媒体”等，确实很有感慨。黑客增长与营销的边界正在模糊，甚至对传统的营销观念正产生巨大的冲击。 但在这样的趋势下，回到我们最开始的问题：增长黑客真的会取代市场总监吗？我的答案是，不会代替，但会融合。新一代市场总监一定要突破原有的营销知识短板，掌握更多的产品、技术、数据等驱动增长手段，而增长黑客也会成为企业市场核心组织，成为与传统品牌、外部广告等共同存在的“三极”之一。 初创公司没有庞大资金来选择优质的推广渠道以及头部内容合作，在这样的情况下可依靠大数据驱动和增长黑客，使之成为助力增长机制。成熟品牌虽然有了市场份额和大批忠诚用户，但仍将面对持续 增长的难题。用市场团队补充增长黑客团队，通过技术和数据的方式，来指导营销广告、创意、投放，也很有必要。 我们看到，无论是传统市场部门还是增长黑客技术部门，必然的趋势都是：企业要想实现流量获取和变现，就必须从自身流量出发寻找控制变量的方法，以存量找增量，以精细化运营获取更多的增长结果。 微信社会化营销的流量改造微信日活跃用户超9 亿，其中55% 的用户每天要打开微信10次以上。这两年，微信的巨大流量让所有人都心动不已，大家都想从这空气级的巨型应用流量池中分一杯羹，企业纷纷自建微信账号，数千万的微信公众号因此诞生。 微信去中心化的体系，让流量变得更直接，同时依托于社交口碑属性，这些流量也更加真实、更有价值。 时至今日，微信营销的基础教育已经完成，几乎每家企业都会开通官方微信公众号，并且有频次地更新内容运营。但这并不代表每个微信营销企业都会合理、有效地利用微信，通过好的手段实现流量改造，使其发挥最大转化效率。 90% 的官方微信都在自嗨 官方微信的“自嗨”是当下微信营销普遍存在的现象。很多企业看似赶在潮流上，实则仍然在用传统广告理念运营微信。比如，注册一个微信公众号（相当于买断一个长久的低价广告位或新闻位），然后雇一位小编定期发图文贴维护（等于雇了一位企业专属的广告投放编辑）。简单两步就解决了常规的广告投放、企业内刊、品牌公关等多种市场传播形式。 网感较强的企业，会紧跟事件热点，借助热点和受众完成一些简单的海报互动，虽然阅读量并不会增加太多；网感不强的企业，微信公众号就会完全沦落为自身的新闻中心，成为企业动态、领导人讲话、企业文化活动的宣传阵地，然后鼓动全员转发朋友圈，以为这样做就能达到传播刷屏的效果。但结果往往是阅读量过千都困难，“粉丝”量不增反降，用户活跃度也没有提升。 “无趣”“无效”“无聊”是当下企业在微信运营时的三个普遍问题。 无趣：由于属性限制，企业微信在一开始就具有天然商业化内容的定位，但受众对于这类内容的接受度和容忍度是有限的。缺少人格化的微信内容定位，没有意思的内容输出，企业和受众之间没有深层互动，这些都是由于无趣导致企业微信关注度不高的直接原因。 无效：即使很多企业微信有了关注度、有了阅读量，却依然无法将阅读量成功转化，让流量成为销量。这是由于企业在移动营销的过程中，仍然保持着传统的广告公关心态来对待微信营销 无聊：由于一些企业微信编辑人员的专业度不够，操作门槛较低，导致产出的内容没有营养价值。无聊的内容最终无法达到获取流量的目标。 微信营销如何才能快速引流并转换，以下将展开讨论。 把微信服务号做成超级 App 请注意，微信服务号不是公关号，也不只是内容号，而是一个 还原 App 功能的服务号。这是微信服务号的基本定位。 微信升级5.0 版本之时，不仅带来了全民上下沉迷的打飞机游戏，更带来了服务号和订阅号拆分。企业如果想要完善建立移动端的营销服务体系，服务号势必成为最佳选项。企业需要通过申请自 定义菜单，开通更多的后台接口，把微信服务号当成轻量级的App来使用，从而完成微信运营的核心思想转化。 对于一款企业 App 产品来说，它至少承载着三大功能。 第一，要承载业务的基本产品功能。 这一点很好理解。比如，神州专车作为移动出行的App，主要功能就是给用户提供出行专车服务。淘宝、天猫、京东App 的基本 产品功能是线上购物平台，饿了么、美团外卖、百度外卖App 的基本功能是线上外卖订餐平台，等等。这些产品功能本身是要关联用户数据和消费数据的。 第二，要承载客服咨询反馈的功能。 App 是企业与用户的主要接触点和沟通平台。企业要想及时获 得用户反馈信息，就必须让自己的产品具备和用户沟通的功能，也就是客服咨询功能。 第三，要承载营销信息的展示告知功能。 当一个App 具备一定的用户基数时，其本身的开屏页、弹窗、轮转图等就是企业免费的广告展示、信息告知的重要渠道。 同理，如果企业微信账号要做成超级App，就得满足以下几个基础功能。 广告信息的展示告知 我一直建议企业做好微信服务号，而不是订阅号。服务号和订阅号的不同之处在于，订阅号每天可推送一条图文信息，会被折叠 在订阅号窗口；而服务号是每月推送4条图文信息，但不会被折叠，可以直达用户。 或许会有企业认为一个月推送4 次，频次不够，内容太少，达不到效果，但其实恰恰相反。当下用户的时间太过碎片化，如果每天的推送内容不够出彩，就很难打动用户，甚至会被认为是一种骚扰而取消关注。 2017 年以来，微信平台本身的订阅号打开率一直在持续下降。如果企业资源有限、人员有限，建议只做好服务号就够了。订阅号可以注册下来，用于企业发布一些紧急性、临时性的信息，以及与用户沟通交流，不做日常更新。 企业在做服务号的推送内容时要珍惜每一次的推送，把内容做成精品，通过一次次的累积叠加最终实现用户的增长。 客服咨询功能 显然，微信的生态环境比App 更适合说明企业服务和管理用户，微信公众平台新版的客服功能提供了即时回复用户咨询、自动回复、客服数据统计等功能，并支持多人同时为一个公众号提供服务，让企业和用户的连接更为方便和快捷。 企业可以利用微信公众平台极大地减少客服人员的工作量，让 用户在微信里自主完成咨询、查询等操作。随着AI（人工智能）客服、语音机器人等技术的成熟，微信客服功能会进一步优质高效。 微信一定要实现企业产品功能微信开发者模式是一个开放式的接口，可以通过产品后台的编写进行后台改造，完成消费数据的接口对接，从而实现产品在微信里的业务转化。 比如，很多连锁餐饮企业就是微信服务号的受益群体。他们将微信服务号进行技术开发和数据对接，增设了订餐、排位、查看菜单、预订外卖等菜单功能，或者添加微信卡券功能，绑定会员卡、发放优惠券等，效果相当不错，能够提高用户消费频次和消费额度。 餐饮企业，尤其是快餐，一个共同特点就是即兴消费，满足“频发”“多选”“短决策”的特性。三者共同作用时，微信在消费者快速做出消费决策时的作用就尤为明显。每一次的推送，再辅助优惠券等福利刺激，都可能立刻转化为消费购买决策。 不仅是餐饮企业，所有具有即兴消费属性的企业，如出行行业、 快消行业、商超便利店等，都适合把微信服务号打造成超级 App。 讲几个实际案例。 肯德基：手机自助点餐 2016 年 3 月 7 日，肯德基与微信支付达成合作，在全国超过 4700 家门店同时上线微信支付，同时在全国30 多个城市超 过 2300 家餐厅开通手机自助点餐。以微信支付为起点，完成微信体内的闭环式营销，打造数字化用餐体验。 用户在肯德基的微信公众号上就能体验“手机自助点餐”的服务功能。这一功能不仅能让顾客不用排队点餐，甚至不用进店就能完成点餐、支付的系列环节。而门店一方只需按照订单准备好菜品，等待用户到店领取即可，大幅度缓解了高峰时段的客流压力。 FlowerPlus 花加：微信大流量带来的留存转化 FlowerPlus 花加（以下简称“花加”）的模式很简单，用 户通过微信下单，每月支付不到100 元就能收到一盒时令鲜花，收花地点选择在办公室或者家中。 就是这样一家关注都市白领日常鲜花消费市场、提出“日常鲜花”概念的公司，借助 微信的巨型流量优势，在短短的一年零4 个月的时间里，公众 号“粉丝”就完成了从0 到 129 万的增长，规模从起步到营收 3000 万元，迅速占领了鲜花 O2O 领域的第一梯队位置。 从传播的角度来说，鲜花消费处在一个受众需求大、消费频次高、自传播触发点广的优势基础上，微信正好为花加提供了传播优势。花加采用的模式是先付款、后发货的订阅模式，这解决了资金流转问题。 微信也为花加的用户留存起到了很大的帮助作用。企业可以通过公众号留下用户的信息和数据，分析客户需求，给不同客户进行用户画像，提供不同的产品和服务送达，从而实现比 自有官网或App 更高的留存率。 比如，花加会给新客户配送比较容易养的或者常见的鲜花，给老客户配送一些有养护难度的花，为孕妇配送鲜花时会避开对胎儿有影响的花种等。 据报道，花加目前的用户来源有 90% 以上都是微信用户的 口碑传播结果，10% 来自微信朋友圈的广告投放。 转载来源：36氪领读 | luckin coffee 侵占你朋友圈的秘密，裂变拉新]]></content>
      <categories>
        <category>传媒</category>
      </categories>
      <tags>
        <tag>软件</tag>
        <tag>移动互联网</tag>
        <tag>市场营销</tag>
        <tag>黑客</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[“排序”机制：区块链原生应用独有的创新设计]]></title>
    <url>%2F2018%2Fa9e75d89%2F</url>
    <content type="text"><![CDATA[具有不同于古典互联网社区产品的设计细节和产品机制，它的“排序”方式使内容第一次有了另一种更公平的筛选算法和发现机制。 区块链付费问答产品 ：Cent ，具有不同于古典互联网社区产品的设计细节和产品机制，它的“排序”方式使内容第一次有了另一种更公平的筛选算法和发现机制。 橙皮书之前介绍了一款区块链付费问答产品 ：Cent 。 最近我仔细看了看这款产品的一些设计，越发感觉：Cent 有不同于古典互联网社区产品的设计细节和产品机制。 我认为Cent代表了区块链原生应用的一个独有的创新设计，这个创新之处就在于“排序”机制。 一、“排序”是一种新的产品设计 从上一次的报道里，我们大概知道了 Cent 是个什么样的产品： 它可以被看作付费问答版的“知乎”（实际上因为是付费的，你也可以上面发布“任务”，而不仅仅只是“问题”，但为了方便讨论，我们还是以问答为主）： 提问者可以在上面发布问题，每个问题绑定一笔征集答案的悬赏金，以 eth 币的形式；1. 答题的人发布答案后，则有机会参与瓜分赏金，赚取 eth 币；1. 最后Cent还有另一个独特的用户角色：排序者。排序者不回答问题，但同样有机会获得 eth 币。我们重点来说说最后一点。 排序者是介于提问者和答题者之间的投票人，系统首先会把每个问题下面的所有答案通过算法分配成两两一组，这个算法会确保每组的两个答案质量相对接近、没有太大的差距，然后排序者需要对所有成对的答案进行投票筛选，在每组中选出最好的那个。 上面把问题下的所有答案分成了 12 对，排序者需要从每一对中挑选出最好的那个答案。 每对包括两个答案，通过左右两个按钮让两个答案进行PK，挑出最好的那个。 被选中的那个答案会变成绿色的。 所有答案排序完成之后，在问题到期限关闭之后，就可以坐等分钱了。 可以看到，排序者每排一次，其实就是对所有答案进行一次整理，最终所有排序者的结果会转化成：根据回答内容的质量，把问题的答案从高到低一一进行了排序，最终让高质量的内容可以浮现出来。 我认为：排序者这种用户角色是之前的互联网产品所没有的。 对内容进行排序、在每组两个答案之间“PK”选出最好的那一个，这种类型的交互，对用户来说也是完全陌生新鲜的产品体验。而且，这种交互最终还会产生不同的投票算法，让优质内容以另一种机制浮现出来。 因此，从各个方面来看，排序都算得上是产品设计上的一个创新。 二、跟知乎点赞同／反对的机制有什么本质不同？ 对社区类产品来说，控制好社区内容的信噪比、让高质量的内容被更多人看到、让有潜力的内容有更多曝光的机会，这些都是产品机制的重点。 在之前的社区产品里（天涯论坛、百度贴吧），采取的方式是让用户尽可能低门槛的发布内容，在主页中以时间排序作为信息流呈现的逻辑，然后通过“版主加精”、给权重的方式让好的内容置顶浮现出来，同时通过删帖等强制管理，消灭垃圾内容。 到了知乎，这种方式就变了。知乎是通过更民主的投票方式，让其他用户对答案点赞同／反对，从而让优质内容浮现出来，让垃圾内容逐渐消失。 表面上看，知乎的赞同／反对也会对答案的排序产生影响。那么，Cent 和知乎本质上还有区别吗？ 答案是：有区别。而且区别很大。 主要有三点： 1.从用户角度来看，点赞同／反对其实是用户一种自我表达的出口。 换句话说，用户在知乎上通过对某个答案点赞同／反对，来表达自己的某种观点和判断的，从而形成自己的社交形象，这是用户肯为内容投票的动机之一；但在 Cent 上，用户愿意成为排序者是出于利益因素——他可以获得实际的经济回报，这更像是一份“工作”，排序者和提问者之间是一种隐性的雇佣关系。 2.知乎采用赞同／反对，最终沉淀出来的是具备热度的头部内容。 知乎的目标，是让每个问题下面都能呈现出一批精彩的答案，这批答案将带来最主要的头部流量，至于 10 赞以下的长尾内容，他们彼此之间的质量和排名是怎样的？知乎并不关心。 而 Cent 不同， Cent 是答案与答案两两进行 PK ，它是对所有答案的一次质量排序，每一个答案具体的排名都是根据用户的操作得出来的，每个排位都是严格的、不可替换的、有迹可循的，不管这个排位是前几名还是倒数后几名。 3.第三点跟第二点是强相关的。 因为知乎点赞同／反对最终是要找出具备热度的头部内容，而点赞数同时也在潜移默化地告诉用户“这个答案是优质”的，是一种心理暗示，因此知乎的头部内容存在马太效应，也就是“赞越多的答案越有可能被后来的用户点赞”；相反，Cent 就不会有这个问题。 首先，Cent 把答案配对分组时，根据此前不同的投票情况，系统就可以对不同的用户展示不同的配对组合，这样总的来说会使得投票过程更加公平；然后——也是和知乎最大的不同点，Cent 每组答案进行 PK 的时候，用户是看不到之前其他用户投票的结果的。每个用户每次投票都是在黑箱中进行，因此不会被其他用户干扰。 这三点，同时也是 Cent 在产品设计上最主要的创新点。社区类产品里，内容第一次有了另一种更公平的筛选算法和发现机制。 三、为什么在区块链之前没有“排序”的设计？ 搞清楚了排序的创新之处，很自然的会想到另一个问题：为什么排序机制一直等到区块链出现之后才诞生？ 最主要的原因是：区块链技术给予了社区类产品更直接的支付能力和交易透明度。之前，要完成 Cent 这样一款产品，借助人民币充值悬赏金、通过微信支付、支付宝这样的工具也可以实现付费问答或者任务认领，但是唯独不能实现的是排序者这个角色的设计。 中心化平台，在这类产品中，大部分的商业模式是中介抽成。中心化平台往往会倾向于自己包办买卖交易双方的撮合和协调，这样才能从提问者的悬赏金中提取一定的中介费。 因此，由中心化平台决定“哪些答案是优质的”是很自然的做法——滴滴总是“宣称”会帮你叫到距离最近的、速度最快的、服务最好的司机，即使事实并非如此，它也会一直这样宣称。因为只有这样，它才具备价值。 那么，既然规则是由中心化平台制定的，显然排序者这样的用户角色就是多余的——最重要的是，你如何确保平台会真的会为排序者分一部分酬劳，以此作为他们辛苦排序的劳动成果？特别是当这部分钱完全可以通过平台内部操作节省下来的时候。 而区块链技术可以确保这一切都是透明的，排序者可以得到规则里承诺的、应有的 token 和报酬。这是一个自动运转的系统，不需要人工运营。提问者、答题者、排序者，只要这三类用户都对平台的规则达成共识，愿意参与进来，那么，智能合约就会自动执行接下来各个环节的资金分配，每个用户都能皆大欢喜。 四、最后 上面主要是我从产品经理的角度分享了对 Cent 排序产品设计的思考。 这是一个非常小的产品细节，也是一个非常具象的例子。从小的细节和具体的例子里，人们更容易感受到区块链技术创新对产品所带来的一些影响。 但实际上，如果你跳出产品经理的视角来看这个问题，“排序”其实不仅是一种产品设计上的创新，它背后代表的是另一块属于区块链的新生的市场。排序代表的是对内容的一种整理和组织，更进一步，我们不仅可以利用区块链和 token 激励用户对答案进行排序，我们还可以激励他们对答案进行补充完善、重新整理（比如，把维基百科代币化）等等。 而这种在社区内借助 token 让用户对内容进行维护的方式，在国外已经有一个统一的称呼了，叫“curation markets”。但在国内，对这种类型的认知还很初期，网上几乎很难找到相关的讨论。橙皮书希望借助这篇产品经理角度的文章，让大家对 curation markets 有一个初步的认识。 作者：橙皮书，微信公众号：橙皮书（ID：taixu_huanjing），关注区块链技术和产品 本文由 &#64;橙皮书 原创发布于人人都是产品经理。未经许可，禁止转载 题图来自 unsplash，基于 CC0 协议 转载来源：“排序”机制：区块链原生应用独有的创新设计]]></content>
      <categories>
        <category>科技</category>
      </categories>
      <tags>
        <tag>经济</tag>
        <tag>区块链</tag>
        <tag>产品经理</tag>
        <tag>苏黎世高工</tag>
        <tag>民主</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RNN和LSTM弱！爆！了！注意力模型才是王道]]></title>
    <url>%2F2018%2Fa71e3994%2F</url>
    <content type="text"><![CDATA[大数据文摘作品编译：晚君、笪洁琼、钱天培循环神经网络。LSTM和RNN被发明于上世纪80、90年代，于2014年死而复生。 大数据文摘作品 编译：晚君、笪洁琼、钱天培 循环神经网络（RNN），长短期记忆（LSTM），这些红得发紫的神经网络——是时候抛弃它们了！ LSTM和RNN被发明于上世纪80、90年代，于2014年死而复生。接下来的几年里，它们成为了解决序列学习、序列转换（seq2seq）的方式，这也使得语音到文本识别和Siri、Cortana、Google语音助理、Alexa的能力得到惊人的提升。 另外，不要忘了机器翻译，包括将文档翻译成不同的语言，或者是神经网络机器翻译还可以将图像翻译为文本，文字到图像和字幕视频等等。 在接下来的几年里，ResNet出现了。ResNet是残差网络，意为训练更深的模型。2016年，微软亚洲研究院的一组研究员在ImageNet图像识别挑战赛中凭借惊人的152层深层残差网络（deep residual networks），以绝对优势获得图像分类、图像定位以及图像检测全部三个主要项目的冠军。之后，Attention（注意力）模型出现了。 虽然仅仅过去两年，但今天我们可以肯定地说： “不要再用RNN和LSTM了，它们已经不行了！” 让我们用事实说话。Google、Facebook、Salesforce等企业越来越多地使用了基于注意力模型（Attention）的网络。 所有这些企业已经将RNN及其变种替换为基于注意力的模型，而这仅仅是个开始。比起基于注意力的模型，RNN需要更多的资源来训练和运行。RNN命不久矣。 为什么 记住RNN和LSTM及其衍生主要是随着时间推移进行顺序处理。请参阅下图中的水平箭头： RNN中的顺序处理 水平箭头的意思是长期信息需在进入当前处理单元前顺序遍历所有单元。这意味着其能轻易被乘以很多次&lt;0的小数而损坏。这是导致vanishing gradients（梯度消失）问题的原因。 为此，今天被视为救星的LSTM模型出现了，有点像ResNet模型，可以绕过单元从而记住更长的时间步骤。因此，LSTM可以消除一些梯度消失的问题。 LSTM中的顺序处理 从上图可以看出，这并没有解决全部问题。我们仍然有一条从过去单元到当前单元的顺序路径。事实上，这条路现在更复杂了，因为它有附加物，并且忽略了隶属于它上面的分支。 毫无疑问LSTM和GRU（Gated Recurrent Uni，是LSTM的衍生）及其衍生能够记住大量更长期的信息！但是它们只能记住100个量级的序列，而不是1000个量级，或者更长的序列。 还有一个RNN的问题是，训练它们对硬件的要求非常高。另外，在我们不需要训练这些网络快速的情况下，它仍需要大量资源。同样在云中运行这些模型也需要很多资源。 考虑到语音到文本的需求正在迅速增长，云是不可扩展的。我们需要在边缘处进行处理，比如Amazon Echo上处理数据。 该做什么？ 如果要避免顺序处理，那么我们可以找到“前进”或更好“回溯”单元，因为大部分时间我们处理实时因果数据，我们“回顾过去”并想知道其对未来决定的影响（“影响未来”）。在翻译句子或分析录制的视频时并非如此，例如，我们拥有完整的数据，并有足够的处理时间。这样的回溯/前进单元是神经网络注意力(Neural Attention)模型组。 为此，通过结合多个神经网络注意力模型，“分层神经网络注意力编码器”出现了，如下图所示： 分层神经网络注意力编码器 “回顾过去”的更好方式是使用注意力模型将过去编码向量汇总到语境矢量 CT中。 请注意上面有一个注意力模型层次结构，它和神经网络层次结构非常相似。这也类似于下面的备注3中的时间卷积网络（TCN）。 在分层神经网络注意力编码器中，多个注意力分层可以查看最近过去的一小部分，比如说100个向量，而上面的层可以查看这100个注意力模块，有效地整合100 x 100个向量的信息。这将分层神经网络注意力编码器的能力扩展到10,000个过去的向量。 这才是“回顾过去”并能够“影响未来”的正确方式！ 但更重要的是查看表示向量传播到网络输出所需的路径长度：在分层网络中，它与log（N）成正比，其中N是层次结构层数。这与RNN需要做的T步骤形成对比，其中T是要记住的序列的最大长度，并且T &gt;&gt; N。 跳过3-4步追溯信息比跳过100步要简单多了！ 这种体系结构跟神经网络图灵机很相似，但可以让神经网络通过注意力决定从内存中读出什么。这意味着一个实际的神经网络将决定哪些过去的向量对未来决策有重要性。 但是存储到内存怎么样呢？上述体系结构将所有先前的表示存储在内存中，这与神经网络图灵机（NTM）不同。这可能是相当低效的：考虑将每帧的表示存储在视频中——大多数情况下，表示向量不会改变帧到帧，所以我们确实存储了太多相同的内容！ 我们可以做的是添加另一个单元来防止相关数据被存储。例如，不存储与以前存储的向量太相似的向量。但这确实只是一种破解的方法，最好的方法是让应用程序指导哪些向量应该保存或不保存。这是当前研究的重点。 看到如此多的公司仍然使用RNN/LSTM进行语音到文本的转换，我真的十分惊讶。许多人不知道这些网络是如此低效和不可扩展。 训练RNN和LSTM的噩梦 RNN和LSTM的训练是困难的，因为它们需要存储带宽绑定计算，这是硬件设计者最糟糕的噩梦，最终限制了神经网络解决方案的适用性。简而言之，LSTM需要每个单元4个线性层（MLP层）在每个序列时间步骤中运行。 线性层需要大量的存储带宽来计算，事实上，它们不能使用许多计算单元，通常是因为系统没有足够的存储带宽来满足计算单元。而且很容易添加更多的计算单元，但是很难增加更多的存储带宽（注意芯片上有足够的线，从处理器到存储的长电线等）。 因此，RNN/LSTM及其变种不是硬件加速的良好匹配，我们在这里之前和这里都讨论过这个问题。一个解决方案将在存储设备中计算出来，就像我们在FWDNXT上工作的一样。 总而言之，抛弃RNN吧。注意力模型真的就是你需要的一切！ 相关报道： https&#58;//towardsdatascience.com/the-fall-of-rnn-lstm-2d1594c74ce0 转载来源：RNN和LSTM弱！爆！了！注意力模型才是王道]]></content>
      <categories>
        <category>科技</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>Google</tag>
        <tag>大数据</tag>
        <tag>王道</tag>
        <tag>亚洲</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[社交关系的裂变不止于电商，「淘淘课」想用它做知识付费]]></title>
    <url>%2F2018%2Fb6e6e083%2F</url>
    <content type="text"><![CDATA[一些有知识生产能力但社会话题度并没有那么高的人被天然排除在了这些大平台之外；此外，对于一些内容生产者来说，平台的确为他们提供了内容运营和流量的支持。 得到、喜马拉雅等知识付费平台的崛起、小鹅通、荔枝微课的火爆又让知识付费从大平台“烧”到了微信生态内，一个肉眼可见的变化是，内容的生产和获取门槛越来越低。 36氪最近接触到淘淘课的创始人及 CEO 周杰认为，中心化、平台化的运行模式下，知识付费更多是一些已经拥有大众影响力的“大V”们对自己的流量的内容变现，一些有知识生产能力但社会话题度并没有那么高的人被天然排除在了这些大平台之外；此外，对于一些内容生产者来说，平台的确为他们提供了内容运营和流量的支持，但综合流量的玩法其实并不适用于内容产品。 淘淘课在做的事情简单来说就是为知识付费产品提供一个出口，帮助一些中小V甚至我们所认为的生活中的普通人将自己的知识进行变现，可以分两方面来看： 内容运营的去中心化：淘淘课选择让知识生产者自己就变成内容营销方，为他们提供“知识店铺”的服务。前面提到，电商领域的综合流量平台玩法其实并不适用于内容产品，主要因为，内容产品（或者说泛教育产品）更多的是供给端主导。在产品一侧，用户更愿意基于口碑、对于内容生产方的品牌认知进行购买，因此，在这一逻辑下，如果要提高课时完成率和复购，就需要内容提供方（机构或个人）来维系用户，沉淀流量，让用户留存下来。 周杰认为，大部分有内容变现动机的 CP 方其实已经有了自己的粉丝用户群体，可以看做是一个“私域流量”，怎么将这部分流量的价值最大化才是最主要的。 通过自己运营“知识店铺”，对于内容生产方而言，一个最为直接的变化体现在收入上，另外一个变化则是，透过知识店铺的运营反馈，对于内容生产方能够直接了解到自己粉丝群体对于内容的需求，进而在新的内容制作当中进行调整，是一个 C2M 的逻辑。 小鹅通、荔枝微课等做的也是类似的事情，但淘淘课的一个区别在于，“知识店铺”除了可以作为自己内容的销售场景外，内容生产方也能够选择淘淘课自己的“精选商城”上的其他内容上架售卖，通过多维度的内容去满足用户对于不同内容的需求，另一方面，也可以理解为“流量共享”。 内容分发及营销的去中心化：做内容领域的“社交电商”。除了用内容生产者本身将平台去中心化外，“知识大使”也是淘淘课在内容营销渠道上的一个重要角色。 创始人周杰曾任蚂蚁-国泰产险 CTO ，参考保险领域的代理人机制，他认为分销不在于线上，而是线上和线下都要保证，“线上”是内容生产者本身的影响力，“线下”则是指的是社交关系。 36氪曾经对社交电商进行过详细的分析和报道，这些“知识大使”可以类比为个人店主，他们有自己的“流量池”，同时也具有一定的将这些流量变现的能力，而淘淘课给到这些知识大使的“供应链”就是精选商城内的知识内容。 我比较好奇的是”知识大使“都是怎样的人群？周杰提到，现阶段，淘淘课主要在宝妈、大学生群体内进行推广。这些群体以“宝妈群”为例，这些群体首先有相对“封闭”的社交关系场景，妈妈群之间的口碑传播效应特别明显。 除了以个人形式运营“知识店铺”外，淘淘课也支持二级分销，用社交关系裂变的模式来促成内容教研。今年 3 月刷屏的新世相已经验证了， 至于淘淘课在线商城中所售卖的课程，主要有两大块来源 ，首先是团，此外，淘淘课的也可以作为大的内容分发渠道存在，对于可能存在的盗版问题，周杰提到，团队下一阶段计划通过区块链的技术来规避。 相对于“有赞”模式，从 SaaS 工具切入到营销侧的服务，淘淘课的服务更加直接，即营销层面的辅助，36氪注意到，小鹅通在营销解决方案上也推出了“知识店铺”的功能。我们认为，当“知识/内容”越来越成为一种商品时，社交电商的模式或许会成为知识付费的新姿势。 团队方面， 淘淘课的创始团队均来自阿里巴巴（蚂蚁金服），参与了余额宝、招财宝、娱乐宝等多项互金领域产品搭建。其中创始人及CEO周杰历任蚂蚁金服总监（M4），蚂蚁-招财宝 CTO、蚂蚁财富事业群创始团队成员，曾负责商户事业线、财富事业线的技术和业务体系搭建。 ———————— 我是36氪作者思齐，教育、消费电商类项目报道交流请联系微信 HannahHQ723 转载来源：社交关系的裂变不止于电商，「淘淘课」想用它做知识付费]]></content>
      <categories>
        <category>科技</category>
      </categories>
      <tags>
        <tag>市场营销</tag>
        <tag>产品运营</tag>
        <tag>电子商务</tag>
        <tag>周杰</tag>
        <tag>蚂蚁</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[海通证券姜超评资管新规：货币超发的时代结束了]]></title>
    <url>%2F2018%2F78a6cd7d%2F</url>
    <content type="text"><![CDATA[海通证券姜超1日观点认为，资管新规的出台确认一个时代的结束，中国货币超发的时代正式结束。如果未来中国的货币不再超发，那么我们的财富创造模式将会发生巨大的变化。靠涨价赚钱将成为过去，靠买房发财的时代就要结束了。而未来只有创造实际增长才能创造财富，这意味着创新才能创造价值，与此同时货币低增将抑制通胀预期，那么债市也会充满机会。 以下为原文： 在上周，央行等四部委联合发布了《关于规范金融… 海通证券姜超1日观点认为，资管新规的出台确认一个时代的结束，中国货币超发的时代正式结束。如果未来中国的货币不再超发，那么我们的财富创造模式将会发生巨大的变化。靠涨价赚钱将成为过去，靠买房发财的时代就要结束了。而未来只有创造实际增长才能创造财富，这意味着创新才能创造价值，与此同时货币低增将抑制通胀预期，那么债市也会充满机会。 以下为原文： 在上周，央行等四部委联合发布了《关于规范金融机构资产管理业务的指导意见》，其中严格规定资产管理机构不得承诺保本保收益、打破刚性兑付；禁止资金池，消除多层嵌套，抑制通道业务，防范影子银行风险；同时按照“新老划断”原则设置过渡期至2020年底，确保平稳过渡。 在我们看来，资管新规的出台确认了一个时代的结束，中国货币超发的时代正式结束了！与此相应，我们的财富创造模式也将发生翻天覆地的变化。 过去发财靠涨价。过去两年，国内的财富创造来源是涨价。比如说大家曾经热捧的茅台，其实主要就是涨价的故事，2016年时53度飞天茅台的零售价只有800多元/瓶，而现在已经涨到了1500元/瓶。而周期行业在过去两年成功逆袭，也在于商品价格大幅上涨，以煤炭、粗钢为代表的商品价格几乎都翻了一倍。 而在过去十年，国内最为成功的财富模式无疑就是买房。从08年到现在，官方统计的全国商品新房的平均售价涨了一倍多，但这个价格其实不可比，因为新房越盖越远，比如10年以前上海卖的新房都在内环内，现在几乎都在外环。按照可比价格来测算，几乎所有一二线城市的房价涨幅都在3倍以上，折算成年化涨幅超过15%。 根本原因货币超发。但是大家有没有想过，凭什么买房可以赚这么多钱？要知道，房子还是那个房子，从房子本身来看其实还有折旧，一套房子放十年以后从质量来看跟以前肯定没法比，但是价格贵了好几倍，这说明房价上涨跟房子本身没有多大关系。 其实，最本质的原因是货币超发，钱不值钱了！在十年以前，中国的广义货币M2总量只有47万亿，而现在的货币总量已经达到了174万亿，货币总量就增加了近3倍，这也是过去10年全国房价实际涨幅平均达到3倍以上的主要原因。 而且，中国的货币统计其实还有遗漏，因为广义货币M2主要统计了银行的存款，也就是银行的表内负债。但是过去几年银行理财大发展，到目前为止的余额接近30万亿，而很大一部分银行理财不统计为银行存款，是以表外影子银行的形式来体现，这意味着包含影子银行以后的中国货币超发情况更严重。 但货币超发并非常态。但是大家有没有想过，其实在经济发展的过程当中，货币超发并非常态现象。发展中的经济体货币增速通常比较高，比如说印度的广义货币M2增速高达30%。但是对于发达经济体，比如美国、欧元区、日本当前的广义货币M2增速都只有3-4%，韩国高一点也就在6%。 而对于这些发达经济体，在历史上其货币增速其实也曾经高过，比如日本70年代货币增速平均是16%，韩国80年代货币增速平均是30%，美国70年代的M2平均增速也有10%，但是到目前为止发达经济体的货币增速几乎无一例外都降到了个位数增长。 为什么货币增速有高有低，如何理解货币增速的变化？ 美国货币史：货币增速先升后降。要理解货币增速的变化，我们可以研究一下美国的货币史，因为美国是全球经济的龙头，而且其拥有最长的经济历史数据，因此其货币增速的变化可以帮我们理解中国货币增速的未来。 金本位时代，货币长期低增。在美国19世纪的100年当中，货币平均增速只有4.5%，远低于20世纪平均7%的货币增速。为什么19世纪美国的货币增速那么低？其实原因非常简单，因为当时处于金本位时代，货币没法超发。 纸币时代，货币增速先升后降。在1933年，美国正式废除了金本位，从此进入了纸币时代。如果观察美国过去80年的货币增速，可以发现前50年货币增速上升，货币平均增速高达9%，但是后30年货币增速下降，货币平均增速仅为6%。 也就是从美国的历史数据来看，货币高增并非常态，其过去200多年的货币增速起初100多年长期在低位，之后50年货币增速上升，但是最近30年货币增速重新下降。 货币作用：从增长到通胀，再从通胀到增长。要理解美国货币增速的历史变化，我们需要思考的是货币的作用。 从宏观来看，货币是经济交换的媒介，因此货币增长主要是满足经济总量增长的需要。而无论是通胀，还是经济的实际增长，都会增加经济名义总量，同时带来对货币的新增需求。因此，货币的增长主要是满足通胀或者经济增长的需要。 金本位时代没有通胀，货币的作用是满足经济增长。在19世纪的100年当中，美国的通胀为零，这意味着100年内美国也没有通胀。19世纪美国GDP实际增速平均约为4.1%，与4.5%的货币增速大致相当，这意味着当时的货币增长主要是满足经济增长的需要。 纸币时代，货币超发导致通胀。但是进入20世纪以后，金本位的缺陷开始显现，因为黄金的产量是有限的，没法满足经济飞速发展的需要，所以主要国家相继都废除了金本位，美国也在1933年进入了纸币时代。 而美国的通胀也在1933年以后产生，其20世纪的平均通胀达到3%，远高于上一个世纪的0%。但从经济增长来看，其20世纪的GDP实际增速平均为3.3%，反而还低于19世纪的4.1%。 这意味着，在进入纸币时代以后，货币增速上升的主要作用只是推高了通胀，但是并没有带来更高的经济增长。20世纪美国的货币增速从4.5%上升到7%，只是把美国的通胀从0推升至3%，而实际经济增速反而还有下降。而这一现象在1970年代达到顶峰，这10年美国的货币增速高达10%，但是GDP实际增速仅为3.2%，而CPI高达7.8%。 在美国20世纪70年代，诞生了著名的货币主义，弗里德曼的名言是“一切通胀都是货币现象”，而这一认识最终改变了美国央行的行为。 央行抑制通胀，货币增速回落。1980年以后，美联储开始转向通胀目标制，把抑制通胀作为最核心的政策目标。1979年沃克尔成为美联储主席，而他持续提高利率紧缩货币也成为里根经济学的重要标志。从那以后30年，美国的通胀大幅回落至2.7%，与之相应美国的货币增速也降至了6%左右水平，但是这一过程当中美国GDP实际增速并没有大幅下降，而是稳定在3%左右。 货币超发，只对涨价有意义。美国的货币史，其实充分证明了一个结论：就是货币超发除了涨价，不创造任何价值。在金本位时代，货币没法超发，其实只是导致了长期通缩、没法涨价，但不影响经济增长。而在纸币时代，货币超发只是带来了通胀的上升，而抑制货币超发也只是降低了通胀，并不影响经济增速。 中国央行：抑制货币超发、货币低增新常态。而美国的历史经验其实也适用于中国。在过去10年，由于美国次贷危机的冲击，我们开始采用刺激需求的方式稳增长，其实本质是采用了货币超发的方式刺激增长。 从08年到17年，中国的GDP实际增速从10%降至了8%，但是这10年当中我们的广义货币增速依然维持在15%左右高位，而且考虑到影子银行的发展，这意味着这10年的实际货币增速还要更高，这期间就不时出现各种价格飞涨，尤其是房价持续上涨导致资产泡沫日益严重。 但是从美国的历史来看，央行的行为是会变的，因为央行会发现货币超发长期没有意义，所以最终会改变货币超发的行为。而在过去两年，中国央行的改变非常明显：首先是政府把打赢防范金融风险作为三大攻坚战的首要任务，与此相应我们建立了金融稳定委员会，实施了资管新规，全力推进金融去杠杆，压缩影子银行。其次是18年的政府工作目标，不再设定任何货币增速，而以往我们每年都有12%以上的货币增速目标。而央行在18年初明确提出，未来货币低增或是新常态。 货币低增时代，创新和债市为王。观察美国过去30年，在美联储下定决心抑制货币超发之后，我们可以发现，靠涨价其实几乎不赚钱。别看最近油价涨的欢，其实过去30年的油价也就涨了一倍。而美国的房价好像创出了历史新高，其实也就回到了08年的高点。也就是无论是商品、还是房子，其实就是涨涨跌跌，长期看每年平均涨幅都非常有限。真正在美国过去30年真正创造财富的是两类资产，一类是创新类资产，美国纳斯达克指数过去30年涨了30倍，另一类是固定收益类资产，因为在控制了货币增速之后，美国利率出现了持续的下降。 今年年初，我们明确提出看好两类资产，一类是创新类资产，另一类是债市为代表的稳定收益资产，到目前为止的表现都非常不错。其实这背后的原因非常简单，如果未来中国的货币不再超发，那么我们的财富创造模式将会发生巨大的变化。靠涨价赚钱将成为过去，靠买房发财的时代就要结束了。而未来只有创造实际增长才能创造财富，这意味着创新才能创造价值，与此同时货币低增将抑制通胀预期，那么债市也会充满机会。 转载来源：海通证券姜超评资管新规：货币超发的时代结束了]]></content>
  </entry>
  <entry>
    <title><![CDATA[专访数学家舒其望：若整天忙着填表评杰青，哪有时间做学问？]]></title>
    <url>%2F2018%2F0543fa4b%2F</url>
    <content type="text"><![CDATA[舒其望资料图“反正聪明人总是很多的，你提供一个好的环境，他们就会冒出来”。美国布朗大学应用数学教授舒其望曾是改革开放以来担任美国名校数学系主任的第一位大陆留学生。 舒其望 资料图 “反正聪明人总是很多的，你提供一个好的环境，他们就会冒出来。” 美国布朗大学应用数学教授舒其望曾是改革开放以来担任美国名校数学系主任的第一位大陆留学生，当澎湃新闻记者问起学科建设的经验，他轻松地回答道。 舒其望衡量一个好环境的最重要标准，就是能让科研工作者不分心。比起竞争数量限定的职位、统计琐碎繁冗的指标、评选层层压身的称号，他相信，同行评议护航下的终身教职制度，能让学者在专心学问的同时，自然而然获得与其水平相称的待遇。 这也许是舒其望今年欣然加入未来科学大奖科学委员会的原因之一。这个由大中华区民间资本设立的科学奖开出单项百万美元奖金，奖励那些由同行评议选出的大中华地区原创基础科学研究。 舒其望参与评选的奖项为“数学与计算机科学奖”，去年刚刚颁发出第一届，得主是时任北京国际数学研究中心教授许晨阳。 数学家向往的“世界顶尖”境界，或许离不开舒其望所说的不分心。 有些孩子就喜欢数学 舒其望现为美国布朗大学讲座教授、美国工业与应用数学学会会士及美国数学学会会士。常春藤盟校之一的布朗大学，是一所古老的顶级私立研究型学府。 他的研究领域包括用于求解双曲方程和对流占优偏微分方程的高精度WENO有限差分及有限体积方法、间断有限元方法和谱方法等。这些方法被广泛应用于计算流体力学、半导体元件模拟及计算宇宙学等领域。 简单来说，微分方程等数学模型可以通过计算机算法应用到上述领域。而舒其望所做的研究，就是用数学方法去检验、提高这些算法的可靠性、准确性和工作效率。 比如，如果对飞机形状做出一些特定的调整，飞机是否能飞得更加平稳呢？在传统上，航空工程师可以在风洞实验中测试气流阻力、噪音等。舒其望与工程师一起设计、分析算法，将风洞实验在计算机上模拟出来。这些数学模型，又往往与天体模拟、半导体元件模拟有相通之处。 现年61岁的舒其望祖籍安徽黟县，他在文革期间度过了中小学，所以也很难说得上有过任何当数学家的童年理想。在这条他个人形容为 “按部就班”的道路上，没有浓重的浪漫主义笔墨，只有数学沿途随意播下兴趣的种子，在多年后竟蔚然成林。 在“胡闹”的上学阶段凭兴趣多看了几本数理化方面的书籍、在恢复高考时报考了相对准备得比较好的数学，1978年，舒其望“按部就班”地进入了中国科学技术大学，开始接受系统性的数学教育。 在舒其望的回忆中，那时的中科大的师生都充满干劲，他在“尝到甜头”后越来越觉得，做数学研究是一件挺好玩的事情。 “但那时候是不是就一定认准了要当数学家呢？好像也不敢说这个话。”舒其望保持着轻松而低调的叙述语气。甚至当被问起后来赴美取得一些研究成果的成功经验时，他也只是笑着说道：“可能是我运气比较好，找老师很顺利，老师也很好，给我的题目也很好，找工作也顺利，可以说是一切都顺利。也许是因为数学比较容易吧，不用牵扯到做实验或其他很多技能。” 舒其望的同辈人在“胡闹”中凭着兴趣自学成长，舒其望下一代的许多孩子，却在奥数班中度过了无数周末。他希望，奥数能褪去高考加分等功利性因素，单纯作为课外活动存在：“有些孩子喜欢画画，有些孩子喜欢打篮球，而有些孩子就喜欢数学。” 分心的事多了，学问就做不好 在美国加州大学洛杉矶分校获博士学位后，1987年，舒其望开始在布朗大学任职。其中1999年至2005年间，他担任应用数学系系主任，为改革开放后中国留学生中第一人。 谈起学科建设，舒其望反复强调不能让科研工作者有太多分心的事情：“要让大家一心一意做自己想做的研究，不需要成天担心自己的教职保不保得住，或是去想是不是可以拿到‘杰青’之类的称号。” 相比国内通常为科研人员设置阶段性的考核指标，由此被一些行政程序和人事关系牵绊精力，他认为美国的终身教职制度能更好地提供专心的研究环境。 “一天到晚填表，说今年发了多少一区文章、二区文章，哪有时间干这种事？我最多花5%的时间在这些对科研毫无用处的事情上。”舒其望直言道。 “一个人的精力有限，分心的事多了，学问就做不好。”他简单直白的逻辑，令人想起苏轼“长恨此身非我有，何时忘却营营”的感慨。 舒其望看到一些中国高校也在尝试终身教职制度，但实际操作中设置了诸多行政性指标：“比如说今年能申两个（职位），五个人都够格申请。一定要从五个人里面评出两个人，这个做法就非常要命。这五个人又不是都做一个研究方向的，你怎么判断淘汰哪三个人呢？” 他担心的情况是，也许五个人都很优秀，却因为学术外的人为因素，无法都拿到职位。 相比起来，美国的终身教职制度始终着眼于考量个人素质是否足够优秀，“你申不上是因为你不太符合这个位置所要求的标准，而不是因为没有位置给你了。”舒其望概括了一下内在的逻辑差异。 终身教职这个科研铁饭碗，会不会存在诱使科研人员混日子的隐患呢？舒其望说道，根据他的个人见闻，这样的制度能保绝大多数的优秀人才在专心科研的同时拿到相应水平的职位，混日子的隐患是必然存在的代价。从现实情况看，这代价已经极小。 数学不需要跟着热点走 不过，即使在实施终身教职的大洋彼岸，足够 “分心”的理由也还有很多。如今舒其望门下有着很多中国学生，保持着用功而成绩优秀的传统，但比起老一辈留学生，他们的七巧玲珑心也许有了更多的疑问，比如，“我是不是应该去赚大钱？” 他门下的桃李绝大部分仍选择了继续做研究，去工业界的不到四分之一。但不可否认，金融和计算机产业的火热，足以令许多数学系学生心动侧目。 有意思的是，舒其望进入未来科学大奖科学委员会参与评选的 “数学与计算机科学奖”，其奖项名称本身就点出了，数学这门古老神圣的基础学科，和计算机科学这门炙手可热的产业新贵，已被紧密地联系在一起。 站在应用数学家的角度，舒其望对计算机科学眼下的人工智能热看得较为平淡。这并不是人工智能第一次掀起热潮：“上世纪60年代就搞过一次，最后发现计算机还没达到那个能力，走不通。现在数学上取得了一些革命性的突破，计算机当然也有很大的进步，两相结合，大家就看到了希望，又说这个东西可以做出来。有热点就会有炒作，现在的人工智能可能存在炒作的情况，到底能坚持多久我也不敢说。” 潮来潮去间，人工智能仍在等待数学世界里的斗转星移。“热点可能几年一换。只要你把数学基础做好了，不管什么来了都能用得上。所以我觉得，数学不是特别需要跟着热点走。”舒其望总结道。 转载来源：专访数学家舒其望：若整天忙着填表评杰青，哪有时间做学问？]]></content>
      <categories>
        <category>教育</category>
      </categories>
      <tags>
        <tag>数学</tag>
        <tag>布朗大学</tag>
        <tag>工程师</tag>
        <tag>UCLA</tag>
        <tag>高考</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[谷歌发布迄今最大注释图像数据集，190万图像目标检测挑战赛启动]]></title>
    <url>%2F2018%2F90c473b2%2F</url>
    <content type="text"><![CDATA[包含190万张图片，共计600个类别，共标记了1540万个边界框，这是迄今的有对象位置注释的最大数据集。 新智元编译 来源：research.googleblog.com 编译：小潘 【新智元导读】今天，谷歌宣布开放Open Images V4数据集，包含190万张图片，共计600个类别，共标记了1540万个边界框，这是迄今的有对象位置注释的最大数据集。基于此数据集，谷歌将在ECCV 2018举办大型图像挑战赛。 2016年，谷歌推出一个包含900万张图片的联合发布数据库：Open Images，其中标注了成千上万个对象类别。从它发布以来，谷歌的工程师一直在努力更新和重新整理数据集，以为计算机视觉研究领域提供有用的资源来开发新的模型。 今天，谷歌宣布开放Open Images V4，其中包含190万张图片，共计600个类别，共标记了1540万个边界框。这个数据集成为现有的带有对象位置注释的最大数据集。这些边界框大部分是由专业的注释人员手工绘制的，以确保准确性和一致性。这些图像非常多样，通常包含有几个对象的复杂场景（平均每个图像包含8个边界框）。 谷歌发起大型开放图像挑战赛与此同时，谷歌还推出Open Image Challenge（开放图像挑战赛），这是一项新的目标检测挑战，将在2018年欧洲计算机视觉会议（ECCV 2018）上举行。Open Image Challenge遵循了PASCAL VOC、ImageNet和COCO的传统，但规模空前： 在170万张训练图片中，有1220万个有框注释，共500个类别。- 比以前的检测挑战更广泛，包括诸如“fedora”和“snowman”之类的新对象。- 除了对象检测这个任务之外，挑战还包括一个视觉关系检测跟踪人物，即在特定关系中检测对象的配对情况，例如“女人弹吉他”。比以前的检测挑战更广泛，包括诸如“fedora”和“snowman”之类的新对象。 训练集现在已经可以使用了。一组包含10万个图像的测试集将于2018年7月1日由Kaggle发布。提交结果的截止日期是2018年9月1日。我们希望这些大型的训练集能够激发对更精密的探测模型的研究，这些模型将超过目前最先进的性能，并且500个类别将能够更精确地评估不同的探测器在哪里表现得最好。此外，有大量的图像和许多对象的注释使我们能够探索视觉关系检测，这是一个正在发展的分支领域的热门话题。 除此之外，Open Images V4还包含3010万的人工验证的图像级标签，共计19794个类别，这并不是挑战的一部分。该数据集包括550万个图像级标签，由来自世界各地的成千上万的用户在crowdsource.google.com上生成。 Open Images V4数据集Open Images是一个由900万张图片组成的数据集，这些图像被标注为图像级标签和对象边界框。V4的训练集包含了600对象类的1460万个图像，其中共标记了174万个标记目标，这使得它成为现有的最大包含对象位置注释的数据集。这些物体的边界框大部分是由专业的注释器手工绘制的，以确保准确性和一致性。这些图像非常多样，通常包含有多个对象的复杂场景（平均每个图像有8.4个标记）。此外，数据集还带有数千个类的图像级标签。 数据组织结构 数据集被分割为一个训练集（9,011,219图像），一个验证集（41620个图像）和一个测试集（125,436张图片）。这些图像被标注了图像级标签和边界框，如下所述。 表1 表1显示了数据集的所有子集中的图像级标签的概述。所有的图像都有机器生成的图像级标签，这些标签是由类似于Google Cloud Vision API的计算机视觉模型自动生成的。这些自动生成的标签有一个很大的假正率。 此外，验证和测试集，以及部分训练集都包含经过人工验证的图像级标签。大多数验证都是由Google内部的注释者完成的。更小的部分是通过图片标签软件来完成的，如Crowdsource app, g.co/imagelabeler。这个验证过程实际上消除了假阳性（但不是传统意义上的假阴性，这种方式会导致一些标签可能在图像中丢失）。由此产生的标签在很大程度上是正确的，我们建议使用这些标签来训练计算机视觉模型。使用多个计算机视觉模型来生成样本，这样做是保证在训练时不仅仅用机器生成的标签数据，这就是为什么词汇表被显著扩展的原因，如表一所示。 总的来说，有19995个不同的类和图像级标签。请注意，这个数字略高于上表中人工验证的标签的数量。原因是在机器生成的数据集中有少量的标签并没有出现在人工验证的集合中。可训练的类是那些在V4训练集中至少有100个正例的人工验证类。基于这个定义，7186个类被认为是可训练的。 边界框 表2 表2显示了数据集的所有分割中边界框注释的概述，它包含了600个对象类。这些服务提供的范围比ILSVRC和COCO探测挑战的范围更广，包括诸如“fedora”和“snowman”之类的新对象。 对于训练集，我们在174 万的图像中标注了方框，用于可用的阳性人工标记的图像级标签。我们关注最具体的标签。例如，如果一个图像包含汽车、豪华轿车、螺丝刀，我们为豪华轿车和螺丝刀提供带注释的标注方框。对于图像中的每一个标签，我们详尽地注释了图像中的对象类的每个实例。数据集共包含1460万个的边界框。平均每个图像有8.4个标记对象。 对于验证和测试集，针对所有可用的正图像级标签，我们提供了所有对象实例详尽的边界框注释。所有的边界框都是手工绘制的。我们有意地尝试在语义层次结构中尽可能详尽地标注注释框。平均来说，在验证和测试集中，每个图像标记了5个边界框。 在所有的子集中，包括训练集、验证集和测试集中，注释器还为每个边界框标记了一组属性，例如指出该对象是否被遮挡。 类定义（Class definitions） 类别由MIDs（机器生成的id）标识，可以在Freebase或Google知识图的API中找到。每个类的简短描述都可以在类中CSV中找到。 统计和数据分析 600个可标记类的层次结构 Open Images数据集&amp;挑战赛地址： https&#58;//storage.googleapis.com/openimages/web/index.html 【加入社群】 新智元 AI 技术 + 产业社群招募中，欢迎对 AI 技术 + 产业落地感兴趣的同学，加小助手微信号&#58; aiera2015_1 入群；通过审核后我们将邀请进群，加入社群后务必修改群备注（姓名 - 公司 - 职位；专业群审核较严，敬请谅解）。 转载来源：谷歌发布迄今最大注释图像数据集，190万图像目标检测挑战赛启动]]></content>
      <categories>
        <category>科技</category>
      </categories>
      <tags>
        <tag>软件</tag>
        <tag>Google</tag>
        <tag>人工智能</tag>
        <tag>Kaggle</tag>
        <tag>Fedora</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[36氪领读 | 零售进化，适者生存]]></title>
    <url>%2F2018%2F248337fc%2F</url>
    <content type="text"><![CDATA[36氪专门为读书设立了栏目，筛选一些值得读的书，并提供一些书摘。内容简介马云说：“纯电商的时代很快就会结束，未来的十年、二十年将没有电子商务，取而代之的是‘新零售’。 36氪专门为读书设立了【36氪领读】栏目，筛选一些值得读的书，并提供一些书摘。希望你手边有一本称心的书，让读书这场运动继续下去。 内容简介马云说：“纯电商的时代很快就会结束，未来的十年、二十年将没有电子商务，取而代之的是‘新零售’。线上线下和物流、大数据结合在一起，才是真正的新零售！” 亚马逊说：“Amazon Go的无须排队、无须结账、无须售货、无收银员才是未来新零售门店该有的样子……” VMC说：“新零售=传统零售业务+新技术+新金融，VMC Anywhere新零售系统是最适合商品流通行业的新零售！” 新零售到底是什么，未来的零售业又是何种形态？我们能否实现人货场重构、消费场景无处不在？大数据、智能化能得到怎样的运用，AR/VR又能带来怎样的虚实消费体验？ 新零售绝不是简单的线上线下融合相通，在零售营销的技术变革中，顾客洞察－身份识别、位置识别、个性化服务，构成了关键驱动要素。 作者简介上海六韬三略咨询机构创始人，上海九逸科技有限公司CEO，曾任联想零售执行委员会专家委员，常年从事零售研究与实践。从2013年起相继出版了《中国为什么没有优衣库》《零售4.0时代，互联网＋零售》等书籍。同时，他及他的团队也为诸多企业提供系统的咨询服务，在新零售带来了无限可能的机遇下，帮助传统企业重新回到同一起跑线上，赋予他们更多的思维和创新能力，谋求以通过商品、服务、渠道、商业模式等方面的创新来重构零售竞争力。 书籍摘录第一章 新零售的号角吹响——新零售时代已经到来 零售是一座桥梁，它搭建的基础是消费者与零售商；零售是一个通道，零售商通过它向消费者传递美好的生活；零售还是一个愿景，在这里将构建一个理想中的生活蓝图。零售自诞生之日起，就与消费者的生活密不可分，只要有消费者的需求，零售就会永远存在。 历史的车轮不断滚滚向前，周边的人与事都发生了诸多变化，城市更加现代，生活更加便捷，通信更加方便，购物方式也更加多元。不知不觉中，零售随着历史的演进渐渐旧貌换新颜。 零售的改变是被历史进程裹挟着前行的，这里有自觉也有不自觉，但每次改变，不管大与小、强与弱，都是为了让生活更美好。消费者乐在其中，他们发现，零售与自己生活的融合度越来越高，零售变得无可替代。 零售是一种表达方式，是一种生活方式，抑或是一种基本法则？ 零售是一个常见词，提到它，几乎每个人都知道，但却少有人深究零售的真实含义是什么。 在给出正式答案之前，我们还是先按照大众的理解来分析一下零售的意义。 零售是零售商把商品与服务卖给消费者，消费者愿意为此买单。零售需要有一个地方出售商品，不管是在一个看得见的地方还是在看不见的网络上，它都需要一个依托，来进行展示和销售。零售的含金量就是把商品与服务卖给那些认同它价值的人。 零售是简单的，也是复杂的交易。简单在于了解它不需要太深奥的道理，直白地说，就是赚取差价。这有点像买股票，低价入，高价出。低价购进的商品，通过零售商的努力，提升其内在价值，再流转到消费者手中。以出售服务为内容的零售商看似是零成本的，但其隐含的成本其实很多，也很复杂。 零售的复杂在于，这么一件看似简单的事情，做得久的人却并不多。世界上最大的零售商之一沃尔玛仅有50余年的历史，而亚马逊、阿里这些后起之秀的历史更是短暂。阿里要做102年的企业，时间将证明一切。 零售是供应链条的最后一个环节，零售商与消费者的距离最近，零售与生活密切相关。 零售物种大爆发 新零售的概念一经提出，与其有关的话题就从未停止过。在争论的同时，更多的零售商投入实践之中。当下的零售业出现了前所未有的活跃状态，几乎每天都能听到新的资讯，也能看到新的零售实践形式，许多零售商试图用实践的结果来回答什么是新零售。新零售还未有一定之规，有的只是对创新的探求。 在线上与线下加速融合的当下，从概念的探讨回归到实践的创新，新零售不是停留于文字表面的静止，而是不断前行、不断添加新内容的实践。 所以，对新零售的认知终归要回归到实践之中。 “忽如一夜春风来，千树万树梨花开”，层出不穷的新的零售形式让人应接不暇。这些零售形式有从线上到线下的落地，也有横空出世的创造，还有线下的探索。每种零售形式都被赋予了不同的理解与卖点，让同行眼前一亮，让消费者相见恨晚。 但这些新的零售形式的内容不管怎样变化，其核心都是为了效率的提升、成本的降低、与消费者更好的互动与体验。这些对新的零售探索像开足马力的马达，轰隆隆地向前驶去。 家乐福关注于“邻家业态”，在上海开出29家“Easy家乐福”后，在无锡开出第30家实体店。家乐福的这个新业态，更贴近于社区，SKU（库存量单位）在4000左右，而面积在300~400平方米，给消费者提供了比便利店更为宽敞的空间。在新业态的探索上，家乐福把找到可持续盈利模式作为关键点，这是业态生存的基础。 大润发也在探索，近期在上海开出飞牛优鲜的第一家实体店，这是依托原有卖场基础进行的改造，后期实体店会独立运营。飞牛优鲜首先借助了大润发的现有资源，线上与线下联动，实现在配送范围之内，1小时送到客户手中。在这个比速度的时代，消费者的耐心被一点点消磨，只有更快，才有可能争夺到机会。 永辉一改传统的形象定位，频频试水新的业态形式，这样的尝试，无疑是在抢夺不同层次的消费者。BRAVOYH是永辉精致超市品牌，主要以经营高端产品和时尚用品为主，这一定位与民生超市永辉已经有了天壤之别。除此之外，永辉还开出了“超级物种”。店如其名，在“超级物种”店内，一个个小物种，如健康有机生活馆、生活厨坊、花坊、鲑鱼工坊、波龙工坊、盒牛工坊、麦子工坊、外卖工坊、择物工坊等组合成了一个大物种。 商圈不同，“超级物种”里的小物种组合方式也略有差异。值得一提的是，与其他零售商过分依赖外来力量助力发展不同，“超级物种”全部是永辉的知识产权，这保证了一定时间内永辉在这个业态形式上拥有更多的自主权。与BRAVOYH相同的是，“超级物种”聚焦的也是高端客群。这两种新兴业态显然与原来的永辉卖场有了很大区别。 老牌零售商也在摸索新的业态形式，可谓老树开出新花。百联集团于近期开出了RISO首店，这家实体店同样无法用传统的零售视角来解读，百联集团把它定义为新零售发现店，是集生鲜、餐饮、书店为一体的新的融合模式，这种前所未有的组合方式，不光给零售业带来了想象空间，还让消费者体验到了各种看似的不可能的情景。 在店内，顾客不用专门到收银台排队结账，只要找到戴着特殊标识的员工就可以实现现场收银，免去了排队的烦恼。顾客用App下单，在配送范围内1个小时就可送达。 各大零售商的业态探索有相同之处，也有不同之处。相同之处是，所有零售商都顾及了消费者的跨渠道体验，将线上与线下融为一体，实现了线上下单、线下快速配送。线上与线下的连接成为一项必备技能，如果缺失，根本无法与消费者达成共识；所有零售商都不约而同地强调了配送的及时性，承诺在配送范围内可以快速到达。 3公里商圈客户，成为线上与线下争夺的重点。这些大品牌零售商开出的小业态都把商圈压缩到触手可及的范围。商圈的缩小，让目标消费者更为集中，同时还能把零售商的运营效率发挥到极致。 不同之处在于，每个零售商的新业态都极具想象力，这考验的是零售商的智慧，他们都在尽量地避免与竞争伙伴重合。这些新推出的实体店业态，每家零售商都有独家秘籍，这表明他们不甘心与他人雷同，残酷的竞争也不允许他们与别人雷同。 业态探索最终探索的是效率与成本，缓行的零售商可能因为一时的迟疑而错过最佳的发展机会。在发展的紧要关头，机会的错失将无法弥补。柯达在由胶卷成像到数码相机的转变进程中，因为迟疑而把最好的机会拱手让人，从而走向了破产的命运；诺基亚自1996年开始，连续14年保持了手机行业的市场份额第一，但面对智能手机崛起，诺基亚未做出及时的转变，终被苹果和三星超越，最后不得不把手机业务卖给微软，从此退出手机市场。 行业的转变带来机会的同时，也暗潮涌动。与风浪同行的人可能会获得机会，但远离风浪的人最终交出的必然是竞争资格，如果资格都失去了，还谈什么竞争？现实如此残酷，这也让各大零售商不敢停下脚步，就算付出资金成本和时间成本，也要坚持探索；就算是一个试错过程，也要不惜代价，坚持前行。 零售第一阶段：有什么买什么 大多数人，对1.0时代的印象极为模糊。翻开历史，那时零售没有形成体系与规模，都是单打独斗，零售形式单一，售卖方式传统古老，单品还不像现在这样极大丰富。零售商大多依附于体制而生存。因为没有竞争，消费者的需求未被充分挖掘出来，紧俏的商品让消费者趋之若鹜。 在1.0时代，零售仅是买与卖的代名词。较为正规的零售商用三尺柜台隔开消费者与营业人员，双方有距离地交流，营业人员不冷不热，消费者也没有太大热情，完成简单的交易后，零售商的使命基本结束。 许多人的记忆中还残存着一些印象：在物资匮乏的年代一物难求，大家都想尽办法买一台电视机，谁家要是有了电视，那就成了邻居们眼中的红人。商品短缺让零售成为毋庸置疑的卖方市场，消费者的需求根本得不到满足。 但随着经济的发展，零售市场斗转星移，消费者很快又要回了主动权。 零售第二阶段：连锁扩张 渐渐地，零售商开始以新姿态示人，最令人印象深刻的是柜台的消失。撤掉柜台，消费者可以自由进出品牌区、超市，不再受遮挡物的干扰。营业人员也变得热情，他们不再对消费者不冷不热、冷眼相对，而是想尽办法“讨好”消费者，消费者由被动变为主动；商品不再供不应求，而是供大于求。于是，零售步入另一个轨道，买方市场来临。 在收集零售2.0时代的资料时，我发现了一个有趣的巧合，那就是“95、96、97元年”现象。在这三年里，中国大地上成立了大量的零售实体，有国资发起的，也有个人创立的，还有国外零售品牌的涌入，现在耳熟能详的零售品牌大多是在那时成立或进入中国市场的。步步高成立于1995年，永辉的前身也成立于1995年，胖东来同样是在这一年开出了前身：望月楼胖子店，家乐福在1995年进入中国市场。 1996年，同样诞生了许多零售商，人人乐和银座都是在1996年开出了第一家实体店，在这一年，沃尔玛和麦德龙进入中国市场。1997年仍很精彩，大润发和银泰分别在上海和北京成立。翻看这些集团的历史就会发现，他们在成立后，并没有满足于现状，都在两三年内迅速开设了分店，由同城到异地，他们凭借最初积累的经验与人气，跑马圈地，渐渐扩大疆域。在他们的引领下，零售市场迎来了2.0时代。 从此，零售商不再单枪匹马，而是树立了集团化战略策略，零售发展呈现连锁化的态势。零售商凭借自身的资源优势，迅速掌握了渠道的话语权。渠道为王，生产商除了传统渠道之外并没有另外的渠道可以销售，必须完全仰仗于零售渠道的分销。这真是传统零售商的“黄金时代”。但是好景不长，剧情很快就发生了逆转，传统零售商不再是“唯一”。 零售第三阶段：电商发展 电商的兴起，让零售市场不再是传统零售商的一枝独秀。电脑的普及、互联网的发展，为电商成立打下了物质基础。同样是巧合，目前电商的两大巨头都在同一个年份涉足电子商务领域，淘宝于2003年上线，同一年京东商城因为“非典”的影响由线下转向线上发展。两大电商的上线，初时并没有引起过多的关注，也未引起实体零售商的警惕。但是其影响力很快就显现出来，渐渐地，线上开始抢占线下的市场，双方虽未呈势均力敌之势，但电商的威力已不能让人小觑。 因为上网需要借助电脑等固定设备，这为购买带来一定障碍，PC时代的网购还没有现在那么疯狂。传统零售商在电商的PC时代并未做出回击之势，只是像对待普通的新事物那样，不屑、好奇，还有些无动于衷。 渐渐地，消费者的购物范围开始扩大，消费者也在线上与线下不停地来回转换；传统生产商的销售渠道也不再单一，而是有了更多的选择。传统零售商此时发现，他们的独有优势正在消失。 随着售卖渠道的增多，消费者开始疯狂地比价。他们潜在的价格敏感度被挖掘出来，谁的价格更低，他们就更愿意到哪个渠道购物。电商凭借价格的优势建立了稳定的客流，更为意义深远的是，他们培养了消费者的购物习惯，消费者对网络购物越来越信任和依赖，这为未来的竞争积累了更多资本。 此时传统零售商依然浑然不觉，大多数零售商还沉浸在“黄金时代”的自鸣得意里，未把消费者购物习惯潜移默化的转变太当回事，从而错过了研究消费者习惯转变的最佳时期。 零售第四阶段：开放与包容、多元与个性并存 这一阶段，线上线下突然变得泾渭分明，线上零售的发展让人不敢小觑，而线下的市场份额一步步被蚕食。线上的发展如果非要用一个标志性事件来总结的话，可以把阿里巴巴的“双11”当成一面旗帜，这面旗帜是电商发展的一个缩影。从2009年初创到2016年，“双11”走过了8个年头，而这8年是电商崛起的一个重要时段。2009年天猫“双11”的销售额仅为5200万元，2017年就达到了1682亿元，几何级数的增长速度让人惊叹不已。 在这些数据中，我们发现了一个有趣的现象。2012年“双11”销售额达到了191亿元，首次突破百亿。查看历年“双11”数据同比值，可以发现2012年是同比销售增长最多的年份，达到了468%；而2012年还有一个数据也不容忽视，那就是移动支付的兴起。移动支付数量达到了900万笔，占到总销售额的6%，与2011年同比增长426%。销售增长与移动支付增长，两个比值竟如此相近，说明消费者正在从PC端悄悄地向手机等移动设备端转移。 过了2012年，移动支付势如破竹，一发不可收拾，到了2016年，无线端支付占到了“双11”全天销售额的近82%。如果说2012年是移动支付迁移的开始，那么到了2016年，就基本完成了转移。消费者从PC端一下跃到了无线端。 这是一次质的飞跃，消费者已然不是昨天的消费者。这看似是使用工具的转移，但背后却是消费者购物习惯、选购方式、需求满足方式的变化。这些变化，让消费者不再具有传统零售模式下的消费习性。他们更善于使用工具，也更善于通过不同渠道满足自己的购买欲望。 消费者成为真正意义上的中心。风云突变，传统零售商仿佛是一夜之间才感知到“春光如此灿烂”，但大多传统零售商根本无暇欣赏这春光如画的美景，因为他们才想起前期忘记种树了。春天没有花赏，秋天没有果吃，那如何“过冬”呢？ 互联网时代的来临，消费的主动权又交还给了消费者，电商在拼命地争取流量，传统零售商正在找回客流，消费者成为各方争夺的焦点。有了流量和客流，就不怕没有销量，这是各方达成的共识。 这个阶段是一个开放和包容的时代，零售商不再拘泥于线上或线下，线上与线下正在快速融合；也不再拘泥于某种形式，大到购物中心，小到一个人的微店，零售的舞台更加多元，也更加精彩。 以上简单梳理了零售的发展进程，这一路走来，每个人都参与其中。不管是商家还是一名普通的消费者，在回顾时才发现自己竟走了这么远的路，一路上的变化是如此之大。而正在经历变化的零售商和消费者竟浑然不觉，真是“只缘身在此山中”。 从近乎原始的第一阶段，到开放包容、个性多元的第四阶段，再到今天提出“新零售”的概念，这是一部零售行业的进化简史。在新零售时代，我们拥有了新的机遇点，同样也会面临新的挑战。 新零售时代：新时代的好零售 评判零售商有两个重要的指标，这两项指标也是上市公司年报上常见的。零售商在日常管理中，也把这两项指标作为重要的参数，公司的经营业绩通过这两项指标就能一下子跃然纸上，而且直观、一目了然，它们就是销售额与利润。但数据仅是结果的表现，只代表前一阶段或当前的状态，并不能全面反映零售商的发展状况。曾有一家上市公司的年报数据显示在上一财年不管是利润还是销售额，公司都取得了令人满意的成绩。但时隔一年，数据就出现了断崖式下降。数字不能代表一切，但也能代表一切。 为什么这么说？ 说数字不能代表一切，是因为尽管上一财务年度的数据还很亮眼，但零售商的内在核心其实早就发生了变化。不过是因为有经营的惯性，数据其实是在缓慢走低的，所以掩盖了深层次的问题。说数字代表一切，是因为核心失去后，直接的体现就是数据严重下跌。 一家好的零售商不仅要做到满足消费者的愿望，提升消费者体验，同时还要建立稳固的专业化市场，保证业绩持续稳定的增长。零售商需要建立长期的市场计划和明确的战略目标，而不能用杀鸡取卵的方式只顾眼前的苟且。得过且过的零售商肯定无法实现长期的盈利。 好的零售商不能患上“数字近视症”，一味强调和追求数据的漂亮，并对数字过分重视与夸大，而不去建立和耕耘长效机制，不主动深入研究数字背后隐藏的问题与原因。长此以往，必定忧患重重。比如有些零售商的员工经常在一个专业论坛里曝光内部管理问题和发泄心中的不满，但因为数据指标表现还算不错，并未引起外界的重视。而当数据不能再掩盖问题的时候，员工的声音又被无限放大。其实问题不是刚刚出现或发生的，“千里之堤，毁于蚁穴”，大厦的倒塌其实早就有了前兆，只是未引起管理层的重视而已。 好的零售商要时刻记得研究顾客的需求与体验，这是零售商立足的根本，在任何时候做任何决定，都不能忘了站在顾客的角度做出思考与选择。马化腾在接受《哈佛商业评论》专访时说，用户需求和用户体验是腾讯研究的重中之重，用户需求与喜好瞬息万变，腾讯每天都在研究。腾讯最早的一个产品是QQ，许多人都有10年以上的“Q龄”，一路相伴走来，会发现QQ也在随时发生着变化，QQ的许多功能与时俱进，与潮流接轨。如果不去改进与变化，QQ可能早就进了历史博物馆。 好的零售就如同慢火煮汤，火势不激进，细工慢火才能打磨出食材深层的味道，并历久弥新，久久让人难忘。 经营深处的拷问，变与不变 突出重围，找到立足地，占领行业或区域行业的制高点，这是许多零售商的梦想。然而现实的残酷与竞争的激烈，却把梦想击得粉碎。 拨开云雾见明月。云雾重重，究竟用什么力量可以把云雾拔开，见到那轮最美的明月？看看零售的发展史，一路走来，做得好的零售商其实是有共性的，不管时代如何演进，消费者怎样变化，表现好的零售商都与这两个字有关。 变 一个主动求变的零售商保持了进取的姿态，不管在思维方式上还是经营策略上都愿意与消费者的需求相磨合。主动求变意味着要或小或大地改变过去的自己，甚至否定自己，这个过程相当痛苦与难熬，像凤凰涅槃，再造一个自己。无印良品当年遇到业绩滑坡时，时任社长松井忠三走访了许多实体店，与员工们座谈，专注于问题的解决。 他还理顺了岗位职责，建立了工作标准手册，并处理了大量滞销商品库存。新制度的建立、与旧有观念的博弈、不良库存的清理与利润损失的阵痛，都让求变之路步履艰难。求变的过程不轻松，但求变后旧貌换新颜，无印良品走上了快速发展之路。如果没有当时的变，就没有今天的无印良品。 主动寻求变化意味着要走出舒适区，时刻挑战自己。不管是线上还是线下，不管是龙头还是草芥，敢于挑战的零售商不是在变化就是在变化的路上。这厢阿里的支付宝刚刚推了VR红包，那厢微信就推出了小程序，让消费者应接不暇。虽然变化无处不在，但也要看到，有的变化长久地生存了下来，有的变化如滑过天际的流星，一闪而过。 变化的根本是洞察顾客的变化，满足顾客的需求，这是所有变化的宗旨。 顺 此处的“顺”可理解为“顺势而为”。零售的发展如一条绵延悠长的水路，消费者和时代的发展、竞争环境的变化是河流里的水，零售商是船，逆水行舟，不进则退。只有顺势而行，才能与消费者、时代、外部环境融合起来，才能有取胜的机会。 零售商如果想做到顺水行舟，首先要顺应消费者的需求，不管是零售商暗合消费者的需求，还是引领他们的需求，消费者的需求始终是零售商前行的方向与目标。只有与消费者的需求合拍，消费者才愿意买单。 还要顺应技术的发展。凯文·凯利说，“技术是世间至强之功”。技术是零售商的内在功夫，拥有先进的技术犹如学会了世界上最高强的武功，靠它行走江湖，就多了份自信。对传统零售商来说，习惯了买货—卖货的简单操作模式和思维模式，在各项新技术不断推出的今天，许多实体零售商显得笨拙和不知所措，显然他们还没有做好学习新技术和应用新技术的准备。 马云提出了“五新”的概念，把新技术也列为重要一项。互联网企业对新技术的热衷有其天然的基因，而实体企业稍显迟钝，在新技术面前要拿出十分的热情热烈拥抱，才能与时代同步。 近期亚马逊实体店Amazon Go利用AI技术真正实现了无人结账。顾客在店里选购商品后可以直接拿回家，他的亚马逊账户随后会收到账单，消费者只要在网上支付即可。亚马逊利用计算机视觉和机器学习技术真正解决了顾客购物后需要等待结账的难题。这种体验在过去几乎闻所未闻，没想到现在梦想很快就变成了现实。 与竞争环境变化相协调，是零售商顺应时代发展的重要体现。零售是一个包容的行业，任何有作为、有想法的企业，都可以在这里大展拳脚。但竞争环境的瞬息万变，又让人不敢有半点松懈。在激烈的竞争中创造优势、突出优势、保持优势，这是与竞争协调发展的必由之路，而竞争是推动进步的发动机。 “顺势而为”还包含另外一层意思，那就是零售商的自我调整。一位有着多年零售经验的前辈曾感叹，每一年的发展变化太快，今年与去年就是天壤之别，去年可能还在犹豫是否引进第三方支付，而今年第三方支付就成了必备之物。零售商要时刻警觉外界的变化，还要警醒自身的不足，这样才能争取更大的机会。 转载来源：36氪领读 | 零售进化，适者生存]]></content>
      <categories>
        <category>科技</category>
      </categories>
      <tags>
        <tag>亚马逊公司</tag>
        <tag>电子商务</tag>
        <tag>家乐福</tag>
        <tag>大润发</tag>
        <tag>沃尔玛</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[美债破3%！中国经济会走剌激房地产老路吗?]]></title>
    <url>%2F2018%2F7fc9456b%2F</url>
    <content type="text"><![CDATA[近日，美国10年期国债收益率涨破了3%，这是自2014年1月以来的首次。受此影响，美国三大股指随后暴跌。标普跌1.37%，纳斯达克跌1.7%，道指跌1.74%；欧洲和亚太股市开盘后，也悉数下跌。而多位华尔街大佬则为此发出警告，如果国债数据超过3%，美股八年大牛市势必会终结。 可能有人会问，美债收益率走高、美元指数升值、全球资本回流美国，就算能够终结美国大牛市，那跟我们中国经济有啥关系呢？… 近日，美国10年期国债收益率涨破了3%，这是自2014年1月以来的首次。受此影响，美国三大股指随后暴跌。标普跌1.37%，纳斯达克跌1.7%，道指跌1.74%；欧洲和亚太股市开盘后，也悉数下跌。而多位华尔街大佬则为此发出警告，如果国债数据超过3%，美股八年大牛市势必会终结。 可能有人会问，美债收益率走高、美元指数升值、全球资本回流美国，就算能够终结美国大牛市，那跟我们中国经济有啥关系呢？对此，我们认为，美国10年期国债收益率突破3%，主要是对中国经济影响还是很大的。 一方面，美债利率走高，意味着中美两国利差会进一步收窄。目前中国10年期收益率为3.6%，利差已从高峰时的150个BP，缩减为现在的60BP，而中美两国长期利差的缩小，会导致国内外资金回流美国本土，这会给人民币汇率造成较大贬值压力。 另一方面，美国社会的通胀率正在逐步上升。由于减税剌激（增加投资）+基建投资+油价飙升，都会推升美国的通胀水平。而因通胀水平上升，导致美联储加快加息和缩表步伐。受此影响，全球资金会加快流向美国的进程，而以人民币计价的资产泡沫就会首当其冲。 就在国内外资金流出已成趋势，人民币蒙受对外贬值压力。同时，中美贸易摩擦持续不断，国内经济转型面临压力，中国经济将往何处去？就值得我们去思考了。 如果中国货币在当下时点选择宽松，这就会导致中国国国债的收益率曲线下移，那么中美两国的利差会进一步被压缩，人民币汇率将遭受空前的贬值压力。 但是如果跟着美国一起收紧货币，人民币汇率稳定住了，而货币收紧之后会导致国内的中小企业生存环境，生存环境更趋恶化。同时，由于中国跟着美国加息之后，房地产将负担不起如此高的利率水平，进而有了方向性下跌的选择。从而引发一系列的系统性金融危机。 从目前情况看，中美贸易摩擦已经对中国经济造成了负面影响，而对这种负面的影响过程，如果再搞货币紧缩肯定是雪上加霜。但如果推出宽松政策，恐怕也不符合当前的“降杠杆，控风险”的目标。所以，未来中国不可能再搞货币剌激政策，更可能的是偏向于中性，就是对于实体经济偏向于宽松，而对于房地产、股市等虚拟经济更倾向性的紧缩。 现在外有美国等西方贸易保护主义的抬头，未来依赖出口来拉动GDP增加更加困难，所以这次中央政治局会议提出了二点重要指示：一是扩大内需。二是要推动信贷、股市、债市、汇市、楼市健康发展，及时跟进监督，守住金融底线，并且消除潜在隐患。 那么，中国该如何扩大内需呢？内需主要包括投资和消费。而投资却又分为制造业、基建与房地产三块。现在情况是，房地产市场是最容易启动的，但加杠杆的空间有限，居民部门的杠杆水平已经非常高，GDP占比已经达到50%！居民债务率从20%到50%，美国花了40年的时间，中国只用了10年。 剩下的还有制造业和基建投资。制造业投资也受到房地产的挤压，而唯有基建投资才勉强的能做点贡献。也就是说在投资方面，只要房地产投资一直过热下去，制造业投资肯定是无法担当助力经济增长大任，只有基建投资尚有可能，只是要中央政府要允许地方政府暂时再扩大一下债务规模，但这也并非长久之计。 讲完了投资之外，还有就是消费，我国居民的消费恐怕也根本拉动不起来，原因是很多购房者把每月收入的大部分都交给了房企和银行，甚至把未来几十年的收入都透支掉了。随着国内房价越来越高，居民消费增加幅度也会大幅放缓。所以，如果要想让民间消费拉动经济，就必须把房地产泡沫给去除掉。 在当前复杂的国际经济和贸易环境之下，中国只能选择扩内需，但是要想把内需真正扩大起来，基础建设投资是托底，而民间投资和消费才是唱主角，但是房地产业对民间投资和消费起到了挤出效应。所以，只有把房地产去掉泡沫，中国经济才能更加平稳、安全的发展。 平说财经 （ID&#58; BZZCAIJING )原创时评 转载来源：美债破3%！中国经济会走剌激房地产老路吗?]]></content>
  </entry>
  <entry>
    <title><![CDATA[小程序风口已至，国家队投资微盟是为名、为利还是为民？]]></title>
    <url>%2F2018%2F3b98a697%2F</url>
    <content type="text"><![CDATA[第一条，是4月20日，“allin小程序”的第三方Saas服务商微盟，完成了高达10.09亿元的D1轮融资。 这两天被小程序刷屏了，而且都是爆炸式的消息。 第一条，是4月20日，“all in 小程序”的第三方Saas服务商微盟，完成了高达10.09亿元的D1轮融资，据说D2轮融资也即将完成，而且金额不低于1.5亿美元。 第二条，在昨天的“小程序&amp;大商业”商业峰会上，明星投资人朱啸虎又放出了炸弹，他认为“小程序流量红利就是今年一年，如果没抓住，那么机会与你无关。” 与这两件大事同样刺激观众眼球的，是小程序在流量萧条时代的逆天数据： 推出一年，1.7亿日活用户、58万个小程序上线、100万个开发者、2300个第三发开放平台，连接微信生态的10亿用户和200多个类目的商家。 如此庞大的商业势能，难怪国家队基金也坐不住了，赶快集体投个第三方服务商微盟“压压惊”。不过互联网行业从来不缺钱，与其说是看中了微信生态庞大的商业跑道，其象征意义其实更值得深思。今天我们就来八一八。 【国家队入局，为利or为名or为民？】 我们不妨先来细数一下微盟10.09亿D1轮融资背后，都有哪些商业大佬： 公开资料显示，微盟10.09亿D1轮融资由上海自贸区基金、国和投资、一村资本、天堂硅谷领投，腾讯双百、东方富海、渤海产业基金、辰韬资本、优势资本等跟投。 其中领投方上海自贸区股权投资基金、国和投资都是国家队级的基金。上海自贸区股权投资基金的身份更有些特殊：它是全国首支自贸区主题投资基金，主要服务于自贸试验区重点产业及‘四新’产业的发展，以及上海全球科创中心建设。 那么，国家队基金对微盟的青眼有加，到底是为名还是为利，亦或是为民？我认为三者兼而有之。 首先，本质上还是金融机构，冲着“国家队”的金字招牌也要做好一个价值投资者。去年就有至少7亿资金押注小程序，今年更是在资本市场所向披靡。据说有投资人放言“没有小程序的电商平台我已经不看了”。小程序如此火爆，其上下游产业链自然也开始蓬勃发展。现象级社交电商拼多多融资30亿美元，朱啸虎6000万投资小程序数据统计平台阿拉丁，第三方Saas服务商微盟受青睐就不足为奇了。 值得注意的是，国家队还是微盟D1轮融资的主力军，这已经不是单纯地看好微盟背后微信生态的大跑道，而是押注微盟在垂直领域的绝对优势。2018年Q1财报显示，微盟2017年全年毛收入近11亿，并且收入、付费客户数、净利润等关键数据都远超于同业者，是货真价实的垂直领域超级入口，未来投资回报不会低。 可以肯定的是，微盟被国家队翻了牌子，这是一次国家战略与市场经济相得益彰的融合，不仅会得到经济和政策上的支持，更有助于其发挥第三方服务商的连接和赋能作用，让微信生态的庞大经济价值惠及各个零售企业和10亿消费者。 【all in 小程序之前，企业首先要解决三大难题】 QuestMobile的报告显示，从2017年1月至2018年3月，微信小程序的月活规模超过4亿，在上个月，小程序日活用户达1.4亿左右，渗透率达43.9%，还有很大的上升空间。 相信已经没有人会质疑，小程序的春天真的来了。 但商家在此时“all in”小程序，恐怕要闯过至少三道关：第一，尽管小程序拥有超过50个流量入口（微信群、朋友圈、公众号、搜索、社交广告、二维码等等），但流量依然集中，精准推广是第一道门槛； 第二道门槛则是场景落地。小程序在内容、社交、门店、工具等多个场景都可以广泛应用，但不同场景下的交易流程和交互体验需求不同。比如零售、餐饮、美业、休闲娱乐等不同行业对应的小程序产品逻辑肯定也各不一样，如何开发出符合自身业务逻辑的小程序就成为当务之急。 第三则是流量转化难。小程序需要下载，又不能像公众号那样主动推送，因此尽管与用户的连接半径最短，但留存成本更大，试错成本高昂，据说新客转化率只有 1%。提升转化率成为攫取红利的第三大门槛。 将业务模式迁移到小程序上，成了所有零售业者头上的达摩克利斯之剑。怎么破？ 最大的关键在于管道。在微信生态中，微信提供的是基础能力。而第三方服务通过互联网技术，为企业提供连接新流量和数据服务的支撑，正是促进微信生态走向繁荣的重要管道。 如果说管道，“all in 小程序”的第三方服务商微盟应该最有优势。去年发布的“新云计划”，微盟小程序矩阵集中亮相，落点在小程序，但眼里盯着的其实是整个企业级服务。SaaS服务体系覆盖了电商、门店、餐厅、外卖、会务、官网、美业、休闲娱乐等多个场景，提供智能化解决方案。 显然，微盟要做的是重度的大连接，以CRM系统为入口，卡位整个智能服务生态。从美国的SalesForce到中国的EC等，CRM都是SaaS最成功的品类。Salesforce市值预计在未来三年将达到1000亿美元。所以，微盟只要执行到位，可以迸发出惊人的商业潜力。 【小程序+公众号+社交广告：微盟跑赢行业的“金三角”】 公平的说，目前企业级Saas服务商各占山头，但许多人更看好微盟。一方面是其先发优势和规模效应所带来的行业护城河极深，另一方面则通过“小程序+公众号+社交广告”所构成的流量“金三角”，为企业级Saas服务在电商在营销上提供一体化解决方案，建立了差异化竞争优势。 一对一的小程序，一对多的公众号、多对多的社交网络（朋友圈），这些都是社交电商的天然渠道。三浪叠加的流量组合，让商家离消费者的距离无限拉近。 社交广告实现流量引入，解决获客难题；公众号助力用户沉淀，解决留存难题；小程序提供服务，帮助提升转化。而最新上线的“分销”功能，是升级，也是裂变，依托微信的社交属性实现裂变传播，激活微信生态的10亿流量，贯穿到“拉新-促活-转化-交易-忠诚”这一社交电商完整链路的始终。 “金三角”的流量闭环和背后巨大的业务推力，正是微盟能够吸引众多电商、零售等合作伙伴的核心原因。 当然，要用好流量“金三角”，前提是要有精准画像。微盟基于微信庞大的社交行为数据和自身平台的消费行为数据，为商家提供更为精准的目标群体画像和丰富的触达通道，助力商家实现更加有效的精准营销。 在挖掘微信生态溢出价值这件事上，微盟下的是苦功夫，更是真干。因为这是他的根本，当然不遗余力。所以，这波小程序流量红利的蛋糕，最先吃到的自然还是微盟。 【结束语】 企业级 SaaS 市场是一个 U 型曲线，前期需要耗费巨大的成本研发产品、培育用户，然后才能迎来增长。 小程序的流量红利释放也才刚刚开始，国家队基金强势参战，领头羊微盟持续赋能并教育市场，也会进一步推动微信生态的繁荣。 相信，小程序会在今后一两年进入大爆发期，企业级Saas也会获得应有的荣光，属于微盟们的舞台很大，加油！ 王冠雄，著名观察家，中国十大自媒体（见各大权威榜单）。主持和参与4次IPO，传统企业“互联网+”转型教练。每日一篇深度文章，发布于微信、微博、搜索引擎，各大门户、科技博客等近30个主流平台，覆盖400万中国核心商业、科技人群。为金融时报、福布斯等世界级媒体撰稿人，观点被媒体广泛转载引用，影响力极大，详情可百度。 转载来源：小程序风口已至，国家队投资微盟是为名、为利还是为民？]]></content>
      <categories>
        <category>科技</category>
      </categories>
      <tags>
        <tag>投资</tag>
        <tag>移动互联网</tag>
        <tag>电子商务</tag>
        <tag>风投</tag>
        <tag>基金</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[“五环外”的拼多多，如何打破圈层壁？]]></title>
    <url>%2F2018%2Fff642dc7%2F</url>
    <content type="text"><![CDATA[连日活 10 亿的微信打破圈层壁都难，拼多多怎么做到的？ 转载来源：“五环外”的拼多多，如何打破圈层壁？]]></content>
      <tags>
        <tag>i黑马</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[微信里的机会，拼多多只是个开始]]></title>
    <url>%2F2018%2F6772d297%2F</url>
    <content type="text"><![CDATA[微信里的机会，拼多多只是个开始 转载来源：微信里的机会，拼多多只是个开始]]></content>
      <tags>
        <tag>虎嗅网</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[消费机器人今年爆发！四大品类瓜分百亿蛋糕]]></title>
    <url>%2F2018%2F9d7ed269%2F</url>
    <content type="text"><![CDATA[特别是视听觉语义理解、自然语言处理、神经网络、情感识别算法等的发展，使得扫地机器人、智能音箱等消费级机器人产品逐渐走入消费者视野。 近两年，人工智能技术的迅猛发展。特别是视听觉语义理解、自然语言处理、神经网络、情感识别算法等的发展，使得扫地机器人、智能音箱等消费级机器人产品逐渐走入消费者视野。根据捷孚凯（GfK）最新报告，中国消费级机器人2018年零售规模将超100亿元。 本期的智能内参，我们推荐来自捷孚凯的中国消费级机器人分析报告，就扫地机器人、智能音箱、民用机器人和智能服务机器人的零售数据出发，盘点市场的消费偏好以及各类产品作为智能家居入口的潜力。如果想收藏本文的报告全文（捷孚凯：中国消费级机器人分析报告），可以在智东西头条号回复关键词“**nc248**”下载。 以下为智能内参整理呈现的干货： 机器人市场都在卖什么 ▲中国机器人市场规模（2015-2020年） 机器人市场主要分为工业机器人、商用服务机器人，以及消费级机器人。 其中，工业机器人主要用于制造业， 包括离散制造和流程制造，在政策和资本的支持下正蓬勃发展，预计市场规模在2020年将达到1110亿美元（IDC数据）。 而商用服务机器人，覆盖医疗、零售批发、公共事业和交通领域等领域，面临着巨大的发展机遇和市场空间，已经处于爆发的临界点。 ▲中国消费级机器人零售规模（2018年将超100亿元） 消费级机器人方面，前两年的产品功能和价格上，与用户的期望还有一定的差距，导致厂商变现困难。但随着智能化技术的发展，产品与需求逐渐匹配，加上多家科技巨头的入局，市场大门正在打开。 目前，市场上消费级服务机器人的主要应用场景包括幼儿教育、助老助残、智能家居、数字娱乐、情感陪护等。在现有的技术成熟度和市场接受度下，扫地机器人、智能音箱、民用无人机、智能服务机器人这四大品类成为主流产品。 ▲消费级机器人的四大主流品类 扫地机器人即帮助消费者从繁琐的家庭劳务中解脱出来，负责清扫、吸尘、拖地的智能化清洁类产品，因需求明确、任务单一，因此发展较为成熟。 智能音箱则是2017年的爆款产品，可以通过语音交互控制智能家居和其他智能设备，并且实现内容搜索、上网购物等第三方服务，成为智能家居的新入口。 民用无人机目前主要用途以航拍摄影为主，兼顾一些商业用途，如电力巡检、环境监测、快递送货等。目前，国内普通民众对无人机的认可程度和需求度逐渐攀升。 智能服务机器人搭载了语音交互、远程视频、本地服务、家居控制等功能，以早教和娱乐为卖点，从而快速地打入消费级市场。 细分市场详析随着人工智能的发展，以及居民可支配收入增加，消费级机器人市场潜力巨大，青年群体（15-49岁的消费人群）的热衷程度与兴奋态度，直接带动了智能音箱、智能服务机器人、无人机、扫地机器人等新物种的跨越式进步。 扫地机器人：智能家居入门级产品 ▲中国扫地机器人市场规模 随着科技的进步和社会发展，特别是受生活节奏加快和工作压力增大的影响，消费者希望从繁琐的家庭日常清洁事务中解脱出来。扫地机器人在这样的背景下应运而生，并且正以惊人的速度普及。 捷孚凯数据显示：扫地机器人零售量五年复合增速高达51%，2017年零售额同比增长率23%；至2017年，扫地机器人零售额已达44.1亿元（零售量332.5万台），预计2018年市场规模将达53.8亿元（435万台）。 ▲中国扫地机器人以线上市场为主 从销售渠道来看，扫地机器人自进入中国市场以来，主要以在线销售为主要销售渠道。2017年，扫地机器人在线零售量占比91%，且零售额中在线市场的占比也达到89%。 在线市场最大的优势就是可以带来巨大的流量，且契合了主要受众（青年群体）的消费习惯，但是通过在线市场了解产品信息和体验产品功能还是具有局限性。捷孚凯指出，在线市场与线下市场融合，是今后整个扫地机器人厂商在渠道布局的重点策略。 ▲扫地机器人带来全智能清扫模式 扫地机器人在不断改善清洁效果的同时，其智能化也在不断发展，包括机器人与智能手机的智能连接和自主导航定位，后者可以实现定位-构图-规划-清扫一站式的智能清扫方案。 智能连接方面，2017年，具有智能连接功能的扫地机器人零售额占比已达50.4%，而这一数字在两年前仅有11.4%；自主导航定位方面，随着越来越多高性价比导航技术的发展与应用，导航系统扫地机器人的零售额占比从2016年的3.3%显著增长到2017年的16.6%。 智能音箱：AI赋能 ▲借助语音交互技术的优势，智能音箱有望成为智能家居入口 2017年，随着苹果等科技巨头加入战局，智能音箱市场规模达到了质的飞跃。 智能音箱具备天然语音属性，指向更为自然的人机交互模式，能够整合多类第三方服务，因此也更容易成为智能家居的入口。 亚马逊市场化的成功尝试，让很多大佬从长远的角度看到了智能音箱作为智慧生活入口的可能性，也让很多内容方看到了语音交互作为服务整合平台的机会。 捷孚凯数据显示：2017年中国智能音箱市场零售量达到165万台，零售额3.1亿元；考虑到百度、阿里、小米这三位玩家在3月分别发布了自家的战略产品，预计2018年销量将持续增长达到588万台（11.8亿元）。 ▲中国智能音箱零售市场 销售渠道方面， 依托规模爆发的智能体验店，智能硬件产业突破了缺乏线下渠道体验的瓶颈，垂直链路全面打通，用户触达、认知、体验机会几何级增长，拉动用户需求，智能音箱行业迎来新机遇。 ▲智能音箱撞上新零售 捷孚凯分析指出，智能音箱从玩具变成真正的智能家居入口，到形成强大的用户群体，势必会经过3个阶段的发展：完善基本功能→集成服务丰富度→建立家居控制体系。除了3个阶段各自的比拼因素之外，在发展过程中都需要渠道，用户基数及资金三大因素加持。 ▲智能音箱发展的三大阶段 我国智能音箱目前还处在第二阶段的初期，产品的体验还有较大的提升空间，且由于我国目前智能家居的环境尚不成熟，进入到第三阶段的比拼还需要几年的时间。 目前，语音交互体验的提升是目前智能音箱领域亟需解决的问题，用户基数是实现该体验优化的核心。 民用无人机：落点户外娱乐 ▲中国消费级民用无人机零售市场始终保持高速增长 随着无人机技术逐渐成熟，制造成本和进入门槛降低（移动终端的兴起，芯片、传感器、电池等硬件产业链成熟），消费级无人机市场已经爆发。 前瞻产业研究院分析指出，2022年我国军用和民用无人机将达22.8亿美元，十年（2013-2022年）需求总额将超过134亿美元，复合增长15.57%。 根据捷孚凯零售追踪数据，2018年无人机的需求量虽持续增长，但涨幅较之前将有所下降，预测2018年全国消费级无人机零售市场同比增长21%，规模将达到34亿人民币。 ▲无人机线上售价趋势 与其他智能硬件相同的是，民用无人机的主要销售渠道为线上。通过价格亲民的入门型号，无人机厂商降低消费门槛，推动消费者普及。 从产品类别来看，民用无人机目前主要用途还是以航拍摄影为主，消费者对于产品的科技感，包括防碰撞性能、4K、VR等，以及便携性的要求逐年上升。起飞重量已经从2016年初的1282克降至2017年末的821克。 除了航拍，诸如电力巡检（预计2020年电力巡检无人机市场规模在不放量的情况下约为58.45亿元）、环境监测（2017年秋冬以来，至少有四川、陕西、安徽等6省都使用了无人机这一“武器”协助监测大气情况）、快递送货（京东、苏宁、顺丰等众多企业纷纷加入无人机送快递的行列）等应用也是民用无人机市场的增长点。 ▲科技感与便携性推动无人机产品升级迭代 智能服务机器人 智能服务机器人搭载语音交互、远程视频、本地服务、家居控制等功能，以教育陪伴、养老助老、生活娱乐和安防为卖点，目前正处在发展期，代表性产品包括软银的Pepper，优必选的Cruzr等。 ▲服务机器人市场前景 根据IFR的数据，服务机器人正处在一个快速上升的应用阶段，到2020年有望达到1322万台的销售量，整个规模可能达到200多个亿。捷孚凯预测，2018年，我国生活陪伴机器人市场规模将超5.2亿元。 ▲智能服务机器人的四大应用场景 2015年的服务机器人风口期，有一大批公司涌入这一行业，他们的产品大同小异。然而经历了两年的发展，服务机器人在落地应用上始终找不到大规模领域，交互仍需迭代改进，成本和售价过高等问题亟待解决。可以说，目前服务机器人领域的当务之急就是寻找落地应用或走定制化路线。 对此，捷孚凯从智能家居的角度指出了一种可行性：基于AI，智能服务机器人有望完成智能化家居控制（门窗管理、照明管理、门禁联动、节能管理等）、家电控制（电视、音响、电饭煲、空调、热水器等控制）和安防保护（防盗监控、防煤气泄漏、紧急报警等），从而成为智能家居的控制终端之一。 智东西认为，青年群体，特别是数字土著（15-19岁消费者）对于新型家庭电子产品热衷的态度为机器人的早期发展提供了良好的消费市场环境，随着这两年来相关技术，特别是人工智能（模式识别、语音交互等）技术的发展，消费级机器人不再局限于数字土著市场：扫地机器人真正的开始解放家务，智能音箱整合了丰富的第三方服务，无人机成为户外娱乐利器，家庭机器人也逐步落点早教和娱乐… 转载来源：消费机器人今年爆发！四大品类瓜分百亿蛋糕]]></content>
      <categories>
        <category>科技</category>
      </categories>
      <tags>
        <tag>人工智能</tag>
        <tag>机器人</tag>
        <tag>扫地机器人</tag>
        <tag>音箱</tag>
        <tag>智能家居</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[从实体去产能到金融去产能：拿走酒杯的人回来了]]></title>
    <url>%2F2018%2Fab4485e9%2F</url>
    <content type="text"><![CDATA[从实体去产能到金融去产能：拿走酒杯的人回来了文／朱振鑫如是金融研究院首席研究员、执行总裁前美联储主席马丁曾说过，金融监管者的职责就是“在盛宴渐入佳境时拿走酒杯”。 从实体去产能到金融去产能：**拿走酒杯的人回来了** （宏观研究猿讲故事第6集） 文／朱振鑫 如是金融研究院首席研究员、执行总裁 前美联储主席马丁曾说过，金融监管者的职责就是“在盛宴渐入佳境时拿走酒杯”。中国的这场金融盛宴嗨了太多年，监管者早已错过了最佳的介入时机，不过庆幸的是，在中国发生金融危机之前，敢于“拿走酒杯的人”终于回来了。这次他们要做的不是拿走酒杯那么简单，因为很多客人已经酩酊大醉，甚至不省人事，他们要做的是拖走那些喝醉的人 如果说2015-2016年是供给侧改革的第一阶段，重点是实体去产能。那么2017年后供给侧改革已进入第二阶段，重点是金融去产能。实体去产能的后果是农民工和企业家受损，产业向龙头企业集中，金融去产能的后果是金融民工和所谓的金融大鳄受损，资本、资源、资金向头部机构集中。 过去十几年，金融跑的太快，实体被彻底甩在身后。从存量看，金融资产规模年均增长15%以上，远远超过同期GDP增速，2009年四万亿时期金融资产增速曾一度达到24.1%。2016年我国金融资产规模约为553万亿元左右，占GDP比率大幅攀升到740%。从增量看，中国金融业增加值占GDP的比重从2005年开始逐年上升，截至2016年已经达到8.4%，不仅远远超出韩国（2.3%）、德国（4.1%）等发展中国家，甚至超过了美国（7.2%）、日本（4.4%）等传统的金融强国。 金融繁荣的后果是泡沫横生。借着泡沫的东风，金融产能快速膨胀。从从业人数来看，金融业队伍不断扩大，2012年-2016年仅仅五年的时间就增加了132万人，达到640万人，金融从业人数占全部就业人数的比例也由0.69%提升到0.82%。从金融机构数量来看，2011到2016年间我国金融机构从3691家增长至10171家。尤其是2014年新一轮宽松以来，金融机构平均每年增长2500家，增速接近50%，其中非银行金融机构增长尤为迅猛。从资产规模来看，我国五大类金融机构的总资产规模在从2010到2016年平均每年增长24万亿元，年均增速高达24%。 2015年是一个拐点，繁荣的金融大厦出现了第一道裂痕：股灾。金融大厦迅速搭建的时候，上层是金融资产泡沫的膨胀，底层是金融机构和人员的产能扩张。反过来，当金融大厦垮塌的时候，先是上层的金融泡沫出清，然后是底层的金融产能去化。 第一阶段是从2015年到2017年，核心是金融去杠杆，挤出金融资产泡沫。从股市来看，2015年的股灾使其成为第一个被戳破的金融泡沫，上证综指从5178点最低跌到2638点，1000多只股票跌幅超过50%，接近100只股票跌幅超过70%。从债市来看，泡沫破分两步，第一步是2014年出现个体的债券违约，这比股灾还早，但真正到大规模债灾是在2016年四季度。由于央行货币政策收紧，10年期国债收益率从2.6%一路反弹到4.0%，跌幅达140bp。与此同时，信用违约事件也愈发频繁，自2015到2018年，债券市场共发生162起违约事件，2016-2017年共有127只债券违约，当中不乏债项评级和主体评级AA+的债券，而2014年只有6只债券违约。 2017年之后进入第二阶段，主要是金融去产能，消灭冗余的金融机构和金融民工。道理很简单，有泡沫的时候蛋糕大了，金融机构都有钱赚，冒险经营的机构也不会踩雷。但泡沫破了就惨了，根本不需要那么多金融机构和金融民工了。举个最简单的例子，比如有机构打着理财的名义非法集资，承诺20%固定收益，然后拿去投股市，当股市大幅上涨的时候，不仅可以兑现高收益，还能赚一大笔，养活一批伪金融家，但当股市暴跌的时候，不仅兑现不了客户的本金和收益，连自己也要破产，机构和人员都要缩编。 实际上，这种去产能在金融的外围圈层已经比较彻底，比如P2P。根据网贷之家的统计，从2015年开始，P2P平台的问题开始集中爆发，仅2015年一年就有1294家平台跑路或出现问题，2016年更是多达1731家，截至2017年多达4000多家P2P平台都爆出问题。不仅数量多，而且很多大平台的资金规模惊人。e租宝涉案581亿，波及90万人，泛亚涉案430亿，波及22万人。这些平台跑路有主观诈骗因素，但也和金融去产能的客观环境有关，泡沫没了，他们的生存失去了根基，最后只能跑路，完成了一种变相的去产能。 下一步金融去产能的重点有两个：一是行业逐步拓展到金融机构的核心圈层，包括一些大家认为很优质的行业，比如基金、券商等。二是形式逐步从温和去产能升级到激进去产能。很多人觉得金融机构是铁饭碗，不可能去产能。但历史告诉我们，覆巢之下无完卵，任何行业在激情之后都逃不了出清的厄运，只是出清的方式不同而已。结合历史经验，金融去产能的方式不外乎四种： 第一，最温和的方式是降薪。金融业和工业最大的不同在于轻资产、重人力，在业绩下滑的时候，工业企业可以出卖固定资产，而金融业只能压缩人力成本。对于很多国有机构来说，不能直接裁员，只能通过降薪来降成本，变相去产能。根据世界银行的统计数据，2016年我国金融业私人报酬支付及公司雇员报酬总体由2015年的445亿美元下降到352亿美元。而2016年四大类金融机构从业人数却由2015年的607万人上升至640万人，表明虽然很多机构还在由于惯性扩张人员，但人均薪酬其实已经下降很多了。 以最早暴露问题的银行业为例，不管是高管还是基层，薪酬都明显下降，尤其是一些股份制银行和中小型银行，过去那种躺着赚钱的日子早就结束了。比如浦发银行人均年薪从2012年的20.6万元一路下降到2016年的12.6万元，每年下降3.1万元。光大银行前些年还不错，但2016年也开始大幅下降，人均年薪较2015年下降将近10万元。中信银行人均年薪在2015年之前稳定在20-25万之间，但2015和2016两年仅有13.5和13.9万元。受限薪影响，高管薪酬的下降更为明显。再加上2015年“限薪令”的影响，大部分银行高管年薪几乎“腰斩”。比如工行董事长总薪酬从2013年的113.9万降到54.68万，中行、建行、交行等一把手的税前总薪酬均从七八十万降到50万元以下。浦发银行董事长吉晓辉2015年的税前总薪酬更是较2013年、2014年下降50%以上。 第二，相对温和的方式是减员。比降薪更有效的方式是减员。中国虽然是一个不太能容忍裁员和失业的国家，但中国式的减员并非没有先例。最激进的就是90年代的下岗潮，在几年的时间里，国企在职人数从1997年的1亿人以上骤降到7000万以下，几千万人被“减员”。对金融业来说，由于机制相对市场化，减员更为激进。目前减员比较多的集中在证券公司，因为证券业在扩张之后最早暴露了风险，2015年的股灾让很多券商遭受重创。从2016年5月方正证券内部宣布裁员之后，有几十家券商都在内部做出裁员预警或通知。 我原来所在的券商分析师行业是一个最典型的例子。持牌分析师都需要在证券业协会备案，根据证券业协会的统计，2011到2014年间，持牌分析师从不到2000人增长到2866人，但现在这个数字已经掉到了2621人。大量卖方分析师离开这个行业，有的去上市公司做实业，有的去创业，就是因为感受到了和过去传统工业那样的产能过剩。 第三，相对激进的方式是重组。多兼并重组、少破产清算不仅适用于实业去产能，也适用于金融去产能。一种是兼并，比如现在的申万宏源，就是三家证券公司重组来的，“申”是申银证券，“万”是万国证券，“宏源”就是宏源证券。90年代末，申银证券和万国证券先合并。2014年，申银万国证券又与宏源证券合并，这是券商里最大的联姻了。还有一种选择是重组，比如当年的南方证券。2004年，因挪用80亿元客户准备金和巨额亏损，证监会、深圳市政府宣布对南方证券实施行政接管。2005年，南方证券重组成为中国建银投资证券，从此退出历史舞台。 第四，最极端的方式是破产倒闭。央行原副行长吴晓灵老师说过一句话，“消灭风险最好的办法是让风险暴露，允许金融机构破产。”很多人可能对金融机构破产没有概念，尤其是觉得银行根本不可能倒闭，但事实上，不管是证券公司、信托公司还是商业银行，都曾经发生过破产倒闭的惨剧。 倒闭的非银机构里最有名的是君安证券和广国投。1999年1月，广东国际信托投资公司由于资不抵债向申请破产，成为中国第一例非银金融机构破产案。当时主要是受亚洲金融危机的影响，导致一大波信托公司倒闭。我看了下数据，1997年的时候中国有242家信托公司，现在只有68家，其他大部分都破产或者被叫停了。 证券公司的情况也差不多，90年代中国的证券公司数量比现在多得多，但那一轮金融去产能之后很多都倒闭了。最有名的就是君安证券，君安号称“创新之王”，是证券业里的“巨无霸”。1998年，因MBO、转移巨资炒作港股等事件而被关闭，董事长张国庆入狱四年，一年后该公司被国泰证券接管，形成现在的国泰君安证券公司。其他一些倒闭的证券公司估计很多人都没听过，比如富友证券、珠海证券、汉唐证券、德恒证券、中富证券、大鹏证券、闽发证券等等。 倒闭的银行大家应该都听过，就是当年的海南发展银行。90年代海南房地产泡沫，多家信用社通过高息揽存的方式开展业务，资不抵债。1997年，28家信用社并入海发行，但海发行宣布不再付高息，于是一些投机者纷纷选择撤资退出，引发其他储户挤兑，储户连续两个月在海发行网点排队取款，兑付压力加上当时房地产泡沫破灭带来的贷款坏账压力最终把海发行压垮。1998 年6 月 21 日，海南发展银行成为国内第一家由于支付危机而倒闭的商业银行。这恐怕是中国最真实的一次金融危机，好在海南是个孤岛，没有让危机扩散，否则后果不堪设想。 图：那些年倒闭的金融机构 数据来源：如是金融研究院 繁荣的时候鸡犬升天，泥沙俱下，危难的时候大浪淘沙，优胜劣汰，天下大势不过如此。过去两年的实体去产能带来了实体产业的集中，比如钢铁行业的集中度（CR5）从22%提升到25%左右。未来两年的金融去产能也必将带来金融业集中度的提升。 第一，从股权的角度看，资本会向大机构集中。从金融机构的股权层面来看，大机构并购小机构、小机构减少会成为趋势。回顾全球金融业的历史，每一次金融去产能都会带来金融业并购的浪潮。上世纪70-80年代滞胀时期，西方国家曾出现过一波并购潮。在美国，仅1979年一年就有217家银行被大银行收购。在英国，60年代还有100家证券机构，到1980年代仅剩下17家。90年代末到21世纪初危机频发，也出现了资本集中的趋势。在美国，弗丽特银行收购波士顿银行，第一银行与第一芝加哥银行联姻，国民银行与美洲银行合并，都是强者愈强的著名案例。在日本，90年代三菱银行与东京银行合并也是经典案例。 在我国，金融行业的兼并也不是新鲜事。就像前面说的，这几年银行业已经出现小型商业银行抱团合并的现象（比如河南13家地方性银行组建成中原银行），期货业已出现兼并重组浪潮，十年内机构数量下降近30家，证券业也已涌现出申万和宏源合并的案例，未来几年这种情况会越来越多。 金融监管的强化可能会加速金融资本集中的趋势。举个最典型的例子，证监会前段时间发了一个关于证券公司股权管理的文件，要求证券公司的控股股东净资产不低于1000亿元。先不说现有的证券公司股东没几家能满足，即便是放眼全中国，能达到这个标准的企业也没几家。显然，政策层希望用严格的牌照管理和资本门槛，把一些不规范的小机构清理出局，资本将加速向大机构集中。 第二，从业务的角度看，资源会向大机构集中。从金融机构的业务层面来看，大机构会享受更多的业务资源，而小机构会受到严重的挤压。以券商为例，近三年IPO数量激增，尤其是2017年IPO数量创下新高，但小券商日子并不好过，大部分项目都被大券商拿走了。广发、中信、海通这前三家保荐了近100个IPO项目，占总量的五分之一。相比之下，很多小券商要么没有项目做，要么只做了一个项目。从IPO主承销收入来看，前十大券商的市场份额2015年是47%，2016年升至51%，到2017年已经达到58%。 为什么业务会越来越集中？一方面是因为监管强化，很多小机构过去业务不规范，靠野路子拉起来的架子肯定要散。典型的就是很多小券商用“包干制”在体外养了很多业务团队，原来监管不严的时候能贡献不少利润，现在恐怕做不到了。另一方面是因为人才流失。小机构的业绩稳定性不如大机构，薪酬水平可能会明显下降，人才失去薪酬激励的时候自然更愿意去大机构，便于积累学习和选择新的工作。 第三，从融资的角度看，资金会向大机构集中。金融机构更愿意给大企业融资，反过来，金融机构自己融资的时候大机构也有明显优势。在监管强化和金融紧缩的过程中，大型金融机构的信用优势会体现出来，客户更愿意把钱交给有坚强后盾的大平台，这使得大机构能享受到更低的负债成本，间接推高了小机构的负债成本。典型的例子就是银行，大银行的资金成本会比中小银行低，导致大银行对企业的贷款成本也会低。如此，便会形成银行找大企业，企业找大银行的循环，大银行融资越来越容易，小银行融资越来越难。 总结来说，过去几年是小金融机构的天下，谁机制活谁就赚得多，未来几年是大机构的天下，谁家底最扎实谁才能活得久。 合作、加群请添加微信：RushiFinance，务必注明机构职务姓名。 转载来源：从实体去产能到金融去产能：拿走酒杯的人回来了]]></content>
      <categories>
        <category>财经</category>
      </categories>
      <tags>
        <tag>金融</tag>
        <tag>光大银行</tag>
        <tag>P2P理财</tag>
        <tag>宏源证券</tag>
        <tag>方正证券</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[麦肯锡：软件驱动重写汽车行业竞争法则]]></title>
    <url>%2F2018%2F1df07ebe%2F</url>
    <content type="text"><![CDATA[当汽车逐渐从一个硬件驱动的机器逐渐进化成软件驱动的电子产品，汽车行业的竞争法则将被重新书写。发动机是整个 20 世纪汽车的技术核心和工程核心，而今天，强大的算力、高级的传感器们正越来越多的扮演着这样的角色：从效率、到互联、到无人驾驶、到电动化、到新型出行解决方案… 当汽车逐渐从一个硬件驱动的机器逐渐进化成软件驱动的电子产品，汽车行业的竞争法则将被重新书写。 发动机是整个 20 世纪汽车的技术核心和工程核心，而今天，强大的算力、高级的传感器们正越来越多的扮演着这样的角色：从效率、到互联、到无人驾驶、到电动化、到新型出行解决方案……它们让更多这样的时代创新成为可能。 可是，随着电子和软件的重要性变强，复杂度也升高了。举个例子就是现在汽车内部暴增的软件代码行数 SLOC（Software Lines of Code，译者注）。2010 年时有些车里的 SLOC 就有千万行，2016 年时这个数字就增长了 15 倍达到大概 1.5 亿行。这样滚雪球似的复杂度上升造成了严重的软件质量问题——有近期数百万级的汽车召回为证。 汽车向提供越来越高级的自动化在定位，汽车厂商们也把车内软件的质量和安防当作确保车辆安全的关键必要条件。行业也因此需要重新对现今的车内软件和电子电气架构进行构思。 说一个较为迫切的行业问题整个行业正在经历一个车从硬件定义到软件定义的转换，每个车平均的软件和电子电气相关内构快速增长。 软件，在今天，能占到一辆 D 级车或说一辆大型车全部内构的 10%（约 $1,200 美元），而这一占比的增速将达到每年 11% 的复合增长率：2030 年将达到 30%（约 $5,200 美元）。 于是也毫不奇怪整个数字汽车价值链上的每个参与者都想在这些软件和电子技术带来的创新中分一杯羹（图 1）。软件公司和其他数字技术公司开始从他们原有的二三级供应商位置跳脱，成为汽车厂商的一级供应商。他们开始在汽车技术的“堆栈”（stack）里扩大自己的参与感，从提供功能、app 进化到操作系统。 与此同时，传统的一级电子系统供应商则大胆踏入由科技巨头们占领的功能、app 地盘；高端车厂们则正进入这“堆栈” 下层，比如操作系统、硬件简化、信号处理等等——他们须要捍卫他们的技术独特性和优势。 图1 这样战略动作带来的后果之一是车辆架构将变为基于广义计算平台的面向服务的SOA（Service-Oriented Architecture，译者注）。开发者们可以加入新的互联解决方案、应用、人工智能元素、高级分析和操作系统。所以未来差异化将不像传统车那样在于硬件，而在于软件和高级电子赋能的 UI、体验部分。 明日之车将转向搭载全新差异化因子的平台（图 2 ）。这些因子要素可能包含信息娱乐创新、自动驾驶能力、以及那些基于“运行失败” 的智能安全功能（例如，系统可以在部分失灵的情况下仍完成关键任务）。软件将在数字技术的分层里逐渐下移，以智能传感器的形式与硬件相互集成。“堆栈”将横向集成，形成让车辆架构向 SOA 转变的新层级。 图 2 最终，新的软件和电子架构会带来多项改变游戏规则的新趋势，驱动复杂度和相互依赖程度的提升。 比如，这些新的智能传感器和应用就会给车辆带来“数据爆炸” ，厂商们若想保持竞争力，就得能更有效率的处理并分析这些数据； 模块化 SOA 和 OTA 更新将成为管理车队复杂软件和功能按需（function-on-demand）商业模式可行的关键条件； 信息娱乐以及低级别 ADAS 在越来越多第三方开发者提供内容的情况下将进一步被“app化”； 对数据安全的要求从聚焦纯访问控制策略向一个可以预测、避免、检测并防御网络攻击的集成安防概念转向； 高级别自动驾驶能力（HAD，Highly automated driving）的来临对功能的融合、高算力以及高集成度都提出了要求。 探寻未来电子电气架构的十个假设技术与商业模式的前路未定，我们还是基于广泛的研究和专家见解对未来的汽车电子电气架构做了十个假设并分析了他们对行业的意义。 一、多 ECU 将被整合 汽车行业将转向整合的 ECU 架构而非与特定功能对应的一大堆特定ECU（即现在这种“加个功能就加个盒子”的模式）。 第一步中，大多数功能将开始被集中在主要车辆域（domain）的集成域控制器上，这些控制器会部分代替当下在不同分布式 ECU 中运行的功能。 这样的发展业已开始且将在两三年内打入市场，这种整合特别适用于跟 ADAS 和 HAD 功能相关的技术层级，基本的车辆功能则可能会继续保持去中心化的状态。 在向自动驾驶的进化过程中，软件功能的虚拟化和硬件的抽象化将变得更加迫切，而这种新的手段可以通过不同几种形式实现。 情况其一是将硬件合并为针对延迟和可靠性提出不同要求的“堆栈” ，比如支持 HAD 和 ADAS 功能的高性能堆栈以及用于基本安全功能的一个独立的、时间驱动的低延迟堆栈。 另一种情况，ECU 被一个冗余的“超级计算机” 替代. 而第三种情况，控制单元概念被彻底放弃以支持一个智能节点计算网络（smart-node computing network）。 这一变化驱动因素有三：成本、市场新入者、通过 HAD 实现的需求。首先无论对于功能开发还是计算硬件，也包括通信硬件，成本减少都会加速上述整合；新入玩家来到汽车领域，通过软件导向的车辆架构对整个行业造成的破坏力也是同样效果；对 HAD 功能以及冗余性日益增长的需求也需要更高集成化 ECU。 几家高端车厂和他们的供应商已经在 ECU 整合问题上积极行动，先行一步升级电子架构，尽管明确的行业定式还未出现。 二、汽车行业将限制特定硬件所用的堆栈数量 伴随整合的将是堆栈限制的规范化，实现车辆功能和 ECU 硬件的分离，提升虚拟化。硬件和嵌入式固件（包括操作系统）将依赖关键非车辆功能条件而非被分配到车辆功能域的一部分。要实现这样的分离和 SOA 架构，以下四个堆栈可能成为未来五到十年内下一代汽车的基本： ·时间（Time-driven）堆栈。在此域中，控制器直接与传感器、执行器连接，系统须支持严格的实时要求和低延迟时间；资源调度基于时间。此堆栈包括达到最高汽车安全完整性等级的系统，比如经典汽车开放系统架构域（AUTOSAR, Automotive Open System Architecture, 译者注）。·事件-时间（Event/time-driven）堆栈。此混合型堆栈将高性能安全应用相连结，比如和支持ADAS和HAD的功能相接。应用程序和外设通过操作系统分离，应用程序按时间进行调度，应用程序内的资源调度则既可以根据时间或是优先级。操作环境确保重要安全应用在独立的容器（container）运行并与其他车内应用明确分隔，现有的例子就是自适应AUTOSAR。·事件堆栈（Event-driven）。这以堆栈以信息娱乐系统为中心，对安全性来说并不关键。应用程序与外设将明确分离，资源和调度遵循最优化或基于事件的调度策略。此堆栈中包含用户可见的、常用的功能且是用户与车辆形成交互的介质，比如像安卓（Android），Automotive Grade Linux， GENIVI 和 QNX。·云堆栈（非板载堆栈）（Cloud-based，off-board）。最后一种堆栈负责和协调从车外获取车内数据和使用车内功能。于是此堆栈负责沟通，同时还负责对应用的安全性和保护性检测（即认证），由它建立一个已定义的车的接口（interface），包括远程诊断。 供应商和技术提供商已经开始在以上堆栈中建立自己的专长，举个值得注意的例子就是信息娱乐系统（事件堆栈），公司们已经在推动人车沟通能力的拓展，如3D或增强现实式导航。另一个例子就是人工智能和传感器在高性能应用程序上的引入，关键供应商在此领域已经和主要的汽车厂家进行合作开发计算平台。 在时驱域，AUTOSAR 和 JASPAR 则在支持着时间堆栈的标准化，而扩展后的中间件层（middlewarelayer）将从硬件中将应用进行抽象。 车不断进化为移动的计算平台，中间件将让车辆的重新配置成为可能，同时允许软件的安装和升级。不像今天在每个不同 ECU 里的中间件只是负责单元间的通讯，下一代汽车中的中间件将是域控制器访问功能的链接，在 ECU 硬件之上运行的中间件层将实现抽象和虚拟化、SOA 和分布式计算。 已有证据表明，汽车厂商正向柔性架构努力，这也包括一个总体的中间件。比如 AUTOSAR 的自适应平台，它是一个动态的系统，包括中间件、对复杂操作系统的支持和最先进的多核微处理器。但是目前这类发展只限制在单个 ECU 中。 三、中期看，车载传感器个数将迅速升高 后面两到三代汽车产品上，厂商们将通过安装多个有相似功能的传感器以确保足够的安全性冗余（图 3）。然而从长远角度，汽车行业将必然开发特有传感器以减少传感器数量以及相关成本。 我们认为接下来的五到八年雷达和摄像头相结合的方案将占据主流，而当自动驾驶能力逐渐提升，激光雷达的引入对于确保物体分析（object analysis）和本地化（localization）的冗余成为必要。 以 SAE 的 L4（高级自动）自动驾驶为例，实现 L4 的初期可能需要 4 到 5 颗激光雷达，包括以城市运营和近 360 度可视为目的的固定在车后方的后置激光雷达。 图 3 长期来看，车辆传感器个数问题将会出现不同的情况：继续增加、数量稳定或数量减少。到底哪一种情况将真正到来则依赖法规要求、不同解决方案的技术成熟度以及在不同用例中使用多个传感器的能力。法规方面，如若要求加强驾驶员监控，则车内传感器必然增多。 可以预见的是汽车内饰中消费电子传感器将会开始应用。动作传感器和用于测心率和困倦成都的健康检测、面部识别、虹膜追踪，这些只是多种潜在用例中的一小部分。当然，随传感器数量上升或稳定，相应的物料成本也将上升，不只是传感器本身的成本，还有车内网络，所以减少传感器数量能带来的成本节约也一定可观。 在高级自动驾驶或完全自动驾驶时代到来，未来的高级算法和机器学习技术将增强传感器的性能和可靠性，结合更强大、性能更高的传感器技术，多余传感器的数量将有希望减少。今天在用的这些传感器可能由于其功能被高性能传感器淘汰而变得过时（例如，一个基于摄像头或激光雷达的停车辅助功能将可能取代超声传感器）。 四、传感器将更加智能 系统架构的需求，决定了智能、集成的传感器们需要为管理和处理高级自动驾驶所需的海量数据而存在。如传感器融合或 3D 定位等高级别功能将需要在中心化计算平台上运行，但数据的预处理、过滤、快速反应等则将更多在传感器周边或直接在传感器内完成。 有估计称对于一辆自动驾驶汽车每小时产生的数据量将达到 4TB，因此，智能化将从 ECU 们逐渐转移至传感器，靠传感器进行基础的、要求低延迟、只要求低算力的预处理，尤其是当权衡数据处理成本时，在传感器内处理数据对比将海量数据在车内传来传去相比更应把这些工作交给传感器。 而且 HAD 下驾驶决策冗余无论如何也需要集成的中心化算力，这更可能是基于已经过预处理的数据。智能传感器将对自身功能进行监督而传感器冗余则将提升传感器网络的可靠性、可用性和安全性。另外，为确保不同情况下传感器的正确运行，需要新一类传感器清洁方案和应用——如需要除冰能力、除尘除垢能力等。 五、全电力和数据网络冗余成为必须 高可靠性要求的关键安全应用和其他类似应用，将充分利用整个冗余圈来实现与安全操控相关的那些关乎巨大的一切内容，比如数据传输和电力供应。 电动车技术、中央计算机以及对电力要求较高的分布式计算网络都会对新的冗余电量管理网络提出要求。支持线控转向和其他 HAD 功能的故障运行系统将需要冗余的系统设计，这也是对现今的故障安全监控实施的巨大架构改进。 六、“汽车以太网”将崛起并成为车的中坚 今天的车辆网络不足以满足未来车辆的需求。 HAD 数据速率和冗余要求的提高，连接环境中的安全性和保障性以及对行业内标准化协议的需求将极可能导致汽车以太网成为关键推动因素，特别是对于冗余中央数据总线。 以太网解决方案将需要通过添加像音频 - 视频桥接（AVB）和时间敏感网络（ TSN ）等以太网扩展来确保可靠的域间通信并满足实时要求。行业参与者和 OPEN 联盟支持采用以太网技术，许多汽车制造商已经取得了这样的飞跃进展。 传统网络（如本地互联网络和控制器区域网络）将继续用于车辆内，但仅用于封闭的低级网络，例如传感器和执行器位置。 FlexRay 和 MOST等技术很可能会被汽车以太网及其扩展，AVB 和 TSN，所取代。 继续发展的话，我们预计汽车行业也同样会拥抱未来以太网技术，比如高延迟宽带产品（HDBP）和 10 千兆位技术。 七、OEM 将始终严格控制用于功能安全和 HAD 的数据连接，但会为第三方访问数据开发界面 发送和接收安全关键数据的中央连接网关将始终直接连接到 OEM 后端，除规定的要求外，第三方可以通过这些进行数据访问。但在信息娱乐方面，受车辆“ APP 化”的驱动，出现新的开放接口来允许内容和应用程序提供商部署内容，而 OEM 将尽可能保持相应的标准。 今天的车载诊断端口将被车联网解决方案取代。将不再需要对车辆网络的物理维护但可以通过 OEM 的后端进行维护。 OEM 们将在其车辆后端提供数据端口用于特定用例，如遗失车辆跟踪或个体保险。但是，售后市场设备对车辆内部数据网络的访问会越来越少。 大型车队（fleet）运营将在用户体验中发挥更强大的作用，并将为终端客户创造价值，例如通过在一套服务（例如周末或每日通勤）中为不同目的提供不同的车辆。这要求他们利用不同 OEM 的后端并开始整合其车队的数据。之后更大型的数据库将允许车队运营商在 OEM 级别无法获取的数据集成和分析上变现。 八、汽车通过云将车载信息与车外数据结合 虽然 OEM 以外的其他厂商可用的数据将取决于未来的监管和相关磋商，但对云计算中不断增加的非敏感数据（即非个人数据或安全相关数据）的处理将可以得到更多深入洞察结果。 随数据量增长，数据分析对于处理信息并将其转化为可操作的知识将变得至关重要。利用数据以实现自动驾驶和其他数字化创新的有效果取决于多个玩家之间的数据共享。目前虽然还不清楚这将如何、由谁完成，但主要传统供应商和技术供应商已经在构建能够处理这种新数据的集成汽车平台。 九、汽车将引入双向通信的可更新组件 车载测试系统将允许汽车自动检查功能和集成更新，从而实现生命周期管理以及增强或解锁售后的产品功能。所有 ECU 将向传感器和执行器发送和接收数据，检索数据集以支持创新用例，例如基于车辆参数的路线计算。 OTA 更新是 HAD 的先决条件; 同时还将因为 OTA 出现新的功能、确保网络安全、并使汽车制造商能够更快地部署功能和软件。实际上 OTA 更新功能是前面介绍的许多车辆体系结构重大变化背后的驱动。 此外，OTA 还需要在从车辆外部堆栈每层到车辆中 ECU 们的端到端的安防解决方案。这种安防解决方案仍有待设计，而由谁做、怎么做都将会是非常有趣的观察。 要实现像智能手机那样的可升级性，业界需要克服限制性经销商合同、监管要求以及安全和隐私问题。这里各种汽车厂商也公布了部署 OTA 服务的计划，其中包括对它们的车辆的无线更新。 OEM 将在 OTA 平台上对其车队进行标准化，并与该领域的技术提供商密切合作。 由于车的互联性和 OTA 平台变得越来越重要，我们可以认为 OEM 将在这个细分市场中占据更多的所有权。 车辆将获得软件和功能升级，同时也会收到针对设计使用寿命的安防更新。监管机构可能会强制要求软件维护以确保车辆设计的安全完整性。这种更新和维护软件的任务将引出关于车辆维护和运营的新商业模式。 十、评估汽车软件和电子体系结构的未来影响 影响当今汽车行业的趋势们为硬件相关的内容带来很多大的不确定性，对软件和电子体系结构来说，看来未来的破坏性可能也不会少多少。 许多战略举措都有可能：汽车厂商可以选择建起行业联盟来规范和标准化车辆架构、数字行业巨头可以引入车载云平台、出行服务商可以自己自己造车或开发开源车辆堆栈和软件功能，汽车厂商则可以引入日渐复杂的互联、自动驾驶汽车。 对于传统的汽车公司来说，从以硬件为中心的产品向以软件为导向的服务驱动型行业的转变尤其有挑战性。然而考虑到本文所述的趋势和变化，汽车行业内的任何人都没有其他选择，只能做好准备。我们能看到几大战略推力： ·解耦车辆和车辆功能的开发周期。 OEM 和一级供应商需要从技术和组织两个角度确定如何开发、提供和部署功能，而且是大部分在车辆开发周期之外。鉴于目前的汽车开发周期，企业需要找到一种管理软件创新的方法。此外，也应该思考如何为现有车队创建改造和升级解决方案（例如计算单元）。·定义软件和电子产品开发工作的目标增值（ value added ）。 OEM 必须确定他们能够建立控制点的差异化特征。另外，明确定义自己软件和电子产品开发的目标附加价值非常重要，同样的还有，找到可以形成商品或话题的区域且仅一家供应商或合作伙伴能够实现。·给软件贴上一个明确的价签。将软件与硬件分离意味着需要 OEM 重新考虑其单独购买软件的内部流程和机制。除了传统已有的设定之外，分析采购过程中如何将敏捷的软件开发方法固定下来也很重要。这里指的供应商（一级，二级和三级）也发挥着至关重要的作用，因为他们需要为其软件和系统产品提供明确的商业价值，以使其获得更大的收入份额。·围绕新的电子架构设计一个特定的组织（包括相关的后端）。除了改变内部流程以交付及销售先进的电子和软件之外，行业玩家们（ OEM 和供应商）还应该考虑针对车辆电子相关的主题设置一个新的不同的组织。主要是，新的“分层”架构要求有可能打破目前的“垂直”流程并引入新的“横向”组织单元。再说一句就是，他们也需要为自己的软件和电子开发团队提升专门的能力和技能。·围绕作为产品的汽车特征设计商业模式（特别是对汽车供应商来说）。为了保持竞争力并在汽车电子领域分一杯羹，分析哪些功能才是为未来架构增添实际价值并可以变现是致关重要的。随后，玩家需要为软件和电子系统的销售推出新的商业模式，无论当时是作为产品、服务或者全新的东西。 随着汽车软件和汽车电子新纪元的开始，它正在彻底改变各种业务模式，客户需求以及竞争性质的行业既有的确定性。我们对将建起来的收入和利润池感到乐观。但要从转变中受益，业内所有参与者都需要重新思考并在新环境中仔细定位（或重新定位）其价值主张。 转载来源：麦肯锡：软件驱动重写汽车行业竞争法则]]></content>
      <categories>
        <category>汽车</category>
      </categories>
      <tags>
        <tag>软件</tag>
        <tag>人工智能</tag>
        <tag>汽车产业</tag>
        <tag>麦肯锡公司</tag>
        <tag>电子技术</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[今日头条一季度教育榜：K12教育新规关注飙升，留学政策大变革]]></title>
    <url>%2F2018%2F93d32ac1%2F</url>
    <content type="text"><![CDATA[发布教育行业季度榜单，透过数据解读教育资讯热点，洞察教育行业目标用户兴趣偏好，用数据带你了解教育的那些事儿。 对于每个有孩子的家庭来说，教育绝对是头等大事。从孩子出生到长大成人，正确的价值观及良好的技能学习都依赖于良好的家庭教育观念，家长是在孩子成长过程中非常重要的角色。 头条指数从早教、K12教育、留学三大领域资讯入手，发布教育行业季度榜单，透过数据解读教育资讯热点，洞察教育行业目标用户兴趣偏好，用数据带你了解教育的那些事儿！ 早教领域关注飙升榜：“压岁钱”成早教热议话题，“双语”教育备受关注。一季度中，“压岁钱”涨幅登顶榜首，相信不少家长在春节也遇到如何让孩子正确面对压岁钱的问题。红包虽小，但背后折射的是一个家庭的教育观念。有的家长直接收走孩子的红包，有的会追问孩子红包金额，聪明的父母会借红包让孩子了解传统文化，也引导孩子正确地对待红包与金钱的关系。 值得一提的是，“双语”教育资讯本季度飙涨877%。3-6岁不仅是孩子性格养成的关键时期，还是学习外语的最佳时期，越来越多的家长意识到从小培养孩子“双语”能力对往后语言学习的重要性，从家庭英语交流到选择双语早教机构，家长们在“双语”教育这件事上可谓是费尽了心思。 早教领域热搜榜：简笔画荣登热搜榜首，德智体美全面发展在早教热搜版块，很多家长不仅搜索简笔画，还经常转发简笔画教程。对于平时工作忙碌的家长来说，简笔画可现学现用确实方便，看完教程可以马上和孩子一起协作画画，这既培养了孩子的想象力及速记能力，也有利于孩子未来的兴趣发展。睡前时间是与孩子交流的宝贵时机，家长们在头条搜索睡前故事，借此与孩子沟通，让孩子在有益的故事中学到知识，促进智力发育，也增进了孩子与家长之间的感情。此外，“手工”、“跆拳道”等培养孩子运动能力的教育方式也受到家长们的关注，德智体美全面发展要从娃娃抓起的教育理念已经深入到家长心中。 K12教育各线城市关注榜：三四线市场潜力大，教培巨头积极布局本季度三四五线城市用户在K12教育领域资讯关注度有所上升，三四线教培市场近年来需求不断增加，家长也有较强的付费意愿，但目前三四线教育仍存在着优质教育资源稀缺，以中小机构为主的教培市场质量参差不齐等痛点。正是如此，得益于互联网及视频直播技术的发展，新东方、好未来两大K12教培巨头借助“双师课堂”模式，改变了以往渠道下沉成本过高、师资人员紧缺等难题，迅速布局三四线教培市场。 K12教育领域热门学科榜：数学登顶关注榜首，三大主科仍最受关注本季度“数学”类教育资讯关注度最高，关于“数学”的资讯更多地集中于数学课的学习方法及技巧，“语文”类资讯则更加集中于作文写作类的案例分享，而语法笔记在“英语”类资讯中也受到用户关注。总体来说，语数英三大主科关注度依旧高于其他科目，从内容看，学习笔记及技巧型的资讯更受用户欢迎。 K12教育领域关注飙升榜：整顿培训机构“超纲教学”，一系列教育新规引关注总体上看，一季度关注度涨幅较高的“超纲”、“托管”、“教师队伍”等关键词源于一系列教育新规的发布。2月22日，教育部办公厅等四部门发布的纠正校外培训机构开展“超纲教学”培训不良行为的通知。2月24日，黑龙江省教育厅首次下发关于推后中小学生早晨到校时间的通知。3月16日，教育部长陈宝生提出今年17个省份将启动新高考改革规划等教育新规的发布迅速引起众多家长关注。此外，本季度用户也十分关注“3点半放学”托管服务问题、促进“教师队伍素质建设”、招聘“特岗教师”等教育资讯。 留学资讯各省用户关注榜：上海北京关注度远超其他省市据教育部发布的2017年中国学生出国留学相关数据显示，2017年我国各类出国留学人员总数首次突破60万，达到达60.84万人，同比增长11.74%，持续保持世界最大留学生生源国地位。从头条指数TGI兴趣偏好数据来看，国内一流大学云集的上海、北京指数均超200，两地用户对留学资讯消费需求大。此外，湖北、海南、广东等省市用户对留学资讯的偏好度也位居全国前列。 热门留学国家关注榜：美澳加三国留学最受关注美国、澳大利亚、加拿大等国拥有多所世界一流大学，发达的教育资源吸引国内学生不断前往留学深造。2017年底，美国大使馆发布的《2017年门户开放报告》数据显示，超过24%的国际留学生把美国选为留学目的国，而中国留学生占在美国际学生总数的32.5％，连续第八年位居榜首。而在澳大利亚，中国留学生占比依旧最高，约占31%，国人依旧热衷着出国留学这件事儿。 留学教育领域关注飙升榜：川普或限发留美签证，加拿大留学政策大变革美国总统川普作为话题人物，在留学资讯领域的影响力也毫不逊色。据美国《华尔街日报》3月17日报道，美国政府正考虑限制给中国留学生发放签证、H-1B工作签证、以及停发10年赴美旅游签证，这则消息迅速引起留学圈的关注。 “GIC”本季度关注度飙升1636%，该计划是加拿大留学政策中的银行投资证明。3月13日，加拿大大使馆微博发布了即将新推出的4大类加拿大留学签证申请类别介绍，新政中强制要求几乎所有的学习许可申请计划都需购买1万加元的担保投资证明。此外，“SDS”作为加拿大签证中的另一重要计划也在本次新政中有所调整，新SDS要求雅思总分与各项分数不低于6分。 本季度教育类热门问答中，家庭教育方式及孩子性格培养类咨询被重点关注，而文理科选择、大学排名等教育类疑问也引发用户热议。 头条指数借助今日头条平台每天百亿级的阅读数据，捕捉行业热点，洞察阅读行为背后的兴趣表达。每个季度，头条指数将发布行业热度榜单，帮你看清行业动态，用户关心的，才是你真正需要的，欢迎关注&#64;头条指数！ 转载来源：今日头条一季度教育榜：K12教育新规关注飙升，留学政策大变革]]></content>
      <categories>
        <category>教育</category>
      </categories>
      <tags>
        <tag>在线教育</tag>
        <tag>加拿大</tag>
        <tag>移动互联网</tag>
        <tag>今日头条</tag>
        <tag>家庭教育</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[被催熟的平安好医生！]]></title>
    <url>%2F2018%2F87907134%2F</url>
    <content type="text"><![CDATA[平安好医生凭借资本力量获取大量客户，主要营收仍来自平安，目前还处于烧钱阶段，此时上市似乎有被催熟的意味。 培育“独角兽”企业，一向都是风投和互联网公司干的活，不过，平安集团仿佛是个例外，一口气培育了四个独角兽企业——平安好医生、陆金所、平安一账通和平安医保科技。4月22日，平安好医生就召开了全球发售会，拟于5月4日登陆香港联交所。平安好医生凭借资本力量获取大量客户，主要营收仍来自平安，目前还处于烧钱阶段，此时上市似乎有被催熟的意味。 4月22日，平安好医生在香港召开全球发售会，公司预期将于5月4日登陆香港联交所。此次共发售1.6亿股，每股发售股份50.80-54.80港元。据知情人士透露，定价在每股54.80港元的最高区间，照此计算，平安好医生拟募资规模87.7亿港元（约合人民币70亿元）。而平安好医生估值也达到75亿美元，折合约588亿港元。 平安好医生自2015年4月正式发布产品至今，仅仅发展了3年时间，估值达到了75亿美元，发展速度着实令人惊叹。不过，探究平安好医生财报，似乎有种被催熟的感觉，营收好看，但是很大一部分来自母公司平安集团的输血，此外仍处于巨额亏损之中。尽管客户数字好看，变现能力堪忧。 根据招股书显示，平安好医生2015年-2017年收入分别为2.79亿元、6.02亿元、18.68亿元，但持续亏损，近三年亏损额分别为3.24亿元、7.58亿元、10.02亿元。近三年经营性现金流也均为负数，分别为-0.45亿元、-2.63亿元、-4.84亿元。 烧钱买来的用户？尽管连续3年亏损，平安好医生仍然受到了投资者的青睐。招股说明书显示，此次平安好医生与多家国际基石投资者签订协议，前十大基石投资者将出资5.5亿美元，认购份额约为53%，占已发行股本8%左右。 或许，这些基石投资者看中的，便是平安好医生迅速增长的用户量以及营收数据。作为一个互联网独角兽，用户数据尤其值得关注。 如上图所示，2015年-2017年，平安好医生月活跃客户分别为560万、2180万和3290万人，而付费客户仅有10万人、40万人和90万人。这样的数据，既可以解读为付费空间巨大，同样可以解读为变现能力差，空有用户量。 有趣的是，打开平安好医生APP，首先看到关键词搜索推荐的是“步步夺金”，首页显眼位置推荐的，也是“步步夺金”。而所谓的“步步夺金”，就是平安好医生2015年12月推出的引流工具，用户通过使用APP，比如第一次参加，走满5步可送1金，作为起步基金。以后每天走路1000步，奖励0.3金，之后每走2000步，奖励0.1金，而这些“现金券”可以在用户商城兑换礼品，比如洗发液、剃须刀、厨具以及牙膏之类的生活用品。 从招股股数据中可以看出，“步步夺金”项目推出后，平安好医生月活量显著提高，直接从2015年560万月活，提升至2016年2180万月活。 根据招股书显示，2015年-2017年，平安好医生推广费用分别为0.07亿元、3.84亿元和2.13亿元。由此可见，2015年12月“步步夺金”项目推出后，平安好医生推广费用剧增。 互联网企业通过补贴拉客户似乎无可厚非，但医疗需求在生活中远没有“衣食住行”那样高频，给用户送钱的方式固然可以粉饰用户数据，如何把这些客户转化为真正关注健康，关注医疗，而非挣钱而来，似乎仍需要平安好医生继续努力。 背靠平安好乘凉事实上，背靠平安这颗大树，平安好医生不仅可以任性地烧钱，也可以通过丰富的集团资源，撑起公司营收。2015年-2017年，平安好医生提供给平安集团业务，占营收比例分别为80.9%、41.4%和46.4%，由此可见，平安好医生的发展，离不了平安集团的扶持。 此外，平安集团对平安好医生的扶持，不仅仅是购买其服务，也包括资本支持和强营销。招股书显示，平安好医生主营业务分为家庭医生服务、消费型医疗、健康商城和健康管理和互动，2017年占营收比例分别为13.0%、35.0%、48.0%和4.0%。 其中，家庭医生服务为平安好医生主打的在线诊疗；而消费型医疗主要是出售体检卡，事实上，很多平安保险经纪人，在推销保险的同时，顺带推销体检卡，从而拉动平安好医生营收。 根据招股书显示，消费型医疗43.8%提供给公司客户，平安集团就占了41.8%，其他56.2%提供给个人客户，而54.5%又通过平安集团的销售代理完成的。平安集团购买的服务加上平安集团代理的，共计96.3%，通过平安好医生自有销售团队和健康商城完成销售的，只占2.9%。可以说，平安好医生的消费型医疗业务，全部靠着平安集团撑着。 健康商城则是一个电商平台，在网上销售养生、居家百货、宝宝用品、健康食品等，有趣的是，平安好医生健康商城还出售电视、手机等家电数码产品。 从招股书可以看出，健康商城为平安好医生发展最为迅速的业务，2015年-2017年，营收分别为190万元、6.31亿元和8.96亿元。扣除提供给平安集团的营收，提供给其他客户的营收分别为190万元、5.66亿元和4.67亿元。 不过，提供给其他客户真实的销售数据，似乎还要扣掉用平安好医生对用户的补贴。上图可知，平安好医生健康商城的付款方式中，有“使用我们奖励计划的优惠券支付”，这便是上文提到的“步步夺金”项目。2016年、2017年“步步夺金”推广费用分别为3.85亿元和2.14亿元，扣除后，提供给其他客户的营收数据分别为1.81亿元和2.53亿元。 2017年，平安好医生营收18.68亿元，其中8.65亿元提供给平安集团。此外，仍有7.72亿元消费型医疗业务来自平安集团代理销售，2.14亿元推广费用也很大程度计入了健康商城收入。 而平安好医生一直宣传和主打的家庭医生业务，则从2015年占比42.6%降至2017年13.0%。为了平安好医生的业绩，平安集团似乎注入了太多的资源，导致平安好医生早熟的假象。 被催熟的平安好医生平安好医生虽然是平安系的，高管却主要来自阿里巴巴。或许，平安好医生的成立，也是源自平安集团董事长马明哲的一次挖角行为。2013年，马明哲从阿里巴巴挖来了王涛，其原为阿里巴巴资深副总裁兼阿里软件总裁，来到平安后，就任平安健康险董事长兼CEO。 健康险在保险品种里并非大类，王涛就任健康险总裁后，便很快干起了老本行软件。2014年1月，他开始在上海组建团队，2015年4月正式发布平安好医生APP。平安好医生的高管团队中，首席产品官吴宗逊、首席技术官王齐和首席运营官白雪均来自阿里巴巴。可以说，平安好医生是拿着平安的渠道和资金，依靠挖角阿里的人才做起来的。 一直以来，看病难和看病贵，是国内患者诟病两大问题。为了解决这些问题，2011年以来兴起了很多互联网医疗公司，比如好大夫在线、春雨医生等。不过，这些互联网公司的都是轻资产运营的，比如在线问诊，它们的作用，就是把医生与患者在网上有效链接起来。 事实上，国内互联网医疗行业发展并不乐观，医疗服务通常需要高素质人才和医疗检查，这些是互联网医疗欠缺的。目前，大部分互联网医疗企业都集中在挂号、在线信息查询等。而平安好医生之所以崛起，主要从医院挖角医师，依靠全职医师给患者提供诊疗服务。 据招股书显示，截至2015年、2016年及2017年，平安好医生的医疗团队人数分别为585名、797名及888名。需要注意的是，截至2017年底，888名的医疗人员中仅有172人为医生（约占总数的19%），其余的是医务助理。 平安好医生的营收主要由消费型医疗和健康商城提供，这些收入绝大部分来自平安集团的购买服务以及资源支持，而公司主打的家庭医生业务，也是靠“重资产”取得竞争优势的，而这些同样离不开平安集团的资金支持。 短短三年，平安集团用资金和平台资源砸起来平安好医生。这样的平安好医生，似乎是被催熟的。不像其他互联网公司，是在市场的腥风血雨中杀出来的，没有得到市场的检验，若离开平安集团的扶持，平安好医生能否正常运营仍值得商榷。 转载来源：被催熟的平安好医生！]]></content>
      <categories>
        <category>财经</category>
      </categories>
      <tags>
        <tag>IPO</tag>
        <tag>平安保险</tag>
        <tag>电子商务</tag>
        <tag>风投</tag>
        <tag>马明哲</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[聚类分析之k-means算法(SSE、轮廓分析）]]></title>
    <url>%2F2018%2F59ab7715%2F</url>
    <content type="text"><![CDATA[在前面我们介绍过了很多的监督学习算法，分类和回归。这篇文章主要介绍无监督算法，通过聚类分析来处理无类标数据。 在前面我们介绍过了很多的监督学习算法，分类和回归。这篇文章主要介绍无监督算法，通过聚类分析来处理无类标数据。我们事先并不知道数据的正确结果(类标)，通过聚类算法来发现和挖掘数据本身的结构信息，对数据进行分簇(分类)。聚类算法的目标是，簇内相似度高，簇间相似度低。有点像LDA降维算法，类内方差最小，类间方差最大。这篇文章主要包括： 1、K-Means算法 2、K-Means++ 3、硬聚类和软聚类 4、聚类算法的性能评价指标 一、K-Means算法在聚类算法中K-Means算法是一种最流行的、使用最广泛的一种聚类算法，因为它的易于实现且计算效率也高。聚类算法的应用领域也是非常广泛的，包括不同类型的文档分类、音乐、电影、基于用户购买行为的分类、基于用户兴趣爱好来构建推荐系统等。 K-Means算法的实现步骤，主要分为四个步骤： 1、从样本集合中随机抽取k个样本点作为初始簇的中心。 2、将每个样本点划分到距离它最近的中心点所代表的簇中。 3、用各个簇中所有样本点的中心点代表簇的中心点。 4、重复2和3，直到簇的中心点不变或达到设定的迭代次数或达到设定的容错范围。 常用的距离度量标准是欧几里得距离的平方： 其中x和y表示不同的两个样本，n表示样本的维度(特征的数量)。基于欧几里得距离，K-Means算法需要优化的问题就是，使得簇内误差平方和(within-cluster sum of squared errors,SSE)最小，也叫簇惯性(cluster intertia)。 下面利用sklearn来实现一个k-means算法的应用，使用sklearn的数据集，数据集中包含150个随机生成的点，样本点分为三个不同的簇 150个样本点的分布如上图所示。下面使用sklearn内置的KMeans算法来实现对上面样本点的聚类分析 二、K-Means++K-Means算法需要随机选择初始化的中心点，如果中心点选择不合适，可能会导致簇的效果不好或产生收敛速度慢等问题。解决这个问题一个比较合适的方法就是，在数据集上多次运行K-Means算法，根据簇内误差平方和(SSE)来选择性能最好的模型。除此之外，还可以通过K-Means++算法，让初始的中心点彼此的距离尽可能的远，相比K-Means算法，它能够产生更好的模型。 K-Means++有下面几个步骤组成： 1、初始化一个空的集合M，用于存储选定的k个中心点 2、从输入的样本中随机选择第一个中心点μ，并将其加入到集合M中 3、对于集合M之外的任意样本点x，通过计算找到与其距离最小的样本d(x,M) 4、使用加权概率分布来随机来随机选择下一个中心点μ 5、重复步骤2和3，直到选定k个中心点 6、基于选定的中心点执行k-means算法 使用sklearn来实现K-Means++，只需要将init参数设置为”k-means++”，默认设置是”k-means++”。下面利用k-means++算法来实现上面三个簇的聚类 通过上面图可以发现k-means++的聚类效果还不错，簇的中心点，基本位于球心。在实际情况中使用k-means++算法可能会遇到，由于样本的维度太高无法可视化，从而无法设定样本的簇数。k-means算法的簇不可重叠，也不可分层，并且假定每个簇至少会出现一个样本。 注意：由于k-means算法是基于欧式距离来计算的，所以k-means算法对于数据的范围比较敏感，所以在使用k-means算法之前，需要先对数据进行标准化，保证k-means算法不受特征量纲的影响。 三、硬聚类和软聚类硬聚类(hard clustering)是指数据集中的样本只能划分到一个簇中，如k-means算法。软聚类(soft clustering)或模糊聚类(fuzzy clustering)可以将一个样本划分到多个不同的簇中，如C-means(FCM)算法。 FCM的计算步骤与k-means相似，只是FCM是使用样本属于不同簇的概率来代替k-means中的类标。样本属于不同簇的概率之和为1。 FCM的计算步骤如下： 1、指定k个中心点，并随机将每个样本点划分到某个簇中 2、计算各簇的中心μ 3、更新每个样本点所属簇的概率(隶属度) 4、重复步骤2和3直至，样本点所属簇的概率不变或是达到容错范围或最大迭代次数 隶属度的计算公式如下： 其中，ω表示的就是样本所属簇的概率，上式表示的簇的个数为3。样本属于簇j的概率。m大于1，一般取2，被称为模糊系数。 FCM算法的单次迭代计算成本要高于k-means算法，但FCM的收敛速度比较快。 四、聚类算法的性能指标1、簇内误方差(SSE) 在对簇的划分中，我们就使用了SSE作为目标函数来划分簇。当KMeans算法训练完成后，我们可以通过使用inertia属性来获取簇内的误方差，不需要再次进行计算。 可以使用图形工具肘方法，根据簇的数量来可视化簇内误方差。通过图形可以直观的观察到k对于簇内误方差的影响。 通过上图可以发现，当簇数量为3的时候出现了肘型，这说明k取3是一个不错的选择。 2、轮廓图定量分析聚类质量轮廓分析(silhouette analysis)，使用图形工具来度量簇中样本的聚集程度，除k-means之外也适用于其他的聚类算法。通过三个步骤可以计算出当个样本的轮廓系数(silhouette coefficient)： 1、将样本x与簇内的其他点之间的平均距离作为簇内的内聚度a 2、将样本x与最近簇中所有点之间的平均距离看作是与最近簇的分离度b 3、将簇的分离度与簇内聚度之差除以二者中比较大的数得到轮廓系数，计算公式如下 轮廓系数的取值在-1到1之间。当簇内聚度与分度离相等时，轮廓系数为0。当b&gt;&gt;a时，轮廓系数近似取到1，此时模型的性能最佳。 通过轮廓图，我们能够看出样本的簇数以及判断样本中是否包含异常值。为了评价聚类模型的性能，可以通过评价轮廓系数，也就是图中的红色虚线进行评价。 转载来源：聚类分析之k-means算法(SSE、轮廓分析）]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>音乐</tag>
        <tag>技术</tag>
        <tag>可视化</tag>
        <tag>欧几里得</tag>
        <tag>推荐技术</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[亚马逊成功的秘密：贝佐斯的决策方法论与“两个披萨原则”]]></title>
    <url>%2F2018%2Facb36bf9%2F</url>
    <content type="text"><![CDATA[同时，我们也注意到，FarnamStreet发表的一篇文章，介绍了亚马逊创始人&amp;首席执行官杰夫·贝佐斯的决策方法论。 编者按：《卫报》近日发表了一系列文章，对亚马逊成功背后的两个披萨原则进行了剖析。同时，我们也注意到，Farnam Street发表的一篇文章，介绍了亚马逊创始人&amp;首席执行官杰夫·贝佐斯的决策方法论。本文的内容主要来自于这两篇文章，由36氪编译，希望能够为你带来启发。 一、贝佐斯的决策方法论我们经常认为，收集尽可能多的信息将有助于我们作出最好的决定。有时这是对的，但有时这也会阻碍我们的进步。甚至在一些时候，这可能是危险的。 许多最成功的人采用简单、多样化的决策方法论，以消除在特定情况下进行审议的必要性。 一种可能是默认说不，就像史蒂夫·乔布斯那样。或者像沃伦·巴菲特那样，拒绝任何需要计算器或计算机的决定。或者是像埃隆·马斯克那样，遵循第一性原理。亚马逊的创始人杰夫·贝佐斯的方法与上面提到的方法都不尽相同。他会问自己，这是一个可逆的还是不可逆的决定？ 如果一个决定是可逆的，我们可以在没有得到充分信息的情况下，快速下决定。如果一项决定是不可逆的，我们最好放慢决策过程，确保我们考虑到充分的信息，并尽可能透彻地理解所面对的问题。 贝佐斯用这种方法论作出了建立亚马逊的决定。他意识到，如果亚马逊失败了，他可以回到他以前的工作中。他仍然会学到很多东西，并且不会后悔尝试。这个决定是可逆的，所以他冒险了。这对他很有帮助，在他以后作出决定的时候，依旧发挥着作用。 在不确定中做决策假设你在网上看到了一个评论后，决定去尝试一家新餐馆。因为你从来没去过那里，你不知道食物会不会好吃，或者气氛会不会很沉闷。但是，你会利用评论中不完整的信息来做出决定，因为你知道，如果你不喜欢这家餐馆，并不是什么大不了的事情。 在其他情况下，不确定性也有一点风险。你可能会决定接受一份特定的工作，但你不知道公司文化是什么样的，也不知道“蜜月期”结束后，你对工作的感觉如何。 你可以很快地做出可逆的决策，而不需要纠结于找到完整的信息。如果这个决策失败了，我们可以用很少的成本从经验中吸取智慧。通常，不值得花费时间和精力去收集更多的信息，去寻找完美无缺的答案。虽然你的研究可能会使你的决策更好5 %，但你可能会错过一个机会。 但是，要注意可逆的决策不是鲁莽行事或不去了解情况的借口，而是一种信念，即我们应该使我们的决策框架适应我们正在作出的决策类型。可逆的决策不需要像不可逆的决策那样做出。 快速做出决策的能力是一项竞争优势。创业公司的一个主要优势是，它们可以随着“velocity”而移动，而老牌的企业通常会随着“speed”而移动。这两者之间的区别是有意义的，往往意味着成功和失败。 “Speed”是以时间上的距离来测量的。如果我们从纽约乘飞机去洛杉矶，从肯尼迪机场起飞，在纽约转圈三个小时，我们的“Speed”很快，但我们什么都没有。“Speed”并不在乎你是否朝着目标前进。另一方面，“Velocity”衡量的是随着时间的推移而产生的位移。要获得“Velocity”，你需要朝着你的目标前进。 这种决策方法论解释了为什么创业公司做出快速决策的时候要比老牌的企业更有优势。这一优势因环境因素(如变化速度)而扩大。环境变化的速度越快，做出快速决策的人就会获得越多的优势，因为他们可以学得更快。 决策为我们提供数据，这样我们就可以更好地做出关于未来的决策。我们在OODA循环中循环得越快越好。这个框架并不是一次性地适用于某些情况；它是一种方法论，需要成为决策工具包的一个组成部分。 编者注：OODA循环理论的基本观点是：武装冲突可以看做是敌对双方互相较量谁能更快更好地完成“观察—调整—决策—行动”的循环程序。双方都从观察开始，观察自己、观察环境和敌人。基于观察，获取相关的外部信息，根据感知到的外部威胁，及时调整系统，做出应对决策，并采取相应行动。 通过实践，我们也能更好地识别错误的决策并进行调整，而不是因为沉没成本谬误，去坚持过去的选择。同样重要的是，我们可以停止把错误或小的失败看作是灾难性的，而把它们看作是将为未来决策提供参考的纯粹信息。 “现在立即执行一个好的计划，比下周执行一个完美的计划要好。”——乔治 · 巴顿将军 贝佐斯把决策比作门。可逆的决策是双向开放的门。不可逆的决策是只允许一个方向通行的门；如果你走过去，你就被困在那里了。大多数决策都是前一种，可以逆转(即使我们永远无法收回投入的时间和资源)。通过一个可逆的门能给我们这样的信息：我们知道另一边是什么。 贝佐斯在前几年的一封股东信中写道： 有些决策是不可逆的或几乎不可逆的单向门，这些决策必须经过深思熟虑和协商，有条不紊地、谨慎地、缓慢地做出。如果你走过去，不喜欢你在另一边看到的东西，你不能回到你以前的地方。我们可以称之为第1类决策。但大多数决策都不是这样——它们是可变的、可逆的——它们是双向的。如果你做了一个次优的第2类决策，你不必忍受这么长时间的后果。你可以重新打开门，然后回去。第2类决策可以、也应该由高判断力的个人或小团队迅速做出。随着组织变得越来越大，在大多数决策中，包括许多第2类决策，似乎倾向于使用重量级的第1类决策过程。这样做的最终结果是决策缓慢，不考虑风险规避，未能进行充分的实验，从而削弱了创新。我们得想办法克服这种倾向。 贝佐斯举了一个例子，向愿意支付额外费用的人提供一小时送达的送货服务。这项服务在这一想法提出后不到四个月就开始了。在111天的时间里，该团队“构建了面向客户的应用程序，确定城市仓库的位置，确定了要销售的25000个项目，为这些项目储备、招聘和配备了新员工，测试、迭代、设计了新的内部使用软件(仓库管理系统和面向驱动程序)，并在假日购物季及时推出。” 作为进一步的指导，贝佐斯认为70 %的确定性是作出决策的适当切入点。这意味着，一旦我们获得了所需的70%信息，就采取行动，而不是等待更长时间。以70%的确定性作出决策，然后进行路线修正，比等待90%的确定性要有效得多。 在《眨眼：思考而不思考的力量》（ Blink&#58; The Power of Thinking Without Thinking）中，马尔科姆·格拉德威尔（Malcolm Gladwell）解释了为什么不确定性下的决策会如此有效。我们通常认为更多的信息会导致更好的决策——如果医生建议进行额外的测试，我们倾向于相信它们会带来更好的结果。格拉德威尔不同意这种说法&#58;“事实上，你需要知道的很少，才能找到一个复杂现象的根本特征。你所需要的只是心电图、血压、肺液和不稳定心绞痛的证据。这是一个激进的说法。” 在医学领域，与许多领域一样，更多的信息不一定能确保改善结果。为了说明这一点，格拉德威尔举了一个例子。一个人来到医院时，胸部会时不时地疼痛，他的生命体征没有显示任何危险因素，但他的生活方式确实如此，两年前他接受了心脏手术。如果医生查看了所有可用的信息，觉得他似乎需要住院。但是除了生命体征之外的其他因素在短期内并不重要。从长远来看，他患心脏病的风险很大。格拉德威尔写道， 其他因素在决定男人现在的状况方面的作用非常小，没有它们就可以做出准确的诊断。事实上，这些额外的信息毫无用处。这是有害的。它混淆了问题。当医生们试图预测心脏病发作时，他们会把太多的信息考虑在内。 我们都可以从贝佐斯的方法中学到东西，这种方法帮助他建立了一个巨大的公司，同时保持了创业的节奏。贝佐斯用他的方法论来对抗许多大型组织内部的停滞。重要的是效率，而不是遵循缓慢决定的规范。 一旦你明白可逆的决定实际上是可逆的，你就可以开始把它们看作是提高学习速度的机会。在公司层面，允许员工做出可逆的决策并从中学习，有助于你以创业的步伐前进。毕竟，如果有人在以“speed”移动，当你以“velocity”移动时，你会超过他们。 这就是贝佐斯的决策方法论，也直接或间接地推动者亚马逊两个披萨原则的形成，因为小团队做决策更多情况下都是可逆的。 两个披萨原则在亚马逊早期，杰夫·贝佐斯制定了一个规则：每个内部团队都应该足够小，两个比萨饼就能解决伙食问题。这并不是要削减餐饮开支，就像亚马逊做的几乎所有事情一样，它专注于两个目标：效率和可扩展性。前者是显而易见的。一个较小的团队，花在管理和让员工了解最新情况的时间会更少，而花在需要做的事情上的时间就更多了。但对亚马逊来说，真正重要的是后者。 拥有许多小团队的好处是，他们能够一起工作，并且能够获得公司的公共资源，以实现他们更大的目标。 用风险投资公司Andreessen Horowitz合伙人本尼迪克特·埃文斯( Benedict Evans )的话来说，这将公司变成了“制造机器的机器”。 “你可以在不添加新的内部结构或直接报告的情况下添加新的产品线，你可以在不用开会、经历一系列项目和流程，就能在物流和电子商务平台上添加它们。”埃文斯指出，“你不需要（从理论上说！）飞往西雅图，安排一场会议，让人们支持你在意大利开展的项目，或者说服任何人将新业务加入他们的路线图。” 亚马逊擅长于成为一家销售商品的电子商务公司，但它最擅长的是，打造新的、销售新产品的电子商务公司。 该公司将这种方法称为“飞轮”：它的规模足以扼杀一个典型的跨国公司，并利用它为整个业务提供日益增长的动力。飞轮旋转得越快，越重，其他人就越难阻止它。 亚马逊位于亚利桑那州凤凰城的分销中心。 AWS (以前称为Amazon Web Services)的诞生和发展也许是这种方法的最好的例证。这是亚马逊的一个部门，为内部和其他公司提供云计算服务——包括那些在其他领域与亚马逊竞争的公司(例如，Netflix和Tesco都使用该平台，尽管亚马逊也销售流媒体视频和杂货)。 就像亚马逊做的很多事情一样，这一切都是从高层发布的命令开始的。贝佐斯下令，每个团队都应该以一种结构化、系统化的方式开始相互合作。如果广告团队需要一些关于鞋类销售的数据来决定如何最好地使用他们的资源，他们就不能通过电子邮件进行分析和索取；他们需要亲自前往分析控制面板并获取信息。如果控制面板不存在，就需要创建它。这种方式需要覆盖到方方面面。 从那里开始，下一个步骤就非常采取了——让其他人使用亚马逊内部提供的相同技术。 那些不起眼的开端孕育了一头野兽。该业务目前占亚马逊总收入的10 %，盈利如此之多，以至于金融法规迫使该公司将其作为自身的一个顶级部门来报告：亚马逊将其公司分为“美国和加拿大”、“国际”和“AWS”。 AWS规模大到可以与亚马逊在其他地区的分公司相提并论；大到Netflix，一家占据北美三分之一互联网流量的公司，只是其另一个客户而已。 大到2016年该公司发布了“雪地车”，一种用来移动数据的卡车。与AWS合作的公司提供了大量的信息，有时互联网根本无法应对。所以现在，如果你想上传大量的数据到亚马逊的云中，公司会开着卡车到你的办公室，装满数据，然后再把它开回去。如果你需要上传100千兆字节的视频，也就是大约500万部4k带环绕声的电影，结果发现没有比以每小时75英里的速度在高速公路上行驶更快的方法了。 当AWS看到亚马逊向外部客户开放其内部技术时，该公司的另一部分也在亚马逊的网站上做着同样的事情。 亚马逊Marketplace于2000年推出，允许第三方卖家在网站上销售自己的产品。多年来，该功能不断扩展，让亚马逊成为“百货店”——在互联网上购买现有产品唯一需要去的地方。 Marketplace要比两个比萨规则更好，允许亚马逊在不需要雇用任何额外员工的情况下扩展到新的领域。 亚马逊上销售的商品种类繁多，其内部的计算机科学家面临着一个问题。“亚马逊等电子商务公司每年处理数十亿份订单，”亚马逊的一个研究团队写道。“然而，这些订单只占所有合理订单的一小部分。“解决办法？训练人工智能纯粹是为了生成似是而非的假订单，更好地猜测如何营销全新的产品。 亚马逊报告说，它从 Marketplace 获得的收入约占公司总收入的20%。但这个指标只计算了第三方卖家向公司支付的费用，低估了业务的巨大规模。“市场现在约占亚马逊销售总量的一半，”Andreessen Horowitz的埃文斯估计。“换句话说，Marketplace意味着亚马逊处理的电子商务份额(但顺便说一句，它本身并没有为其定价)是其报告的收入份额的两倍。” 因此，亚马逊越来越不像 Tesco或沃尔玛那样的大型零售商，徘徊在城市边缘，扼杀当地的商业街，而更像是一家购物中心：独立的零售商可以存在，甚至可以维持一个整洁的生活，但前提是他们在购物中心有一席之地，而且他们永远记住真正的赚钱者是房东。 自2014年以来，亚马逊为其业务增加了第三个飞轮：人工智能。该公司一直处于行业领先地位，最明显的是其基于神经网络的推荐算法。但是，直到最近，这种方法还是漫无目标的、分割的、几乎不是世界级的(想想你上次在亚马逊上买的东西，几个星期后才推荐给你。“你喜欢羽绒被吗？为什么不多买10个呢？”)。 当该公司决定建造将成为Echo的硬件时，情况发生了变化。在亚马逊的经典模式中，它从最后开始，然后从那里向后努力，为未来的概念产品撰写一份“新闻稿”，然后试图找出需要开发或收购什么样的专业知识才能实现这一目标。需要私人助理吗？收购Cambridge-based True Knowledge。需要远场语音识别，让Echo听到房间另一边的人的声音？现在就开始解决这个问题吧，因为没有人真正解决这个问题。 从制度上讲，Alexa 人工智能团队的大部分成员仍在AWS之下，使用其基础设施，并向希望在其设备中构建语音控制的第三方提供另一部分数字服务。但人工智能的规模经济是独一无二的。当然，数据的价值在于：使用Echo的人越多，需要训练的语音样本就越多，因此Echo越好。除此之外，机器学习技术是如此的基本和通用，以至于亚马逊的每一个进步都会在整个业务中产生反弹，提高效率，开拓新的领域，并提出进一步的研究方向。 结语：亚马逊的弱点但是没有什么是永恒的，亚马逊也有它的弱点。例如，两个比萨原则可能是建立一家无限扩张的公司的一个好策略，但它并没有带来一个令人愉快、无压力的工作环境。 长期以来，亚马逊在对待仓库工人方面一直面临着批评&#58;与该行业的许多公司一样，巨大的估值和高技术抱负与低收入、低技能的工作并驾齐驱。 亚马逊与Deliveroo、苹果和Facebook等公司的不同之处在于，在总部工作的高技能员工几乎有同样多的抱怨。 《纽约时报》2015年的一篇报道称，亚马逊的员工在办公桌前哭泣，承受着近乎崩溃的压力。其员工的快速流动是传奇式的，内部人士描述了这样的一个场景：有人离开，其他人不得不重写他们的所有代码，以使仍然在那里的人能够理解——但重写完成时，重写的人员也离开了，需要其他人重新开始整个过程。 但从第一天起，杰夫·贝佐斯就一直处于食物链的顶端，直接控制着7400亿美元( 5300亿欧元)的业务，几乎没有其他公司的老板能与之匹敌。 原文链接：https&#58;//www.theguardian.com/technology/2018/apr/24/the-two-pizza-rule-and-the-secret-of-amazons-success 编译组出品。编辑：郝鹏程 转载来源：亚马逊成功的秘密：贝佐斯的决策方法论与“两个披萨原则”]]></content>
      <categories>
        <category>科技</category>
      </categories>
      <tags>
        <tag>亚马逊公司</tag>
        <tag>成功的秘密</tag>
        <tag>乔布斯</tag>
        <tag>巴菲特</tag>
        <tag>Velocity</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[平安好医生上市创香港今年最大IPO 超额认购600倍]]></title>
    <url>%2F2018%2Fb417f5f9%2F</url>
    <content type="text"><![CDATA[中国健康科技公司平安好医生在香港今年以来规模最大的一桩首次公开募股中募得87.7亿港元，这是今年将进行的一系列大型科技公司上市交易中的第一桩。 转载来源：平安好医生上市创香港今年最大IPO 超额认购600倍]]></content>
  </entry>
  <entry>
    <title><![CDATA[酷码编程完成天使轮融资，将搭建在线编程平台]]></title>
    <url>%2F2018%2Fda43d115%2F</url>
    <content type="text"><![CDATA[由头头是道基金领投、第四象限基金跟投，具体金额尚未透露。据悉，本轮融资将主要用于课程研发、在线编程平台搭建以及线下直营校区扩张等方面。 图片来源：摄图网芥末堆4月27日讯，近日，少儿编程教育品牌酷码编程宣布完成天使轮融资，由头头是道基金领投、第四象限基金跟投，具体金额尚未透露。据悉，本轮融资将主要用于课程研发、在线编程平台搭建以及线下直营校区扩张等方面。 酷码编程成立于2016年，主要面向4到18岁的青少年，为其提供趣味编程、初级至高级算法的编程课程，可供学员完成3到5年的学习。分别涵盖图形化编程、机器人编程、手工代码JS/H5、Python、C++等课程。 酷码编程采用线下小班课（不超过8人）的教学模式，重点关注青少年思维模式的培养和相互之间的合作竞争。酷码编程创始人马丛认为，青少年编程教育从课程设计、教学流程等都没有足够成熟的资源可直接复制，为此酷码编程前期采用线下面对面授课的形式，获取不同城市、不同家庭对编程教育的需求，并根据市场反馈完善其课程产品和教学服务。 马丛表示，“未来，少儿编程教育将会成为像英语一样的必备技能，其市场规模将会和英语培训比肩。此次和头头是道基金将展开多层次的合作，发挥其在教育领域、媒体、科技板块的资源，帮助酷码编程完成高效运营。” 目前，酷码编程拥有8家直营校区。后续酷码编程将提升运营速度，计划在2018年末完成10到15家直营校区的扩张。同时，为了方便学生对于课程的体验，酷码编程也在积极寻找社区型的加盟商。 酷码编程计划将于今年8月推出自主研发的在线编程平台，为学生提供在线教学服务，与线下教学形成互补，完成“线上+线下”的业务布局。 转载来源：酷码编程完成天使轮融资，将搭建在线编程平台]]></content>
      <categories>
        <category>科技</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>英语</tag>
        <tag>科技</tag>
        <tag>基金</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[下一个千亿级财富金矿？IPFS挖矿到底有何魔力！]]></title>
    <url>%2F2018%2F4f900001%2F</url>
    <content type="text"><![CDATA[想起前段时间比特币市场行情一片低迷，矿工及矿机经销商更是叫苦不迭，能不赔就已经非常幸运了。在这种大环境下，不少矿工选择退场，或者寻找新的机会，比如IPFS。 正文 最近小编在群里或者币友圈都有听到很多人讨论IPFS。想起前段时间比特币市场行情一片低迷，矿工及矿机经销商更是叫苦不迭，能不赔就已经非常幸运了，在这种大环境下，不少矿工选择退场，或者寻找新的机会，比如IPFS。 可能是IPFS主网即将在6月份上线，越来越多的人开始关注IPFS，矿工们的呼声早已在各大论坛、社群炸开了锅，提前屯硬盘、矿机的人比比皆是，大家都相信2018年将是IPFS矿工获取最大收益的一年，并且第一批矿工将会是收益最大的人。 那么这个让矿工朋友如此兴奋的IPFS，到底是个啥！为何这么多人为之疯狂？IPFS到底是概念模式的炒作，还是真的能给世界带来什么样的改变？ IPFS是什么？ 星际文件系统IPFS（InterPlanetary File System）是一个面向全球的、点对点的分布式版本文件系统，目标是为了补充（甚至是取代）目前统治互联网的超文本传输协议（HTTP），将所有具有相同文件系统的计算设备连接在一起。 原理是用基于内容的地址替代基于域名的地址，也就是用户寻找的不是某个地址而是储存在某个地方的内容，不需要验证发送者的身份，而只需要验证内容的哈希，通过这样可以让网页的速度更快、更安全、更健壮、更持久。 它是一个协议也是一个网络，已经运行了2年半，并非虚无缥缈的空气。它就像比特币网络一样，并没有发明什么新技术，他只是将很多种技术(P2P网络技术、BitTorrent传输技术、Git版本控制，自证明文件系统的数据传输协议等等)加以结合，并在这些技术上改进创新，集成了一个去中心化的，IPFS网络。 接下来是IPFS最为引人瞩目，最被大家炒作的概念：IPFS要补充甚至取代过去20年里使用的超文本媒体传输协议(HTTP)。要知道，咱们现在使用的互联网协议，全都是http，而且已经用了20多年了，从HTTP1.0 到现在的HTTP5，网页的展示越来越美观丰富，但它背后的Browser/Server 模式是从来没变的。 如果，我说的是如果，IPFS能做到更改这项模式，无疑会是颠覆性的，甚至是重构性而存在的：网络将会更快、更安全、更开放；隐私将得到极大的保护，人人都可以成为服务器，IPFS，可以从本质上改变网络数据的分发机制，这一切，是在是太amazing了！ IPFS的挖矿方式，其实很简单，IPFS矿工通过检索或贡献存储空间来得到奖励（FileCoin），再将FileCoin兑换成比特币或者以太币套现，这就是IPFS挖矿的基本流程。 IPFS为何如此火爆？ 天下熙熙皆为利来，天下攘攘皆为利往！已经说了，区块链最大的共识就是赚钱，IPFS这里显然有了极好的概念，极好的模式，咱们可以赚一笔啊，为何不来。IPFS的中国热，其实是由由新矿工群体和新矿机商群体带动的。 1、IPFS市场之所以如此被看好，重要的一个原因就是其奖励机制，70%的募资基金将奖励FileCoin 矿工（挖矿奖励），这么有诚意的分配比例当然会被大家看好。这样高份额的奖励自然吸引着矿工们前来挖矿。 2、大部分矿工觉得购买比特币和以太坊矿机，价格太高而且还订不到，电费价格也奇贵无比，所以只能退而求其次，买个”IPFS“矿机咱先挖着玩玩，说不定这个概念一起来，猪都起飞了呢。 3、IPFS，说句实话，现在突然热的要命，主要还是因为这个概念真他么的好圈钱。怎么圈？搞个盒子卖矿机，加上IPFS的概念，高价卖货你懂得！光卖矿机就能发大财你能信？ IPFS是一个开源版+增强版的玩客云 1.去中心的云存储不会随时关停 金山快盘、酷盘、360云盘、快传、各种云盘，当你在上面存满自己私人东西，然后公司说关就关的时候，是多么的痛苦啊。 而IPFS是一个去中心的云存储，不会有哪个公司可以「因业务调整」而关停。 2.挖矿不再是消耗人类能源的浪费 比特币的挖矿，其实是算一个lucky number，使得这个lucky number跟一堆转账记录的hash值符合特殊规律（比如前10位都是0）。 这个挖矿算法很优雅，解决了区块链历史记录极难被更改的问题。但它有个致命的缺点，算lucky number本身毫无社会贡献，只是增加了大量的电费。 而IPFS的挖矿更优雅，矿机硬盘的文件被别人使用了（比如一次下载或一次视频播放），可以收获一点点filecoin（文件币）的奖励。如果你想获得很多很多的filecoin，那么你就要准备很大很大的磁盘空间和很多很多的带宽。而你的这些存储和带宽，是别人上网冲浪需要访问的文件。 所以IPFS的挖矿其实就是BT做种的过程，我为人人，人人为我。 3.集成了Git版本控制功能，文件的历史版本都有保存 区块链的一个超级帅的概念，就是可以追溯历史，之前干了什么是无法抵赖的。集成了Git概念的IPFS，一个文件的任何改动都有版本可查，做了坏事想抹掉可是很难的了。 4.你的数字资产真正被你所拥有 在http的互联网里面，你在网络上留下的痕迹、产生的数据、保存的文件，其实都是存在「别人公司」的服务器上的，数据的所有权其实是数据商业公司的。 举个例子，你想销毁在Facebook上传的照片，或者是想把微信好友的关系保存起来，都没办法。因为这些数据虽然是你产生的，所有权是「公司」的。这些数据很有价值，但是你却没办法把它带走。 而在IPFS里面，你拥有一把数据的私钥。需要给哪个应用开启，不想给哪个应用使用，掌控权都属于你自己。 转载来源：下一个千亿级财富金矿？IPFS挖矿到底有何魔力！]]></content>
      <categories>
        <category>财经</category>
      </categories>
      <tags>
        <tag>区块链</tag>
        <tag>数字货币</tag>
        <tag>比特币</tag>
        <tag>Git</tag>
        <tag>360云盘</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[小米IPO保荐人花落中信、高盛、摩根斯丹利三家]]></title>
    <url>%2F2018%2Fa2e3b089%2F</url>
    <content type="text"><![CDATA[小米上市的主要投行目前已最终确定。不同信源表示，此次负责小米赴港IPO的保荐人，中资投行为中信证券，外资保荐人为高盛和摩根斯丹利。 转载来源：小米IPO保荐人花落中信、高盛、摩根斯丹利三家]]></content>
  </entry>
  <entry>
    <title><![CDATA[2018中国新一线城市排名出炉！成都稳居第一！]]></title>
    <url>%2F2018%2F94a512a0%2F</url>
    <content type="text"><![CDATA[近年来，无数文字和图片，试图解构这一网络热点。昨日，2018中国城市商业魅力排行榜出炉。四个一线城市在各自的两个梯次中调换了位置——由“北上广深”变为“上北深广”；15个“新一线”城市中成都排位第一，依次是成都、杭州、重庆、武汉、苏州、西安、天津、南京、郑州、长沙、沈阳、青岛、宁 什么是成都？ 什么是成都魅力？ 近年来，无数文字和图片，试图解构这一网络热点。 实实在在的 数据痕迹，更能说明。 昨日，2018中国城市商业魅力排行榜出炉，依据最新一年的170个品牌商业数据、19家互联网公司的用户行为数据及数据机构的城市大数据，第一财经·新一线城市研究所对中国338个地级以上城市再次排名。 最引人瞩目的，是一线城市和“新一线”城市两个榜单。 根据最新排行榜，四个一线城市在各自的两个梯次中调换了位置——由“北上广深”变为“上北深广”； 15个“新一线”城市中成都排位第一，依次是成都、杭州、重庆、武汉、苏州、西安、天津、南京、郑州、长沙、沈阳、青岛、宁波、东莞和无锡。 用数据描绘成都 ↓↓↓↓↓ 商业资源集聚度·排名第一 消费品牌门店总数连续三年超过广州 在第一财经·新一线城市研究所看来，门店选址是商业社会中最为精明且谨慎的逻辑。今年，这套方法得到了进一步升级。 在今年的商业资源集聚度中，考察大品牌青睐度——大品牌如何选择城市，代表着品牌对城市商业品质氛围的认可。成都依然是西南地区大品牌进驻的首选城市，成都的消费品牌门店总数连续三年超过广州及其他“新一线”城市。 城市枢纽性·排名第一 交通是联通城市的物质基础 若是把城市之间的关联比喻成一张网，那么每一座城市都是网络中的节点。强辐射力的城市向周边城市输送更多的商品、资源与人才，弱辐射力的城市往往处于被动接收辐射的地位。这种输送的能力——即枢纽性，是城市重要的竞争力之一。 交通是联通城市的物质基础，在这个维度，榜单既考虑了城市的高铁站数量、民航可直达城市数、经过高速公路条数等城际交通基础设施类数据，也用城市之间通过铁路、民航与高速公路等交通工具的城际往来矩阵分别计算了城市在交通网络中的枢纽性。 今年的物流通达度指数在物流网点数量之外，新增了各城市收寄包裹的数据。 商业资源区域中心度指数计算的是城市中各商业品牌与其所在区域内其它城市联系度的总和。华南的广州和深圳、西南的成都和重庆、东北的沈阳和大连商业资源分配相对“均势”，而上海、北京、武汉和西安在各自区域内则具有绝对优势。 城市人活跃度·排名第一 成都是日均观影规模最大的新一线城市之一 消费活跃度指数是衡量城市人是否活跃的基础指标，也意味着城市人的支付能力以及整座城市在线上线下同步提供商业服务的能力。成都、武汉和重庆等中西部城市是日均观影规模最大的新一线城市，足够的人口规模为当地的观影消费市场提供了充足的潜在客源。 不安分指数衡量的是城市向上生长更新的欲望，它代表了一种积极的生活状态。成都是不安分指数最高的新一线城市。成都人乐于在旅行平台上分享自己的旅游记录，也迅速接纳了新生的共享单车，并保持着很高的日常骑行活跃度数据。 生活方式多样性·排名第一 成都等城市更愿意在音乐上投入资金 今年榜单建构了生活方式多样性指数的算法框架，从出门新鲜度、休闲丰富度和消费多样性三方面更聚焦地衡量这个与城市人生活感知密切相关的指数。 更多的人开始跑步、健身、阅读、听音乐和旅行，这些休闲活动类数据都可以用来衡量城市人的休闲丰富度。 通过电影票房、音乐App的付费意愿、淘宝线上消费商品的多样性、对星级酒店的偏好与旅游产品的购买意愿，能观察到城市人多样的消费类型。从数据看，成都、南京和杭州相比其他城市更愿意在音乐上投入资金。 未来可塑性·排名第一 成都是创业环境最好的“新一线”城市 创新能力是城市可塑潜力的重要一环，初创公司是最主要的创新主体之一。数据显示，杭州、成都创业平台数量和融资规模仅次于一线城市，是创业环境最好的新一线城市。 这一指数还考虑了城市人消费行为中的商品信息关注度、会员用户情况。越来越多追求理性与品质的消费行为，会给城市商业带来新的升级空间。 城市的GDP和人口数据也纳入考量。在考虑规模基数的前提下，不同级别的城市突破各自增长瓶颈的能力多少给了人们对未来的信心，也让人们相信自己的选择。成都商报记者 叶燕 相关： 一套评估体系包含五大指标 在最新的中国城市商业魅力排行榜中，记者看到，四个一线城市为上海、北京、深圳、广州。15个“新一线”城市依次是成都、杭州、重庆、武汉、苏州、西安、天津、南京、郑州、长沙、沈阳、青岛、宁波、东莞和无锡。 第一财经·新一线城市研究所说明，这是利用城市数据建立一套评估体系，依据最新一年的170个品牌商业数据、19家互联网公司的用户行为数据及数据机构的城市大数据，对中国338个地级以上城市再次排名。 为保证榜单的延续性与可比性，这份2018年最新的城市商业魅力排行榜沿用了上一年的商业资源集聚度、城市枢纽性、城市人活跃度、生活方式多样性和未来可塑性五大指标，并维持了原有的算法框架：一级指数的权重以新一线城市研究所专家委员会打分的方式计入，二级指数以下的数据则采用主成分分析法。 中国城市格局悄然发生变化。在新一线城市中，成都排名第一，在多个维度成绩亮眼；无锡经过一年的蛰伏重返新一线；重庆、苏州、郑州是位次连续3年上升的3个城市。 越来越多城市都充分意识到，人才是城市发展的核心。今年年初，南京、杭州、成都、西安和武汉等新一线城市都相继出台人才新政，吸引高校学生和专业技术人员落户。这是一场“人才争夺战”，更是城市发展核心要素的抢滩。 转载来源：2018中国新一线城市排名出炉！成都稳居第一！]]></content>
      <categories>
        <category>房产</category>
      </categories>
      <tags>
        <tag>音乐</tag>
        <tag>创业</tag>
        <tag>大数据</tag>
        <tag>中国联通</tag>
        <tag>酒店</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[K12社群运营容易忽视的几个细节和思考]]></title>
    <url>%2F2018%2Fbcfe4894%2F</url>
    <content type="text"><![CDATA[社群邀请了教育行业专家、学者、创始人、老师等人，不定期对当下的热点话题、细分领域、教育现象进行探讨和分享。 【智囊分享】是“芥末堆｜教育智囊团”组织的社群活动。社群邀请了教育行业专家、学者、创始人、老师等人，不定期对当下的热点话题、细分领域、教育现象进行探讨和分享。 在教育行业，社群运营也被视为招生获客的最佳路径。很多教育公司也一早锁准了这条低成本的获客之道。比如传智博客、好未来的家长帮、宝宝玩英语的微信生态等。在本期中，国内某知名K12公司的小明老师（化名）分享了大家在社群运营过程中容易忽视的几个细节和思考。 以下是分享实录，供大家参阅。 对于社群运营，今年我们看到了很多刷屏级的活动，大家很兴奋，想复制这种模式，很多做社群的小伙伴貌似又看到了做社群的希望。我个人觉得虽然社群运营操作门槛低，我不讲如何能做出现象级刷屏的活动，第一，我觉得毕竟还是少数人可以操盘，我们更多的是吃瓜群众，应该脚踏实地做好每一步的运营；第二。我们多一些最基本的思考，如何设计属于适合自己的增长模型，如何在每一个环节达到最大的转化，从而实现营收。 现在做社群相信市面也有很多很成熟的体系，大家也听过很多社群营销的课程，接下来我的分享，仅限于自己对于K12社群运营的简单思考。 一、对于K12社群运营的理解科学、系统、高效是我对K12用户的社群运营结构最基本的理解。科学是指社群依据在线教育产品如何利用内容、模式以及产品实现增长和转化；系统是指社群运营如何解决从用户参与到转化的过程达到系统化运转；高效是指如何利用工具、团队以及内容实现社群的高效运转。 用户结构根据中小学生对于学习有着长期、稳定、刚性需求这三个特点：学生作为直接参与者以及效果的反馈者，家长作为产品选择者和付费决策者，老师作为内容生产者，根据不同的角色，社群运营出现了不同的运营策略。 二、关于社群运营基本思维、工具、技巧（一）社群运营最基本的思维：势 1.运营和推广：比如早前几年朋友圈推出的各种测试、H5以及近期的何种刷屏活动，裂变手段，好友互动、成绩比拼、微信群PK、朋友圈晒成绩，如果你第一时间掌握了，将瞬间实现流量的爆发。 2.热点和爆款：综艺节目类似与朗读的节目、政策的红利（高考改革）、内容（稀缺性资料以及课程）等等，如何策划出爆款活动，懂得势在哪里。 3。工具和产品：现在技术的发展远远比我们想象的快，对于社群裂变、个人号裂变、公众号裂变等，每一个细分的领域都会有更先进的工具出现，在第一时间掌握最新的工具，结合自己的优势，实现高效的增长。 （二）出现的社群模式 群裂变模式 对于7天搞定xx、21天xx训练营,领取xx课程等等活动，借助活码群裂变工具，成为很多做社群运营忠实的方式，看起来能获得一大批对于某项福利感兴趣的用户，但是这些群变现的转化率较低，用户很难产生黏性，一旦入群发现是推广课程、公众号，用户产生了抵触情绪，使得变现和留存成为最大的难题。 2、打卡模式 相信很多做在线教育的小伙伴并不会陌生，利用学习训练营打卡模式，打卡的平台有小程序、公众号、H5等产品。形式有点击打卡（小程序、H5）、分享链接、海报等打卡方式，分享的渠道多集中于朋友圈。打卡在社群中意味着一种态度还有短时间养成一种好习惯，根据打卡的格式、时间以及规则，要求的是对于社群高效组织。 例如：对于刷屏的薄荷阅读，薄荷阅读通过微信公众号打卡、带班老师每日讲解+答疑、社群成员督促等多种机制来确保用户完成每日的学习任务。 在“社群老师”的引导下，群成员每天完成“每日一句”、“今日词表”、“边听边读”、“课后习题”、“答疑讲课”等学习环节,在最后一天完成所有阅读的成员可获得本次课程笔记的永久阅读权限，未完成者无法再查看阅读内容，通过激励/惩罚措施来提高群成员达成目标的完成率。 （三）社群需要掌握的工具：托 1、整合资源 比如在举办某些大会或者课程时，我们需要一些大咖或者嘉宾出面，如果我们没有太多的经费和资源，这个时候，我们可能会先联系最有可能搞定的嘉宾，同时我们会联系和他同样级别的嘉宾，然后分别对双方说都会出席同一场活动，这种通过对方当托，实现资源协调的方式利用好，实现事半功倍的效果，再比如大家熟知的吃火锅的案例。 2、旁敲侧击 用户在群内发出一些不利于群规的链接和信息等，出于对用户意识到问题的目的，我们在群内让托犯同样的错误，予以警告或者剔除，通过旁敲侧击，让用户意识到问题，不再犯类似的错误。所以当我们不想直接处理，但又不得不这么做的时候，我们可以用“托”的方式去旁敲侧击一下。 3、以假乱真 对于一个社群刚刚建立的时候，用户有很强的戒备心，如何破冰和解决信任度的问题。 这个时候我们就要利用托的身份假扮成用户，主要目的就是以用户的身份产生互动，产生交流，增强彼此的信赖度，用户的心理防线就会慢慢降低，我们在进行宣传和推广。 4、以身作则 针对福利活动，出现了运营流程的错误或者由于参与人数太少，导致用户怀疑能不能按照预期获得他应该得到的东西。流程上由于一些特殊原因没能及时准备。这种时候如果不及时处理的话，会造成大面积的用户反抗行为。对于活动如果由于参加的用户较少，用户对活动的奖励也存在着半信半疑的态度。 此时，我们可以利用托进行澄清说明，声称自己已经拿到了奖品。同时要说明奖品是依据规则获得，会陆续发放，请大家耐心等待。用户看到了其他人已经拿到，而自己只是时间上稍微晚一点的话，会比较容易接受。这不是为了骗人，而是为了给我们争取时间来弥补过失。 （四）社群需要掌握的分享技巧 利用内容和福利刺激用户实现转发分享，对于增强用户的接受程度，目前常见的几种方式如下： 1、前置分享 A、对于用户要获取福利就必须按照要求转发，通过审核之后才可以获取利益。需要工具的支持和人工不阶段的审核。 缺点在于，随着群内人群的增多，对利益的需求就会越来越大。比如增送小礼品，人数增多，成本增大，最好的选择就是我们能够提供复制不产生成本的利益，比如工具、视频、电子书、网络教程、人脉资源等等。 B、用户自己参与条件不够，需获得更大的利益，需要邀请让人分享获得抽奖机会或者免费名额，比如小米的一分抢购助力。利用了人贪便宜和赌博的心理实现裂变。 2、后置分享 A、大部分用户为了获得福利，朋友圈觉得对于其他的好友来说不合适，比如集赞、助力等，所以在这个时候我们让转发的人和被吸引人的人同时获取福利，就会打消转发者的心理顾虑，这也是满足了社交中的利他属性。比如：报一次课需要100，但是双人同时报每人只要60元，用户的接受程度就比较高了。 转载来源：K12社群运营容易忽视的几个细节和思考]]></content>
      <categories>
        <category>教育</category>
      </categories>
      <tags>
        <tag>在线教育</tag>
        <tag>移动互联网</tag>
        <tag>朋友圈</tag>
        <tag>市场营销</tag>
        <tag>薄荷</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[焦点解读 | 大疆10亿美元融资样本：三大关键点存疑，投资者分歧加剧]]></title>
    <url>%2F2018%2F14467c74%2F</url>
    <content type="text"><![CDATA[科技独角兽的高溢价时代 焦点解读 | 大疆10亿美元融资样本：三大关键点存疑，投资者分歧加剧_36氪 转载来源：焦点解读 | 大疆10亿美元融资样本：三大关键点存疑，投资者分歧加剧]]></content>
  </entry>
  <entry>
    <title><![CDATA[1分钟知识锦囊 | 听说开发者都喜欢用游戏来训练AI，为什么？]]></title>
    <url>%2F2018%2F870b0c9e%2F</url>
    <content type="text"><![CDATA[AI 在训练场中可以学习如何跟人类一起合作。 1分钟知识锦囊 | 听说开发者都喜欢用游戏来训练AI，为什么？ _36氪 转载来源：1分钟知识锦囊 | 听说开发者都喜欢用游戏来训练AI，为什么？]]></content>
  </entry>
  <entry>
    <title><![CDATA[从无监督构建词库看「最小熵原理」，套路是如何炼成的]]></title>
    <url>%2F2018%2Fb8f4fc62%2F</url>
    <content type="text"><![CDATA[在深度学习等端到端方案已经逐步席卷NLP的今天，你是否还愿意去思考自然语言背后的基本原理？ 转载来源：从无监督构建词库看「最小熵原理」，套路是如何炼成的]]></content>
      <tags>
        <tag>PaperWeekly</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[小学一年级语文题碾轧家长智商 专家：小学生作业不宜太难]]></title>
    <url>%2F2018%2F862625ba%2F</url>
    <content type="text"><![CDATA[一道小学一年级的语文作业题，不仅难倒了大学本科毕业的小学生爸妈及众多家长，还在一个从事文字工作的编辑群里引发了热烈讨论，说法不一。 来源&#58;燕赵晚报 小学一年级语文题碾轧家长智商 专家：小学生作业不宜太难，应少些标准多给空间，培养开阔思维 “人多，什么游戏都能玩，拔河，老鹰捉小鸡，打排球，打篮球，踢足球……连开运动会也可以。”请问，这段选文一共有几句话？一道小学一年级的语文作业题，不仅难倒了大学本科毕业的小学生爸妈及众多家长，还在一个从事文字工作的编辑群里引发了热烈讨论，说法不一。 近来，关于小学生作业频频碾轧爸妈智商的案例屡见不鲜，不少父母发帖吐槽，是小学生作业太难了，还是家长们知识落伍了？相关教育专家认为，小学生作业不宜太难，也不宜标准太僵化，在答案上应该多给空间，才不致于把孩子的思维“圈”得太死。 家长们每天讨论孩子们的作业题 梁女士和爱人都是大学毕业，儿子上小学一年级，平时梁女士负责给孩子辅导作业。按常理，小学一年级的作业是最简单的，但是却经常让她感到挠头，比如一道数学题：“数字按从小到大的顺序排列，7的前面有几个数？”梁女士就弄不清是“7”还是“6”，因为不知道是不是包括“0”。还有一道数学题：32-6=？答案肯定是26，但是试卷要求写出做题分解过程，儿子分解的是32拆分成30和2，30-6=24，24+2=26，被判错了；正确答案是32拆分成20和12，12-6=6，20+6=26。这叫梁女士很是不解：“为什么32只能分解成20和12才算对？这个标准也太僵化了！” 在梁女士儿子所在班的家长群里，家长们每天都在为孩子们的作业题而讨论，哪道题怎么做，正确答案应该是什么？为什么正确答案是这样的…… 最近，一道语文题让梁女士和家长们颇费脑筋，是老师发的期中模拟卷上的题：“人多，什么游戏都能玩，拔河，老鹰捉小鸡，打排球，打篮球，踢足球……连开运动会也可以。”请问，这段选文一共有几句话？梁女士是中文系毕业，从事的也是文字工作，她认为这段选文应该是一句话，因为通篇表达了一个意思，但是她又不确定。到底应该是几句话，家长们在群里集思广益后也没有讨论出结果。梁女士又发到了朋友圈，朋友们给出的答案也不一样，有的说是一句话，有的说是两句话，还有的说是三句话。昨日，梁女士又把这道题发到了一个编辑工作群里，群里都是从事文字工作的专业人士，大家议论纷纷，仍然没讨论清楚到底是几句话，有人说是一句话，理由是通篇表达了一个意思；有人说是两句话，因为中间出现了省略号，省略号代表一句终结。在讨论中，大家纷纷感慨，现在的小学生语文题竟然让专业的文字工作者都“拿不准”，是成年人知识结构过时了？还是小学生的题出得太难了？ 小学生作业频频碾轧爸妈智商 梁女士的作业难题一抛出，引来了一片共鸣声，很多爸妈吐槽孩子作业太难了，智商受到无情碾轧，甚至开始怀疑所受的教育。 在一家事业单位工作的黄女士说，她的儿子上小学二年级，前一阵子试卷上的题也把她难住了，是卷子里拓展题中的图型填字题，上面是“车”、左边是“齿”、右边是“流”、下面是“班”，要求在中间填个字，使上下左右的字均可组成词语，她想了很长时间，想出来一个“轮”字；后一道题型类似：上边是“大”、下边是“风”、左边是“日”、右边是“思”，中间要求填个字，使上下左右的字都能组成词语，她就怎么也想不出来了。只能告诉儿子，等老师公布答案吧。黄女士说，她最怕的还是数学题，比如“一堆桃子，5个5个地数余3个，7个7个地数也余3个，这堆桃子至少多少个？”还有填字游戏，要求把1-10这几个数填入图中，使图型每条线上的四个数之和都相等……这些题都让自己很蒙，觉得学过的数学完全不够用。 同为小学生家长的王女士则主要依赖作业帮和百度，每有不会的题，就上网搜索答案，再给孩子讲，但即使如此，也经常会有弄不明白的题，只能向老师求助。 在网上，因为孩子作业难而吐槽、求助的帖子更是数不胜数。一个叫“红火火”的网友发帖说：“求各位大神帮帮忙，一道小学三年级的题目把我们难倒了，我和她爸爸都是大学本科毕业，现在大学生被小学题难住了，感觉自己辅导孩子学习没有信心了”。这个帖子很火，后面跟帖一片，一些网友给出自己的答案，更多的网友则发出共鸣，讲述自己遇到的各种奇葩的小学题，想去网上搜索答案都搜不到，很无奈很崩溃。网友纷纷感慨：“感觉这些小学生作业是对自己智商的无情碾轧”“想想自己上了十几年学都白上了”…… 专家： 小学生作业应弱化标准 让梁女士挠头多日的语文题究竟答案是什么？她终于忍不住给老师发短信询问，老师的回复是：两句，因为省略号代表一句话。 石家庄市教育科学研究所高中组的李慎老师则做了更详细的解释。她说，这道题考的其实是学生对标点符号的认知，在标点符号使用规范中，标点符号分为“标号”和“点号”，“点号”又分“句中点号”和“句末点号”，“句中点号”包括顿号、逗号、分号等，“句末点号”包括句号、问号、叹号、省略号等。在这段话里，出现了省略号和句号两个“句末点号”，就是两句话。如果再细分一下省略号的归属，用于一串名词后面表示列举的不能单独成句，用于带有动词、形容词的句子后面的可单独成句，这段话中省略号列举的正是可单独成句的动词组。 李慎老师说，这样的题应该是小学三、四年级的题型，如果是小学一年级，还是有些难了。一段文字包括几句话应该是标点符号和内容结合来判断，小学一年级的孩子很难做到这一点。小学生的理解能力有限，一些题目超出他们的理解能力后就很难讲解，经常有小学的教研员来请教她，如何对学生解释明白一道题的答案，她也很为难。她认为现在小学生的作业题、考试题很多是形式大于内容，过于追求形式上的标准，而不是内容上的实用，同样的一段话，与其让孩子们回答这是几句话，不如让孩子们去体会这段话的意思，体会文字的美感。在答案上也该多给些空间，只要是合理的能够自圆其说的都该鼓励，不能用过于僵化的标准来“圈”住孩子们的思维。 老师： 家长可与老师多交流 针对家长们经常被小学题目难住的困惑，在省会一所重点中学任教的刘老师也提出了自己的看法。她说，小学生的作业题多数是根据课程教材来出的，考量的是课堂上学过的某个知识点，家长们虽然受过高等教育，但很可能已经忘记小学学过的知识了，又对教材不熟悉的话，有时候的确会蒙。她建议家长多跟孩子交流，只有熟悉小学生的课本才能更好地帮助他们，如果家长的确觉得难，还可以跟老师交流，要求降低作业难度。而且，尽量鼓励小学生自己动脑动手解决问题，家长不要包办太多。 此前，石家庄市教育局还专门出台了《关于进一步加强小学生作业管理工作的通知》，要求学校不得布置超越学生能力的作业，以尽力解决小学生作业方面的困扰。文件还明确提出，小学生作业布置数量要适当，难易要适度。加强作业形式的灵活性和情趣性，调动学生的学习积极性和创造力。（记者 刘文静） 转载来源：小学一年级语文题碾轧家长智商 专家：小学生作业不宜太难]]></content>
      <categories>
        <category>教育</category>
      </categories>
      <tags>
        <tag>大学</tag>
        <tag>数学</tag>
        <tag>语文</tag>
        <tag>排球</tag>
        <tag>升学考试</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[中国教育O2O服务行业白皮书]]></title>
    <url>%2F2018%2Fb6a554fb%2F</url>
    <content type="text"><![CDATA[市场特征：中国教育培训市场整体呈现潜力大、机会多、创新性强且竞争激烈的特点，未来市场整体将走向深度融合之态。 核心结论 教育培训行业 发展驱动力：中国教育培训行业的发展受频发的利好政策、不断更新迭代的技术和越发多元的市场需求共同驱动。- 市场特征：中国教育培训市场整体呈现潜力大、机会多、创新性强且竞争激烈的特点，未来市场整体将走向深度融合之态。- 营销痛点：市场营销渠道不断丰富，但企业面临获客成本高、市场竞争强和用户转移成本高的不足，而用户缺乏一个具有公信力的、用户信任的第三方渠道来为用户筛选信息、做决策等活动提供参考。市场特征：中国教育培训市场整体呈现潜力大、机会多、创新性强且竞争激烈的特点，未来市场整体将走向深度融合之态。 教育O2O平台 核心价值：教育O2O模式提升了用户选择培训机构的效率，增加了机构/教师的曝光度，为“互联网+”与教育的结合提供了宝贵的经验和借鉴意义。- 发展趋势：未来，教育O2O平台将优化升级平台服务能力，平台功能向重量级方向发展；平台整体上将会重构教学流程和服务模式，线上线下业务走向深度融合的OMO形态。发展趋势：未来，教育O2O平台将优化升级平台服务能力，平台功能向重量级方向发展；平台整体上将会重构教学流程和服务模式，线上线下业务走向深度融合的OMO形态。 中国教育培训行业的特征 线下教育市场步入理性思考，并积极拥抱互联网 中国教育培训行业始于20世纪80年代，改革开放使得国门打开，外企得以入驻，人才变得紧缺且复合化，职业培训市场开始兴起。随后市场逐步开化，留学培训市场开始繁荣。进入2000年，中国加入WTO，社会对人才的要求越来越高，终生学习的理念开始盛行，家庭经济的好转也催生出”不让孩子输在起跑线”的意识，一大批中小学教育辅导机构开始涌现，市场迎来高速发展。2013年以后，线下培训市场竞争趋于白热化，一方面，为吸引生源“保分保效果”等代名词开始出现，但实际效果不佳；另一方面，市场门槛低，一大批人入行掘金，行业赛道混乱、口碑下滑，因此用户对市场的信任度下降，倒逼企业反思。同期，随着互联网元年的到来，受新兴科技与互联网的影响，教育+互联网的新业态开始形成，在线教育浪潮开始兴起。 利好政策频发，市场秩序不断规范，助力市场健康有序发展 中国教育培训市场的发展无法脱离国家政策的保驾护航。新修订的《民办教育促进法》明确了对民办学校举办者合法权益的保护，同时“负面清单”和联合监管机制规范了校外教育培训机构，市场将在迎来一轮清洗后进入良性的健康发展期。针对新兴技术与教育的结合，国家大力推行教育信息化，以将优质教育资源输出，同时在学习行为数据化的基础上利用人工智能技术实现智能教育、个性化教育，促进教育公平而有质量。在人才引进/输出方面，一方面，减少留学流程，优化回国人员服务；另一方面，促进中外合作办学，学习借鉴各国教育发展成功经验，取长补短，互利互补，提高办学质量。 技术创新更迭快，不断与教育融合，提升教与学的效率 技术进步是中国教育培训发展的基础，每一次技术的变革都是提升教学效率和教学体验升级的催化剂。互联网的普及与完善使得多媒体教学得以推广，让部分教学活动从粉笔板书中解放出来，提升了教与学的效率，同时也为大范围在线教育提供可能，降低了用户知识获取的成本；语音图像识别、在线测评、互动直播，不仅覆盖了教学活动中的教-学-练-测-评各个环节，丰富了学习场景，而且将用户学习行为记录下来，为个性化教学、自适应学习提供了数据支撑。未来，当人工智能技术深度融合于教育教学环节时，教学活动各个环节的时间成本将有效释放，高质量的、有效的智能化教学时代将开启。 生活水平逐步提升，催生市场各类需求，推动市场发展 改革开放后经济复苏，人们生活水平得到极大提升，人们在人均文教娱乐方面的支出持续增长，自2010年开始，全国人均教育文化娱乐支出保持年均10%左右的增速，市场消费意愿持续高涨。经济基础和时代环境的变化催生着教育需求的变化，为满足市场需求，教育培训市场的企业数量和品类逐渐丰富，并且随着人工智能等新兴技术的发展，用户对个性化、定制化的需求急剧显现，对减少重复学习提升学习效率的自适应学习等智能化的学习期望也越来越强烈，表现为资本市场和创业市场对线上教育的不断深耕，教育培训市场以新兴科技为基础的新科技萌芽时代已经开启。 音乐、外语、兴趣等领域的需求大于供给，市场可拓性强 从美团点评平台的机构和用户分布来看，音乐、外语、兴趣生活和留学培训等市场领域目前需求大于供给，市场的可拓性强，市场机会也相对较多。但从市场的更替来看，目前兴趣生活、驾校、外语培训的市场淘汰率相对较高，其中兴趣生活的替换率最高。 资本和技术推动教育培训市场不断创新，市场潜力大、机会多、创新性强且竞争激烈，市场整体将走向深度融合之态 从整体来看，教育培训的需求市场不断变化进而推动整个教育培训市场的繁荣发展。现阶段，中国教育培训市场潜力巨大，而且基于国内互联网的快速发展，新兴技术与教育的不断融合，教育培训市场不断有新的基因注入。同时在资本的推动下，新的商业模式、深度垂直细分的赛道以及创新教学及管理等频频引领市场新的竞争热点，整个教育培训市场呈现资本热、机会多、创新性强、竞争激烈的特点，未来提升教学效率和效果将是永恒的主题，市场整体将走向深度融合。 营销方式随时代变迁而丰富，营销的本质是利用多种运营手段让用户产生消费，并在此过程中建立企业品牌形象 营销的本质在于让用户产生消费，AIDA模式（“爱达”公式）是一个经典的营销模型，它将销售阶段化，即吸引用户注意—引起用户兴趣和认同—激发用户购买欲望—产生购买行为。但让用户产生购买行为并非营销终点，用户的重复购买和推荐不仅能够引导潜在用户池的有效转化，而且能促进企业品牌形象的树立，进而形成用户认知-转化-推荐的闭环。现阶段基于互联网，营销的手段越发丰富，营销方式也变得更加灵活多变，传统的营销方式如发传单、广播、电视等，与线上的搜索、弹窗广告、社群运营等网络营销手段相结合，企业将有更多渠道和机会接触达用户进而完成转化，营销将在企业运营和品牌建立中发挥越来越大的价值。 获客成本高、市场竞争强和用户转移成本高是企业运营痛点 中国教育培训行业发展至今，其市场营销的本质虽然未变，但随着市场的成熟和行业宏观环境的变化，市场营销面临着越来越多的挑战。目前获客成本高是各培训机构普遍面临的挑战，一方面是用户常用的营销渠道（如百度）竞争格局相对单一，为寡头垄断型，故用户的导流作用受营销成本影响大；另一方面，不同教育领域或同一教育领域不同角色的用户做决策时侧重点不同，如家长用户对品牌知名度、名师的要求高，看重服务的权威性，而学生则对营销内容的娱乐性等更为敏感，且存在家长与学生意见相左的情况，营销成本无形增加。从市场竞争角度来看，拥有口碑的机构已形成较强的竞争壁垒，在市场竞争中占据强势地位；而同品类的长尾机构同质化明显，让用户无从抉择。同时教育培训市场的营销也受行业本身的慢所限，当用户选择了合适的老师时，再次更换机构/老师的可能性较低，即市场的客户转移成本高。 一个具有公信力的、用户信任的第三方渠道可为用户筛选信息、做决策等提供有效参考 目前中国教育培训市场企业林立且在教学内容、师资力量和产品服务上同质化已十分明显，而用户通过专业的与教育相关的途径去了解相关信息的可行性低，大部分用户做决策时都是随大流。除了从众心理，从诸多繁杂的培训机构中做出决策的未知性本身就会给用户带来一定焦虑感，而该种焦虑感容易被利用。此外，互联网时代营销无处不在，用户身处各种营销触点之下，市场上常见的能够为用户提供信息的媒介大部分都是企业营销的途径，一定程度上带有目的和导向性。因此市场急需一个具有公信力的、权威的、用户信任的第三方渠道为用户筛选培训机构时提供参考，以降低用户与培训机构信息的不对称，节省用户的时间成本和试错成本。 提升了教学及管理效率，降低了运营成本 互联网+教育打破了空间限制，优化了资源配置，缓解了教育资源分布不均的问题。在提升效率方面，互联网+教育使得用户学习行为和机构管理可线上化、数据化，一方面，为用户精准推荐学习内容逐步成为可能；另一方面，教育SaaS的应用提升了企业的运营管理效率。在营销推广方面，互联网丰富了企业的推广渠道，增加了企业触达用户的途径，降低了企业的推广成本，同时也倒逼企业优化服务、寻找差异性，从而增加用户转化。 中国教育O2O服务平台的特征 平台主要负责提供教育培训的相关信息，并在线下完成上课 普遍意义的教育O2O模式是指教育培训企业在各种互联网技术的基础上，通过不断整合线上线下学生资源和教育资源，实现学生线上支付课程费用而在线下完成学习活动的一种教育模式。本报告所定义的教育O2O服务平台的主要特点是平台以提供教育培训信息为主，而不生产或不主要负责对外生产教学内容，并且相关学习活动是在线下进行，它既包括疯狂老师、轻轻家教、艺好学等垂直类教育领域平台，也包括决胜网、美团点评这样的综合类教育导购平台。 纵观教育O2O模式的发展历程，其繁荣发展阶段和市场洗牌阶段出现的都很快，没有长时间的跨度，主要是因为互联网思维模式的快与教育行业的慢相结合时产生了极大的不对等，因此在未来的一段时间，教育O2O赛道上的玩家将处于思变期，蓄势待发，谋求破局。 教育O2O平台基于线下教学，平台主要是将信息聚集并分发给用户，而其他平台则发挥着聚合或自营主体的作用 教育O2O平台与其他在线教育平台相比，主要是在教学形式和平台作用方面有很大的差别。在教学形式上，教育O2O平台的学习场景主要在线下进行，而其他在线教育平台按照自己的业务或课程特色可以选择线上教学、线下教学或线上线下相结合的方式，具有自主性。在平台作用方面，教育O2O平台主要是将机构/教师信息集中起来，然后分发给用户，提升用户筛选效率和选择空间，并且为中小机构带来流量；而B2B2C和B2C教育平台则分别承担着教育载体的角色和自营主体的角色。 教育O2O模式提升了用户的选择效率，增加了机构/教师的曝光度，是“互联网+教育” 的先行者 教育O2O平台充分挖掘线下资源，将具有不同教学特色、不同教学方式的教师信息及用户评论通过平台展现给用户，不仅拓展了用户的选择空间，降低了市场信息的不对称，而且为用户做决策时提供了参考评论，增加了用户对业内企业/机构的了解。从机构/教师角度来讲，教育O2O增加了机构/教师的曝光度，降低了运营成本，并为中小机构和个体教师在大的教育机构垄断大部分市场的前提下增加了机会。从行业整体来讲，教育O2O模式不仅丰富了在线教育的商业模式，更重要的是教育O2O平台的不断试错为互联网与教育的不断融合与深度发展提供了宝贵的借鉴意义，将引导教育+互联网正向发展。 教育客观环境的不足以及产品端在服务体系和服务能力等方面的缺陷是教育O2O平台所面临的主要挑战 教育O2O平台发展之初创业市场和资本市场纷纷急于抢占赛道而缺少对教育与互联网结合的思考，故教育O2O平台的不足快速显现。从客观环境来讲，教育O2O平台既无法脱离教育本身的特性，又无法摆脱互联网的要求，对师资、内容和流量的要求都很高。从产品模式来看，平台的用户粘性不高，平台对用户提供的服务更像一次性买卖，用户一旦与教师建立联系后完全可以脱离平台，而且平台在用户想要找到最适合的老师的这项需求上发力不足，产品无论是在服务体系上还是在服务能力等方面都亟待升级，以提升用户粘性，增加行业竞争壁垒。 教育O2O平台功能持续优化，升级服务能力，提升竞争壁垒 现阶段大部分教育O2O平台对用户的服务止于找到适合的老师，而未将服务延伸至找到老师后，并且用户普遍认为平台评价内容的参考价值有待提升，平台整体技术竞争壁垒也不高。随着业内企业的不断反思与用户对信息有效性的需求越发旺盛，教育O2O平台将持续优化并完善平台的功能和服务体系，从而为用户提供更有效的、更精准的、更个性化的信息和教学活动服务，而平台的功能和服务也势必向重量级方向发展。目前美团点评在平台的服务能力和服务体系方面不断优化并升级，一方面平台为用户提供了海量的信息供给，而且针对评价体系和评价真伪性两方面双管齐下，增加了用户信任度的同时也提升了用户的筛选效率。另一方面，平台还致力于打通机构间的合作，试图为用户提供更多的增值服务，以向用户传递可持续的服务价值。 未来教育O2O平台将是线上线下深度融合的OMO形态 教育O2O平台的用户具有低频且对平台低依赖性的特点，究其原因在于平台无法将用户从购买服务-享受服务-服务后的各个环节形成闭环。经过市场经验的总结及摸索，以美团点评学习培训为例，作为教育O2O服务行业的头部平台，不断向打通教学活动各环节、对接上下游企业等方向发力，试图将企业线上线下数据进行深度融合，进而形成用户、教师、平台的共生价值：一方面，平台通过打通各个企业间的合作实现教学流程的重构，既为用户提供基本信息服务，又满足了用户对优质师资和教育资源的需求；另一方面，完善的服务体系将吸引师生重返平台，平台得以再次掌握用户的学习行为数据，进一步提升推荐精准度，实现教师和用户之间的良性反馈。艾瑞认为平台全量信息的聚集将使得平台服务能力和服务体系向更加稳定而成熟的服务闭环发展。 中国教育O2O服务平台用户洞察 升学考试类用户重视预约且下单较为慎重 从美团点评平台的用户购买行为来看，升学考试类用户与兴趣技能类用户的购买行为有较大差异。升学考试类用户面临较大的升学及考试压力，重视教学效果，故做决策时更加慎重，他们往往通过平台获取培训机构信息后预约到培训机构了解完具体情况后再判断是否交易，占比为74.3%。而兴趣技能类用户相对而言压力小，更多的是为了发展自己的兴趣或者提升自己的素养，并且该类产品需要通过试听体验来判断产品/服务是否适合自己，故用户对试听的性价比要求较高，因此用户除了通过平台预约外，更喜欢在平台团购体验课。 师资力量是用户预约试听时考虑的首要因素 从用户预约试听时考虑的因素来看，“师资力量”是家长用户和成人用户预约试听时考虑的首要因素，占比分别为63.0%和50.1%，紧随其后的就是“机构知名度”，占比分别为53.1%和41.9%。 相比于家长用户，成人用户更加看重“培训体系/教材”和“资料完整程度”等因素，这可能是由于成人用户的学习自主性和目的性强，故对所学教材类型和资料的完整度有更高的要求。 家长用户更重课程体系设置，成人用户更重试听课体验 从用户判断产品/服务质量好坏的标准来看，家长用户更加重视“课程体系设置”，占比为63.8%，其次为“试听课体验”、“教学方法”和“学习环境/氛围”。而成人用户最为注重“试听课体验”，其占比最高为50.5%，“课程体系设置”和“学习环境/氛围”紧随其后，分别为48.8%和45.1%。这或许是由于家长用户的孩子处于K12阶段，更需要系统的教学，故更重视课程设置，而成人用户则更多的是有目的性的学习，试听课有助于他们判断课程或服务是否符合他们的需求，故试听体验对他们的影响更大。在这个过程中，教育O2O平台既可以让商户提供更优质的服务，满足单次体验的标准化，又可以让用户高效获取信息，降低用户的时间成本和决策成本。 师资力量是用户做出付费决定的最主要的因素 从用户付费的决定因素来看，家长用户和成人用户在付费前的考虑因素大致相似，“师资力量”从诸多因素中脱颖而出成为最主要的因素，其占比分别为57.0%和48.8%，紧随其后的就是“品牌知名度”和“教学方法”这两方面。 整体来看，家长用户考虑的因素更加宽泛，而成人用户则相对集中在“师资力量”方面。家长用户和成人用户对“价格”和“地理位置”的敏感度均较低。 用户在购买产品/服务前均会货比“三”家 根据本次调查，用户在购买产品/服务前均会对比其他家的同类产品/服务，无论是家长用户还是成人用户，其对比的平均机构数为3家。 15人以内的小班课是用户最偏爱的班型 从用户班型的偏好来看，2-15人的小班课备受用户偏爱，其中，6-10人的班型最受家长用户和成人用户欢迎，其占比分别为38.5%和31.5%。一方面，小班课的容量易让每个学生受到老师的关注，从心里产生对学习的积极性；另一方面，小班课的费用较一对一教学要低，性价比高。 平台信息丰富选择空间大是用户使用教育O2O平台的主因 从用户使用教育O2O服务平台的原因来看，“平台信息丰富，可选择的企业/机构多”是家长用户和成人用户使用该类平台的主要原因，其占比分别为66.1%和51.3%。除此之外，家长用户对平台的“可对比不同机构的价位、师资等”、“可参考其他用户评价”等功能比较注重；而成人用户则相对更加关注平台的“可免费或低价试听”及“可对比不同机构的价位、师资等”功能。选择被广告导流进入或随手点击进入的用户占比较少，这说明用户对教育O2O平台的使用具有主动性。 平台信息丰富全面但存在教师刷课时、刷评论等现象 从用户对教育O2O服务平台的直观印象来看，“平台信息丰富全面”是家长用户和成人用户最为直观的印象，用户平均评分分别为4.42和4.49（满分为5分制），紧随其后的则是平台基于教师性格和地理位置的匹配。但用户普遍认为平台存在教师刷课时、刷评论等现象，这说明教育O2O服务平台在教师管理规范上还有很大的提升空间。 超过九成的用户对教育O2O平台整体持满意的态度 从用户对教育O2O服务平台整体评价来看，用户整体对教育O2O服务平台的满意度都较高，其中家长用户和成人用户对教育O2O服务平台满意度的TOP2（非常满意+比较满意）值分别为92.0%和90.4%。这可能源于教育O2O平台不仅能够给用户提供较为丰富的企业/机构/教师的选择，而且用户的评价和购买量能够给用户带来一定的参考，降低用户决策时对机/教师的认知盲点和试错成本。 超九成的用户对从教育O2O平台上所购的产品/服务满意 从用户对通过教育O2O服务平台上购买的产品/服务的满意度来看，家长用户和成人用户对其所购买的产品/服务持满意的态度，其TOP2（非常满意+比较满意）值分别为92.9%和90.4%。 严审老师资质、精准推荐适配机构/教师以及简化筛选条件是用户对教育O2O平台的三大期望 从本次调研结果来看，家长用户整体对教育O2O服务平台的期望较为迫切，这可能是由于家长用户的孩子处于K12阶段，整体上升学压力比较大，因此对能够提高孩子与机构/教师的适配度、提升筛选优质师资的几率的各项功能有迫切的需求。而成人用户可自由支配的时间较少，因此对教学的有效性要求较高，故对机构/教师的适配性要求也随之提升。 转载来源：中国教育O2O服务行业白皮书]]></content>
      <categories>
        <category>科技</category>
      </categories>
      <tags>
        <tag>在线教育</tag>
        <tag>移动互联网</tag>
        <tag>市场营销</tag>
        <tag>美团网</tag>
        <tag>O2O</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[资源｜DMLC团队发布GluonCV和GluonNLP：两种简单易用的DL工具箱]]></title>
    <url>%2F2018%2Fea9b9244%2F</url>
    <content type="text"><![CDATA[GluonCV文档地址：http&#58;//gluon-cv.mxnet.ioGluonNLP文档地址：http&#58;//gluon-nlp.mxnet.io/自去年以来，MXNet的动态图接口Gluon凭借着它的简单易用、并行效率高和节省显存等特点，成为了非常受欢迎的一个开源工具。 近日，DMLC 发布了简单易用的深度学习工具箱 GluonCV 和 GluonNLP，它们分别为计算机视觉和自然语言处理提供了顶级的算法实现与基本运算。本文简要介绍了这两个工具箱，并提供了基本的使用示例，更多详细的内容请查看它们的原文档。 GluonCV 文档地址：http&#58;//gluon-cv.mxnet.io - GluonNLP 文档地址：http&#58;//gluon-nlp.mxnet.io/GluonNLP 文档地址：http&#58;//gluon-nlp.mxnet.io/ 自去年以来，MXNet 的动态图接口 Gluon 凭借着它的简单易用、并行效率高和节省显存等特点，成为了非常受欢迎的一个开源工具。此外，Gluon 最大的特点就是文档和教程齐全，李沐及 MXNet 团队还发布了一系列「动手学深度学习」的公开课。 GluonCV 和 GluonNLP 继承了 Gluon 的优良传统，它们都能使用简单易用的 API 构建复杂的深度神经网络。此外，这两个项目目前都处于开发的早期阶段，它们的更新频率会比较高。因此，各位读者对该项目的贡献将极大地完善用户体验和工具性能。 GluonCV 项目地址：https&#58;//github.com/dmlc/gluon-cv- GluonNLP 项目地址：https&#58;//github.com/dmlc/gluon-nlpGluonNLP 项目地址：https&#58;//github.com/dmlc/gluon-nlp GluonCV 提供了计算机视觉领域顶级深度学习算法的实现。设计上，GluonCV 是为了帮助工程师、研究人员、学生快速的做出产品原型、验证新思路、学习计算机视觉。 训练脚本从而重现最新论文中的顶级结果； 大量的预训练模型； 细心设计的 API，便于理解实现； 社区支持。 GluonNLP 提供了 NLP 领域顶级深度学习模型的实现，且建立了文本数据管道和模型的模块。设计上，它同样也是为了让工程师、研究员和学生能快速的实现研究思路，做出产品原型。该工具箱提供以下四大特征： 训练脚本来重现研究论文中的顶级结果； 通用 NLP 任务的预训练模型； 仔细设计的 API，极大的减少了实现的复杂性； 社区支持。 安装 安装 MXNET GluonCV 和 GluonNLP 都依赖最新版的 MXNet，最简单的方式是通过 pip 安装 MXNet，运行下面的命令行将安装 CPU 版本的 MXNet。 安装 GluonCV 使用 pip 是安装 GluonCV 最简单的方式： 当然，我们也可以使用 Git 复制 GluonCV 项目并在本地安装： 安装 GluonNLP 同样，通过以下 pip 命令安装 GluonNLP 也是最简单的： 这两个工具目前都提供了案例或教程，但 GluonNLP 假定了用户对深度学习与 NLP 有基础理解，GluonCV 的教程假定用户对深度学习与计算机视觉有基础了解。以下简要展示了这两个工具的使用案例。 以下的案例将使用 20 层的残差网络在 CIFAR10 上从头开始训练，我们这里只展示了模型架构和最优化方法。使用 GluonCV 首先需要导入这个库： 选择模型架构可以简单地从已有模型中导入，以下将从 GluonCV 的模型库中导入用于 CIFAR10 的 20 层残差网络： 而剩下的优化方法及损失函数的配置就可以通过一般的 Gluon 接口完成，这同样也是非常简明和高效的使用方法。 对于 GluonNLP 来说，一般的任务都可以分为加载数据、构建词表、搭建模型和加载词嵌入等。以下将针对这些步骤展示该自然语言处理库的简单使用过程。 首先，以下代码将导入 GluonNLP，并加载 Wikitext-2 数据集： 随后，我们可以根据上面导入的数据集创建词表： 创建词表后，我们就能继续构建神经网络模型。如下将从模型仓库中导入一个标准的 RNN 语言模型，并将其应用到上面加载的数据集上： 最后，加载词嵌入表征就能馈送到模型并进行训练。如下将加载 GloVe 词嵌入表征，它是一种顶级的英语词嵌入方法： 转载来源：资源｜DMLC团队发布GluonCV和GluonNLP：两种简单易用的DL工具箱]]></content>
      <categories>
        <category>科技</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>Word</tag>
        <tag>机器学习</tag>
        <tag>人工智能</tag>
        <tag>Git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[以史为镜 『中兴之困』 与三星半导体30年崛起之路]]></title>
    <url>%2F2018%2Ff0862c95%2F</url>
    <content type="text"><![CDATA[如何避免下一次中兴事件？ 转载来源：以史为镜 『中兴之困』 与三星半导体30年崛起之路]]></content>
      <tags>
        <tag>爱否科技</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[别了美国！华为突然宣布！]]></title>
    <url>%2F2018%2Fe14be960%2F</url>
    <content type="text"><![CDATA[别了美国！华为突然宣布！ 转载来源：别了美国！华为突然宣布！]]></content>
      <tags>
        <tag>CMA</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[作业盒子完成C轮1亿美元融资 自适应学习受资本青睐]]></title>
    <url>%2F2018%2F2ab007b1%2F</url>
    <content type="text"><![CDATA[4月25日，作业盒子正式对外宣布，已于今年2月份完成C轮共计1亿美元融资。本轮融资由云锋基金领投，好未来等机构跟投，泰合资本担任独家财务顾问。 本报讯 记者喻剑报道：4月25日，作业盒子正式对外宣布，已于今年2月份完成C轮共计1亿美元融资，本轮融资由云锋基金领投，好未来等机构跟投，泰合资本担任独家财务顾问。这是作业盒子四个月内完成的第2轮融资，创造了在线教育领域最快的连续融资纪录。此前的2017年10月，作业盒子刚刚宣布完成由贝塔斯曼亚洲投资基金领投，新世界集团、好未来、BV百度风投跟投的2亿元人民币B+轮融资。 120天内完成两轮融资，一方面表明资本对作业盒子AIOC战略的认可和加持，另一方面也印证了教育行业的持续升温。作业盒子创始人兼CEO刘夜表示，“教育”一直是引发社会普遍焦虑的高频话题，国家提倡减负、学校要保证效率效果、家长渴求优质教育资源，这些矛盾背后的根源是“优质教育资源供需之间的不平衡”。作业盒子成立于2014年7月，最早从作业工具切入K12公立学校教学场景，2015年正式面向公立学校师生推出了“作业盒子”系列产品，目前已经构建了从工具到数据再到内容的“教—研—学—辅”完整教育生态。根据作业盒子公布的最新数据显示，截至2018年4月，作业盒子累计注册学生用户已经超过2700万，教师用户超过200万，覆盖了全国31个省市自治区、400多座城市的70000所学校，日均采集学习行为数据超过1亿条，每天活跃用户超过370万。在小学数学领域，作业盒子拥有全网最大规模、最全维度的学生学习数据库，构建了超过30维的学生数据肖像，这也是作业盒子实施其“AIOC战略”最具行业优势的场景和数据基础。 “教育的终局是创造供给，而非搬运供给”，这是作业盒子此前在B+轮融资发布会上提出“AIOC”战略时想要推动解决的问题，即：借助AI等技术手段来大规模地创造教育供给，让每个老师都有他个人的AI助教，也让每个家庭拥有专属的AI老师，把相对重复、繁琐的知识“传递”的工作交由更智慧的机器来完成，解放老师和家长，让他们更专注教育当中情感和精神的传达，让孩子的学习更高效，成长更科学——这也是教育的本质。在作业盒子构建的未来教育场景中，“机器将成为最好的‘老师’，为尽可能多的群体提供优质教育资源”。 记者了解到，自适应算法如今在教育创业领域被广为采用，如论答公司开发了国内第一个以国际顶尖算法为核心的自适应学习系统，主持了国内第一个有关自适应学习有效性的实证研究。为客户提供最前沿的个性化学习解决方案。论答创始人王枫博士说，论答的人工智能学习引擎，相当于名师的大脑。“就像AlphaGo，围棋棋盘有361个格子，每个格子有黑子、白子和不落子三种可能，围棋就有3的361次方种可能，这个量级无法用深蓝那种穷举的方法算出来，只能通过算法优化，这个跟K12解读阶段的知识点是类似的，中考数学有181个知识点，每个知识点有掌握、未掌握两种状态，中考数学就有2的181次方种可能，也必须通过算法来优化。”王枫说，“科技能够在差异化、个性化、精准化方面，为教育提供新动力。比如算法和知识图谱的配合，规划出一条最优学习路径”。 德联资本合伙人贾静表示，中国是一个教育大国，在教育产品选择中，经济成本只占一小部分，更重要的是时间成本、机会成本的考量。如何因材施教，作业、练习题、课程内容根据每个受教育者的特点而设计，是现在教育产品的一个趋势。 转载来源：作业盒子完成C轮1亿美元融资 自适应学习受资本青睐]]></content>
      <categories>
        <category>财经</category>
      </categories>
      <tags>
        <tag>人工智能</tag>
        <tag>在线教育</tag>
        <tag>数学</tag>
        <tag>好未来</tag>
        <tag>云锋基金</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[教你用PyTorch实现“看图说话”（附代码、学习资源）]]></title>
    <url>%2F2018%2Fa36020f3%2F</url>
    <content type="text"><![CDATA[作者：FAIZANSHAIKH翻译：和中华校对：白静本文共2200字，建议阅读10分钟。本文用浅显易懂的方式解释了什么是“看图说话”。 作者：FAIZAN SHAIKH 翻译：和中华 校对：白静 本文共2200**字，建议阅读10分钟**。 本文用浅显易懂的方式解释了什么是“看图说话”(Image Captioning)，借助github上的PyTorch代码带领大家自己做一个模型，并附带了很多相关的学习资源。介绍 深度学习目前是一个非常活跃的领域—每天都会有许多应用出现。进一步学习Deep Learning最好的方法就是亲自动手。尽可能多的接触项目并且尝试自己去做。这将会帮助你更深刻地掌握各个主题，成为一名更好的Deep Learning实践者。 这篇文章将和大家一起看一个有趣的多模态主题，我们将结合图像和文本处理技术来构建一个有用的深度学习应用，即看图说话(Image Captioning)。看图说话是指从一个图像中基于其中的对象和动作生成文本描述的过程。例如： 这种过程在现实生活中有很多潜在的应用场景。一个明显的应用比如保存图片的描述字幕，以便该图片随后可以根据这个描述轻松地被检索出来。 我们开始吧！ 注意： 本文假定你了解深度学习的基础知识，以前曾使用CNN处理过图像问题。如果想复习这些概念，可以先阅读下面的文章： Fundamentals of Deep Learning – Starting with Artificial Neural Network- Architecture of Convolutional Neural Networks (CNNs) demystified- Tutorial&#58; Optimizing Neural Networks using Keras (with Image recognition case study)- Essentials of Deep Learning – Sequence to Sequence modelling with Attention (using python)Architecture of Convolutional Neural Networks (CNNs) demystified Essentials of Deep Learning – Sequence to Sequence modelling with Attention (using python) 目录 什么是Image Captioning问题？- 解决任务的方法- 应用演练- 下一步工作解决任务的方法 下一步工作 什么是Image Captioning问题？ 设想你看到了这张图： 你首先想到的是什么？下面是一些人们可以想到的句子： A man and a girl sit on the ground and eat . （一个男人和一个女孩坐在地上吃东西）A man and a little girl are sitting on a sidewalk near a blue bag eating . （一个男人和一个小女孩坐在蓝色包旁边的人行道上吃东西）A man wearing a black shirt and a little girl wearing an orange dress share a treat .（一个穿黑色衬衣的男人和一个穿橘色连衣裙的小女孩分享美食） 快速看一眼就足以让你理解和描述图片中发生的事情。从一个人造系统中自动生成这种文字描述就是Image Captioning的任务。 该任务很明确，即产生的输出是用一句话来描述这幅图片中的内容—存在的对象，属性，正在发生的动作以及对象之间的互动等。但是与其他图像处理问题一样，在人造系统中再现这种行为也是一项艰巨的任务。因此需要使用像Deep Learning这样先进复杂的技术来解决该任务。 在继续下文之前，我想特别感谢Andrej Kartpathy等学者，他们富有洞察力的课程CS231n帮助我理解了这个主题。 解决任务的方法 可以把image captioning任务在逻辑上分为两个模块——一个是基于图像的模型，从图像中提取特征和细微的差别， 另一个是基于语言的模型，将第一个模型给出的特征和对象翻译成自然的语句。 对于基于图像的模型而言（即编码器）我们通常依靠CNN网络。对于基于语言的模型而言（即解码器），我们依赖RNN网络。下图总结了前面提到的方法： 通常，一个预先训练好的CNN网络从输入图像中提取特征。特征向量被线性转换成与RNN/LSTM网络的输入具有相同的维度。这个网络被训练作为我们特征向量的语言模型。 为了训练LSTM模型，我们预先定义了标签和目标文本。比如，如果字幕是A man and a girl sit on the ground and eat .（一个男人和一个女孩坐在地上吃东西），则我们的标签和目标文本如下&#58; 这样做是为了让模型理解我们标记序列的开始和结束。 具体实现案例 让我们看一个Pytorch中image captioning的简单实现。我们将以一幅图作为输入，然后使用深度学习模型来预测它的描述。 例子的代码可以在GitHub上找到。代码的原始作者是Yunjey Choi 向他杰出的pytorch例子致敬。 在本例中，一个预先训练好的ResNet-152被用作编码器，而解码器是一个LSTM网络。 要运行本例中的代码，你需要安装必备软件，确保有一个可以工作的python环境，最好使用anaconda。然后运行以下命令来安装其他所需要的库。 git clone https&#58;//github.com/pdollar/coco.gitcd coco/PythonAPI/makepython setup.py buildpython setup.py installcd ../../git clone https&#58;//github.com/yunjey/pytorch-tutorial.gitcd pytorch-tutorial/tutorials/03-advanced/image_captioning/pip install -r requirements.txt 设置完系统后，就该下载所需的数据集并且训练模型了。这里我们使用的是MS-COCO数据集。可以运行如下命令来自动下载数据集： chmod +x download.sh./download.sh 现在可以继续并开始模型的构建过程了。首先，你需要处理输入： Search for all the possible words in the dataset and# build a vocabulary listpython build_vocab.py # resize all the images to bring them to shape 224x224python resize.py 现在，运行下面的命令来训练模型： python train.py –num_epochs 10 –learning_rate 0.01 来看一下被封装好的代码中是如何定义模型的，可以在model.py文件中找到： import torchimport torch.nn as nnimport torchvision.models as modelsfrom torch.nn.utils.rnn import pack_padded_sequencefrom torch.autograd import Variable class EncoderCNN(nn.Module)&#58; def init(self, embed_size)&#58; “””Load the pretrained ResNet-152 and replace top fc layer.””” super(EncoderCNN, self).init() resnet = models.resnet152(pretrained=True) modules = list(resnet.children())&#91;&#58;-1&#93; # delete the last fc layer. self.resnet = nn.Sequential(*modules) self.linear = nn.Linear(resnet.fc.in_features, embed_size) self.bn = nn.BatchNorm1d(embed_size, momentum=0.01) self.init_weights() def init_weights(self)&#58; “””Initialize the weights.””” self.linear.weight.data.normal_(0.0, 0.02) self.linear.bias.data.fill_(0) def forward(self, images)&#58; “””Extract the image feature vectors.””” features = self.resnet(images) features = Variable(features.data) features = features.view(features.size(0), -1) features = self.bn(self.linear(features)) return featuresclass DecoderRNN(nn.Module)&#58; def init(self, embed_size, hidden_size, vocab_size, num_layers)&#58; “””Set the hyper-parameters and build the layers.””” super(DecoderRNN, self).init() self.embed = nn.Embedding(vocab_size, embed_size) self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True) self.linear = nn.Linear(hidden_size, vocab_size) self.init_weights() def init_weights(self)&#58; “””Initialize weights.””” self.embed.weight.data.uniform_(-0.1, 0.1) self.linear.weight.data.uniform_(-0.1, 0.1) self.linear.bias.data.fill_(0) def forward(self, features, captions, lengths)&#58; “””Decode image feature vectors and generates captions.””” embeddings = self.embed(captions) embeddings = torch.cat((features.unsqueeze(1), embeddings), 1) packed = pack_padded_sequence(embeddings, lengths, batch_first=True) hiddens, _ = self.lstm(packed) outputs = self.linear(hiddens&#91;0&#93;) return outputs def sample(self, features, states=None)&#58; “””Samples captions for given image features (Greedy search).””” sampled_ids = &#91;&#93; inputs = features.unsqueeze(1) for i in range(20)&#58; # maximum sampling length hiddens, states = self.lstm(inputs, states) # (batch_size, 1, hidden_size), outputs = self.linear(hiddens.squeeze(1)) # (batch_size, vocab_size) predicted = outputs.max(1)&#91;1&#93; sampled_ids.append(predicted) inputs = self.embed(predicted) inputs = inputs.unsqueeze(1) # (batch_size, 1, embed_size) sampled_ids = torch.cat(sampled_ids, 1) # (batch_size, 20) return sampled_ids.squeeze() 现在测试我们的模型： python sample.py –image=’png/example.png’ 对于样例图片，我们的模型给出了这样的输出： a group of giraffes standing in a grassy area . 一群长颈鹿站在草地上 以上就是如何建立一个用于image captioning的深度学习模型。 下一步工作 以上模型只是冰山一角。关于这个主题已经有很多的研究。目前在image captioning领域最先进的模型是微软的CaptionBot。可以在他们的官网上看一个系统的demo. 我列举一些可以用来构建更好的image captioning模型的想法： 加入更多数据当然这也是深度学习模型通常的趋势。提供的数据越多，模型效果越好。可以在这里找到其他的数据集： http&#58;//www.cs.toronto.edu/~fidler/slides/2017/CSC2539/Kaustav_slides.pdf- 使用Attention模型正如这篇文章所述(Essentials of Deep Learning – Sequence to Sequence modelling with Attention),使用attention模型有助于微调模型的性能- 转向更大更好的技术研究人员一直在研究一些技术，比如使用强化学习来构建端到端的深度学习系统，或者使用新颖的attention模型用于“视觉哨兵（visual sentinel）”。使用Attention模型正如这篇文章所述(Essentials of Deep Learning – Sequence to Sequence modelling with Attention),使用attention模型有助于微调模型的性能 结语 这篇文章中，我介绍了image captioning,这是一个多模态任务，它由解密图片和用自然语句描述图片两部分组成。然后我解释了解决该任务用到的方法并给出了一个应用演练。 对于好奇心强的读者，我还列举了几条可以改进模型性能的方法。 希望这篇文章可以激励你去发现更多可以用深度学习解决的任务，从而在工业中出现越来越多的突破和创新。如果有任何建议/反馈，欢迎在下面的评论中留言！ 原文标题：Automatic Image Captioning using Deep Learning (CNN and LSTM) in PyTorch 原文链接：https&#58;//www.analyticsvidhya.com/blog/2018/04/solving-an-image-captioning-task-using-deep-learning/ 译者简介 和中华，留德软件工程硕士。由于对机器学习感兴趣，硕士论文选择了利用遗传算法思想改进传统kmeans。目前在杭州进行大数据相关实践。加入数据派THU希望为IT同行们尽自己一份绵薄之力，也希望结交许多志趣相投的小伙伴。 转载来源：教你用PyTorch实现“看图说话”（附代码、学习资源）]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>编程语言</tag>
        <tag>GitHub</tag>
        <tag>Python</tag>
        <tag>机器学习</tag>
        <tag>Git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[小程序：BAT的下一个新战场]]></title>
    <url>%2F2018%2Fcf711999%2F</url>
    <content type="text"><![CDATA[自从张小龙把应用号改为“小程序”后，关于小程序的产品动向一直令开发者和创业者心驰神往。连独角兽捕手朱啸虎在不同场合表态，“别看区块链了，赶紧去小程序创业啊！ 自从张小龙把应用号改为“小程序”后，关于小程序的产品动向一直令开发者和创业者心驰神往，当前的小程序开发热逐渐有了七年多前的安卓开发的盛况。从做一个App走上人生巅峰，到做一个公众号估值千万，到如今已有爆款小程序、小程序服务平台等拿到一线VC的巨额融资，连独角兽捕手朱啸虎在不同场合表态，“别看区块链了，赶紧去小程序创业啊！” 当我们谈起“小程序”时，在语境一般指代的是“微信小程序”，这有点儿不公允，支付宝在去年也做了小程序，淘宝会员码店铺也像小程序；百度才是最早的小程序鼻祖，从轻应用（lightApp）到直达号再到今年4月重新上线部分企业的小程序，现实很残酷，与用户称呼公众号一般特指微信订阅号一样正常。 微信在基本完成连接“人”（社交）、连接“内容”（社媒）任务之后，已把重心放在连接“产品”（小程序）上，尽管目前主流产品形态还是App，但微信小程序增速非常亮眼，根据QuestMobile报告显示，从2017年1月至2018年3月微信小程序的月活规模超过4亿，在上个月小程序日活用户达1.4亿左右，渗透率达43.9%，还有很大的上升空间。越来越多App开发了小程序版，以致于以后移动产品评测机构做用户数据得做“双份”对比了。 那么，BAT三家之中，为何只有微信小程序具有强大的用户流量以及生态场景能力？在微信生态之中，小程序如何才能存活下来？ 阿里**小程序、百度小程序为什么没有那么火？**本质上说，小程序是以超级App为母App不断孵化出无数多的子App，使得超级App成为新的移动互联网基础设施，当各种需求被不同的小程序满足之后，很容易长出类似苹果应用市场（App Store）那样的生态。所以从微信小程序诞生起，巨头们就很焦虑，阿里担心小程序的社交电商在腾讯体系内形成闭环，百度担心人们搜索需求被小程序搜索满足。 1.阿里**小程序押注“新零售”，**拓展线下场景 马云曾这样评价微信，刚出来时“被吓了一跳”，再后来觉得“仅此而已”，而这几年，微信支付在碎片化支付场景之中对支付宝的冲击显而易见；微信小程序在下很大一盘棋，仅从小程序之中孵化的一个拼多多，就成了腾讯战略冲击淘宝电商垄断地位的又一王牌。 阿里做小程序的孵化基地是支付宝和淘宝，支付宝的小程序更多是在便民属性上工具应用，属于“用完即走”性质。今年淘宝的一个显著变化是不再是纯粹的电商平台，而是线上新零售基地。当前淘宝中已经整合了“淘宝外卖”、“飞猪旅游”等O2O业务，挨着淘宝搜索框的“会员码”实际上是一个向用户附近位置线下门店导流的本地生活服务业务。 在淘宝平台模式之中，要实现复杂的、非标的到店或到家服务场景，就得采用免下载安装包的“小程序”产品形态以更好实现流量闭环，由于淘宝本身就是阿里系产品导流基地，做轻应用有比较强的技术积淀，产品体验并不差。但阿里做小程序也只能打通在小程序上的电商场景，而没有办法丰富其他业态，阿里电商的盈利模式决定了商户依然需要自购广告买平台流量或者返佣。 （淘宝首页本地生活服务类增多，会员码实际上是门店小程序，目前支付宝使用较为高频的共享单车小程序） 2.百度小程序的出路是做信息流广告落地页 百度链接信息的强项更准确的说是链接官网或者站长，这种优势在微信生态之中被公众号和自媒体削弱，接下来企业做官网的趋势可能就是做一个小程序（开发App的成本相对较高），毕竟以前PC端4亿用户每个企业都有官网，和现在微信小程序用户量级差不多。所以，百度再不推小程序，确实就有点晚了。 此前的轻应用没有做起来是由于百度在移动端没有拳头级App带，过去2年百度移动端转型已经完成，2017年手百信息流广告业务以环比20%增速发展，与大搜业务并驾齐驱，是百度长期投入AI技术研发的现金流业务。后续在百度移动端的信息流之中，有望接入一些体验较好的小程序站点，首页向用户Push（推荐）时呈现，无需跳转到至客户的App，使用体验要比H5式的广告落地页要好很多。 笔者发现4月12日百度App的“常用服务”中上线了“优信二手车”小程序，还没有介入首页。 百度小程序的缺憾在于其移动支付市场份额较小，没有办法在自身生态体系内完全打通支付场景；并且或许只有与百度合作的小程序商家才能得到类似“品牌专区”的推荐以及首页的流量推荐，这将制约很多企业开发者进驻的兴致。 （百度小程序目前仅作为常用服务，优信二手车作为样板，体验App化，暂无支付功能） 长尾小程序如何在微信“去中心化的产品生态”中生存？目前阿里小程序、百度小程序均无法与微信小程序相抗衡，更多只是出于战略防守的目的，并且其他移动端产品也不可能再做小程序了，自身的流量不够。 微信小程序独特禀赋在于：用户基数、打开高频度、用户粘性第一，拥有真实的个人账户和社交关系链，微信支付用的人越来越多，而最为难得是微信试图培育出的“去中心化”的小程序生态，而非是中心化流量入口进行系统分配。微信会让用户自主选择小程序，微信首页下拉呈现是用户常用的小程序，便于让用户养成使用习惯。 阿星认为，微信小程序也是为腾讯的商业模式服务的，但创业者大可以放心，正如微信营建公众号庞大生态，它本身并不需要从某个公众号中抽佣或者收费，腾讯本身靠游戏、投资拥有丰裕的利润，腾讯无意与小程序运营者争利。它反而更需要所有用户、企业、产品等在腾讯自己的生态玩下去，这样才有源源不断的“流量”为其商业模式提供肥沃的土壤。 很多创业者和企业已经敏锐觉察到，微信小程序正复制公众号成功的逻辑，很快小程序也会成为企业的标配，正如2000多万个公众号中真正能够生存下来极少数，而小程序运营者痛点也与微信定位的“去中心化产品生态”息息有关：- 目前头部小程序很多是腾讯自己开发或投资项目，很多创业者会担心的我的用户留存在微信，数据在微信上跑，哪天封号怎么办。- 如何结合自身的业务逻辑设计出体验度不比App差的工具，依然需要行业解决方案，市面上的小程序开发商大多良莠不齐。- 小程序不能像公众号那样向用户推送消息，如何唤醒留存用户，提升转化率成为运营难题。而去中心化产品生态的好处在于这些小程序的痛点会微信第三方服务商加以解决，他们本身既是与微信有着深度合作的开发者，熟悉小程序游戏规则，又是其他微信小程序创业者的服务平台，因而在小程序风口成为资本市场投资的重点。 笔者发现此前做公众号服务商的平台已经转型做小程序平台，他们与传统中小企业打交道的多，更了解他们的痛点和需求，并且小程序的开发与此前的公众号微商城成熟经验可以很好的结合，4月份微盟完成10.09亿元D1轮融资，可见资本市场对于小程序企业服务（to B业务）的看好。 移动互联网从流量上更像“微信互联网”，小程序是微信目前主推的用户交易、交互产品，微盟创始人孙涛勇“小程序+”概念或许更容易理解张小龙小程序“连接万物”的野心，小程序可以加官网、加公众号、加电商、加门店、加广告，这么多的玩法没有开发和运营经验的企业单独去玩，可能没法充分释放其“全渠道”价值，另外如何遵守微信相对复杂、变动的产品规则也得有小程序服务商引导，而小程序服务平台崛起满足了这种市场刚需。 而微信小程序服务平台，最重要的不是技术，微信底层技术已经比较简易化了，真正的壁垒是具备不同垂直行业的运营经验积累，只做少数小程序案例的服务商很难应付，小程序的红利就是眼前，但是真正会玩的、能抓到仍然是少数，这种两年左右的红利窗口期试错成本高昂，使得小程序服务平台容易出现“头部化”的马太效益。 提供小程序应用服务是一门技术活，比如做微电商、智慧餐厅、智慧外卖、智慧美业等不同行业的小程序产品逻辑肯定不一样，在不同场景做交易、官网、门店或者会务小程序的交互也不一样。此前帮助不同行业创业者和企业做微网站、微分销的团队对业务理解优势凸显了，笔者认为，这是资本市场看好微盟成为中长尾企业主做小程序开发和运营的赋能者的重要原因。 尽管小程序要超过50个流量入口，要解决小程序推广难的问题，还得依靠“公众号+小程序”组合拳的办法，公众号粉丝留存度高、但入口比较深、打开率低，小程序入口多、使用可以高频，长尾中小企业用户如果能发挥出公众号存量和小程序增量优势，可以节约很多推广成本、并丰富小程序的推广玩法，比如公众号插入小程序，二维码设计粉丝的分配机制，拼多多式的社交+拼团模式，铂涛酒店式的微信卡券营销、礼物说式的送礼等方式可以借鉴和模仿，“专业人做专业事”，阿星相信，目前小程序服务商已经掌握了足够多运营工具和解决方案。 张小龙说小程序不是为电商准备的，意思是小程序在内容、社交、门店、工具等多个应用场景都可以广泛应用，或许内容电商、社交电商、粉丝营销以及本地生活化服务可以通过小程序真正引爆。 当腾讯成为中小企业连接线上线下的“赋能者”的时候，腾讯才会最终与阿里拉开市值差距。 结语小程序会成为巨头争夺的新战场，阿里和百度之所以没有办法与微信小程序分庭抗礼，主要与其商业模式、在连接用户多元化需求以及中心化流量入口产品思维有关，据了解，微信小程序在2018年商业化进程会加快，这是移动互联网创业者流量红利，也是线下门店和企业拥抱微信互联网机会，如何在微信复杂流量宇宙里生存下来，必须借鉴专业微信生态服务商，而第三方服务平台能否把小程序电商及营销能量输出给长尾中小创业者，是微信小程序生态能否繁荣重要角色。（本文首发于钛媒体） 【钛媒体作者：靠谱的阿星（李星），公众号：靠谱的阿星，靠谱汇创始人、科技媒体专栏作家，CMO训练营认证导师，获2017年钛媒体年度作者「最具人气奖」，个人微信即QQ号：1598145405】 更多精彩内容，关注钛媒体微信号（ID：taimeiti），或者下载钛媒体App 转载来源：小程序：BAT的下一个新战场]]></content>
      <categories>
        <category>科技</category>
      </categories>
      <tags>
        <tag>软件</tag>
        <tag>移动互联网</tag>
        <tag>电子商务</tag>
        <tag>支付宝</tag>
        <tag>移动支付</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Yann LeCun：距离“真正的” AI，我们还缺什么？]]></title>
    <url>%2F2018%2Fa693f421%2F</url>
    <content type="text"><![CDATA[他讲述了关于深度学习的最新研究成果，同时也描述了深度学习的未来，以及机器智能所要面临的的挑战。训练监督学习模型需要向它展示各种例子，并告诉它正确答案。 【AI科技大本营导读】今天是 GMIC Beijing 2018 大会第一天，首个演讲者是 Facebook 首席 AI 科学家 Yann LeCun。他讲述了关于深度学习的最新研究成果，同时也描述了深度学习的未来，以及机器智能所要面临的的挑战。 # ▌监督学习无可替代 如今的 AI 系统都是使用的监督学习，所有的 AI 应用，不管是图像识别、声音识别还是人脸识别，或者机器翻译等等，这些都是监督学习的应用。训练监督学习模型需要向它展示各种例子，并告诉它正确答案，如果你想让机器学会将汽车和飞机区分开来，比如你给它展示一辆车的图像，它说这不是一辆车，然后你可以对参数进行调整，下次再向机器展示同一张图像的话，你就会得到接近正确的答案。 我们可以对机器进行端到端的训练，来完成特定的任务，feeding 原始的 inputs，就会自动给出 outputs。机器学习这个任务的过程是端到端的学习过程。通过这种方式机器，计算机能更好地了解这个世界。 比如卷积网络，实际上这个想法是可以回溯到上个世纪八十年代。它可识别图像，同时也有很多其他的应用，比如说可以用于语言处理、语言识别和其他很多的应用。我们知道对于神经网络是非常大的，只有在非常强大的计算机上才可以运用，需要有 GPU 加以辅助。 在深度学习变得比较普遍之前，我们首先要确保这样的一些系统可以用于这些情况，比如一个例子是我们在 2009 年、2010 年在纽约大学合作的一个实验，可以看到它可以识别马路上的建筑、天空以及路上的车和人等等，这个在当时并没有被称为最好的系统。再过几年之后，越来越多人相信深度学习是可以奏效的，可以发挥作用的。 在这里大家可以看到在网络当中使用的几个层，比如说有 100 层或者 180 层的一些人工神经网络，在 Facebook 当中我们就会广泛使用。这上面大家可以看到错误率是在不断下降的，有的时候表现的甚至要比人还要好。它的性能非常好，已经成为了一种标杆。 这是 Facebook 人工智能部门所做的研究，叫做 Mask R-CNN，可以看到它的结果，它可以标记这样的图像，就像我刚才给大家展示的例子，展示出非常好的性能。它不仅仅可以识别出每个人，同时它会为每个人加一个标记，所以可以很容易区分出是一个人还是一只狗。 在这里大家可以看到这个系统可以识别电脑、酒杯、人、桌子，也可以数出来到底有多少，而且也可以识别出道路、汽车。如果五年之前问系统这些问题的话，我们当时可能认为需要 10-20 年时间才能达到今天呈现的效果。 这也是 Facebook 所做的一些研究，叫做 Detectron。大家可以下载上面的代码，它可以探测 200 多种不同的类别，这也是 Facebook 在 AI 方面的一些研究，我们不仅仅发布了一些论文，同时连代码也都发布出来了，这样的话世界各地都可以更好的熟知这种技术。 当然还包括其他很多项目，在 Facebook 我们利用 DensePose 这样的技术，预测人类的行为。我们现在有一个系统能够实时的运行，在一个单一的 GPU 上运行。它可以跟踪很多人的行为，生成视频，非常的准确，可以实时地生成一些相应的数据和信息，并且相应的代码也是可以用的，这些都是一些最新的应用。 当然利用的这样的技术不仅仅可以进行识别图像，面部识别，也可以识别人的行动，也可以用来翻译，这是 Facebook 在加州所做的研究（FairSeq）。我们可以用这个系统进来行翻译的工作。 我觉得对于行业说进行这样的开发研究将是会是一个非常有用的过程，同时我们也希望自己所开发的技术能够引导整个社区，解决我们所感兴趣的问题。我们认为 AI 不仅仅会帮助我们解决问题，同时还会帮助我们解决很多人类自己无法解决的挑战，所以我们会与科学团队一起朝这方面努力。 这里是在过去的几年里，FAIR 所发布的一些开源项目，包括像深度学习网络，还有深度学习框架等等。 我刚才讲到每天都会有一些新的应用发布，而深度学习的广泛应用也进一步推动科学方面的研究。在接下来几年里深度学习会发生更大的革命。 接下来为大家举一个例子，这段视频表现出来的是一种加速过程，它可以训练车去进行驾驶，而且可以调整车轮的方向。这样可以让车自己去进行驾驶，而不需要有人去进行校正。 # ▌可微分编程 接下来我们再来看一下可微分编程，这个编程可以用人工神经网络解释。 编者注：程序员不写代码，或者仅写出少量 high-level 的代码，但是提供大量输入数据 X 与对应运算结果 Y 的例子。神经网络根据提供的数据集，自动学出从输入数据X到最终运算结果 Y 的映射（既整个程序）；或者结合程序员提供的 high-level 的代码，用神经网络作为中间函数，补全得到整个程序。来源：知乎&#64;殷鹏程（https&#58;//www.zhihu.com/question/265173352/answer/291994649） 我们通过研究可以实现这样的一种编程，可以利用这样的系统或者培训系统，来完成某一个具体的任务。 这是几年前所开展的工作，是由 Facebook 和纽约大学一起合作做的项目。这个项目是训练模型，让它能够回答相关的问题。在自然语言处理过程中，也可以看到人工神经网络是动态的，在不断变化的。 这是另外一个例子，如果你要建立一个能够回答复杂问题的系统，比如说关于图像的复杂问题等。为了回答这个图片是不是有更多的立体形状，之后我们就会让系统来进行计算。比如说这里有多少是方形体，或者有多少颜色，最后告诉你具体的答案是什么。通过这么做我们可以建立起一个端到端的解答的途径，而且也允许你提出更多新的问题。根据你输入的数据不同，它会有所变化。 大家看到这里是我们最近开发所得到的一些深度学习最新的成就，之后我们来看一下关于 AI 有没有我们触及到的。 ▌机器学习需要常识 对于新技术，我觉得可以进入到更多的领域，比如进行更多的影像分析。在一定程度上，我们觉得机器可能确实拥有一定的人工智能，但具体细节上，我们还需要进行更多探讨。 比如在机器学习方面，我们怎么做呢？在这儿可以看到有一些具体的图像，我们有些新的方法。在实际的生活当中其实这种方式不太成功，因为关于深度学习方面我们要进行深入的挖掘，因为对于机器本身它会有不同的解决方案，比如在实际生活中是不能够去实施的。 有时候让机器学习很长时间才能玩游戏。所以在核心功能方面，现在确实还没有触及到。但这些机器是能做到的，只是我们还没有挖掘出来。我们也可以对机器本身进行更深入的训练，比如我们要让系统进行成千上万次的训练之后，它们才能够进行学习。 有些学习它是与力学相关的，但是在实际的生活当中不可能实时进行，所以我们只能够进行模拟，但它也需要我们进行很多的尝试才能够让机器学到。 婴儿们是怎么学习的呢？比如就像右下角这幅图向他们所展示的，六个月以下的婴儿可能不太了解物理运动，可当他们满了八个月之后，他们已经知道自由落体这个动作了。 所以像右下角的这个小女孩非常了不起，我的一位朋友她给我们展示了婴儿怎么学会一些概念，而且他们也能够了解一些最基本的物理原理，这是他们在生活中最初学到的一些概念，这是凭借人们常识获得的，婴儿们所学会的是就是一些常识。 另外我们向动物展示这样的情景，比如大家看一下这个大猩猩，它们在幼年的时候由培训员给它们进行展示一些东西，所以大猩猩面对这样的魔术会笑出来，而人们会把这当做世界最初的原型。 但我们希望机器能够建立一些样本，使得系统运行，最终机器就能进行一些预测，像人一样有效运行。我们有这样的监督或者学习就能够使得机器得到训练和规划，这是我们所需要建立的一个系统。 不管下次的变革出现在哪里，我觉得它们应该是自我监督或者无监督学习，而且在这样的变革当中也会出现一些常识性的学习。 我总结一下，这是我们最近做的一些非常有意义的事情，这是一些预测性的模型，来由机器进行规划，根据它们的尝试进行预测。 我们进行了对抗性训练，比如说我们可以训练机器来了解哪个分项是更可能的，或者在实际生活中会产生什么样的结果。对于发生的可能性它也会来做出预测，可能有的时候有的结果是虚假的，不是真实的。通过这么做我们就能够得到不同机器产生的结果，之后得到了很多的影像和图片。 我们的系统在进行训练之后，生成了一系列的人脸，大家看看这些名人的面孔，里面有一些是假的图像，是由机器生成的，但看起来是真实的。 我们将在下周会议上向大家展示最新的结果，得到的成果非常好。总之，我们希望把这个工具之后能够融入到我们的机器学习当中。 最后，我想做一下总结，我觉得监督学习是不能够被替代的，不管是无监督学习还是其他的学习方式都不能够替代它，这点已经引起了很多人的兴趣，我们也要进行更多尝试。还有一点我需要强调的是，我们要让机器能够推理，来看深深度学习能带给我们什么样的推理能力，同时也要了解在AI时代，机器的推理能力有多高，逻辑性有多强。 接下来我们也要来朝着可微分编程的智能学习的方向持续发展，这就需要进行做更多对抗性训练的研究。当然，还会出现更多的有关深度学习的变革，比如一些多渠道发展或者是复杂的架构，在这个领域也会出现更多的理论。 关于技术监督的趋势很显然是不断的弱化，甚至监督会消失，这就会导致出现一些新理论的产生，比如新语言，或者是一些新的并行文本，我相信之后应该有多维度的可能性。可能会出现一些新框架，包括了一些动态影像。我们会和微软，和亚马逊会进行更多合作，我们也会不断进行开源。 当然，现在我们的工作量很大，但是关于我们的移动工具和其他工具越来越流行了，Facebook 的用户他们每天能够推出大概 20 亿张不同的影像，所以我们希望能充分发挥这方面的能力，它可能是一种很强的驱动力。另外，我们也要不断强化硬件，以使用户需求能够得到专业化的处理。 转载来源：Yann LeCun：距离“真正的” AI，我们还缺什么？]]></content>
      <categories>
        <category>其它</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>机器学习</tag>
        <tag>人工智能</tag>
        <tag>GPU</tag>
        <tag>Facebook</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[俄罗斯的新战略：三分欧亚大陆]]></title>
    <url>%2F2018%2F5185426b%2F</url>
    <content type="text"><![CDATA[俄罗斯在融入西方全球政治秩序失败后，似乎迅速找到了新的定位：欧亚大陆的能源中心。莫斯科为此提出了“三分欧亚大陆”的战略：北极和俄罗斯、里海和中东地区，构成中部能源生产区。 转载来源：俄罗斯的新战略：三分欧亚大陆]]></content>
  </entry>
  <entry>
    <title><![CDATA[百行征信与中国征信的未来]]></title>
    <url>%2F2018%2F90135fb4%2F</url>
    <content type="text"><![CDATA[就金融领域而言，征信系统可以说是金融信息的高速公路，各种金融信贷平台，可以理解成各种交通工具，风控就是驾驶技术。 【财新网】（专栏作家 刘新海）作为市场经济的产物，征信系统对于经济的健康发展和金融市场的稳定的重要性不言而喻，就金融领域而言，征信系统可以说是金融信息的高速公路，各种金融信贷平台，可以理解成各种交通工具，风控就是驾驶技术。但是如果没有一个好的征信系统做基础，再好的驾驶技术也是没有意义。 2017年4月21日，央行征信管理局举行个人信息保护的国际学术研讨会，宣布全国八家个人征信机构没有一家合格，给征信业带来很大的冲击，震撼至今。时隔一年，中国征信领域发生一些变化，引起社会的关注，本文将围绕一些重要的征信现象简单来讨论一下中国的征信问题，并展望一下中国征信的未来。 央行征信系统：非赢利、非市场化的国家基础数据库 中国征信体系的起步是从2006年设立央行征信中心（维护央行征信系统）开始，从征信机构、征信监管者、数据提供商、征信用户、数据主体这些角度来看，形成基本的中国征信行业格局，如图一所示。 图一 中国征信体系（2006-2017） 央行征信中心是2018年之前国内唯一的一个官方征信机构，央行征信系统的名称是国家金融信用信息基础数据库。虽然定位为国家级的基础数据库，但央行征信中心实质上就是肩负着为市场化信贷机构提供征信服务的征信机构。 央行征信中心具有非常独特的一面，它既具有个人征信系统和企业征信系统功能，还承担央行信贷登记系统（服务于央行监管）的作用。目前它的征信数据覆盖约四亿消费者和两千多万企业的信贷记录，提供的主要征信产品有（个人和企业）的信用报告，个人信用报告数字解读（实际上就是个人信用评分）和企业关联关系查询。 央行征信中心在成立之初，在很短时间内就把全国银行的信贷数据进行采集和整合，在解决处理信贷金融风险问题上发挥了重要作用，大幅度降低银行信贷的不良率，同时促进了国民经济发展，清华大学课题小组研究显示，央行征信系统促进了GDP0.33%的增长。 同时，央行征信中心也存在一些问题，有的与金融市场环境有关，有的是自身体制的问题。因为央行征信中心是一个非赢利，非市场化的国家事业单位，而不是独立的商业机构。 首先是整个金融市场服务不完善，金融信贷服务供给不足，2/3的消费者和大量小微企业无法从信贷机构获得贷款，导致央行征信系统覆盖的人群和企业有限。 其次与金融机构监管范围有关系，央行征信中心服务的对象直接受到监管的金融机构，即仅限于持牌的金融机关，对于没有受到监管的新金融机构，央行征信中心按照监管规定，无法为其提供服务。 再次是自身的体制问题，非赢利、非市场化的定位，导致其无法高效率地运行，提供更多的征信服务。深层次的原因是央行征信系统作为国家金融信用信息基础数据库主要任务是采集征信数据和生成最基本的征信产品-信用报告，期望其能够像美国的三大个人征信机构（盈利性的，市场化运作）一样每年能研发出上百个征信产品也的确是勉为其难。 征信“大跃进”（2015-2017）：互联网金融泡沫的衍生品 从2015年到2017年，可以称为是征信大跃进的三年。征信属于相对比较小众的行业领域，在中国引起前所未有的关注度，无论欧美发达国家还是东南亚等新兴国家，从来没有哪个国家的征信能这么大幅度的引起社会大众关注、促使许多新金融机构、大数据公司、互联网公司积极参与。在这三年里，各种背景的征信机构纷纷成立，大量的人力、物力和资本投入，所以称之为征信大跃进。 这种现象的发生并非偶然，首先和国内的互联网金融与大数据泡沫密切相关，在“征信行业躺着赚钱”的社会舆论刺激下，以及“先开枪，再瞄准”的互联网思维引导下，大量的新兴机构在不了解征信商业本质的情况下涌入征信行业寻找商机。其次是因为互联网金融和消费金融的兴起，这些新金融机构需要征信服务，央行征信中心或者由于覆盖人群不足没法给他们提供直接服务的，或者提供的服务不够。 另外一方面，虽然国内只有1/3的消费者有传统信贷数据，但进入大数据和互联网经济时代，消费者有丰富的其它非信贷信用数据或者其它信用相关数据。将央行征信系统、三大电信运营商、微信、支付宝和京东等消费者的数据进行比较，就会发现三大电信运营商和微信的活跃用户远远超过央行征信系统覆盖的信贷人群，这些替代数据是可以用来辅助信用分析。 在上述因素的推动下，央行监管层面对市场化征信开始有所松动。首先从个人征信来说，央行在2015年1月5日通知八家个人征信机构准备征信工作，掀起征信建设高潮的序幕。其中，大量的互联网，大数据等高科技公司和新兴的金融机构都要开展征信业务，特别是个人征信，这也是非常具有中国特色和时代背景的独特现象。 其次从企业征信来讲，目前国内约有130家企业征信机构备案，但这些机构都是比较弱小的，而且商业模式单一。而整个社会，有约50万家公司的名称中包含征信的字眼，积极涉足征信业。 同时征信公司也获得资本市场的热捧，甚至还出现一些证券分析师推出的“中国征信第一股”的咄咄怪事。 这种全社会的征信大跃进式行业投入存在着明显的弊端。首先大量机构涌入征信领域有着很大的盲目性，浪费了大量的人力和大量的资金。其次导致“征信”概念的混乱，造成了征信概念满天飞。征信与风控混淆不清，很多人理解的征信其实是风控，而且“一切数据皆信用”，只要有点数据都想着开始做征信，征信机构变得没有门槛了。 为什么征信大跃进这个泡沫灭掉了？主要原因是金融监管的加强，特别是互联网金融的整治的不断展开和深化。同时征信行业的专业性和积累性的特点使这些机构的大量投入不能很快地变现 征信大跃进的背后，是我们对征信本质的理解欠缺。征信系统建设需要时间的积累，需要前期大量地投入，需要和先进的信息技术进行整合，而且具有一定的专业门槛，并非适合许多企业参与，并非能够短期见效。 对征信大跃进这种现象值得深入地反思，个人征信产品是有公共属性的商业化产品，那么民营征信机构如何保证公信力？如何做一个有责任的公司？如何保证消费者的权益？保证公平正义，赢得监管层、传统金融机构的信任？ 百行征信启航：未来新金融的基石，任重道远 百行征信（信联），这是一个最近比较热的话题。百行征信作为第一家拿到个人征信牌照的机构，引起国内外的广泛关注，国外的主流媒体像美国华尔街日报，英国的金融时报，和国内的许多权威媒体连续追踪报道，对百行征信进行解读和展望。甚至还有很多大数据公司和金融科技公司也想加入这个百行征信。 百行征信有它的明显的优势，首先是具有一定的社会公信力，因为它有央行背景，这一点对于开展具有公共属性的个人征信服务来说，这种得天独厚的优势，是民营征信机构所无法比拟的。其次互联网金融领域的征信服务目前基本上是空白，百行征信的未来业务开展有很大的空间。同时百行征信的企业定位，也会在市场化的道路上走得远一点。 但百行征信的未来仍存在很多挑战：一是能否制定长效的商业机制，例如不完全利用央行的行政权力把数据搜集上来，用商业的力量让不太成熟的新金融机构积极参与信息共享。二是能否充分利用市场化的激励机制，让征信系统更加有效率，能够及时开发出一些更加满足实际需要的产品。三是结合大数据、人工智能和区块链等先进的信息技术做一些创新，满足未来飞速发展的中国互联网经济的需要。 图二 中国（个人）征信体系（2018） 2018年之后随着百行征信的启航，中国的征信体系发生了真正的变化，如图二所示。可以看出，新成立的百行征信在覆盖信贷人群和新金融机构上将作出重要贡献，但是能不能成为一个有效率的征信体系，还需要时间的验证，还有待观察。建成一个高效的征信系统并非一件容易的事情，不是简单地把数据搜集整合成信用报告，而是需要很长时间的技术和业务积累，需要大量的征信产品研发投入，更重要的是商业游戏规则和激励机制的制定。 对于百行征信的未来，既不能因为其天然的优势期望过高，也不能因为其面临的挑战而过度悲观，作为互联网经济时代的重要基础设施，大家应该群策群力，使其更快更好地服务社会，造福我们每一个消费者。 未来中国征信：继续探索，继续完善，寻找市场和政府的平衡点 纵观现实，目前的征信服务和快速发展的互联网经济是不匹配的，不仅缺乏丰富的征信产品和服务，甚至还没有向全社会提供类似美国FICO评分一样可以用于基本自动化信用决策的信用评分，因此中国征信格局并未成型，还没有走向成熟，还需要继续探索。特别是（个人）征信产品是一个半公共属性的商业产品，需要政府的监管和市场的参与，兼顾公平和效率。美国作为全球的征信最发达的国家，就充分发挥了市场和政府的作用。我国的特点是强势的政府，活跃的市场，如何把这两方面有效地结合起来，双轮驱动中国征信体系建设？由于经济发展阶段等原因，感觉目前还需要继续寻找征信体系建设中政府和市场的最佳平衡点，未来可能会继续博弈。 未来个人信息保护对个人征信冲击也会比较大。相比于欧美发达国家，中国征信或者是风控的机会在于，一是强劲的市场需求，二是相对宽松的个人信息保护政策。但是目前似乎有种趋势，某些（个人信息保护）政策研究人员，没有充分考虑消费者的实际需求，不考虑大数据企业真正运营的情况，计划全盘引进欧盟最新出台的个人数据保护措施（GDPR）。过于苛严和不切实际的个人信息保护，对个人征信，对风控都是很大的挑战。 企业征信继续面临挑战。国内目前注册了将近130多个企业征信公司，相比个人征信来说，企业征信的特点是业务非标性，信息维度比较高，信用评估情况也比较复杂。目前企业征信的基本商业模式还存在很大的挑战，即使央行有很多的企业征信数据，但是企业征信的产品仍很难推出来，因为业务理解和产品开发难度还是相当大。 另一方面，国内的征信也面临一些新机遇。虽然国内消费者缺乏传统的信贷数据，但是有很多丰富的非信贷信用数据，或者信用相关的数据。比如电信数据、支付数据、社交数据、电商数据和心理测量数据等。过去几年，互联网金融公司也开始开发和利用这些非信贷类信用信息进行风控，但是每个公司都要自己投入，数据太分散而且成本很高。其实可以从征信角度来看这个问题，通过一些专业化的机构把数据整合，提供一些通用的基础产品，降低风控成本，提高效率。服务于不同消费场景的专业征信机构也是未来值得探索的方向。 作者曾提出非金融征信平台的政策建议。电信数据不仅能反映消费者的经济活跃性，也有“先用后买”的征信含义在里面。根据国内目前的情况，有必要建立一个基于电信运营商数据的征信平台，不仅可以解决电信运营商内部的信用风险问题，也可以对外输出，作为一个基础的征信平台给信贷机构提供服务，而且，可以还为很多“先用后买”的互联网经济提供征信服务。 图三 基于电信运营商数据的征信平台 未来征信行业和社会信用体系建设会产生一些联系。从2003年开始的社会信用体系是中国特有的，征信体系目前是作为社会信用体系的一部分。政府希望通过信用管理的一些机制用于政府管理和社会治理，范围更广，不限于金融领域，2020年的目标是初步建成中国的社会信用体系建设。未来的社会信用体系建设，和征信继续会有交叉、联系，但两者还是有明显的区别。前面提到的50万个征信公司，2017年4月21日之后，部分公司改名了，把征信改为信用，向社会信用体系建设靠拢。 中国征信业的未来充满了挑战，也面临着机遇。面对现实，中国的征信服务还是比较滞后，中国的征信业还未走向成熟。但是创新、专业精神以及政府与市场的平衡将为中国征信业的未来发展注入活力。■ 作者为某大型金融机构副研究员、《征信与大数据》作者。北京大数据研究院李铭博士对本文亦有贡献 转载来源：百行征信与中国征信的未来]]></content>
      <categories>
        <category>财经</category>
      </categories>
      <tags>
        <tag>经济</tag>
        <tag>金融</tag>
        <tag>大数据</tag>
        <tag>运营商</tag>
        <tag>中国人民银行</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[鸿茅药酒发布企业自查整改报告:已停播全部广告]]></title>
    <url>%2F2018%2F6c31b234%2F</url>
    <content type="text"><![CDATA[今天（4月26日）下午，鸿茅药酒在其官方账号发布企业自查整改报告，其中表示，目前已停播全部广告，并表示认识到鸿茅药酒在近五年的广告投放中存在广告投放量大、下游经销商和零售药店广告违规等问题。对于社会关注的安全性、豹骨来源及生产质量等问题，鸿茅药酒表示&quot;豹骨&quot;的购买及使用符合法律法规，并称&quot;按照药品说明书的用法用量使用鸿茅药酒是安全&quot;。 新京报快讯 今日(26日)下午，鸿茅药酒生产方，内蒙古鸿茅国药股份有限公司发布企业自查报告，面向社会公众致歉。 以下为自查报告全文： 内蒙古鸿茅国药股份有限公司(以下简称“鸿茅国药”)，作为鸿茅药酒市场与安全的责任主体，按照原国家食品药品监督管理总局、国家药品监督管理局和内蒙古自治区食品药品监督管理局的要求，认真查找自身问题，强化质量安全，加强风险控制，规范生产经营，坚决做到对消费者负责。 在近期鸿茅药酒事件的舆情发生后，我公司没有认真研判风险，主动发声，主动处置，主动回应社会关切，导致舆情进一步扩大。由此，给社会各界带来的问题和影响，我公司高度重视并向社会真诚道歉，恳请公众谅解。 本次事件，对于鸿茅国药是一次深刻教训。我公司清醒地认识到，作为一家药品生产企业，必须承担主体责任，对产品全生命周期的安全性和有效性负全部责任;承担不良反应直报和对药品质量持续提升的任务，以更高质量的产品和服务保障消费者的安全和健康。 我公司正在认真组织自查，并积极配合国家相关职能部门的检查，现将截至目前的自查及整改情况向社会各界通报如下： 一、对于社会高度关注的鸿茅药酒安全性、豹骨来源及生产质量等问题的说明 1.鸿茅药酒组方中的“制附子、制半夏、制天南星”都是炮制品,按照药品说明书要求的日服用量，折算成药材剂量，上述药材日用量均在药典规定的安全用量范围之内。因此，按照药品说明书的用法用量使用鸿茅药酒是安全的。 鸿茅药酒组方中制附子和制半夏同用在中成药制剂中具有普遍性。 2.针对鸿茅药酒的安全性，我公司主动开展过毒性试验、主要药效学研究和临床试验，研究和试验结论是安全有效的。根据不良反应中心数据，2004年至2018年2月底，鸿茅药酒共有不良反应报告137例，不良反应主要表现为头晕、瘙痒、皮疹、呕吐、腹痛等。 3.鸿茅药酒中“豹骨”的购买及使用符合法律法规。生产过程中，物料平衡符合要求。并在2007年启动了鸿茅药酒中去豹骨研究工作并完成了药效学对比研究实验。 4.我公司始终严格按照《药品生产质量管理规范》的要求进行全面管理。在生产质量管理过程中，不断完善生产质量管理保证体系，保证产品质量稳定。在历次产品监督抽检中，未出现产品不合格情况。今后，我公司会持续按照GMP标准要求组织生产，承担不良反应直报和产品质量持续提升的主体责任。 今后，我公司将进一步加强药品安全性的风险评估，主动监测药品风险，并对鸿茅药酒持续进行安全性和有效性研究，守好“品质关”和“用药安全关”，更好地对消费者负责。 二、关于社会公众及媒体关注的鸿茅药酒广告问题的情况说明 我公司经过严格自查及深刻反省，认识到鸿茅药酒在近五年的广告投放中存在广告投放量大、下游经销商和零售药店广告违规等问题;企业经营管理过度依赖广告、广告发布管理存在漏洞等问题。我公司对由此引发的媒体争议、社会批评，负有不可推卸的主体责任。 1.近五年来，我公司作为广告投放主体，发布的广告平台为中央电视台、中央人民广播电台、各卫视及地方媒体。经认真自查，广告发布过程均依法合规。但鸿茅药酒的全国各地经销商和零售药店存在违规发布广告的问题。通过查询近五年的《违法药品医疗器械保健食品广告公告》，涉及鸿茅药酒的公告共计17个，其中2015年9月1日新《广告法》颁布之后的公告有4个。 2.关于 “鸿茅药酒祝您：每天两口 健康长寿”广告语的创意，是因为鸿茅药酒产品说明书中的用法用量为：口服一次15毫升，一日2次。因药酒剂型特殊，有酒的特性，容易出现消费者超过规定用量服用的情况，所以广告语表述为“每天两口”，是特意提醒消费者每天服用量不要超过“两口”，以保证用药安全;“祝您健康长寿”，则是一句祝福语。为避免歧义，2017年12月，内蒙古自治区食品药品监督管理局按照原国家食品药品监督管理总局有关部门的要求，及时与我公司约谈，要求企业对凡是含有有可能引起误解、歧义、争议广告用语的广告全部改版、尽快停播。我公司已按要求停播有关广告，并交回了相关广告批文。 2018年改版后播出的广告内容特意增加“鸿茅药酒提示您，购买药品前请阅读说明书”的安全用药公益性宣传，同时也明确了鸿茅药酒是药品的属性。 3.近年来，鸿茅药酒饱受侵权产品、假冒产品的侵害，我公司从未发布过“喝鸿茅百病消”、“鸿茅药酒包治百病”等广告内容，也从来没有宣称过“鸿茅药酒能够治疗高血压、糖尿病”，对此我公司将进一步调查其发布主体及来源，并进行追责。 目前，为了消除不良影响，承担企业主体责任，我公司采取了一系列从严整改措施。现已停播中央电视台、中央人民广播电台、各卫视及地方媒体的全部广告;同时派出检查小组，深入全国31个省市自治区开展自查自纠工作;从速建立了从企业到零售终端的完整、严格、可控的广告宣传管理链条;对假冒鸿茅商标或商号的违法经营行为进行维权;积极宣传药品用药安全;承担社会责任、关爱患者健康、大力参与和推动社会公益活动。 作者：王煜 原标题：鸿茅药酒生产方发布自查报告 向公众致歉 转载来源：鸿茅药酒发布企业自查整改报告:已停播全部广告]]></content>
  </entry>
  <entry>
    <title><![CDATA[美国司法部调查华为违法制裁令 华为将步中兴后尘休克]]></title>
    <url>%2F2018%2F7ce32885%2F</url>
    <content type="text"><![CDATA[据华尔街日报4月25日最新报道，知情人士透露，美国司法部正在调查华为是否违反了美国的制裁禁令，和伊朗进行了贸易。 报道称，目前还不清楚美国政府的调查具体指向哪个方面，华为也拒绝对此事作出任何置评。 但是华尔街日报认为，调查将极大的增加华为所面临的风险，目前，华为在美国的贸易已经遭到了诸多限制，如果形成了连锁反应，华为的海外生意将遭到重创，尤其是欧洲地区。另外，因为美国政府的调查，华为在其他部分重要国家的合作伙伴已经开始变得越发警惕。 在中兴遭遇制裁之后，很多网友对华为和中兴一个顶上天，一个踩到底，甚至给中兴冠以买办的名目，认为华为有了麒麟所以不怕制裁。 但实际上，这个看法是值得商榷的。 诚然，由于华为的执行力更强，资金实力更加雄厚，在布局上更具前瞻性。在芯自给自足的比例上要比中兴更高，但也还是要大量进口芯片。像CPU、FPGA、RF、数模/模数等芯片也是大量进口。 根据工信部下属机构的报告显示 华为的芯片采购总量不逊色于联想，高达140亿美元左右，其中，采购高通芯片18亿美元，采购英特尔芯片6.8亿美元，采购镁光芯片5.8亿美元，采购博通芯片6亿美元，采购赛灵思芯片5.6亿美元，采购Cypress/Spansion芯片5.4亿美元，采购Skyworks和Qorvo芯片各4.5亿美元，采购德州仪器芯片近4亿美元…… 可以说，由于美国在半导体产业的强大，在CPU、GPU、FPGA、DSP、基带芯片、射频芯片、高端交换路由芯片、高速接口芯片，以及数模转换芯片、电源管理芯片、光模块等核心元器件方面占据绝对优势，使中国华为、中兴、联想、步步高、小米等整机厂高度依赖美国元器件。 即便是华为海思能够自己设计的芯片，其实也有很多是高度依赖国外授权的，一旦遭遇制裁就悲剧了。 以华为最负盛名的麒麟芯片为例。 华为海思麒麟芯片基于ARM的技术授权，从2009年的K3到2017年的麒麟970，取得了骄人业绩。但在技术上是反复购买ARM的CPU核与GPU核。一旦特朗普政府制裁华为，就会遭遇釜底抽薪的困局。 不要以为传说中孙正义祖先来自中国就会对华为网开一面，毕竟ARM主要研发中心在美国，且美国在日本和韩国都有驻军，在这种情况下，韩裔日本人孙正义只能唯特朗普马首是瞻。 之前制裁中兴的时候ARM也一并参与了制裁。也就是说，海思麒麟其实是建立在沙滩上的洋房，虽然非常漂亮，但根本经不住外部的风吹雨打。 事实上，中兴也有自己的芯片设计单位——中兴微电子，在2017年实现营业收入达75亿美元，同比增长约30%。而且中兴已实现 2G 和 3G 基带芯片和数字中频芯片的自主配套，包括多模软基带芯片、数字中频芯片、中低端光交换芯片、中低端路由芯片、中低端交换芯片、 3G 基带芯片等。但依旧在美国的制裁下不堪一击，原因就在于整个产业不如人。 即便是爱立信、诺基亚等国外大厂，其实也是无法依靠自己的力量包打天下，在核心元器件上也是依赖全球采购。华为中兴在很多核心元器件受制于人，不能把责任全部推给他们。真正的问题在于中国的半导体产业技术不如人。 网络舆论一个现象就是一下吹上天，一下踩到底。即便华为未来遭遇制裁休克，也不会改变华为是一家高科技公司的事实。 毕竟华为和中兴是通讯公司，不是半导体公司，两家公司在通讯领域有很深的技术积累。 就中兴来说，中兴是全球第四的通信厂商，在通信行业的技术积累非常深厚。在国际专利体系（PCT）中，连续8年申请量全球前三。截止2017年12月31日，中兴通讯拥有6.9万余件全球专利资产、已授权专利资产超过3万件。其中，4G LTE 标准必要专利超过815件，全球占比超过13%；5G战略布局专利全球超过2000件。 华为的实力和底蕴还在中兴之上，两家企业都是在通信领域有较深厚技术积累的企业，经不住美国制裁的根本原因，在于中美半导体产业的差距，而非华为中兴在通信技术上的不足。 最后，希望华为和中兴都能度过难关，并在今后更多的扶持本土供应商。 就像李国杰院士说的，国产元器件并非一定要追平国外厂商后才能用，作为深受政策扶持成长壮大起来的大企业，必须有一定担当，做出一定牺牲扶持国内供应商。这一方面是“先富带后富”，承担社会责任；另一方面，也是给自己留一条后路，避免在今后重蹈覆辙。 转载来源：美国司法部调查华为违法制裁令 华为将步中兴后尘休克]]></content>
      <categories>
        <category>科技</category>
      </categories>
      <tags>
        <tag>ARM</tag>
        <tag>CPU</tag>
        <tag>通信</tag>
        <tag>华为海思</tag>
        <tag>中兴</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度长文：反思互联网的黑暗世界]]></title>
    <url>%2F2018%2F443dcd60%2F</url>
    <content type="text"><![CDATA[本文作者James Bridle在“Something is wrong on the internet”一文中从YouTube上供孩子们观看的视频入手，剖析了互联网通过自动化、规模化框架所产生的一个黑暗世界。 文/木木子 互联网真的可能毒害下一代。 编者按：反思是必要行为，越及时越好，越清醒越好。本文作者James Bridle在“Something is wrong on the internet”一文中从YouTube上供孩子们观看的视频入手，剖析了互联网通过自动化、规模化框架所产生的一个黑暗世界。而重要的是，我们该如何行动。 我随着互联网成长，确信它是对于今天我之所以为我的最重要影响之一。我从13岁开始上网，当时电脑就在我的卧室里。它让我有机会接触到了许多完全不适合青少年所了解的东西，但是这都还好。互联网一直以我认为的于己有益的方式对我的身份产生至关重要的影响，它塑造了我的人际关系、文化与政治。我一直是互联网及其所带来的一切影响的重要支持者，并明显认为它是有益的且具有解放意义。在一开始说明这一点是因为在下文我将描述互联网所产生的影响以一种意味深长的方式驳斥了我之前的假设和偏见。 我经常问自己的一个假设性问题是，如今我对于自己的孩子用同样方式上网有什么感受。我发现这个问题越来越难回答。我明白这是随着年龄增长而自然发生的演变，而且某种程度上这个问题的假设性因素越来越少。我不想在这个问题上成为伪君子。我希望我的孩子拥有与我一样去探索、成长和表达他们自己的机会。我希望他们拥有这个选择。这种信念放松了人们对互联网在公共生活中扮演角色所持有的态度。 我也意识到，在这些时间里年轻孩子与YouTube之间的共生关系与日俱增。我看到孩子们无论是在婴儿车里，还是在餐馆里，都整天关注着屏幕，同时伴随着一种类似于勒德分子（Luddite，十九世纪初英国手工业工人中参加捣毁机器的人，现泛指强烈反对机械化或自动化的人）的阵痛。但我不会为他人或者是他人父母做出判断。我也看到家人和朋友的孩子投入地看《小猪佩奇》和童谣视频，这使他们变得开心并给每个人带来放松的机会，所以这也还好。 我至今没有孩子，而现在只想一把火把所有东西烧掉。 一些人、一些事抑或是一些人与事的结合体正以自动且大规模的方式通过YouTube系统性的恐吓、伤害和虐待儿童，这迫使我从任何角度质疑自己对于互联网的看法。接下来，我要说的大部分内容已经在其他地方报道过了，虽然在我所看的主流报道中没有一个能真正理解那些似乎正在发生事情的含义。 我将由此开始：专为孩子开辟的YouTube内容清楚且引人注目地充满怪异。我意识到它的古怪已经有一段时间了。去年，有许多文章都提到了令人吃惊的惊喜蛋（Surprise Egg）热潮。惊喜蛋的相关视频通常以一种极其痛苦的长度展示了打开金德和其他玩具蛋的过程。就是这么回事，但是孩子们对此着迷。即使没有数百万，也有成千上万的孩子们在观看成千上万这类视频。 自2010年以来，惊喜蛋的制作者已经累积了370万订阅用户，这个孩子友好频道仅仅致力于打开惊喜蛋和拆箱玩具就达到低于60亿的浏览量。视频标题是一种模糊品牌线和搭卖广告的模式：“Blu Toys Surprise Brinquedos &amp; Juegos”“Cars Screamin’ Banshee Eats Lightning McQueen Disney Pixar”“Disney Baby Pop Up Pals Easter Eggs SURPRISE”。 在我写这篇文章的时候，他一共制作了4426个视频。与贾斯汀·比伯官方频道超100亿的浏览量相比，全职YouTube名人PewDiePie拥有近120亿的浏览量，就好像这个男人用一双温柔地打开惊喜蛋的双手为生。（惊喜蛋视频都配有前贴片广告、中断广告和其他广告形式。） 这让你了解到孩子们的在线视频世界是多么怪异，而这一系列视频标题则暗示了这一情况所涉及的不同寻常的范围和复杂程度。我们马上就会讲到后者。但是，就目前而言它已经显得非常奇怪。 另一种尤其对于最小的孩子而言很受欢迎的巨量内容，是童谣视频。 制作这种视频的Little Baby Bum在YouTube最受欢迎频道排名第7。他们共产出515个视频，拥有1150万订阅用户和130亿浏览量。同样，我很快就会说道这些数据的准确性也存在问题，在这里我想说明这是一个庞大的网络产业和世界。 点播视频对于父母和孩子来说都是“猫薄荷”，对于内容创作者和广告商来说也是如此。小一些的孩子无论是出于熟悉的人物还是歌曲，或者是简单的色彩和舒缓的声音而被这些视频迷住。这些视频的长度（一个常见的处理方法是将许多童谣或卡通片段汇编在一起+编辑）和将视频长度作为吸引点而成为营销手段的一部分，指向了那些孩子们与这些视频一起度过的时间。 因此，YouTube广播公司想出大量策略来吸引家长和孩子们的注意力，并获得伴随这些行为而生的广告收入。第一个策略是复制和剽窃其他内容。以我的尝试为例，在YouTube上搜索“小猪佩奇”会收到“大约10400000个搜索结果”，而首页内容几乎全部来自经过认证的“小猪佩奇官方频道”，同时有一个视频来自于未经认证的频道，叫做Play Go Toys，除非你刻意寻找它否则很难注意到。 据我猜测，Play Go Toys频道里的《小猪佩奇》和其他卡通片、玩具拆箱是盗版的，此外还有一些我认为应该是频道主自己孩子的视频。我并不是要说Play Go Toys有什么不好；我只想简单说明YouTube的框架以怎样的方式导致内容与创作者的分离，而这将如何影响我们对于信源的认识和信任行为。 正如另一位博客作者所指出的，品牌内容所扮演的传统角色之一是其信源可信性。无论是儿童电视上播放的《小猪佩奇》还是迪士尼电影，不管人们对于这种娱乐生产的工业模式有什么感受，这些视频通过精心制作和受到控制而让孩子们能以基本安全的方式观看它们，并且被赋予信任。而这不再适应于品牌和内容被平台分离的情况，已知的可信内容为那些未经认证和潜在有害内容提供了无缝衔接的网关。 （同样，这与Facebook和谷歌上对可信新闻媒体的分离过程完全相同，而这些结果正在对我们的认知和政治体系产生严重破坏。我不打算在这里进一步探讨这种关系，但是它显然也非常重要。） 增加视频点击率的第二种方法是关联关键字或是主题标签，这是一种全然黑暗的艺术。当某种内容，比如惊喜蛋视频，成为趋势而能够抵达绝对多的大众时，内容生产者就会蜂拥而至，制作出成千上万类似的视频。这就是上面所提到的那些奇怪名字产生的缘由：官方内容、童谣标题和“惊喜蛋”都被塞进了同一个单词中以出现在搜索结果、侧边栏、以及“下一个”自动排名的位置中。 一个奇怪的例子是Finger Family视频，我不知道他们从哪里冒出来，也不知道这些童谣旋律的起源。但是目前YouTube上至少有1700万个版本，它们也涵盖了包括上百亿拼接场景在内的所有可能类型。 再说一次，必须认真对待视频的浏览量情况。其中大多数视频本质上是由机器人所制作，被机器人所观看，甚至被机器人所评论。这是一个完全陌生的世界。而不应该掩盖的事实是，实际上还有许多孩子通过iPhone或者是平板电脑一遍又一遍地观看着，这在某种程度上解释了这些视频的浏览量数值。孩子们学着在浏览框中输入基本搜索词，或者是在侧边栏中简单添加另一个视频。 令我觉得有些不安的是，即使是相对正常的儿童视频在网上的扩散也无法解释其自动化的程度，如何剖析人与机器之间的差距呢？有近200万订阅者的Bounce Patrol Kids频道显示了这种影响。其内容有专门的真人演员表演，以每周一个的制作速度发布专业制作视频。 有一群人在无休止地演绎着由算法生成的关键字组合的意义所促使的行为，这很奇怪：“Halloween Finger Family &amp; more Halloween Songs for Children | Kids Halloween Songs Collection”, “Australian Animals Finger Family Song | Finger Family Nursery Rhymes”, “Farm Animals Finger Family and more Animals Songs | Finger Family Collection - Learn Animals Sounds”,还有很多很多。这是算法搜索时代的内容制作模式，即使你是人类，也必须去迎合机器。 其他视频也会与人类演员一起，不断制作出无限可重构的相同视频版本。当然，自动化在其中扮演重要角色。各类库存、音频轨道和这些关键字列表以数千种方式拼接在一起源源不断制作出视频。上图中所展示的频道，Videogyan 3D Rhymes — Nursery Rhymes &amp; Baby Songs通过组合这些日益复杂的关键词，以每周几次的频率发布视频内容。他们有近500万订阅者，是Bounce Patrol订阅者数量的二倍不止，然而我们依然不知道是谁或者是什么累积起了这些数以百万计的浏览量。 我试图不让这篇文章被没完没了的例子所充斥，但是我们需要了解这个体系有多阔大、以及是什么决定了此种行动、过程和观众，这很重要。这也涉及国际事宜：有许多Finger Family和Learn Colours视频的不同版本，如Tamil epics和Malaysian cartoons它们不太可能出现在任何英语使用者的搜索结果中。这种极其不确定性和搜索行为是其存在和产生影响的关键。这种广延性让这种行为捉摸不定，甚至难以真正对其形成思考。 我们已经看到了那些确之凿凿的例子，用以说明完全自动化所带来的令人不安的结果，谢天谢地是其中一些已经经由黑色幽默所发酵，而另有一些还没有。从T恤到咖啡杯，从婴儿睡衣到手机壳，许多都是由图片库和按需制作的算法构建而成。上图中最近在Amazon上售卖的物品就是一个例子，它如何产生的故事既迷人又怪异，但从本质而言可以理解。没有人打算用药物和医疗设备制作手机壳，这可能只是一个非常怪异的数字或概率结果。但是，这个需要一段时间才能被人们注意到的事实敲响了警钟。 同样，“冷静地强奸”（Keep Calm and Rape A Lot）T恤以及“冷静被对她持刀”和“冷静地打她”的结果令人沮丧且痛苦，但是可以理解。没有人打算制造这样的T恤，它们只是由一个未经核查的动词和代词列表与一个在线生成器所配对。这些T恤相当有可能从没有出现过、曾经被买过或是穿过，这样就不会造成任何伤害。但是又一次，制作这个内容的人没有注意到，分销商们也没有注意到。他们根本不知道自己在做什么。 基于这些案例以及那些我将要进一步说明的案例，我想说的是，这个系统的规模和逻辑是产生如此结果的同谋，我们需要好好思考它们的含义。 （再次说明，我不打算深入探讨这些内容除这篇文章之外的更广泛的社会影响，但是很明显，人们可以从这些例子中找到一条解决大数据和机器智能驱动系统中存在的种族和性别偏见等当代问题的明显界限，这些问题需要迫切的关注但是我们却没有任何简单的或者是更好的解决方法。） 我们可以在这些成堆视频中选择看一段，然后试着分析它来自哪里。需要重点强调，我并没有打算去找某个特定视频：在一个匿名浏览器窗口（例如，不应该受之前搜索行为的影响）中搜索“finger family”，它的排名很高。这种自动化把我们带到了非常非常奇观的情况中，“兔子洞”如此之深以至于我们不知道这样的东西如何得以形成。 这个视频标题为“Wrong Heads Disney Wrong Ears Wrong Legs Kids Learn Colors Finger Family 2017 Nursery Rhymes”。标题本身就证实了它来自于自动化合成。我不知道为什么有“Wrong Heads”这样的表达，但是可以猜想，就像“Finger Family Song”一样，它通过与Learn Colors,、Finger Family和Nursery Rhymes这些词汇拼接渐渐提高自己的算法排名，这最终形成了我们所看到的这个标题。 这段视频由一个普通的Finger Family歌曲组成，画面中呈现出一个由迪士尼《阿拉丁》动画中的角色头部和身体拼接而成人物。这的确很怪异，但是坦白讲，并不比惊喜蛋视频或其他童谣视频奇怪到哪去。但是我发现这种想法太天真了。一个非《阿拉丁》动画的角色艾格尼斯出现了，她是《卑鄙的我》中的人物。 这段视频的制作者BABYFUN TV已经制作了许多类似的视频。《头脑特工队》的角色Hope与蓝精灵和食人妖交换头部。BABYFUN TV只有170位订阅者和非常低的浏览量，但是平台上有成千上万个这样的频道。长尾数字在抽象情况下并不能显示出重要性，重要的是它们所实际产生的累积效果。 所以问题变成：这些是怎么来的？在BABYFUN TV的频道中也出现“Bad Baby”的修辞。虽然我觉得令人不安，但是我能理解它是如何通过提供一些旋律、节奏或与他们自己经历相关的东西而使得真正的小婴儿被这些内容所吸引，虽然它经由算法的反复与重组而以一种没有人会想要让其发生的情况所扭曲和拉扯。 Toy Freaks频道截图 Toy Freaks是一个非常受欢迎的频道（平台排名第68位），主角是一位父亲和两个女儿。除了学习儿歌和辨认颜色之外，Toy Freaks擅长于制作一些令人恶心的内容，以及令许多观众都觉得自己受到虐待和剥削的行为，其中还不算特别严重的情节包括儿童呕吐和疼痛的视频。Toy Freaks是一个经YouTube认证的频道，无论这意味着什么。 就像Bounce Patrol Kids一样，无论你对这些视频内容有什么感觉，我们不可能知道自动化涉入的起点与抽身的终点，不可能知道谁来提出这些想法以及谁在扮演这些角色。相反，在如Toy Freaks这类受欢迎的、由人类主导的频道中，其所产生的影响使得它们通过越来越古怪且扭曲的重新组合方式在网络中不断出现。 而有些视频比Toy Freaks和与其类似的内容更令人不安。下面是一个相对温和，但依旧令人不安的例子： 比之前所提到的盗版《小猪佩奇》更甚，还存在一种山寨品。这些内容也充斥着暴力。在官方版本中，佩奇得到了和蔼牙医的适当安慰。而从上图的对比版本中看出，她受到了折磨。搜索“peppa pig dentist”会在首页上发现这些视频，而这些让事情变得更糟。（此处引用视频已经被YouTube删除，但是平台上仍有许多类似视频。） 《小猪佩奇》这些令人不安的视频内容，包括佩奇吃自己的父亲和喝漂白粉在内，隐含着极端暴力和恐惧因素却被广为传播。它们是整个YouTube亚文化的一部分。 这段视频以对于佩奇的拙劣模仿开始，后来进入到我们已经见过的那种情节的不断重复中。也许这就是钓鱼行为（Troll，最初作为网络用语用来描述在公共论坛等讨论区故意用激烈言辞引起别人进行没有意义的争论的行为，后来释义延伸为几乎所有做出令人厌恶举止的行为，不论主动还是被动的）。我希望它是。但是我不这样认为。“钓鱼”并不能解释这些人类参与者的交集之处和更多自动化的例子。它在其中显现，却不是故事的全部。 我想，不去深思熟虑这个问题是幼稚的行为，但是有如此多像牙医情节一样未认证的发布，许多孩子们正在观看它。我知道大多数内容并不是想把孩子弄得一团糟，但是它们确实会带来如此效果。 我尝试去理解原因是什么，这并不单单是对于“不会有人想到孩子”的束手无策。显而易见，这些内容不合适。显而易见，有一些不好的演员。显而易见，有些视频应该被删除。显而易见，这引出了关于合理使用、挪用、言论自由等问题。但是那些仅仅通过镜头内容了解问题的报告并没有完全理解正在发生的机制，因此也就无法从整体上考虑其影响，并作出相应回应。 《纽约时报》所写的关于这些问题的文章“On YouTube Kids, Startling Videos Slip Past Filters”强调了那些令人不安的视频中的假冒人物和童谣现象，并将其放置在适度性和立法的问题上面。官方应用YouTube Kids声称其内容对于儿童来说是安全的，但显然不是如此，这即是问题所在，因为它误用了用户的信任。英国小报《太阳报》的一篇文章“Kids left traumatised after sick YouTube clips showing Peppa Pig characters with knives and guns appear on app for children”也采用同样思路，又增加了右翼技术恐惧和自以为是的分析。但是这两篇文章都是从表面上评价了YouTube声称这些结果非常罕见且很快被删除的行为。 Good Baby Toys频道 这是亚洲地区制作的Toy Freaks的基本情况。还有俄罗斯的。我不想再用“人主导”这个词来形容这些视频，尽管这些内容包含了相同的修辞和实际的人类角色。我想不明白到底发生了什么，我认为这就是问题的关键。这就是我为什么开始深思熟虑所发生的一切的部分原因。我们需要作出许多努力。谁在写这些脚本，谁在编辑这些视频？再一次，我想强调：这与许多东西相比仍然是相对温和，甚至是有趣的东西。 有些事情令我不安： 第一件事情是恐怖和暴力被展示的程度。互联网提供一种放大且促进我们潜在欲望的方式；事实上，这是它看起来最擅长的。我花了很长时间来论证这种倾向，关于人类的性自由、个人身份和其他问题。而在这里，绝大多数人有时会觉得，这种倾向本身就有一种暴力和破坏性的倾向。 第二件事情是被剥削的程度。不是因为他们是孩子而成为孩子，而是因为他们的无权利性而成为孩子。像YouTube算法这样的自动奖励系统必定以资本主义的剥削方式进行剥削。如果你对于对等式的后半部分感到恼火，也许这就是让你信服真相的原因。剥削被编码到我们正在建构的系统中，使其更难被发现、更难于对其进行思考和解释、更难于对抗和防御。这不是工厂由机器人构成以及人工智能统治一切的未来，而是此刻、现在，在你的屏幕上，在你的起居室里，在你的口袋里。 其中部分例子都试图证明没有人真的在看这些视频，都是机器人在背后操作。即使人类只出现在这个系统的生产方面，我也对他们表示担心。 这段视频“BURIED ALIVE Outdoor Playground Finger Family Song Nursery Rhymes Animation Education Learning Video”里包含了我们上面所提到的所有元素，并产生比之前更严重的影响。熟悉的人物角色、童谣、关键词、完全自动化、暴力、以及孩子们最糟糕的梦想。当然，这种视频大量存在着。一个频道接着一个频道，以每周数百个新视频的更新速度进行播放。以工业化的水平生产噩梦。 最后：也有比这些更暴力和含有更多性内容的东西。我并不打算给出链接。我不相信它会给别人带来精神上的创伤，但是有必须要继续强调，不要忽视这些黑暗的、怪异的对成人没有多少干扰的东西对于孩子的影响。 一位从事于数字视频的朋友向我描述了制作这些内容需要些什么：一个小型工作室（可能由6人，或者更多人组成）通过大量制造低质量内容而满足这个系统的某些要求（长度似乎是其中一个因素）来获得广告收入。根据这位朋友的说法，供儿童观看的内容是3D动画为数不多的赚钱方法之一，因其审美标准较低并可以通过大规模的独立生产而获利。它使用现有的且易于获得的内容（如人物模型和动作捕捉库），通过不断重复和修改而完成制作。它可以不具有任何意义，因为算法不会对此歧视，孩子们也不会。 这些视频，无论是在哪里制作，无论是如何制作，无论是否有明确目（例如，积累广告收入）都在以有意识向孩子们展示视频的方式获取利润。而不自觉的涌现出来的后果到处都是。 让孩子暴露在这种内容之中是虐待。我们讨论的不是电影或是视频游戏暴力对青少年存在争议却毋庸置疑的影响，也不是色情或极端形象对于青少年的影响，这属于那种我在开篇所描述的青少年时期使用互联网影响。这是重要且值得辩论的内容，但是却不是我在此想要讨论的东西。我们所谈论的是非常年幼的孩子，从出生开始，成为会对他们造成伤害和干扰的目标之物，而互联网则提供了这种进行虐待的极其容易的方式。这不仅关于“钓鱼”问题，而是内在暴力与数字系统和资本主义激励的结合。 我的观点是：这个系统是虐待行为的同谋。 此时、此地，YouTube和谷歌在这个系统中沆瀣一气。他们为了从在线视频中获取最大利益而建立起的框架正在被虐待儿童的不确定分子所入侵，也许这不是故意行为，但是却以大规模的方式存在着。我认为他们需要担负起解决这个问题的绝对责任，正如他们有责任去处理那些意图政治说服的极端视频对年轻人的激进化问题所产生的影响。到目前为止，他们完全没有表现出这种倾向，而这本身就是卑鄙行为。然而，我对于这个议题想做出的最大回应是，我不知道他们如何在不关闭服务本身以及其他类似系统的情况下做出解释。我们建立了一个规模化运作的世界，在那里，人类的监督无法触及到每个角落，也没有任何非人形式的监督能够发现我在本文中所提到的大部分例子。 这是一个非常黑暗的时代，我们为服务于自我而建立起的框架正在以系统且自动化的方式转而对付我们，我们所有人。当网络制造出恐怖的时候，我们很难对网络保有信心。尽管人们很容易将这些更为疯狂的例子视为“钓鱼”行为，肯定有相当数量的人是这样认为，但是这并不能解释那些特别怪诞的内容的绝对数量。就像越来越多人关注俄罗斯对社交媒体的干涉一样，它所带来的纷繁复杂的危险被用作加强控制、增加审核等行为的理由。这并不是我们想要的结果。 而我想说的是： 尽管对于孩子所遭受的暴力问题让我十分担忧，但是我所担忧的不仅仅是这些。我所担忧的是对于这种在任何时间对于我们所有人产生影响的基础设施暴力，我们依然在努力寻找谈论它、描述它的机制、它的行为和它的影响的方式。正如我在开篇所说：这是由人、由事、由人与事的结合所导致。对于这种结果所需要承担的责任无法合理被分配，但是伤害是如此、如此的真实。 原文链接：https&#58;//medium.com/&#64;jamesbridle/something-is-wrong-on-the-internet-c39c471271d2编译组出品。编辑：郝鹏程 转载来源：深度长文：反思互联网的黑暗世界]]></content>
      <categories>
        <category>育儿</category>
      </categories>
      <tags>
        <tag>Google</tag>
        <tag>人工智能</tag>
        <tag>英国</tag>
        <tag>YouTube</tag>
        <tag>玩具</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[极端图像压缩的生成对抗网络，可生成低码率的高质量图像]]></title>
    <url>%2F2018%2Ff69244ef%2F</url>
    <content type="text"><![CDATA[本文提出了一个基于生成对抗网络的极端学习图像压缩框架，能生成码率更低但视觉效果更好的图像。此外，该框架可以根据原始图像的语义标签映射，在解码图像中完全合成非主要的区域。 极端图像压缩的生成对抗网络，可生成低码率的高质量图像 转载来源：极端图像压缩的生成对抗网络，可生成低码率的高质量图像]]></content>
  </entry>
  <entry>
    <title><![CDATA[三年半后，快播王欣拾起来的仍是当年的“区块链”]]></title>
    <url>%2F2018%2F28e4f56a%2F</url>
    <content type="text"><![CDATA[2014年4月22日晚，王欣被警察带走，称被举报涉嫌传播淫秽色情信息，再之后就是法庭上那些曾经刷爆朋友圈的桥段。 提到快播，众多宅男都会心一笑，想起无数个夜晚通过网络种子看过的那些不可描述。 而这个视频软件的缔造者王欣，也被称为中国“最有种的男人”。 2014年4月22日晚，王欣被警察带走，称被举报涉嫌传播淫秽色情信息，再之后就是法庭上那些曾经刷爆朋友圈的桥段。 狱中三年半，外面的世界已经被区块链的浪潮包围。 不过，P2P、流量矿石、互动币，以及王欣狱中的一系列动作都表明，他带头的“快播系”并未错过区块链。 惊艳出场的P2P 王欣出狱后，据传与小鹏汽车创始人何小鹏、 58 同城CEO姚劲波等人畅谈AI、视频、区块链等技术的发展，何小鹏更是表示“他思维完全和大家在一起”。 其实这也在意料之中，尽管在狱中三年多，但区块链对于王欣来说恐怕不是什么新鲜事物，因为他以前玩过的P2P对等网络技术就是区块链系统的重要基石。 2003年，王欣23岁，当时他是“盛大盒子”研发团队的一员。这个花费了陈天桥许多心血的项目，最终没有在市场上获得成功，而王欣也决定另谋出路，和其他技术牛人一样，走上了创业之路。这也导致了之后快播的诞生。 快播巅峰时期拥有5.3亿用户，是国内市占率第一的播放器。而其最吸引用户的点是让电影和游戏的下载变得更容易：用户在线观看电影的同时，该文件同时也被静悄悄地下载下来了，这个功能受到众多用户的追捧。 不知道快播的用户在享受“边看边下”带来的快捷时，是否有思考其背后的技术？其实这种快捷的下载方式就是使用的P2P技术。 P2P又被叫做对等互联网技术，这种技术最大的特点是：它并没有与各计算机相连的中央服务器，只有一套允许数据共享的端对端协议。它依靠的是网络中各个节点的计算能力和带宽，而不是依赖较少的几台服务器。 更形象的来说，一般情况下，如果我们要在网络上下载一部资源，通常用我们的电脑从提供资源的中央服务器上获取，是有一个中心化的组织存在。但在P2P模式下，只要这个网络中的任何节点里有这个资源，我们都可以下载下来，反过来我们下载的资源也可能成为其他网友获取的对象。 区块链网络系统和P2P理念拥有高度默契感，区块链的出发点之一是去中心化，所有的交易都是点对点进行，每个节点都是公平的；而P2P网络的天然属性，就是全网节点平等，无特殊节点。 可以看出，王欣在创办快播并引用P2P技术的时候，他的思维理念已经和区块链不谋而合。 “骗局”流量矿石 早在2013年，快播原科技产品总监黄胜就接触过比特币，在对区块链进行研究后，他和王欣共同发起了流量矿石项目。 快播出事半年后，流量矿石原团队新成立公司云帆科技，流量矿石背后真正的团队正是云帆加速科技有限公司。 对于王欣，黄胜也曾表示团队将邀请他以“原项目发起人、投资人”的角色回归，希望能够在他的支持下，把项目做好。 其实流量矿石平台本质是一种利用共享经济模式玩转CDN（内容分发网络）的云计算方案，让闲置的资源重新恢复它应有的价值。 具体怎么玩？其实它主要汇集个人手持设备、家庭带宽、企业节点等众多闲散的带宽和CPU资源，通过云计算的方式将实时的部署利用达到最优化，这样可以帮助各大视频网站解决带宽不足的问题。 流量矿石团队声称：“我们的使命是通过区块链与网络加速技术，打造一个世界级的去中心化共享CDN网络生态。” 但这种说法本身似乎是自相矛盾：既然流量矿石致力于“汇集个人手持设备、家庭带宽、企业节点、IDC机房等众多闲散的带宽和CPU资源”，那怎么会是一个去中心化的网络生态呢？ 这里要提一下快播的老对手迅雷。 去年，迅雷发布了一款产品“玩客云“，它的玩法和流量矿石类似，核心都是往CDN模式发展，收集用户闲散的宽带资源。玩法也都是卖硬件给用户挖矿，然后通过代币给奖励。 然而，玩客币可以回购，但流量矿石却不支持回购。挖到的矿石，其团队自己不收购，要通过会员充值进来的钱来购买，并且最终还是只能消费在平台上。这也是被许多用户诟病为流量矿石是一场骗局的原因所在。 根据最新的信通院行业研究报告，全球2010-2015年间的CDN市场复合年增长率为27.7%，2016年达到60.5亿美元，预计2020年将达到157.3亿美元，这无疑是一块很大的蛋糕。但流量矿石这一步棋没有走好。 反人性互动币 1月8日，流量矿石宝盒—一款前快播团队，同时也是流量矿石的兄弟团队开发的产品在苏宁开启预约，在1小时内出现了火爆的一幕：这款售价599元的产品被25万人疯抢。而一款名为互动链的项目随后也被推出，其宣称致力于用区块链技术撬动应用流量分发市场。 其官网宣称，互动链应用开发者的目标不再是创造用户价值，而是鼓励用户从应用开发者提供的服务中创造自身需要的价值，从而带动产品价值的提升。 事实上，所谓的互动链有点像游戏里面的“做任务”：开发者可在基于区块链的任务中心上设定好流量任务，在任务发布时根据人群属性的不同进行投放。投放完毕后，用户可以领取系统匹配好的任务，完成任务后智能合约就会触发给予奖励互动币HDT。 也就是说，站在开发者的角度，在互动链上他们掌握了更多的互动性：不用去捉摸如何为用户带来价值，而是让用户在自己提供的任务里面创造自己需要的价值。 但这本身是一种反人性的理念：用户本身是抵触学习，抵触创造的，如何让用户自己由被动变为主动不是一件简单的事情。 【结束语】 王欣，这个宣称“技术无罪”的男人曾赢得了无数技术同行的支持喝彩，当他出狱后，众多曾经的粉丝高喊“我们欠你一个快播币”。然而，如今的互联网格局毕竟已经不能和三年前相比，区块链能否为王欣打一场翻身仗仍有待时间考验。 转载来源：三年半后，快播王欣拾起来的仍是当年的“区块链”]]></content>
      <categories>
        <category>科技</category>
      </categories>
      <tags>
        <tag>软件</tag>
        <tag>区块链</tag>
        <tag>云计算</tag>
        <tag>快播</tag>
        <tag>迅雷</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[徐明星的OKEX涉嫌“非法交易”与“诈骗”全调查 | 钛媒体深度]]></title>
    <url>%2F2018%2F092b1e23%2F</url>
    <content type="text"><![CDATA[声讨这家公司和其背后的控制人：徐明星，国内比特币交易所OKCoin创始人，以及OK系主要公司OKEX实际控制人。 图片来源&#64;视觉中国 钛媒体注：本文来自钛媒体集团旗下区块链资讯与数据平台“链得得”。 赴京维权、上门理论、服毒威胁、集体报案、众人上访，过去的两个月间，络绎不绝的数字货币投资人聚集在OKCoin位于北京海淀区上地的总部办公地，声讨这家公司和其背后的控制人：徐明星，国内比特币交易所OKCoin创始人，以及OK系主要公司OKEX实际控制人，如今外出时常有保镖陪护。 前赴后继的维权行动，看似在一场场徐明星的反击声明和“投资者应该愿赌服输”的争辩中变成了口水战，投资者到底有没有责任？OKEX到底是什么性质的平台？中国的法律边界到底在哪里？为何徐明星会突然来那一场“献给国家”的言论，到底孰是孰非，围绕徐明星的迷雾始终没有散开，链得得研究团队在过去的一个多月也围绕平台交易的各个环节在全国展开了调查，逐渐有了完整而清晰的事实真相。 3月9日，国内比特币交易平台OKEX及OK集团创始人徐明星在员工群里发表言论称,“未来随时准备捐给国家。”这一突然的在很多人看来有些莫名其妙的言论引起轩然大波。 但是鲜为人知的是，据链得得App调查查证，就在此前的2018年2月24日，东莞市公安局以“诈骗案”，正式对OKEX平台可能涉嫌“非法期货交易”或者更严重的“诈骗罪”，展开刑事立案侦查。 另据链得得了解，中国其他地方也有陆续立案。 东莞公安局出示的立案告知书 在链得得APP行情追踪结果中，4月9日，OKEX以约为47.98亿人民币的每日交易额位居世界数字货币交易所第三名。在近65天的日交易额排名中，OKEX也以超过80%的频率稳居前三。巨大的交易额、平台流量和品牌影响力，令OKEX成为了广大普通中国数字货币投资者最主要的交易渠道。 众多投资人上门维权，让OKEX这家长期占据世界数字货币交易所排位前三甲的公司，持续牵动着舆论最紧绷的神经。来自百余位维权投资者所有的指控都来源于一个爆发点，OKEX平台提供的“合约交易”产品。 被爆仓或者利益严重受损的投资者的维权理由，都是基于一点，OKEX提供了非法的期货交易，这在中国违法；而徐明星和OKEX的声明皆认为，平台提供的“合约交易”并非期货产品。不过，根据链得得研究团队多方取证和调查、分析，OKEX的“合约交易”可以说具有比较清晰完备的期货交易属性。 横向对比目前为止世界前五大交易所，完全出海的币安、主体在中国国内的火币均未开设期货，或所谓“合约交易”的产品。但2014年8月至今（截止链得得发稿），OKCoin旗下OKEX交易所平台中，那明显的“合约交易”入口始终没有停息。 OKEX“合约交易”究竟是不是期货交易？区块链及数字货币问世至今所标榜的“去中心化”，在数字货币交易所层面却实现了诸多资本金融权力的集中。数字货币交易所集银行、银联、证券交易所、券商、发行审核部门、甚至监管者的角色而存在。在资产规模日益壮大的数据货币交易市场中，占据资金流动和融通规则的把控核心。 2014年8月18日，OKCoin发布消息，称旗下的比特币、数字货币“合约交易”平台OKEX上线，在“合约交易”币种上，包含比特币、莱特币、以太币。 OKCoin在自己站内发布的推文显示：旗下交易平台OKEX隆重上线，比特币、莱特币、以太币合约交易请到OKEX上来。 当时，OKCoin在“合约交易”业务的对外说明中写到：此次推出比特币合约，可实现比特币的套期保值，利用比特币合约对冲在支付过程中比特币价格波动导致的风险，进而解决比特币在支付领域的最大问题；此外由于比特币市场每日成交额有限，导致当前比特币支付能够承载的业务量十分微小，通过比特币合约放大杠杆，可以变相增加比特币每日成交额。 这也是投资者维权时指摘OKEX违法期货交易的重要源头，虽然这并不被OKEX官方承认。今年3月22日，OKEX官方在对媒体的回复声明中表示：OKEX法律团队认为平台的比特币虚拟合约业务不属于传统期货，交易过程没有法币，属于币币兑换，不符合传统期货定义，用户和OKEX公司之间也没有资金往来。 不过，打开现在OKEX的“合约交易”页面，链得得App也发现，OKEX的“合约交易”业务从交易逻辑到操作规程，均满足期货的显著特征：高杠杆、保证金、强制平仓、周月季度交割期、标准化合约、平台集中交易、套期保值功能宣传等。在OKEX的英文版页面介绍中，始终使用futures一词来指代“合约交易”的业务，而futures在金融交易的使用语境中，对应的中文翻译就是期货。 2011年，国务院出台《国务院关于清理整顿各类交易场所切实防范金融风险的决定》。其中明确提到，“除依法经国务院或国务院期货监管机构批准设立从事期货交易的交易场所外，任何单位一律不得以集中竞价、电子撮合、匿名交易、做市商等集中交易方式进行标准化合约交易。” 链得得研究团队搜集和整理近年来中国整治非法期货交易场所的所有案例，总结其特征可以发现：这些平台通常以当下热门概念投资产品或金属、大宗农产品现货交易为诱饵，面向社会公众开展业务，主要有以下几种类型： 一是单纯提供交易平台，以投资小、收益大等噱头或以配资为名提供更高杠杆率，吸引投资者参与，为买卖双方提供撮合交易，收取佣金。部分场所还为境外交易机构（俗称“外盘”）充当代理，或者以外盘代理为名，行内盘交易之实。 二是不仅提供交易平台充当中介，而且参与交易，成为买方或卖方（俗称“对赌”），剩余头寸进入合法交易场所保值。有的在对赌时，提供虚假行情或者不同步的交易行情，有的在交易系统恶意设置障碍。 三、交易过程中存在的机器人虚假交易、数据造假、欺诈、限制平仓等行为的证据。 反观近期深陷维权风波的OKEX，链得得APP在走访了近二十位维权投资人和业内从业者，了解到OKEX与此三点特征高度的吻合性。 以下，链得得App研究团队的深度调查，从期货特征交易的几大重点环节，来还原OKEX平台“合约交易”的真实面貌。 危险的“合约杠杆”在全球范围内，加密货币期货交易规范化进程中，芝加哥期权交易所（Cboe）开启了最受瞩目的运营尝试。2017年12月11日，美国芝加哥期权交易所正式上线比特币期货交易，当天便引发比特币现货价格突破1.6万美元。作为正规化尝试且最被效仿的芝加哥期货交易所，目前提供的比特币期货合约最高的杠杆倍数为5倍。 而在其他游离各国监管和合法边缘之外的期货交易所中，投机和对赌的色彩就更为显著。这一点在杠杆叠加的倍数上窥见一斑。 在目前全球超过250家规模以上数字货币交易所中，开设期货合约业务并有规模人群参与的交易所不超过10家。其中，具有代表性的平台： 成立于2014年的BitMex，属于纯期货领域交易量最大的交易所。光比特币调期合约24小时就有3亿美元以上的交易量。具备一定交易盘深度，也成为诸多资深期货玩家主要交易的战场。其期货合约杠杆为1至100倍任意选择。 Bitfinex被誉为最具比特币流动性的交易所之一，杠杆倍数最高为3.33倍。 Bitstar现货合约与期货类似，是一种保证金交易的差价合约，用户可以通过判断涨跌来赚取差价。最高提供5倍杠杆。 极端案例中，俄罗斯的一个数字货币期货交易平台更是存在惊人的最高1000倍杠杆，但该平台的交易量和品牌知名度非常有限。 OKEX的“合约交易”杠杆有两个选择，10倍与20倍。 来自湖北的姚彬自进入OKEX的交易合约至今，先后被爆过三次仓，这里面包括自己的钱和亲戚朋友的钱约合人民币80余万。如今的他已然身无分文，靠朋友接济度日。用他的话说，“OKEX的合约杠杆既是瞬间想象暴富的可能，也是让你瞬间倾家荡产的存在。” 今年两会上，前央行行长周小川在讲话中提到了未来监管方向将是动态监管，他说不喜欢创造纯投机产品，让大家有一种一夜暴富的幻想。 基于OKEX全球前三大数字货币交易所的影响力认知，这个倍数无论在合规体系中还是非监管体系下，都属于杠杆居高，且用户倾家荡产风险极大的交易平台。20倍杠杆意味着只需缴纳5%的保证金即可建仓交易，在用户下期货单的数字货币上，该数字货币市场波动只要超过5%，便面临爆仓的风险。期货单有周、月度和季度的平仓时间期限，这些期限分别对应着用户期货单必须平仓的最终时间点，但凡是在对应时间期限之内，只有不被爆仓都可以随时平仓以获取盈亏。 数字货币市场是一个365天、24小时不间断的交易市场，且没有涨停、跌停的限制，亦没有熔断的保护机制。关注过数字货币行情的人都能发现，相比于传统证券市场，数币市场币价的波动可谓“天方夜谭”。 以链得得APP汇总的2018年4月15日每日数字货币涨跌排行榜为例，该日涨幅前三十的币种全部超过20%的涨幅，前四名的涨幅超过133%；跌幅前三十的币种全部超过13%，前四名跌幅均超30%。即便作为期货合约主要标的的比特币、ETH、莱特币等，24小时内的涨跌波动都能够接近或超过5%，更不用说在期货合约标定的周、月度和季度的更长时间周期内，有多少次可以随意穿越5%，这个纸糊的爆仓线了。 因此，在数字货币市场，任何超过200%的杠杆都存在极大的设计风险。 毫无实物交割，一场以空搏空对赌“这就是一个自定规矩的赌场，只是它把人的赌性放得更大”，一位资深从事数字货币期货交易的操盘手对链得得App说，“我上周刚刚被在一个平台上被爆了200个比特币，也愿赌服输，反正再过几笔交易我只要赢一把，前面输的钱都回来了。大部分参与数字货币期货的人都有同样的想法。” 关于为什么不在OKEX上做期货交易的疑问，上述人士回答，“在我们重度参与者眼中，OKEX期货交易深度太浅，真实交易和成交活跃度不高，而且规则与产品设计也存在许多漏洞，这种情况下很容易形成一个庄家市场，风险系数和投资回报不成比例。OKEX主要还是一些初出茅庐又赌性十足的韭菜在里面玩期货。” 期货市场之所以能够被各主要主权经济体视为资本市场不可缺少的一部分，并加以规范建设和监管，是由于期货市场对于实体经济对冲风险、大宗商品价格预判、货币波动方向预测、优化流动性配置、维护市场稳定等方面有调节推动的作用。以期货合约交易标的性质来区分，大致分为商品期货与金融期货。 与传统合规的农业、金属、工业等大宗商品、原油期货等市场不同，OKEX的“合约交易”不存在实物交割的能力和选项。 理论角度看，套期保值和价格发现是期货市场的两大功能。以商品期货为例，在约定平仓时间点以转移合约标的实物的所有权来完成交割。这种情况下对于实体企业在生产资料环节的中远期价格对冲将产生实际套期保值好处，稳定了成本价格，从而保障利润可持续，进而推动实体经济发展。 即便像股指期货这样可以进行现金交割的金融期货，往往也会提供实物交割的选项。这里的实物，指的是一份实实在在通过非杠杆实价购买的ETF指数基金标的产品。 回看OKEX的合约交易，没有提供任何实物交割的选项，在合约交易过程中你也购买不到，看不到任何类似于证券市场ETF股指基金的产品份额。 至少得有现货支持实物交割的产品，而不是空手对空手的虚交易。很大程度上，OKEX的合约成交价格是由买卖双方在固定时间、封闭池子里的成交价决定，无法反映和影响真实数币市场价格。只要有足够的资金和筹码，平台就能够在临近交割日期时，向交割价格进行收敛。这个过程就会时不时引发合约市场较大的波动。 平台所提供的比特币、莱特币、以太坊等交易标的也没有铆钉任何被广泛认知的实物资产，平台通过算法和交易程序撮合，匹配一对多空两向的对手盘交易，以10倍或20倍的杠杆配上一定数额的保证金，便开始了一场既无实物标定又无价格真实传导的对赌冒险。 “合约交易”指数不透明，有鲜明庄家痕迹，价格操纵空间大为何说“又无价格真实传导”？价格发现作为期货主要功能之一，承载了价格对冲稳定和价格预判的功能。股指期货价格的形成有一套完整、公开、明确且备受规程监管的体系。集合了股票现货市场价格、成交量、影响现货市场的各种因素，以及交易者对影响现货市场价格因素的预期和投资者心理和行为等重要信息，连同期货市场的价格走势、持仓量、成交量等因素综合而形成。不可能出现与股指现货价格完全脱节的情况。 OKEX仅凭一套自行定义的“指数价格”对合约进行交割。 在OKEX网站上公示的最新合约交易指数介绍中，只表示从2018年9月30日后，采用以Bitstamp、Coinbase、Bitfinex、Kraken四家平台价格数据标本的新合约“价格指数”。并没有明晰“指数价格”的产生机制；是取四家平台指数的平均值？还是加权求值？加权的各平台权重如何？还是说有自己的一套参考算法？所有的具体环节都没有答案。对指数价格形成过程的第三方监管和审查更是无从谈起。 若期货合约的交割价格计算方式不透明，直接将造成交易价格数据调整和操纵的空间，进一步导致在合约交割时间（比如强制平仓时间节点）上人为介入的机会。 2018年3月30日凌晨5时许，OKex上出现近1个半小时的极端交易行为，BTC季度合约一度比现货指数低出20多个百分点，最低点逼近4000美元。根据OKEX爆仓记录统计，短短一小时瞬间爆破多头46万个比特币的期货合约。跌到最低点后瞬间又拉涨10几个点，部分空头也被爆仓。而在整个异常波动中，现货最低价格也没有跌破6000美元。期货现货差价最高逼近30%。 OKEX不支持实物交割，即使作为金融衍生品也缺乏定价功能。平台合约交易均直接平仓对冲了结，实际交易不以实物交割为目的。无法对数字货币市场价格环境进行有效调节和预测，空转的资本流通更不能对实体经济产生哪怕些许的利好。 由于合约交割价格形成机制的不透明和人为介入的可能性，很难实现OKEX平台声称的“套期保值”功能，不能做稳定有效的对冲操作。最后均以数字货币交割的方式完成，监管、运营、规则设定都集于平台一身，为暗箱操作和非法洗钱创造了巨大的想象空间。 至此，一个可能带有鲜明庄家痕迹的赌性合约市场将繁荣滋长。 2017年3月28日，清理交易场所部际联席会议办公室印发《关于做好清理整顿各类交易场所“回头看”前期阶段有关工作的通知》指出，商品类交易场所的分散式柜台交易“一般为杠杆交易，合约具有标准化特征。交易场所既不组织商品流通、又不发现商品价格，实为投机炒作平台，对实体经济没有积极作用。” 中国经济脱虚向实的政策大背景下，若大量民间资金借由数字货币的渠道流入类似的“合约交易”对赌中，对本就脆弱的实体经济环境将是进一步的打击。 诡异的爆仓K线图显示，当天交易价最低点为68206.52元。 投资人被爆仓的截图。可以看到被平仓的价格为67263元，下面一列是该投资人约30秒前追着行情卖，却没能卖出的价格68843元价格。 有投资人公开质疑，OKEX爆仓价格不透明、不规范。在这位投资人提供的自己爆仓当天，OKEX合约交易的全屏界面。图形数字显示，当天交易价最低点为68206.52元。而当天成交记录显示该用户当天的爆仓价为67263元，也就是说，哪怕当天全天最低的价格都没有触发该用户的爆仓价67263元。没到爆仓线却爆了个精光，究竟这个爆仓价是怎么计算得出的？系统又是怎样自动执行操作的？ 请输入图说 投资人用户与客服沟通的聊天记录显示，该用户质疑OKEX合约交易平台爆仓价格的设定机制，并要求对爆仓价定价方式进行公开。用户表示自己在全过程都盯着合约交易价格看，不存在疏漏，但还是眼睁睁地看着自己的合约在没达到约定爆仓线时，就被系统强行爆仓。 关于爆仓价格是如何计算得出的质问，OKEX客服的答复是：委托数量换成张数进行计算。可委托数量和合约张数是成交量，而爆仓价是价格计算。一个是价格，一个是量，两者完全不属于同一维度。客服这样的答复令人一头雾水。 该投资人表示，虽然方向看错是自己的问题，但之后的反弹也不会让我全部亏完，且如果价格到爆仓附近自己也有补仓的想法。本人之前合约有亏有赚，也被爆仓过。如果是其自己的问题可以接受，但平台这样无故爆仓，无法接受。 原海证券交易所CTO、中科院软件室主任、软件方向首席科学家白硕，在近日针对交易所监管的文章中指出：中心化的数字货币交易所仍然是主流，虽然也有去中心化的交易所，但性能跟不上，要想效率高，只好中心化。一旦中心化，安全就没有保证，交易的真实性也存疑。这时如果没有监管，光靠自律肯定不行，监管必须要介入。倘若传统交易所出事故，不仅内不要通报批评，对外还有有所交代。但现在的虚拟货币交易所一出事就各种推诿搪塞，这种讨论很像游戏网站。这种机制怎么可能颠覆金融？ 在文章的最后，白硕同样呼唤：监管必须介入。 美国当地时间2018年4月17日周二，纽约州总检察长办公室公布，致信13家数字货币交易平台或相关实体，最迟今年5月1日答复相关问卷调查，披露交易规则、使用交易机器人、内部管控、断电等交易被迫中止情形、对客户资产保障措施、利益冲突等重要业务信息，以便普通投资人更好地了解风险和获得的保障。 纽约州已然注意到了数字货币交易所日益积累的诸多顽疾。数币交易所这类轻重有别，普遍存在的问题需要监管进一步找到对应措施。 “独树一帜”的穿仓平摊制度不仅会遭遇爆仓，还有一个奇特的穿仓平摊制度。这张流传甚广的截图相信很多人都见到过： “穿仓分摊制”是OKEX平台在“穿仓”情况下，自己设定的损失分摊机制。正如图片对话中投资者抵触的一样，大部分投资人在莫名被平摊了与自己无关的穿仓费之后，才第一次发觉OKEX的这套机制。整个扣费过程毫无通知、毫无协商的余地。“穿仓分摊制”指的是将所有合约的爆仓单产生的穿仓亏损合并统计，并按照合约所有盈利用户的所有收益，作为分摊基数进行的操作模式。 在加10倍或20倍杠杆的情况下，若方向看反，标的价格大幅波动就容易触及到准备金的爆仓警戒线，系统便会强行按市场价格将该合约卖出。如果当前强行卖出的市场价格低于保证金底线，就发生穿仓。系统显示爆仓数为负，也就是OKEX平台实际上将承担穿仓导致的账面亏损。 此时，为了转嫁穿仓带来的平台亏损。OKEX就强行让平台上的盈利用户来对这部分平台亏损进行分摊。因此，这种躺着中枪的“薅羊毛”行为经常遭到盈利用户的强烈反弹。交易规则和系统设计都是平台构建运营维护的，在传统期货交易中，规范化的期货平台会设置“两条线”。第一条是强行平仓线，这条强行平仓线会比用户保证金高一些，当系统强行平仓时就不会发生穿仓的事件。在OKEX这里，自身产品设计的缺陷却用一套用户分摊机制去弥补自己的草率。 消失的交易记录同样草率的还有用户个人账户里的“合约交易”记录。李铭来自北京，他根据自己在OKEX上合约盈亏记录的统计，发现在一个月内，他“合约操作”应得的币余额比实际少了10个比特币。李铭在对历史交易数据查询时发现， OKEX的交易清单无法下载完全。比如每天下载只有1000条，无法用Excel完整统计，加剧了账务的混乱。OKEX的合约分页功能始终无法显示第二页以后的内容。以上质疑OKEX客服长期无人回复。 前文中，被三次爆仓的姚彬同样寄希望于搜索交易记录，来找到其怀疑平台“定点爆仓”的证据。可在OKEX“合约交易”中，手机客户端的交易数据仅保留一个月的，PC客户端的合约交易数仅显示三个月内。 横向对比，李铭在2013年11月份进入火币交易，当时至今的交易记录，一笔不差都都能查到。李铭说，“大多数期货参与者没有固定证据的意识，一旦他们意识到交易数据证据的重要性时，要么被OKEX恶意删除交易历史，要么被OKEX以超短的3个月数据保留期限自动删除交易历史。” 被清退的OKCoin战地转移OKEX：用户转移和品牌背书2017年中国境内的“9·4监管”之后，包括OKCoin在内的国内几大数字货币交易所被清退，暂停数字货币与人民币兑换业务。OKCoin就此在交易所业务上进军海外市场，以OKEX的品牌身份继续承担数字货币交易所的功能。 OKCoin上线于2013年10月，是中国最早且最大的比特币交易所之一，品牌隶属于北京乐酷达网络科技有限公司。在清退实施后，其用户资源、页面流量、品牌权益通过各种渠道导入了OKEX，这个注册地在美国伯利兹，办公地在香港的公司。使得OKEX迅速在交易量、用户规模上冲进了全球数币交易所三强。OKEX最受关注的业务之一，“合约交易”也借此良机乘帆起航。 林旬来自广东，2016年底注册OKCoin并进行比特币交易。在“清退出海”事件后，林先生通过OKCoin页面上的链接，使用原有账号便直接登录了OKEX，并于2017年11月首次尝试了“合约交易”。谈及对OKEX产生最初信任的原因，林旬解释到，首先OKEX是OKCoin旗下公司，同时两家公司人员有重叠，尤其是创始人和实际控制人均为徐明星。 其次OKCoin首页至今包含了OKEX的链接，OKCoin的用户账号及数字资产可无缝且免手续费地迁移至OKEX，于此同时，两者在招聘启示的公告相同，人事关系的电子邮箱地址后缀一致，在形式上看起来就是两家利益共同体的公司。 再次，OKEX网站上标注了许多境内知名投资人和机构的信息，包括史玉柱的巨人网络、王亚伟的千合资本、蔡文胜的隆领资本等。这些罗列的投资人与OKCoin页面列举的投资人信息有高度相似性。 最后，OKEX在国内网络媒体、社交媒体、自媒体上的推广力度很大，很多身边的投资人是通过网页、微信群和H5推文获悉的OKEX。 请输入图说 违规开放中国国内用户的投资入口OKEX在注册设计上，会根据用户在KYC（know your custmer）上传的信息确定是否支持相关用户的平台交易行为。 在不支持交易的国家和地区用户列表中，办公地位于香港的OKEX没有排除中国内地用户的交易，反而不支持所在地香港的用户交易。因此，中国大陆用户可以畅通无阻地在OKEX平台进行法币交易，币币交易，以及合约（期货）交易。为承接OKCoin原有大量国内用户创造了迁移的有利条件。 目前，在OKCoin交易平台上的数字货币只能够提币至OKCoin中国站账户、OKCoin国际站账户和OKEX站当中。彼此账户系统之间的数字货币转账仍然不需手续费。 与OKCoin不同，OKEX的公司主体注册在美国，办公场地在香港。OKCoin官方常以此来切割与OKEX的关系，并为OKEX撇清非法设立期货业务的责任，表示OKEX经营不在中国境内，因此不受国内政策和法规的约束。但这不能否认OKEX大量“合约交易”的投资人用户在中国境内，OKEX在国内网络、社交、内容平台上大量宣传推广业务的事实。 有趣的是，OKEX官网上服务条款细则中，第十四条仲裁一栏；中文版写的原文是，与OKEX相关的任何仲裁将在香港发生。如将版本切换到同网页同位置的英文版；内容赫然写着，任何相关仲裁将在北京发生。同一公司、同一主体、同一业务，发生仲裁的地址就这样混淆视听，各不相同。 法律上，同一公司主体约定仲裁的地址不可能有两个，各类语言的对应地点必须统一。香港和北京的仲裁方式甚至都不同：在北京，仲裁双方需自己指定仲裁员，国内大多仲裁员不是专职。在香港，当事人双方直接去仲裁机构立案，香港仲裁员一般为专职，由仲裁机构指定。由于效率高、过程专业，因此，国外经济贸易纠纷喜欢仲裁而不喜欢打官司。OKEX的仲裁地点中文写香港、英文写北京，这种明显的混淆方式，也给两边的投资人同时制造了意识困境。 据一位不愿透露姓名的国内知名刑事与经济案件律师观察，OKEX“合约交易”被东莞市公安局以“非法期货”和诈骗罪立案，在法理上是没有任何问题的。 该律师表示，无论其实际工作人员是在境内或境外，所隶属公司是境内公司法人或境外公司法人，是境内服务器或境外服务器，以及国家是否对其境内IP封闭，均不对国内执法部门的管辖权构成影响。责任认定的标准在于用户上网转账的地点是否处于境内。即“属地原则”：案件受害对象在哪儿，作案行为发生在哪儿，就可以在哪儿立案或上诉司法裁决。只要OKEX“合约交易”存在国内用户，就符合《刑法》的管辖适用范围。 该律师介绍，期货交易是受国家严格监管和实际控制的业务，即便平台在国外获得了运营资格，但凡在国内的业务开展没有得到中国官方授权和批准，就存在欺诈的行为。 这种情况下，交易平台不可能向国内的协议投资人，提供其国外交易的所有真实状况；公平性、真实性、和承诺没有任何保障。 “为什么国内没有传统金融机构参与其交易，却几乎都是散户在其中买卖？”律师反问到。“只因大型金融机构均设立风控部，风控部里都是有经验的律师，深知其中巨大的法律和交易风险，是绝不允许有这种投资决策出来的。” OKEX也缺乏牌照授权和经营资质综上，OKEX的这种典型的期货交易属性很难用它自己所说不是期货交易来做解释，那它又到底是否具备期货交易的资质呢？ 链得得在工商查询中获知，OKEX作为境外法人不具备任何国内金融牌照、期货交易资质、办公场所、工作人员、合法经营资质。与此同时，其主要客户群体在境内，其创始人、实际控制人徐明星也在境内。 2018年1月15日，运营主体为北京烽火创杰有限公司的OKEX官微和APP因“通过登记经营场所无法联系”为由而被北京工商局列入经营异常名录。 不仅如此，注册地在国内的OKEX母公司平台OKCoin，及OKCoin所隶属和资本关联的北京乐酷达网络科技有限公司、北京欧凯联创网络科技有限公司，同样没有相关执业资格。 在部分维权投资人向中国证监会关于“北京乐酷达网络科技有限公司，北京欧凯联创网络科技有限公司是否具备期货合约业务资格”的信息公开申请中，证监会回复称，根据《期货交易管理条例》，目前国内合法期货交易场所分别为上海期货交易所、大连商品交易所、郑州商品交易所和中国金融期货交易所。我会未批准其它交易场所组织开展期货交易。 而在证监会深圳监管局的回复中，更直称“中国证监会未批准任何交易场所开展比特币等虚拟货币期货交易，北京乐酷达网络科技有限公司和北京欧凯联创网络科技有限公司不具有中国证监会核准的期货业务相关资格。” 依照《刑法》第二百二十五非法经营罪，第三款：“未经国家有关主管部门批准，非法经营证券、期货或者保险业务的。”其中，“非法经营证券、期货业务”，主要是指以下几种行为：非法设立证券交易所、期货交易所进行证券、期货交易；非法证券、期货经纪行为，如未经工商行政管理部门核准登记，擅自开展证券或者期货经纪业务；证券交易所、期货交易所、证券公司、期货经纪公司超越经营权限非法从事证券、期货交易；从事证券、期货咨询性业务的证券、期货咨询公司、投资服务公司擅自超越经营范围从事证券、期货业务。” 数字货币市场酝酿着巨大的利益交易和价值前景。中心化的交易所在其中扮演着撮合、流动和杠杆的角色。在如此巨大的利益诱惑前，设计、运营、交易、存储、流通、审核、监控、发放、裁定甚至最终权责解释权的全过程，都只依靠交易所自身的自律性来独木支撑。连基本的行业共识规范都没能达成的当下，政府或第三方机构的监管缺位，会加速数字货币市场的失控，从而影响金融和经济秩序的稳定。 另外，投资人自身也应肩负起对投机和赌性行为负责的责任。在OKEX“合约交易”事件中，许多投资人在抗风险能力弱、专业判断能力缺失、极端操作、风险评估不足、交易经验浅薄，甚至在连基本游戏规则都没能吃透的情况下贸然入市，并轻易地动用高杠杆交易，造成了个人巨大损失。 悲剧永远不是孤立产生的，一定是群体性的放纵欲望和无序贪婪造成的共同恶果。平台的责任在于引诱了欲望、放大了欲望、创造了新欲望。可如果投资人自身能理性控制欲望的泛滥，平台也害不了你。 当前，包括中国在内的大部分数字加密货币交易市场主要国家，都没有在法律和监管层面明确定义数字货币的性质和功能定位；同时，明确的监管参考案例又极少，无法有效对应既有法规和判例执行定性追惩。在监管执行层内部也尚未统一对数字货币交易形态的认识和判断，系统化的官方调查追责程序暂时都按兵不动。 正是由于这样的野蛮生长窗口期，一些数字货币交易平台在民间炒币市场繁荣，且监管政策和落实未至的不对称空间内，利用包括高杠杆、高风险、高覆盖面却丧失有效监管的灰色手段攫取利益，并能站在法规与市场的模糊地带继续牟利。 包括比特币、以太坊、莱特币、EOS等大交易额主流数字货币都对应着法币和实物资产，在现实生活中有各种便捷稳定的法币兑换渠道；随着普通人对数字货币可交易、可牟利性认知的迅速普及，在越来越多的市场参与和信任支撑下，主流数字货币对映美元、人民币等法币的交换性将进一步紧密。因此，在大众参与的数币交易市场里，流通的不是积分或服务交换等虚拟产品，而是真金白银。 此时，如果那些涉及高资金杠杆、高赌性风险、无有效监管、无明确规范、影响面巨大且游走于法律之外的数字货币交易平台不断增多，对于普通投资人权益、民间投资环境、货币流通环境、数字货币交易市场健康发展、乃至社会稳定将是一场灾难。这种情绪、收割策略和流动性波动将直接传导致一级和二级资本交易市场，进而干扰更大范围内金融秩序的稳定。 近日，链得得APP联系到一位OKCoin内部人士： 问：“为什么压力这么大的情况下，你们还不关停OKEX的合约交易业务？” 答：“因为这块业务实在太赚钱了，大部分数字货币交易所都盯着想吃这块大蛋糕。” 问：“能有多赚钱？” 答：“具体我也不知道。”（文/ 链得得内容合伙人李非凡，本文独家首发链得得App） （后注：出于保护投资者和线索提供者的原因，故将真实姓名隐去，文中涉及的用户人物皆为化名。） 更多精彩内容，关注钛媒体微信号（ID：taimeiti），或者下载钛媒体App 转载来源：徐明星的OKEX涉嫌“非法交易”与“诈骗”全调查 | 钛媒体深度]]></content>
      <categories>
        <category>财经</category>
      </categories>
      <tags>
        <tag>数字货币</tag>
        <tag>比特币</tag>
        <tag>期货</tag>
        <tag>OKCoin</tag>
        <tag>风投</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[医药界再现“蛇吞象”并购 武田制药豪掷640亿美元收购英国药厂夏尔]]></title>
    <url>%2F2018%2F66d43553%2F</url>
    <content type="text"><![CDATA[日本药业龙头、也是亚洲数一数二的制药公司武田制药再次修改要约出价之后，罕见病药物制造商夏尔董事会今日宣布愿意接受目前估值高达640亿美元的收购要约，并将推荐提交股东大会审议。 转载来源：医药界再现“蛇吞象”并购 武田制药豪掷640亿美元收购英国药厂夏尔]]></content>
  </entry>
  <entry>
    <title><![CDATA[“头条”激战三四线：谁占领下沉人群，谁占领明天 | 深氪]]></title>
    <url>%2F2018%2F2d47a6a6%2F</url>
    <content type="text"><![CDATA[文|方婷编辑|杨轩漩涡中心谈判桌的另一端，摆着最少9份投资意向书，还坐着BAT三家。10亿美金，12亿美金，14亿美金，到最后的16亿，这家叫趣头条的公司，谈判中估值一直在往上涨。 文 | 方婷 编辑 | 杨轩 （感谢36氪作者闫浩、王雷柏对本文的贡献） 漩涡中心谈判桌的另一端，摆着最少9份投资意向书，还坐着BAT三家。 10亿美金，12亿美金，14亿美金，到最后的16亿，这家叫趣头条的公司，谈判中估值一直在往上涨。 趣头条最想拿腾讯的投资。2018年春节前后，趣头条与BAT的融资谈判已经进入开价阶段，为了等腾讯，它拒绝了百度、阿里，腾出一段不算短的时间，专门等待腾讯的回音。 这款同样定位为信息流资讯的产品，只花一年多时间，DAU（日活跃用户数）就过了千万。今日头条不能不心生警惕。去年12月，张一鸣把一张趣头条凶猛增长的图发到高管群里。有传言称，今日头条也曾试图投资趣头条，但趣头条一方没有回应。 最终腾讯还是拿下了这笔交易，以16亿美元的估值领投，交易金额近2亿美元，跟投的还有顺为资本、尚珹资本等。 当流量越来越贵、越来越难出现爆发性增长的产品时，不管情不情愿，趣头条已经来到漩涡中心。 “投资趣头条未必是腾讯对今日头条的防御，只是在现在的态势之下，任何大的流量入口都不能被放过。”光源资本CEO郑烜乐对36氪说。光源是趣头条本轮融资的FA，同时也担任过快手、哈罗单车的融资顾问，通过此前的案例早早意识到了三四线下沉市场的价值。 在一二线城市，大家默认“头条”＝“今日头条”。但在下沉市场里，还有一整个庞大的“头条系”矩阵，它们指的不是抖音、懂车帝、西瓜视频、火山小视频等北京字节跳动公司旗下的产品，而是趣头条、惠头条、淘新闻、东方头条等以返利补贴为主要运营手段的信息流产品。趣头条是其中做得最早，起量最快的一家。 根据趣头条创始人谭思亮的说法，这个行业里的第一名在三线城市及以下市场的渗透率一直不超过20%，“并没有大家想的那么夸张”。剩下的80%，是留给趣头条们攻城略地的空白，它们所接触到的大多数用户，手机里很可能还没有安装过任何新闻资讯应用。 “趣头条们其实是给今日头条盖天花板。现在它们扩张得有多大，未来今日头条在下沉市场的天花板就有多低。”一位要求匿名的投资人说。 但对创业者和投资人们，这真是个好消息。 今年4月初，在一家著名VC的闭门会议里，下沉市场的流量成为诸多圆桌环节里最受欢迎的话题，拼多多、趣头条是被拿来反复剖析的案例。 回顾最近进入极速增长的公司，无论是拼多多还是趣头条，他们共通的特点，就是流量来自“下沉市场”——那些当看似无处不在的淘宝、今日头条还没有触达的人群。 这是个多达五六亿人口的市场，长久以来处于互联网世界的边缘，被认为很难发掘和“不够有价值”。他们是三四线城市里面四十岁以上的主流用户，也包括一线城市里面那些年纪更大的用户。 而如今，有人找到了一套搞定他们的办法。 师徒关系和时间性价比趣头条之所以要等腾讯，一方面是因为腾讯较为宽容的投后管理风格，另一方面，是因为依托于微信、QQ的人际传播才是搞定“下沉”的关键，而非互联网世界过去十多年通行的、买流量的方法。 “我不是从这些大平台采购的，而是从一个一个人手里来组成流量，个人是不会跟我们溢价的。”谭思亮告诉36氪，这是他当时觉得趣头条这件事情可做的原因。在创办趣头条前，谭思亮曾在盛大集团负责过在线广告平台业务，他太清楚线上流量价格如何一路走高。 趣头条尝试过从巨头掌握的渠道里买量，平均获客成本在10元以上。对左手用户右手广告的生意而言，这么高的成本不足以支撑起一个快速成长的公司。令谭思亮更为忧虑的是，如果依赖巨头掌握的分发渠道来获客，渠道只会不断抬价，将属于创业公司的利润不断压缩。 要在下沉市场抢份额，关键点从掌握渠道变成了理解“人”。 86岁的王老太太又开始整夜抱着手机，手指不时在屏幕上滑动，从晚上九点多一直划拉到凌晨两点，在儿子的制止下，才不情不愿地放下手机。下载趣头条之前，她的作息是晚上十一点多入睡，但现在，凌晨一两点睡对她来说也很常见。她得意于自己有个月通过看新闻赚了30多块话费，喜滋滋地将这个消息告诉给别人，家人才知道她口中整天念叨的“头条”，不是今日头条，而是趣头条。 王老太太退休前在兰州当麻醉师，发展她用趣头条的，是她以前炒股时的“师傅”，也是她现实中的熟人。收徒首得3000金币，此后徒弟的阅读收益也能进贡给师傅。王老太太的师傅已经收了数十个“徒弟”。 “金币”刺激带来了大量在微信群和朋友圈的分享行为： 有缘人加我.B7PYYXXX，立得1元。CRB8YFXXX，需要邀请码请复制，输入你得一元。新闻类收徒，3元以上一个徒——大概是嫌邀请码一条条发得太慢，有人干脆直接甩出一张截图，里面包含数个主打阅读得金币的App。 如果认真翻看这些群里每半个小时就新增出的99+条信息，就不太可能错过任何一个市面上新推出的赚钱平台。它们的名目千奇百怪，从看新闻赚零花，收徒就赚888，到每天抢红包，互相帮忙做任务。负责运营维护的群主对这些信息似乎也已经是见怪不怪，很少有踢人的情况出现。 大多数返利平台对机器羊毛党都深恶痛绝，趣头条更是在创业初期就清理了20多万个机器人帐号，对一个DAU刚到百万的平台来说，这在当时是个艰难的决定。但对这些想要薅羊毛的真实用户，平台却显得宽容得多。谭思亮对此想得很明白，“如果说他本来就想贪点小便宜，这些用户其实也很容易被我们的积分机制吸引，后面我们通过阅读，通过算法，去培养他的使用习惯。” 王老太太也想收徒，不过被她的家人强力阻止了。但家人并不反对她看新闻，可以不停往下刷的图文和视频替代了电视和报纸的位置。当她跟随儿孙从兰州到北京生活时，刷新闻更是帮她打发了大量无聊的时间。 用户量的裂变就是这么开始发生的。 在趣头条的排行榜上，排名第一的用户收了四万多名徒弟，获得了八万多元的推广奖励。以这种直接给予金钱刺激的方式，趣头条完成了第一个百万DAU，也冲过了千万DAU的大关。 裂变的基础设施，是王老太太们都已经用上了微信，会相互转发信息，还会发红包和用微信支付。无论是拼多多还是趣头条，它们的商业模式都离不开这一系列基础功能。此前，这群多达五六亿的“互联网边缘人”，虽然有智能手机，但除了微信和手机预装软件之外，很少会主动去下载一个新App。 趣头条的一众跟随者，无论是惠头条、淘新闻、东方头条，都沿用了师徒体系的叫法。这个叫法在各种网赚平台里早已有之，趣头条不过是沿用，创始团队认为对于三线及以下用户而言，师徒体系更容易理解。但这个名称不免让人想到传销，所以趣头条需要反复跟外界解释，传销是向用户收钱，而趣头条是付钱给用户，这里面有本质的不同。 师徒体系相当于平台的拉新成本。根据平台规则的不同，“师傅”每邀请一位“徒弟”下载注册，一般可以获得2块到8块不等。高于2块的额度通常需要分三次返还，激发条件是“徒弟”在平台上的阅读行为。换句话说，师傅除了发邀请码邀请徒弟注册之外，还需要鼓励徒弟阅读信息流。尽管也有开宝箱（签到）、晒收入（分享链接）、阅读资讯、优质评论等令人眼花缭乱的任务体系，但如果想真的依靠这些App来挣钱，拉来新用户才是最有效的手段。 只要在摊子前面摆出一张邀请二维码，扫描下载送西瓜，水果摊主就是他们的王牌地推；卖保险的代理也会发展出十几个徒弟，每天按时按点在微信群里督促徒弟们阅读新闻。他们在现实生活中颇为活跃，都是一张张真实社交关系网上的节点人物。他们的“徒弟”，又会通过朋友圈分享、群发邀请码等方式来扩展自己的队伍。 依靠这种传播方法，趣头条将平均获客成本降低到3－4元。 刚退休不久的老上海人于虹，也是被她的“师傅”一起带着用起了趣头条的。她开始在她朋友圈一天五六条的美食、植物、度假村、碧海蓝天的澳洲旅游照中间，夹着“看新闻，赚零花，填我邀请码：24784XXX”的链接；她把每天花一个小时来刷新闻当成一种游戏，当手机屏幕上跳出那个堆着金币的图标，她和她的“师傅”都能有进账，虽然折算下来，平均一天也就几毛钱的收益。 下载趣头条10个月，于虹的收益总共是60块，尚有28元在帐户里没有提现，她就已经将兴趣转移到补贴更高的惠头条上，当然，是跟着她的“师傅”一起转移的。 “我也不知道我们的第一个100万DAU怎么达到的。”惠头条创始人Mingu Kang想了半天，也没找到这个问题的答案。这位在中国创业有年头的外国人，普通话说得异常娴熟，只是语气里经常有一种亢奋感，在描述惠头条的成绩时就更是如此。 这家公司的上一款产品是惠锁屏，一款解锁手机屏幕就会跳转弹出广告的返利产品，于2013年上线。在很长一段时间，惠锁屏经营惨淡，还引发过提现和信用崩塌的问题。惠头条的模式本质上跟惠锁屏一样，都是以返利索取用户注意力，换取广告主投放。有惠锁屏的案例在前，整个花动传媒对惠头条都没有报太大的希望。 因此，Mingu Kang 2017年年底做新年KPI计划的时候，给惠头条制定的目标是DAU达到300万，当时惠头条上线刚两个月。谁也没有想到，刚进入2018年2月，300万DAU就达成了。 算账惠锁屏当年之所以失败，是因为每天的用户时长只有几分钟，无法吸引足够多的广告。广告投放太少，给用户的返利不够多，这个游戏就无法滚动运转下去。 但信息流产品不同，凭借看似永远也刷不到底部的内容，趣头条们可以将用户黏在手机上，再加上金币刺激，这几款产品的平均用户时长几乎都在60分钟以上。 “这些人对相对价格没那么看重，对绝对价格很敏感。”糖豆CEO张远说。糖豆目前的DAU在300万左右，是广场舞领域里的第一，它所切的人群也是下沉市场，只不过更为细分，以40岁以上的人群为主，张远称之为“小镇中年”。这群用户通过刷手机看新闻，哪怕一个月能挣上20块钱，也是额外的收获。 换句话说，他们更在乎净挣多少钱，而不是时间和收入之间的性价比。这就贡献了长达60分钟以上的使用时长，以及源源不绝的信息流广告位。 说到底，最终要落到能不能算得过来账上。 “虽然他们财富的绝对值比不上一线城市（的人），但是他们的可支配时间和可支配财富其实是比较大的。”糖豆CEO张远说。 更重要的是，他们都没被“洗”过。 此前，赚到这部分人钱的，是插播大量专题广告的电视台，是电视购物，是小镇商场里巧舌如簧推荐不知名品牌的推销员。 “这两年因为变现的手段越来越多，变现的深度越来越深，这个人群的机会还是很大的。”顺为资本合伙人程天对36氪说。流量变现的手段始终没变，主要是直播、广告、游戏、电商，深度是指从单个用户身上可获得的日收入或月收入。 这原本是由今日头条和百度瓜分的生意。2017年，今日头条的广告收入大约为150亿元，2018年的KPI据传翻了3倍，定为500亿（包含抖音等所有今日头条系产品在内）。信息流也是百度过去两年的业务重点，就连百度云盘里都内嵌信息流模块。信息流业务分发量的增长，也是整个2017年度百度财报会上李彦宏和陆奇强调的重点。 今日头条和百度切掉的只是大蛋糕的一块，在下沉市场里狂奔的趣头条和惠头条，单月营收高点均已破亿元。 在不曾被反复“洗过“的下沉市场里，效果广告比品牌广告更有发挥的空间，尽管比起后者，效果广告往往以其粗糙的形式，诱导性的界面被视为急功近利。但就结果而言，以转化率为考核标准的效果广告显然更有效率。 “对这些用户来说，广告甚至也是信息的一部分。”一位资讯类公司高层向36氪指出。相较于一二线典型的城市白领，下沉市场的消费者并不拥有充足的品牌知识和消费选择。他们在做出购买决策时相对盲目，这使得整个市场呈现出广告营销驱动的特征。 网传趣头条融资BP截图 与其等待着消费者上门，不如将品牌和产品推到他们面前，所以信息流广告比搜索广告有着更多驱动购买的机会。这正是趣头条们能够迅速崛起的先决条件。 根据 Mingu Kang 的说法，惠头条的DAU从0到500万的过程中，没有动用过公司任何存量资金，几乎是上线第一天就开始挣钱，趣头条传达出的也是类似的信息。起码现阶段，做下沉市场的信息流广告还是一门看上去稳赚不赔的好生意。获客成本可以控制在5元以下，也不需要今日头条、百度那样的万人销售团队，对于现阶段的它们来说，接入代理投放的广告联盟是消化流量的最佳方案。 下沉市场的“头条系”上，当然不止有退休人员，40岁以下的女性才是这些App的主流用户。瞄准她们的广告主既有爱奇艺、新氧、58同城、大众汽车，也有“双眼皮抽脂价格”、“牙齿种植会痛吗”——各式各样层次不同的广告混杂在一起，等待着用户在领完每日签到金币和阅读完各种本地新闻和娱乐短视频之后，再打开它们。 头条们的竞争在Mingu Kang 如今的计划里，惠头条的2018年度目标变成了DAU 2000万。 这同样也是谭思亮为趣头条定下的未来3到6个月的目标。他心里的DAU生死线是5000万，在他看来，只有过了这道槛，趣头条才算到安全区域。 试图挤入这个新市场的人正陡然变多。之前，很多人对趣头条的态度，是从看不懂到看不上，但当这家公司顺利拿下腾讯投资时，风向变成了跟着做。 今日头条已经不可复制，趣头条看起来还有追击的机会。 据趣头条官方数据，它的注册用户为7000万，这跟创新工场管理合伙人汪华提及的多达五六亿人口的第三波人口红利相比，只占十分之一左右。还有一大片商业价值待开发的下沉人群，尚未被触达到。 在金币体系的刺激下，这些“头条系”产品都呈现出高留存的特征。邀请好友送金币，一元提现——在惠头条的用户群里，能不时看到一个叫微鲤头条的广告。产品上线刚一个多月，微鲤头条已经是盈利状态。 就连今日头条也在“复制”趣头条。先看和今日头条极速版是今日头条推出的两款阅读返利产品，只有安卓市场才搜索得到。它们有着跟趣头条一模一样的师徒体系和金币玩法，在算法和内容上延续今日头条的已有基础，起量迅速。 今日头条极速版的安装包很小，只有2M，适合低配置手机，在头条内部定位本来就是一个下沉市场的策略型产品。在趣头条爆火之后，今日头条极速版也全面拥抱趣头条所仰仗的师徒模式。根据QuestMobile的数据，2017年年底时，今日头条极速版已经有2000万日活。 这也是趣头条把“5000万日活”定为安全线的原因之一。 略带反讽的是，大家都在学习金币体系，内容反而不是做一款资讯类产品最难的一步——今日头条起家时，张一鸣为版权问题遭受非议，乃至于频繁打官司。6年过去，在这个信息冗余的年代，平台甚至不需要花费前期成本，只要愿意开放流量，广告分成，就可以轻松达到10万条以上的采购量。 趣头条每日更新的内容量是30万条，惠头条的单日内容量是15万条，就连刚上线1个月的微鲤头条，日更新量也能达到10万条。它们的内容多数是从腾讯、百度等内容聚合平台上采购，经过双重审核，话题以本地新闻、健康养生、娱乐资讯为主，不够有“营养”，但足够安全。这个数量足以满足用户覆盖80%热点的需求，相比之下，谁都没有太大优势。 看似简单可复制的金币体系，才是现阶段决定差距的关键。只有做过的人才知道，其中隐藏着多少运营的窍门。 30元提现－15元提现－1元提现－无门槛提现；转发链接送30枚金币－阅读送10枚金币－点击视频30枚金币……这些App的奖励规则往往以周为单位变化，有的规则写在明面上，有的则是只有后台的产品和运营才懂得的窍门。比如金币兑换成现金的比例并不是固定的，平台通称为“汇率”。 两个返利“头条”产品的任务对比 “每天赚多少钱进来，给多少出去，需要算得很清楚，这里面涉及到数值的设定，甚至是金融体系的知识。”微鲤头条创始人孙建说。这家人数在100人左右的公司同时运营着数个产品，中华万年历是其中最拳头的一个，7年来累计3亿注册用户，因为主打农历功能，在两广地区和小县城里都不乏忠实用户。 如果没有中华万年历这款下沉产品打底，孙建肯定不会进入返利信息流的混战中。在不确定的运营规则下，唯一确定的一点是，做这件事的门槛正在不断抬高。腾讯旗下的天天快报直接将拉新奖励提高到6到8元，这是趣头条拉新成本的双倍，也超过了今日头条极速版开出的奖赏。 好在下沉这个市场，由于体量过于庞大，发令枪刚打响，暂时还不是赢家通吃的局面。否则这不过又是一个谁的弹药更多，谁就能获胜的乏味故事。 “流量的获取、内容的运营，玩法设计上的竞争，这是个综合性的结果。内容是其中一个方面。”顺为合伙人程天说。顺为同时投资了今日头条、趣头条和一点资讯，所占份额都不大。从不同角度出发，这三个App分别切下了蛋糕中的一块，暂时还没到短兵相接的那一步，但总有一天，它们伸向流量的手会碰到一起。 “现在流量聚拢的趋势太明显了，所以大家都拿不到资源，创新一出来就被扼杀掉。那这个时候，只有拼命地在这个时间段，把你的一席之地给拿下来，不然可能再没有机会了。”微鲤头条的孙建说。返利阅读算是运营手段上的微创新，但它的窗口期也只有半年到一年。错过这个时间段，哪怕是趣头条，也不会有太多优势。 下沉市场还有另外一个典型特点，用户一旦使用了某一款产品，忠诚度往往比一二线城市用户更高——对还处于互联网小白阶段的用户来说，从一款App迁移到另一款上是个麻烦的行为。这多少给了趣头条们喘口气的时间。 获得腾讯投资后，趣头条暂时还算弹药充足，谭思亮表示构建内容体系将是趣头条接下来一段时间的重点工作。据趣头条投资方之一，红点创投管理合伙人袁文达介绍，趣头条还从硅谷重金挖了技术团队改进算法。 但比起这点，业界显然更关注另外一则小道消息：趣头条挖了一个拼好货高管，就连拼多多创始团队也在四处打听这个人的名字。已经长成“下沉三巨头”之一的趣头条，已经无法像一年前那样低调行事。他们想要挖的这个人到底是谁，想做什么，尚无一个准确的说法。但可以看到的是，在利用下沉市场的已有经验复制出一个拼多多之前，趣头条已经在使用拼团的方法来拉新了。 趣头条拼团收徒玩法 趣头条在复制拼多多，也在复制自己。诸多筑起护城河的方法里，模式的复制依然是最简单易操作的一种。趣头条正在做的一件事情是鼓励用户下载趣多拍和麻花语音。前者是短视频产品，后者的产品说明是“情感倾诉互助社区”，一模一样的金币玩法被套用这两款产品上。惠头条也推出了类似的惠动漫，主旨都是填充下沉市场用户的娱乐时间。它们的目标出奇一致，都是为了建立一个丰富的娱乐生态，尽量以低成本占据用户尽可能长的时间。 新一批被瞄准的用户和新一批入局的公司，都在进入“被洗”的过程中。 转载来源：“头条”激战三四线：谁占领下沉人群，谁占领明天 | 深氪]]></content>
      <categories>
        <category>科技</category>
      </categories>
      <tags>
        <tag>投资</tag>
        <tag>创业</tag>
        <tag>移动互联网</tag>
        <tag>今日头条</tag>
        <tag>张一鸣</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[汤晓鸥说全球顶尖AI哪有BAT身影，商汤发新产品正面刚苹果谷歌]]></title>
    <url>%2F2018%2F0809c21e%2F</url>
    <content type="text"><![CDATA[商汤不仅专门为这次大会买了竞价排名，还在现场醒目提示：员工不要占用现场座位。今天，商汤推出了一些新玩法，包括能在视频中瘦脸瘦腿美颜美形——归结起来就是，以后不止有P过的照骗，还会有看不出真身的视频。 李根 发自 凹非寺 量子位 报道 | 公众号 QbitAI 商汤，今天召开了一场“人工智能峰会”。 这家刚刚创下AI领域融资新纪录的公司，对这场活动有多重视？看看百度搜索广告和现场标语就知道了。 商汤不仅专门为这次大会买了竞价排名，还在现场醒目提示：员工不要占用现场座位。 为啥如此重视？看完整场大会就知道了。 因为这不光是一场大型技术产品发布会，也是一场大型招商会。 而且也是在这场会上，商汤创始人汤晓鸥也首次提出了商汤的愿景和目标——一个国产手机厂商发布会上司空见惯的目标。 一起围观下详情。 商汤新产品SenseAR开放平台 最先登场的是SenseAR，这是商汤之前AI商业化应用的核心平台。 今天，商汤推出了一些新玩法，包括能在视频中瘦脸瘦腿美颜美形——归结起来就是，以后不止有P过的照骗，还会有看不出真身的视频。 但是，这算是开胃小菜而已。 商汤在AR方面更大的野心是正式推出SenseAR开放平台，基于商汤技术引擎，与OEM手机厂商合作，开放API等工具，打造基于AR的App应用和体验。 对于这个SenseAR开放平台，商汤目光高远，直接对标苹果的ARKit、Google的ARCore，并且放出对比图正面刚，结果上完胜——至少发布会上是这样。 △ 商汤AR正面刚苹果、谷歌 当然，特殊的时间点，民族主义的节奏牌也能带来特殊意义。 商汤方面强调说：这是属于中国原创的AR平台，在当前这个特殊的时间点可能也有不一样的意义。 内容审核系统SenseMedia 其次登台的是SenseMedia，一个内容审核系统，可以鉴黄、集锦剪辑。 基于深度学习，SenseMedia可以实时读懂文字、图片和视频，抓取并过滤其中色情、暴力和敏感内容等有害信息。 之前，这样的工作有专门的“鉴黄师”和审核编辑，但商汤坚信利用AI，可以大幅度提升效率、降低成本。 另外，SenseMedia还具备视频摘要功能，能在无需人工参与的情况下，制作智能视频内容集锦。 趁着2018俄罗斯世界杯将至，商汤宣布推出足球集锦系统，尝试用机器解救熬夜剪片的体育编辑。 安防开放平台 接下来还是开放平台，但这次是商汤营收大头的安防领域。 商汤科技联合创始人杨帆会上发布了拥有城市监控和轨迹还原等功能的SenseFace 3.0，并透露已经在深圳等城市投入使用，最近还在3小时内帮助找到了走失老人。 此外，杨帆还宣布推出名为“SenseFoundry”的方舟城市级视觉开放平台，商汤会把之前应用在城市安防的相关技术引擎对外开放，打造城市安防为核心的开放生态圈。 卡车应用的SenseDrive系统 最后一个新发布的产品是商汤SenseDrive系统，也是商汤在智能驾驶汽车领域的首款产品，运用深度学习技术和嵌入式芯片优化技术结合，实现对驾驶员疲劳驾驶、驾驶分心、危险动作等驾驶员状态的实时智能检测与提醒。 实际上，去年11月，百度在世界大会上也推出了一样的产品，同样也是卡车货运场景，同样针对卡车司机目前存在的多个痛点。 所以也意味着商汤将在该业务上与百度直接竞争，商汤怕不怕？ 答案是：不仅不怕…… AI领域BAT何在？不仅不惧与百度直接竞争，商汤还强调技术上的领先。 在商汤创始人汤晓鸥的压轴演讲中，汤教授再次祭出AI顶会论文数量图，并表示“BAT都说是AI公司，但在国际上，存在的只有商汤。” 汤教授还说，虽然这两年BAT都在紧锣密鼓布局AI，天价挖人才发论文，但今年为止，发力最猛的腾讯AI有20多篇论文中标CVPR，但商汤有44篇。 商汤的自信，也能从投资人那里找到。 前来现场助阵的IDG资本合伙人牛奎光说，汤教授曾以钻石为喻，认为“钻石”商汤身处石头中间——不过汤晓鸥其后解释称说法有误，他当时只是表示大家都是钻石，只不过商汤更优秀一些。 这也能解释商汤的自我定位。 汤教授更倾向于把商汤看做中国原创的AI公司在国际顶尖竞技中的代表，还是在发布会上，商汤宣布与MIT达成合作，成立人工智能联盟。 “我们要做最好的公司，合作伙伴也找最好的，这样次啊能最大限度发挥我们人才的作用。” 牛奎光则透露，商汤之所以囤积了150多名顶尖AI博士，是因为把最初融资的钱都用来挖人了。 但汤晓鸥也强调，即便截至目前为止已累计融资10多亿美元，拥有70多个投资人，但商汤并不是一家烧钱的公司，甚至商汤已经实现了自负盈亏，迈入盈利状态。 最后，作为商汤科技创始人，汤晓鸥也对商汤的文化和愿景做出了明确。 他认为商汤不是一家狼性文化公司，而会是一家强调爱和同理心的“羊”文化公司，并且更希望以“黑羊”（Black Sheep）自居，强调中国原创。 汤教授说这个英文的意思虽然不尽正面，但也有“捣蛋鬼”的意思。他想强调的是一种特立独行、没有羊群跟随效应的意味。 如何证明这种“原创”？ 汤老师举例，2017年底以来，AR大热，但最早推出AR平台的……其实是商汤。 商汤的对手是谁？在段子、玩笑和举例最后，这位商汤科技创始人明确： 要做一家吃“苹果”的公司。 — 完 — 诚挚招聘 量子位正在招募编辑/记者，工作地点在北京中关村。期待有才气、有热情的同学加入我们！相关细节，请在量子位公众号(QbitAI)对话界面，回复“招聘”两个字。 量子位 QbitAI · 头条号签约作者 վ’ᴗ’ ի 追踪AI技术和产品新动态 转载来源：汤晓鸥说全球顶尖AI哪有BAT身影，商汤发新产品正面刚苹果谷歌]]></content>
      <categories>
        <category>科技</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>Google</tag>
        <tag>人工智能</tag>
        <tag>苹果公司</tag>
        <tag>商汤</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[腾讯视频开卖搜索广告，又一种付费也躲不开的广告]]></title>
    <url>%2F2018%2Fa75be634%2F</url>
    <content type="text"><![CDATA[当用户总量增长见顶，视频网站们正在想别的办法赚钱。过去一年她正在筹备更多平台上的新增广告资源，包括本月推出的APP内搜索广告产品“搜易达”。 转载来源：腾讯视频开卖搜索广告，又一种付费也躲不开的广告]]></content>
  </entry>
  <entry>
    <title><![CDATA[机器学习（十四）——协同过滤的ALS算法（2）、主成分分析 - CSDN博客]]></title>
    <url>%2F2018%2Fab6af7d9%2F</url>
    <content type="text"><![CDATA[&#91;http&#58;//antkillerfarm.github.io/&#93;(http&#58;//antkillerfarm.github.io/) Kendall秩相关系数（Kendall rank correlation coefficient）对于秩变量对(xi,yi),(xj,yj)(x_i,y_i),(x_j,y_j)： (xi−xj)(yi−yj)⎧⎩⎨&gt;0,=0,&lt;0,concordantneither concordant nor discordantdiscordant(x_i-x_j)(y_i-y_j)\begin&amp;#123cases&amp;#125 0, &amp; \text&amp;#123concordant&amp;#125 \=0, &amp; \text&amp;#123neither concordant nor discordant&amp;#125 \ &lt;0, &amp; \text&amp;#123discordant&amp;#125 \\end&amp;#123cases&amp;#125 τ=(number of concordant pairs)−(number of discordant pairs)n(n−1)/2\tau = \frac&amp;#123(\text&amp;#123number of concordant pairs&amp;#125) - (\text&amp;#123number of discordant pairs&amp;#125)&amp;#125&amp;#123n (n-1) /2&amp;#125 注：Sir Maurice George Kendall，1907~1983，英国统计学家。这个人职业生涯的大部分时间都是一个公务员，二战期间出任英国船运协会副总经理。1949年以后担任伦敦大学教授。 参见： &#91;https&#58;//en.wikipedia.org/wiki/Kendall_rank_correlation_coefficient&#93;(https&#58;//en.wikipedia.org/wiki/Kendall_rank_correlation_coefficient) Tanimoto系数T(x,y)=|X∩Y||X∪Y|=|X∩Y||X|+|Y|−|X∩Y|=∑xiyi∑x2i−−−−√+∑y2i−−−−√−∑xiyiT(x,y)=\frac&amp;#123|X\cap Y|&amp;#125&amp;#123|X\cup Y|&amp;#125=\frac&amp;#123|X\cap Y|&amp;#125&amp;#123|X|+|Y|-|X\cap Y|&amp;#125=\frac&amp;#123\sum x_iy_i&amp;#125&amp;#123\sqrt&amp;#123\sum x_i^2&amp;#125+\sqrt&amp;#123\sum y_i^2&amp;#125-\sum x_iy_i&amp;#125 该系数由Taffee T. Tanimoto于1960年提出。Tanimoto生平不详，从名字来看，应该是个日本人。在其他领域，它还有另一个名字Jaccard similarity coefficient。（两者的系数公式一致，但距离公式略有差异。） 注：Paul Jaccard，1868～1944，苏黎世联邦理工学院（ETH Zurich）博士，苏黎世联邦理工学院植物学教授。ETH Zurich可是出了24个诺贝尔奖得主的。 参见： &#91;https&#58;//en.wikipedia.org/wiki/Jaccard_index&#93;(https&#58;//en.wikipedia.org/wiki/Jaccard_index) ALS算法原理&#91;http&#58;//www.cnblogs.com/luchen927/archive/2012/02/01/2325360.html&#93;(http&#58;//www.cnblogs.com/luchen927/archive/2012/02/01/2325360.html) 上面的网页概括了ALS算法出现之前的协同过滤算法的概况。 ALS算法是2008年以来，用的比较多的协同过滤算法。它已经集成到Spark的Mllib库中，使用起来比较方便。 从协同过滤的分类来说，ALS算法属于User-Item CF，也叫做混合CF。它同时考虑了User和Item两个方面。 用户和商品的关系，可以抽象为如下的三元组：&lt;User,Item,Rating&gt;。其中，Rating是用户对商品的评分，表征用户对该商品的喜好程度。 假设我们有一批用户数据，其中包含m个User和n个Item，则我们定义Rating矩阵Rm×nR_&amp;#123m\times n&amp;#125，其中的元素ruir_&amp;#123ui&amp;#125表示第u个User对第i个Item的评分。 在实际使用中，由于n和m的数量都十分巨大，因此R矩阵的规模很容易就会突破1亿项。这时候，传统的矩阵分解方法对于这么大的数据量已经是很难处理了。 另一方面，一个用户也不可能给所有商品评分，因此，R矩阵注定是个稀疏矩阵。矩阵中所缺失的评分，又叫做missing item。 针对这样的特点，我们可以假设用户和商品之间存在若干关联维度（比如用户年龄、性别、受教育程度和商品的外观、价格等），我们只需要将R矩阵投射到这些维度上即可。这个投射的数学表示是： Rm×n≈Xm×kYTn×k(1)R_&amp;#123m\times n&amp;#125\approx X_&amp;#123m\times k&amp;#125Y_&amp;#123n\times k&amp;#125^T\tag&amp;#1231&amp;#125 这里的≈\approx表明这个投射只是一个近似的空间变换。 不懂这个空间变换的同学，可参见《机器学习（十二）》中的“奇异值分解”的内容，或是本节中的“主成分分析”的内容。 一般情况下，k的值远小于n和m的值，从而达到了数据降维的目的。 幸运的是，我们并不需要显式的定义这些关联维度，而只需要假定它们存在即可，因此这里的关联维度又被称为Latent factor。k的典型取值一般是20～200。 这种方法被称为概率矩阵分解算法(probabilistic matrix factorization，PMF)。ALS算法是PMF在数值计算方面的应用。 为了使低秩矩阵X和Y尽可能地逼近R，需要最小化下面的平方误差损失函数： minx∗,y∗∑u,i is known(rui−xTuyi)2\min_&amp;#123x_,y_&amp;#125\sum_&amp;#123u,i\text&amp;#123 is known&amp;#125&amp;#125(r_&amp;#123ui&amp;#125-x_u^Ty_i)^2 考虑到矩阵的稳定性问题，使用Tikhonov regularization，则上式变为： minx∗,y∗L(X,Y)=minx∗,y∗∑u,i is known(rui−xTuyi)2+λ(|xu|2+|yi|2)(2)\min_&amp;#123x_,y_&amp;#125L(X,Y)=\min_&amp;#123x_,y_&amp;#125\sum_&amp;#123u,i\text&amp;#123 is known&amp;#125&amp;#125(r_&amp;#123ui&amp;#125-x_u^Ty_i)^2+\lambda(|x_u|^2+|y_i|^2)\tag&amp;#1232&amp;#125 优化上式，得到训练结果矩阵Xm×k,Yn×kX_&amp;#123m\times k&amp;#125,Y_&amp;#123n\times k&amp;#125。预测时，将User和Item代入rui=xTuyir_&amp;#123ui&amp;#125=x_u^Ty_i，即可得到相应的评分预测值。 同时，矩阵X和Y，还可以用于比较不同的User（或Item）之间的相似度，如下图所示： ALS算法的缺点在于： 1.它是一个离线算法。 2.无法准确评估新加入的用户或商品。这个问题也被称为Cold Start问题。 ALS算法优化过程的推导公式2的直接优化是很困难的，因为X和Y的二元导数并不容易计算，这时可以使用类似坐标下降法的算法，固定其他维度，而只优化其中一个维度。 对xux_u求导，可得： ∂L∂xu=−2∑i(rui−xTuyi)yi+2λxu=−2∑i(rui−yTixu)yi+2λxu=−2YTru+2YTYxu+2λxu\begin&amp;#123align&amp;#125\frac&amp;#123\partial L&amp;#125&amp;#123\partial x_u&amp;#125&amp;=-2\sum_i(r_&amp;#123ui&amp;#125-x_u^Ty_i)y_i+2\lambda x_u\&amp;=-2\sum_i(r_&amp;#123ui&amp;#125-y_i^Tx_u)y_i+2\lambda x_u\&amp;=-2Y^Tr_u+2Y^TYx_u+2\lambda x_u\end&amp;#123align&amp;#125 令导数为0，可得： YTYxu+λIxu=YTru⇒xu=(YTY+λI)−1YTru(3)Y^TYx_u+\lambda Ix_u=Y^Tr_u\Rightarrow x_u=(Y^TY+\lambda I)^&amp;#123-1&amp;#125Y^Tr_u\tag&amp;#1233&amp;#125 同理，对yiy_i求导，由于X和Y是对称的，因此可得类似的结论： yi=(XTX+λI)−1XTri(4)y_i=(X^TX+\lambda I)^&amp;#123-1&amp;#125X^Tr_i\tag&amp;#1234&amp;#125 因此整个优化迭代的过程为： 1.随机生成X、Y。（相当于对迭代算法给出一个初始解。） Repeat until convergence &amp;#123 2.固定Y，使用公式3更新xux_u。 3.固定X，使用公式4更新yiy_i。 &amp;#125 一般使用RMSE（root-mean-square error）评估误差是否收敛，具体到这里就是： RMSE=∑(R−XYT)2N−−−−−−−−−−−−−√RMSE=\sqrt&amp;#123\frac&amp;#123\sum(R-XY^T)^2&amp;#125&amp;#123N&amp;#125&amp;#125 其中，N为三元组&lt;User,Item,Rating&gt;的个数。当RMSE值变化很小时，就可以认为结果已经收敛。 算法复杂度： 1.求xux_u：O(k2N+k3m)O(k^2N+k^3m) 2.求yiy_i：O(k2N+k3n)O(k^2N+k^3n) 可以看出当k一定的时候，这个算法的复杂度是线性的。 因为这个迭代过程，交替优化X和Y，因此又被称作交替最小二乘算法（Alternating Least Squares，ALS）。 隐式反馈用户给商品评分是个非常简单粗暴的用户行为。在实际的电商网站中，还有大量的用户行为，同样能够间接反映用户的喜好，比如用户的购买记录、搜索关键字，甚至是鼠标的移动。我们将这些间接用户行为称之为隐式反馈（implicit feedback），以区别于评分这样的显式反馈（explicit feedback）。 隐式反馈有以下几个特点： 1.没有负面反馈（negative feedback）。用户一般会直接忽略不喜欢的商品，而不是给予负面评价。 2.隐式反馈包含大量噪声。比如，电视机在某一时间播放某一节目，然而用户已经睡着了，或者忘了换台。 3.显式反馈表现的是用户的喜好（preference），而隐式反馈表现的是用户的信任（confidence）。比如用户最喜欢的一般是电影，但观看时间最长的却是连续剧。大米购买的比较频繁，量也大，但未必是用户最想吃的食物。 4.隐式反馈非常难以量化。 ALS-WR针对隐式反馈，有ALS-WR算法（ALS with Weighted-λ\lambda-Regularization）。 首先将用户反馈分类： pui=&amp;#1231,0,preferenceno preferencep_&amp;#123ui&amp;#125=\begin&amp;#123cases&amp;#1251, &amp; \text&amp;#123preference&amp;#125 \0, &amp; \text&amp;#123no preference&amp;#125 \\end&amp;#123cases&amp;#125 但是喜好是有程度差异的，因此需要定义程度系数： cui=1+αruic_&amp;#123ui&amp;#125=1+\alpha r_&amp;#123ui&amp;#125 这里的ruir_&amp;#123ui&amp;#125表示原始量化值，比如观看电影的时间； 这个公式里的1表示最低信任度，α\alpha表示根据用户行为所增加的信任度。 最终，损失函数变为： minx∗,y∗L(X,Y)=minx∗,y∗∑u,icui(pui−xTuyi)2+λ(∑u|xu|2+∑i|yi|2)\min_&amp;#123x_,y_&amp;#125L(X,Y)=\min_&amp;#123x_,y_&amp;#125\sum_&amp;#123u,i&amp;#125c_&amp;#123ui&amp;#125(p_&amp;#123ui&amp;#125-x_u^Ty_i)^2+\lambda(\sum_u|x_u|^2+\sum_i|y_i|^2) 除此之外，我们还可以使用指数函数来定义cuic_&amp;#123ui&amp;#125： cui=1+αlog(1+rui/ϵ)c_&amp;#123ui&amp;#125=1+\alpha \log(1+r_&amp;#123ui&amp;#125/\epsilon) ALS-WR没有考虑到时序行为的影响，时序行为相关的内容，可参见： &#91;http&#58;//www.jos.org.cn/1000-9825/4478.htm&#93;(http&#58;//www.jos.org.cn/1000-9825/4478.htm) 参考参考论文： 《Large-scale Parallel Collaborative Filtering forthe Netflix Prize》 《Collaborative Filtering for Implicit Feedback Datasets》 《Matrix Factorization Techniques for Recommender Systems》 其他参考： &#91;http&#58;//www.jos.org.cn/html/2014/9/4648.htm&#93;(http&#58;//www.jos.org.cn/html/2014/9/4648.htm) &#91;http&#58;//www.fuqingchuan.com/2015/03/812.html&#93;(http&#58;//www.fuqingchuan.com/2015/03/812.html) &#91;http&#58;//www.docin.com/p-714582034.html&#93;(http&#58;//www.docin.com/p-714582034.html) &#91;http&#58;//www.tuicool.com/articles/fANvieZ&#93;(http&#58;//www.tuicool.com/articles/fANvieZ) &#91;http&#58;//www.68idc.cn/help/buildlang/ask/20150727462819.html&#93;(http&#58;//www.68idc.cn/help/buildlang/ask/20150727462819.html) 主成分分析真实的训练数据总是存在各种各样的问题。 比如拿到一个汽车的样本，里面既有以“千米/每小时”度量的最大速度特征，也有“英里/小时”的最大速度特征。显然这两个特征有一个是多余的，我们需要找到，并去除这个冗余。 再比如，针对飞行员的调查，包含两个特征：飞行的技能水平和对飞行的爱好程度。由于飞行员是很难培训的，因此如果没有对飞行的热爱，也就很难学好飞行。所以这两个特征实际上是强相关的（strongly correlated）。如下图所示： 我们的目标就是找出上图中所示的向量u1u_1。 为了实现这两个目标，我们可以采用PCA（Principal components analysis）算法。 数据的规则化处理在进行PCA算法之前，我们首先要对数据进行预处理，使之规则化。其方法如下： 1.μ=1m∑mi=1x(i)\mu=\frac&amp;#1231&amp;#125&amp;#123m&amp;#125\sum_&amp;#123i=1&amp;#125^mx^&amp;#123(i)&amp;#125 2.x(i)&#58;=x(i)−μx^&amp;#123(i)&amp;#125&#58;=x^&amp;#123(i)&amp;#125-\mu 3.σ2j=1m∑i(x(i))2\sigma_j^2=\frac&amp;#1231&amp;#125&amp;#123m&amp;#125\sum_i(x^&amp;#123(i)&amp;#125)^2 4.x(i)j&#58;=x(i)j/σjx_j^&amp;#123(i)&amp;#125&#58;=x_j^&amp;#123(i)&amp;#125/\sigma_j 多数情况下，特征空间中，不同特征向量所代表的维度之间，并不能直接比较。 比如，摄氏度和华氏度，虽然都是温度的单位，但两种温标的原点和尺度都不相同，因此需要规范化之后才能比较。 步骤1和2，用于消除原点偏差（常数项偏差）。步骤3和4，用于统一尺度（一次项偏差）。 虽然上面的办法，对于二次以上的偏差无能为力，然而多数情况下，这种处理，已经比原始状态好多了。 PCA算法推导回到之前的话题，为了找到主要的方向u，我们首先观察一下，样本点在u上的投影应该是什么样子的。 |—— 上图所示是5个样本在不同向量上的投影情况。其中，X表示样本点，而黑点表示样本在u上的投影。 很显然，左图中的u就是我们需要求解的主成分的方向。和右图相比，左图中各样本点x在u上的投影点比较分散，也就是投影点之间的方差较大。 由《机器学习（十一）》一节的公式4，可知样本点x在单位向量u上的投影为：xTux^Tu。 因此，这个问题的代价函数为： 1m∑i=1m(x(i)Tu)2=1m∑i=1m(x(i)Tu)T(x(i)Tu)=1m∑i=1muTx(i)x(i)Tu=uT(1m∑i=1mx(i)x(i)T)u=uTΣu\begin&amp;#123align&amp;#125&amp;\frac&amp;#1231&amp;#125&amp;#123m&amp;#125\sum_&amp;#123i=1&amp;#125^m\left(x^&amp;#123(i)^T&amp;#125u\right)^2=\frac&amp;#1231&amp;#125&amp;#123m&amp;#125\sum_&amp;#123i=1&amp;#125^m\left(x^&amp;#123(i)^T&amp;#125u\right)^T\left(x^&amp;#123(i)^T&amp;#125u\right)\&amp;=\frac&amp;#1231&amp;#125&amp;#123m&amp;#125\sum_&amp;#123i=1&amp;#125^mu^Tx^&amp;#123(i)&amp;#125x^&amp;#123(i)^T&amp;#125u=u^T\left(\frac&amp;#1231&amp;#125&amp;#123m&amp;#125\sum_&amp;#123i=1&amp;#125^mx^&amp;#123(i)&amp;#125x^&amp;#123(i)^T&amp;#125\right)u=u^T\Sigma u\end&amp;#123align&amp;#125 即： maxus.t.uTΣuuTu=1\begin&amp;#123align&amp;#125&amp;\operatorname&amp;#123max&amp;#125_&amp;#123u&amp;#125&amp; &amp; u^T\Sigma u\&amp;\operatorname&amp;#123s.t.&amp;#125&amp; &amp; u^Tu=1\end&amp;#123align&amp;#125 其拉格朗日函数为： L(u)=uTΣu−λ(uTu−1)\mathcal&amp;#123L&amp;#125(u)=u^T\Sigma u-\lambda(u^Tu-1) 转载来源：机器学习（十四）——协同过滤的ALS算法（2）、主成分分析 - CSDN博客]]></content>
  </entry>
  <entry>
    <title><![CDATA[在Github惨遭404后，那位不能提名字的大学生又选择了区块链]]></title>
    <url>%2F2018%2F8b3a85eb%2F</url>
    <content type="text"><![CDATA[昨天，公共区块链平台以太坊上发生了一笔可能永载史册的交易。通过这笔交易，敢言者的声音被宣扬，正义在某种程度上得到伸张。 昨天，公共区块链平台以太坊 (Ethereum) 上发生了一笔可能永载史册的交易。 通过这笔交易，敢言者的声音被宣扬，正义在某种程度上得到伸张。 区块 5490403，一位用户用自己的钱包给自己转账了 0 以太币。 以太坊每笔交易都需要一笔手续费，也即所谓的“燃料”。为了这笔交易，该用户支付了 0.0007787 个以太币，这笔手续费在交易发生之时大约价值 53 美分，或者三块三毛人民币。 花了手续费，最后却没有完成任何价值的转移，听起来这位用户脑子抽了。 实际上并非如此。这笔交易的关键在于，其正当性和信息的准确性，已经得到遍布全球超过 1.6 万个以太坊节点的认证。没人可以删除，没人可以篡改。这正是以太坊以及大部分公链的特性。 通过这笔交易，一段文字永久地存在了遍布全球的超过 1.6 万个以太坊节点中。而这一切仅耗费了一套煎饼的钱。 为众人抱薪者，必将铭刻于区块链上。 区块链新应用：写文章人们都听说过区块链，但大部分人都不知道这玩意到底能干嘛用。而通过这笔交易，区块链终于证明了自己：它有能力，让一段信息“恒久远，永流传”。 当你在支付宝上转账，可以附上一句话。在支付宝上，这句话只有你和转账对象（或者加上支付宝）知道。 和支付宝一样，在以太坊上转账同样可以留下一句话。然而不同之处在于，这段话并非双方之间的“悄悄话”，而将成为区块链上，甚至整个互联网上一段的“永不消失的电波”。 在炒泡沫、割韭菜和云养猫之外，区块链技术终于发挥了应有的作用。 事实上，在区块链上放置永久的信息，并不是什么新鲜技术，反而是以太坊这类公链平台的基础能力操作起来都简单的不可思议。 今天，我们就来教你怎样在区块链上写文章。 准备你至少需要四样东西： 钱包——相当于银行卡号，唯一，生成起来很方便。 以太币 (Ether)——在以太坊上交易使用的 token，也即所谓“数字货币”，兑换起来也很方便 内容——你需要发布的内容。 编码转换器——后面告诉你为什么需要，以及怎样使用。英文内容可以直接转成 16 进制码，中文内容则需要先转 Unicode 码然后再转 16 进制码。 首先我们需要一个以太坊钱包。你可以用 MetaMask 这样的浏览器插件，或者 MyEtherWallet 网站来生成并管理自己的钱包。今天的教程就围绕 MetaMask 进行。 （如果你是硅星人/PingWest品玩的忠实读者，你可能会记得 MetaMask 就是上次那个在区块链上养猫所用的工具。） 交易本质上，在区块链上做大部分事情，包括发文章，都是交易。 很简单，点击 MetaMask，按照流程完成注册，然后点击 Send 按钮发起一个交易。 什么，没有以太币？你也可以点击 Buy 按钮，购买以太币。因为我们在美国，此处就用美国兑换方式 Coinbase 演示。 输入你想花多少钱，输入你的电子邮件地址，买完之后会邮件告知，以太币会打入你的钱包。 你的银行可能会阻止你以太币这样的数字货币，请你通过和银行联系解决问题，或者找会玩数字货币的朋友给你转一点以太币。 接下来，你有以太币了，可以给自己转账了。 你需要在下面的 Transaction Data (Optional) 里面填写你想发布的内容。 内容需要转换为 16 进制码，还要在代码之前加上固定头部“0x”以表示 16 进制。这里建议你把文字翻译成英文，因为英文 (UTF-8) 转换到 16 进制是最方便的。区块 5490403 就是这样做的。 其实这个功能本来是用来做备注用的，因为区块链交易没有什么名目，最后要靠备注来标记这笔钱用来投资还是买酒了。 接下来，MetaMask 会提醒你设置这笔交易的手续费 (Gas)。 “手续费”是个值得说的东西。理论上，在以太坊或者任何公链上交易，是可以不支付手续费的，但实际不给手续费不可能交易成功。因为每笔交易的背后都是所谓的矿工在执行，当交易量过大的时候（其实一直很多）就会出现交易阻塞，这时矿工先执行谁就要看手续费的多少了。 设置好手续费，点接受，就好了！ 永不消逝的电波前面提到，以太坊是一个公链，交易完成后就登录在一个账本上，而这个账本也是公开的。 全球有 16646 个以太坊节点，也就是 16646 本完全同步的账本。一旦写入，除非这 16646 个节点都下线并被摧毁，你的文章总有一份保存在世界的某个地方，就像永不消逝的电波一样…… 查看这段信息，以及整个交易，也有很多方式。比如 Etherscan 网站，你可以理解为区块链是一个硬盘，而像 Etherscan 这样的网站或者软件，就是文件浏览器。 不必多说，给自己设个小目标吧：在这星期内完成一次交易，把你喜欢的文章保存到以太坊或者任何其他的公链上。 只需要一套煎饼的钱，你也可以让那些站在阴影里的人明白：正义永不缺席，正义的声音也不会消失。 本文转自硅星人（ID：guixingren123），作者邢逸帆、宋图样，文章为作者独立观点，不代表芥末堆立场。 转载来源：在Github惨遭404后，那位不能提名字的大学生又选择了区块链]]></content>
      <categories>
        <category>其他</category>
      </categories>
      <tags>
        <tag>投资</tag>
        <tag>GitHub</tag>
        <tag>区块链</tag>
        <tag>数字货币</tag>
        <tag>支付宝</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[缺只眼睛也能补回来！英伟达这个自动修补图像的AI完虐PS]]></title>
    <url>%2F2018%2F9b89c63c%2F</url>
    <content type="text"><![CDATA[然而，Photoshop不一定是最好的选择~这次英伟达，就搞出了一个新AI，拥有技惊四座的P图本领。 问耕 发自 凹非寺量子位 出品 | 公众号 QbitAI 在修图这件事上，Photoshop有着崇高的地位。 前一阵子，流传过这样一个段子：“甲方不要PS！让我们用Photoshop做！”足以说明开头的结论。 然而，Photoshop不一定是最好的选择~ 这次英伟达，就搞出了一个新AI，拥有技惊四座的P图本领。 比方，给你这样一张图。 嗯，对，就是这样，画面严重缺失。请用AI把这张图修复一下，P得要看起来像原图一样真实。 难不难？这画面上是啥都不一定能看清。 普通的AI也就能自动修补成下面这样或那样。 英伟达的AI呢？可以P成这样： 对比下原图。 再来一个，还是同样画面确缺失严重。 普通的AI也就能自动修补成下面这样或那样。 英伟达的AI，可以P成这样： 再对比下原图。 英伟达的AI是不是更赞？ 但，这个技术有什么用？ 其实与大段大段的补全缺失画面相比，快速且优秀的进行局部P图，才是这个技术真正发挥实力的地方。 静态展示不过瘾，动态展示才更令人惊叹。 比如下面这个动图，把左图中的人、线、红旗、石头、棍子都P掉，应该怎么做？很简单，就是直接涂抹掉，然后AI就自动修复了。 还比如这样。 其实这个AI的本事，就是能在一片空白之中，更好的填补上缺失的内容。 极端情况，比方处理人脸时候，眼睛被遮蔽了。 英伟达的AI就能重新补上一双眼睛，当然不可能是原来那个人的眼睛，但是至少能弥补的也算相对完美吧~ 以及，对于也能让头发更浓密。 也能让发际线更高~ 英伟达的新AI是真么做到的？ 答案就在这家公司新发布的论文里。这篇论文标题：Image Inpainting for Irregular Holes Using Partial Convolutions 。 英伟达的研究团队提出了一种新的模型，使用部分卷积的方法，其中卷积被掩蔽，并且仅基于有效像素进行重新归一化等处理。 论文中，还与现有其他方法进行了对比，有很多公式。 比如这种。 还有这种。 详细的方法和原理，可以前往阅读论文，地址： https&#58;//arxiv.org/abs/1804.07723 总而言之就是一句话：我不是针对谁，在座的都是…… 最后顺便提一下，这篇论文的一作Guilin Liu，在加入英伟达之前，还曾在Adobe Research实习过。 Adobe，就是搞出Photoshop（和一堆其他软件）的那家公司~ — 完 — 诚挚招聘 量子位正在招募编辑/记者，工作地点在北京中关村。期待有才气、有热情的同学加入我们！相关细节，请在量子位公众号(QbitAI)对话界面，回复“招聘”两个字。 量子位 QbitAI · 头条号签约作者 վ’ᴗ’ ի 追踪AI技术和产品新动态 转载来源：缺只眼睛也能补回来！英伟达这个自动修补图像的AI完虐PS]]></content>
      <categories>
        <category>其它</category>
      </categories>
      <tags>
        <tag>软件</tag>
        <tag>人工智能</tag>
        <tag>英伟达</tag>
        <tag>Photoshop</tag>
        <tag>Adobe</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Twitter也在用的分布式日志系统 值得一用]]></title>
    <url>%2F2018%2F51d03556%2F</url>
    <content type="text"><![CDATA[日志，可以说是程序员最熟悉的一种数据结构了。网络设备、系统及服务程序等，在运作时都会产生一个叫log的事件记录;每一行日志都记载着日期、时间、使用者及动作等相关操作的描述。它存在于大家每天的工作中，是一组只追加，严格有序的记录序列。日志是一种很有效的数据结构，可用来解决很多分布式系统的问题。 在5月11日-13日，北京国际会议中心隆重举行的第八届中国数据库技术大会(DTCC 2017)——开源技术分会场上，Twitter消息组技术主管郭斯杰向大家分享了“Apache DistributedLog(分布式日志系统)的强一致性简化”。 ▲Twitter消息组技术主管郭斯杰 郭斯杰毕业于中科院计算所，加入 Twitter 之前，就职于 Yahoo。专注于分布式消息中间件和分布式存储系统方向。同时，他也是Apache DistributedLog的联合创始人，Apache BookKeeper的PMC Chair。 郭斯杰先生在演讲中指出了目前分布式系统中存在的一些挑战：磁盘/服务器的年故障率高达10%、不可用、不一致以及Split-Brains(“脑裂”现象)。 脑裂现象指：本来一个大脑的两半球互相配合，变成了分裂成两个独立的大脑，都认为对方已死。在集群环境中，有这么几种可能造成”Split-Brain”现象：1、在集群环境中的节点间的心跳线同时断掉后，集群系统所处于的一种特殊状态。例如节点1和2组成一个集群，突然1和2间的心跳同时都断了，如果此前节点1正在运行应用，心跳都断掉后2开始去接管应用，强行加载数据，此时就是split-brain。2、集群中节点因为处理器忙或者其他原因暂时停止响应时，其他节点可能误认为该节点“已死”。后果：节点间争夺共享磁盘(即资源)的访问权，都对共享文件系统产生读写操作，从而导致共享磁盘文件系统损坏。 在谈到这些问题的解决办法时，郭斯杰首先表示，目前有一些算法的使用已获得了大家的共识，如Paxos、Zab、Raft……等等。但在此之后，仍有许多困扰。关于命令问题，哪种改变应该首先考虑?至于确定性，命令不会改变，甚至不会多次读取。还有一个问题是，如何保持复制日志的一致性? 为真正解决这些问题，郭斯杰向大家分享了Apache DistributedLog。该系统具有如下特性： 高性能。Apache DistributedLog可以在具有大量并发日志的持久写入中提供毫秒延迟，并且可以从数千个客户端处理每秒大量的读和写操作。- 持久性和一致性。消息被持久化到磁盘上，并复制以存储多个副本以防止数据丢失。在严格的排序下，它们保证了写入者和读取者之间的一致性。- 高效的扇入和扇出。Apache DistributedLog提供了一个高效的服务层,优化运行的多租户数据中心环境便或纱等。服务层能够支持大规模的读(扇入)和写(扇出)。- 不同的工作负载。Apache DistributedLog支持各种工作负载，从那些对延迟敏感的联机事务处理(OLTP)应用程序(如分布式数据库和内存复制状态机),实时摄取和计算，到分析处理。- 多租户。为了支持多租户的大量日志，Apache DistributedLog专为I/O在实际工作负载中隔离而设计。- 分层体系架构。Apache分布式日志系统有一个现代的分层架构设计，它将无状态的服务层与有状态的存储层分离。支持大规模写入(扇入)和读取(扇出),允许扩展存储独立的CPU和内存。持久性和一致性。消息被持久化到磁盘上，并复制以存储多个副本以防止数据丢失。在严格的排序下，它们保证了写入者和读取者之间的一致性。 不同的工作负载。Apache DistributedLog支持各种工作负载，从那些对延迟敏感的联机事务处理(OLTP)应用程序(如分布式数据库和内存复制状态机),实时摄取和计算，到分析处理。 分层体系架构。Apache分布式日志系统有一个现代的分层架构设计，它将无状态的服务层与有状态的存储层分离。支持大规模写入(扇入)和读取(扇出),允许扩展存储独立的CPU和内存。 ▲Apache DistributedLog软件栈 ▲日志流解析 在接受笔者采访，谈及Apache DistributedLog的未来规划时，郭斯杰先生表示也将顺应“时代趋势”走开源的路线。去年五月份，他们就已经将Apache DistributedLog开源，并且希望以开源的方式壮大社区，同时也在推动与如雅虎、百度等公司的合作，以期共同推动技术的进步，帮助他人更好地适应自己的应用场景。 最后，郭斯杰先生以一个互联网行业技术人员的身份，向广大的“同僚”们提出了自己关于职业发展方向的一些建议。郭斯杰认为，计算机领域是一个非常细分的领域，作为此领域内的人士，最重要的应该是抓住自己的兴趣点。如果对底层技术比较感兴趣，可以深入到其中，通过参与社区的方式去学习了解相关的底层技术。而对于上层技术比较感兴趣的人，则可通过参与开源技术锻炼自己的技术能力，提升自身水平。 转载来源：Twitter也在用的分布式日志系统 值得一用]]></content>
      <categories>
        <category>科技</category>
      </categories>
      <tags>
        <tag>软件</tag>
        <tag>Twitter</tag>
        <tag>Apache</tag>
        <tag>数据结构</tag>
        <tag>雅虎</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[以太坊中的账户、交易、Gas和区块Gas Limit » 论坛 » EthFans | 以太坊爱好者]]></title>
    <url>%2F2018%2F7fbcbb4a%2F</url>
    <content type="text"><![CDATA[以太坊中的账户、交易、Gas和区块Gas Limit » 论坛 » EthFans | 以太坊爱好者 转载来源：以太坊中的账户、交易、Gas和区块Gas Limit » 论坛 » EthFans | 以太坊爱好者]]></content>
  </entry>
  <entry>
    <title><![CDATA[新论文：首个基于决策树集成的自动编码器，表现优于DNN]]></title>
    <url>%2F2018%2F48ca02d0%2F</url>
    <content type="text"><![CDATA[新论文：首个基于决策树集成的自动编码器，表现优于DNN 转载来源：新论文：首个基于决策树集成的自动编码器，表现优于DNN]]></content>
  </entry>
  <entry>
    <title><![CDATA[【值得收藏的深度学习思维导图】全面梳理基本概念与11大模型关系]]></title>
    <url>%2F2018%2Fe80f32a8%2F</url>
    <content type="text"><![CDATA[【值得收藏的深度学习思维导图】全面梳理基本概念与11大模型关系 转载来源：【值得收藏的深度学习思维导图】全面梳理基本概念与11大模型关系]]></content>
      <tags>
        <tag>新智元</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[床抬高一米，小户型变大户型！]]></title>
    <url>%2F2018%2Fc0527d8f%2F</url>
    <content type="text"><![CDATA[床抬高一米，小户型变大户型！ 转载来源：床抬高一米，小户型变大户型！]]></content>
      <tags>
        <tag>设计吧</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[EOS白皮书 - 简书]]></title>
    <url>%2F2018%2F8ea15bc4%2F</url>
    <content type="text"><![CDATA[EOS白皮书 - 简书 转载来源：EOS白皮书 - 简书]]></content>
  </entry>
  <entry>
    <title><![CDATA[朝鲜买房指南]]></title>
    <url>%2F2018%2Fd462d30b%2F</url>
    <content type="text"><![CDATA[朝鲜买房指南 转载来源：朝鲜买房指南]]></content>
      <tags>
        <tag>地球知识局</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[您要找的页面不存在 - 简书]]></title>
    <url>%2F2018%2Fca035c5d%2F</url>
    <content type="text"><![CDATA[您要找的页面不存在 - 简书 转载来源：您要找的页面不存在 - 简书]]></content>
  </entry>
  <entry>
    <title><![CDATA[EOS DAWN-V3.0.0 智能合约开发之Hello World]]></title>
    <url>%2F2018%2F8c8082e7%2F</url>
    <content type="text"><![CDATA[C、C++、Java还是任何其他语言，一般刚开始学习的时候，我们都会从HelloWorld开始，这篇文章主要讲解EOS DAWN-V3.0.0智能合约开发之Hello World。 1. 编写合约代码在桌面创建一个文件夹，比如：0418，用Atom打开0418文件夹。新建文件Hello.cpp文件，并将下面的源码拷贝到Hello.cpp文件中。 1#include &lt;eosiolib/eosio.hpp&gt;#include &lt;eosiolib/print.hpp&gt;// 视频网站：http://kongyixueyuan.com// 个人博客：http://liyuechun.org// 公众号：区块链部落// 进技术群，请加微信（kongyixueyuan）//用eosio命名空间using namespace eosio;//所有的智能合约都继承自contract类class Hello : public eosio::contract &#123; public: using contract::contract; /// @abi action void hi( account_name user ) &#123; print( &quot;Hello, &quot;, name&#123;user&#125; ); &#125;&#125;;EOSIO_ABI( Hello, (hi) ) 2. 生成.wast文件1liyuechun:Project yuechunli$ eosiocpp -o Hello.wast Hello.cppliyuechun:Project yuechunli$ lsHello.cppHello.wastliyuechun:Project yuechunli$ 3. 生成.abi文件1liyuechun:Project yuechunli$ eosiocpp -g Hello.abi Hello.cpp Generated Hello.abi ...liyuechun:Project yuechunli$ lsHello.abiHello.cppHello.wastliyuechun:Project yuechunli$ 1&#123; &quot;____comment&quot;: &quot;This file was generated by eosio-abigen. DO NOT EDIT - 2018-04-18T08:15:50&quot;, &quot;types&quot;: , &quot;structs&quot;: [&#123; &quot;name&quot;: &quot;hi&quot;, &quot;base&quot;: &quot;&quot;, &quot;fields&quot;: [&#123; &quot;name&quot;: &quot;user&quot;, &quot;type&quot;: &quot;account_name&quot; &#125; ] &#125; ], &quot;actions&quot;: [&#123; &quot;name&quot;: &quot;hi&quot;, &quot;type&quot;: &quot;hi&quot;, &quot;ricardian_contract&quot;: &quot;&quot; &#125; ], &quot;tables&quot;: , &quot;clauses&quot;: &#125; 4. 创建钱包账号4.1 创建钱包1liyuechun:Hello yuechunli$ cleos wallet createCreating wallet: defaultSave password to use in the future to unlock this wallet.Without password imported keys will not be retrievable.&quot;PW5J3rx7Bfg9zb8Kf2owTytccFyJqtDTrqnUX8iBRRUvbwM8RyzRL&quot; PW5J3rx7Bfg9zb8Kf2owTytccFyJqtDTrqnUX8iBRRUvbwM8RyzRL必须保存好，解锁钱包时需要使用到这个密码。 4.2 创建两组key1liyuechun:Hello yuechunli$ ./cleos create key-bash: ./cleos: No such file or directoryliyuechun:Hello yuechunli$ cleos create keyPrivate key: 5K7QdknUZsF9apdBhD8TDMZGJjw8zJ8esYwS173YyFRv2453Z9tPublic key: EOS5RU8VsYBLnN5snGeUKmt1sDDzpvQbGyW7LPP6qEryaFctYieCKliyuechun:Hello yuechunli$ cleos create keyPrivate key: 5J8kComGiQHZyNmH6VvkHgtFggeQemazLpihKR4QW75DNkWTVdAPublic key: EOS5fqiC3VFAJ1riMrKf8vzD28nqd4EpXvZGpXt6YewEBnH8DYinG 4.3 向钱包导入私钥1liyuechun:Hello yuechunli$ cleos wallet import 5K7QdknUZsF9apdBhD8TDMZGJjw8zJ8esYwS173YyFRv2453Z9timported private key for: EOS5RU8VsYBLnN5snGeUKmt1sDDzpvQbGyW7LPP6qEryaFctYieCKliyuechun:Hello yuechunli$ cleos wallet import 5J8kComGiQHZyNmH6VvkHgtFggeQemazLpihKR4QW75DNkWTVdAimported private key for: EOS5fqiC3VFAJ1riMrKf8vzD28nqd4EpXvZGpXt6YewEBnH8DYinG 4.4 创建账户1liyuechun:cleos yuechunli$ ./cleos create account eosio liyc111 EOS5RU8VsYBLnN5snGeUKmt1sDDzpvQbGyW7LPP6qEryaFctYieCK EOS5fqiC3VFAJ1riMrKf8vzD28nqd4EpXvZGpXt6YewEBnH8DYinG 5. 部署合约1liyuechun:build yuechunli$ cleos set contract liyc111 ./contracts/HelloReading WAST/WASM from ./contracts/Hello/Hello.wast...Assembling WASM...Publishing contract...executed transaction: 21d891e425f3d65852432e2b6a78146e2e2992a267c9f28c8ce56cd5dbea98f2 1632 bytes 2200576 cycles# eosio &lt;= eosio::setcode &#123;&quot;account&quot;:&quot;liyc111&quot;,&quot;vmtype&quot;:0,&quot;vmversion&quot;:0,&quot;code&quot;:&quot;0061736d0100000001370b60027f7e0060027e7e006001...# eosio &lt;= eosio::setabi &#123;&quot;account&quot;:&quot;liyc111&quot;,&quot;abi&quot;:&#123;&quot;types&quot;:,&quot;structs&quot;:[&#123;&quot;name&quot;:&quot;hi&quot;,&quot;base&quot;:&quot;&quot;,&quot;fields&quot;:[&#123;&quot;name&quot;:&quot;user&quot;,&quot;t...liyuechun:build yuechunli$ cleos get code liyc111code hash: e387951f9a18870f2c151fbceea5b279a3861bdabab58ea87a67296a8a6583d0liyuechun:build yuechunli$ 6. 执行合约6.1 解锁钱包PW5J3rx7Bfg9zb8Kf2owTytccFyJqtDTrqnUX8iBRRUvbwM8RyzRL是创建钱包是的密码。 1liyuechun:build yuechunli$ cleos wallet unlock --password PW5J3rx7Bfg9zb8Kf2owTytccFyJqtDTrqnUX8iBRRUvbwM8RyzRLUnlocked: default 6.2 执行合约 转载来源：EOS DAWN-V3.0.0 智能合约开发之Hello World]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>技术</tag>
        <tag>Java</tag>
        <tag>Bash</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Page Not Found :( | Hyperledger Composer]]></title>
    <url>%2F2018%2Fb3e9f1cb%2F</url>
    <content type="text"><![CDATA[Page Not Found :( | Hyperledger Composer 转载来源：Page Not Found :( | Hyperledger Composer]]></content>
  </entry>
  <entry>
    <title><![CDATA[图灵奖得主Sivio Micali的Algorand区块链协议简介]]></title>
    <url>%2F2018%2Fd0773b71%2F</url>
    <content type="text"><![CDATA[图灵奖得主Sivio Micali的Algorand区块链协议简介 转载来源：图灵奖得主Sivio Micali的Algorand区块链协议简介]]></content>
      <tags>
        <tag>南湖互联网金融评论</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[算法与产品：抖音、快手的“气质”成因]]></title>
    <url>%2F2018%2F55d84e14%2F</url>
    <content type="text"><![CDATA[同一类型的两款产品，从功能上看，似乎没有明显差别，但为什么给人的“感觉”却是完全不同呢？ 算法快抖的视频内容分为推荐（发现）、附近（同城）和关注三个模块，这里主要说明推荐模块的算法机制。 视频与用户画像的匹配程度1. 热度（赞、评论、转发等）1. 发布时间根据用户数据和内容标签计算两者的匹配程度，是每个内容产品的算法核心，已经被总结很多次了，其理论大体一致，在本社区搜索“算法”关键词就能找到，这里就不再赘述了，下面主要介绍热度和发布时间两点。 打开你的抖音，你会看到系统已经为你推荐好了一系列内容，再仔细观察一下，你会发现这些视频的获赞数量基本都在50万以上，中位数大概在100万（刷多了会减少）。 而打开快手呢？ 你会发现视频获赞数量基本维持在1万到10万这个区间，有的甚至会出现几千几百，但很难找到过10万的视频。 出现这种差距，难道是因为快手用户少吗？ 显然不是，快手的用户已经是其他短视频产品的用户之和了。之所以出现这种状况，其实是因为快手算法特有的“热度权重“。 视频发布初期，随着其热度提高，曝光机会也会跟着提高，此时，“热度权重”起到“择优去劣”的作用。而在视频热度达到一定阈值后，它的曝光机会将不断降低，此时，“热度权重”起到“择新去旧”的作用（其实是为了给用户平等的展示机会，后面会讲到）。 与此同时，快抖对于“发布时间”的看法也是不一样的。 在抖音，你会发现很多视频其实几个月前就发布了（验证这一点，只需要在评论区不断下翻即可，可根据早期评论的发布时间进行推断）。因为用户一般不会在乎短视频是不是最新的，只要足够精彩即可。 而在快手，大部分视频都是近期发布的，再远的视频也是一个月内（在视频界面右下方有时间标注）。 那么，前面提到的“热度权重”和“发布时间权重”对于用户来讲又会有怎样的影响呢？ 首先，短视频的用户大体分两种：一种是“看视频”的看官，一种是“拍视频”的制作方。这里，我们把看官的注意力比作一个蛋糕，而制作方比作分蛋糕的人。 首先，我们来看分蛋糕的人。在抖音，分到大量蛋糕的用户还会继续加快分蛋糕的速度（高热度会不断提高曝光机会），头部用户集中了大量的用户注意力资源，这种中心化会让普通制作者、草根制作者难以被关注（这与微博颇为类似）。 而在快手，头部用户分到的蛋糕被设了上限（高热度和旧视频曝光机会会大大降低），因此会有更多的人分到蛋糕，这也体现了快手的理念——希望所有用户都能展示自我，任何一位普通用户都有被关注的权利。 对于看官来说，抖音给了他们大量的优质资源，这些都是经过大量用户检验，而放到推荐模块的内容池的视频。而快手只是经过初步检验就根据用户喜好开始推荐了，所以会有很多小众和“乡土”的内容。 抖音和快手，一个是精致的台上表演，一个是平凡的街边才艺。 相对来说，抖音是看官导向的，而快手则更偏向于制作方，尤其是草根用户。这也就不难理解为什么快手会沦落到被整改的尴尬境地，因为“三俗”生产者总能找到自己的市场。 产品除了算法，我认为以下两点也是快抖气质差异的诱因。 录制功能1. 交互设计 1. 先音乐后录制的妙处与其他短视频不同，抖音的录制功能别具一格，先选音乐再根据音乐录视频，而不只是充当背景音乐。 每当视频的动感与音频的调子相重合时，会大大刺激观众的感官，带来不一样的体验。同时，也产生了更多玩法，比如：对口型、拍同款等，增加了趣味性、可看性。因此，抖音会给人一种“酷炫”的感觉（但是拍摄门槛也抬高了）。 2. 不断上滑的“沉浸式体验”一打开抖音，便进入了播放界面，接着依靠上下滑动来更换视频，嗯 … 这种状态可以维持一个多小时 … 这种懒人式交互大大提升了用户粘性，不过也削弱了用户改变“状态”的意愿，即附近模块、关注模块的使用几率将会降低。由此，用户的注意力又被“粘”在了头部用户的优质内容上，中心化进一步加剧。 与之相反的，快手的推荐（发现）对用户并没有那么大的粘性，三个模块的交互方式相当，都是瀑布流布局，并且快手的启动页是用户上一次退出的页面。 比如：上一次在同城离开，下一次启动页会是同城模块，关注模块也是如此，这样，用户选择的自由度“无形”地增加了。同时，快手也因其同城和关注的高使用频率，而在社交属性上更胜一筹，而不仅仅是一个娱乐软件。 其实快抖的算法与交互设计是相辅相成的，抖音的算法决定了它的视频更加优质，因此不需要用户做太多的选择，适合逐个播放，也减少了用户操作负担和选择负担。而快手视频的优质密度没有那么大，需要用户选择播放，在操作上会繁琐一些。 回忆一下我们使用抖音的时候，是不是一般会看完整个视频再播放下一个，很少掠过。而使用快手的时候，我们通常要掠过几个，才能选出自己想看的视频。 因此，“滚动播放”更适合抖音，而瀑布流适用于快手（其实快手也可以尝试美拍的“瀑布流+滚动播放相关视频”的交互设计，或者抖音附近模块的“瀑布流+滚动播放下一个视频”的交互设计）。 结尾除了算法和产品设计，还有着其他因素导致快抖的风格差异，比如：运营和品牌公关。 不过从效果上看，算法和产品设计更像是产品的“基因”，从最开始就影响着“两个宝宝”的未来走向（抖音的精致范和快手的平民化）。 如何设计出我们想要的产品，让“宝宝”长成我们想要的样子，抖音和快手的实例值得我们借鉴。 本文由 @ 信管专业学生 原创发布于人人都是产品经理。未经许可，禁止转载 题图来自网络 转载来源：算法与产品：抖音、快手的“气质”成因]]></content>
      <categories>
        <category>其它</category>
      </categories>
      <tags>
        <tag>音乐</tag>
        <tag>软件</tag>
        <tag>蛋糕</tag>
        <tag>甜品</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[晚上 11 点肝要排毒？你被这 30 个谣言坑了多少年……]]></title>
    <url>%2F2018%2F9a715849%2F</url>
    <content type="text"><![CDATA[晚上 11 点肝要排毒？你被这 30 个谣言坑了多少年…… 转载来源：晚上 11 点肝要排毒？你被这 30 个谣言坑了多少年……]]></content>
      <tags>
        <tag>华为运动健康</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用文本挖掘实现站点个性化推荐]]></title>
    <url>%2F2018%2F7bb4adc9%2F</url>
    <content type="text"><![CDATA[使用文本挖掘实现站点个性化推荐 转载来源：使用文本挖掘实现站点个性化推荐]]></content>
      <tags>
        <tag>CSDN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LSTM、GRU与神经图灵机：详解深度学习最热门的循环神经网络]]></title>
    <url>%2F2018%2Ffb1efad0%2F</url>
    <content type="text"><![CDATA[循环神经网络是当前深度学习热潮中最重要和最核心的技术之一。近日，Jason Brownlee 通过一篇长文对循环神经网络进行了系统的介绍。 循环神经网络（RNN/recurrent neural network）是一类人工神经网络，其可以通过为网络添加额外的权重来在网络图（network graph）中创建循环，以便维持一个内部状态。 为神经网络添加状态的好处是它们将能在序列预测问题中明确地学习和利用背景信息（context），这类问题包括带有顺序或时间组件的问题。 在这篇文章中，你将踏上了解用于深度学习的循环神经网络的旅程。 在读完这篇文章后，你将了解： 用于深度学习的顶级循环神经网络的工作方式，其中包括 LSTM、GRU 和 NTM。- 顶级 RNN 与人工神经网络中更广泛的循环（recurrence）研究的相关性。- RNN 研究如何在一系列高难度问题上实现了当前最佳的表现。顶级 RNN 与人工神经网络中更广泛的循环（recurrence）研究的相关性。 注意，我们并不会覆盖每一种可能的循环神经网络，而是会重点关注几种用于深度学习的循环神经网络（LSTM、GRU 和 NTM）以及用于理解它们的背景。 那就让我们开始吧！ 概述 我们首先会设置循环神经网络领域的场景；然后，我们将深入了解用于深度学习的 LSTM、GRU 和 NTM；之后我们会花点时间介绍一些与用于深度学习的 RNN 相关的高级主题。 循环神经网络- 完全循环网络（Fully Recurrent Networks）- 递归神经网络（Recursive Neural Networks）- 神经历史压缩器（Neural History Compressor）递归神经网络（Recursive Neural Networks） 长短期记忆网络（LSTM）- 门控循环单元（GRU）神经网络- 神经图灵机（NTM）门控循环单元（GRU）神经网络 循环神经网络 首先让我们设置场景。 人们普遍认为循环（recurrence）是给网络拓扑结构提供一个记忆（memory）。 一种更好的看法是训练集包含一种样本——其带有一组用于循环训练样本的输入。这是「传统的惯例」，比如传统的多层感知器 X(i) -&gt; y(i) 但是该训练样本得到了来自之前的样本的一组输入的补充。这是「非传统的」，比如循环神经网络 [X(i-1), X(i)] -&gt; y(i) 和所有的前馈网络范式一样，问题的关键是如何将输入层连接到输出层（包括反馈激活），然后训练该结构使其收敛。 现在，让我们来看看几种不同的循环神经网络，首先从非常简单的概念开始 完全循环网络 多层感知器的分类结构得到了保留，但该架构中的每个元素与其它每个元素之间都有一个加权的连接，并且还有一个与其自身的反馈连接。 并不是所有这些连接都会被训练，而且其误差导数的极端非线性意味着传统的反向传播无法起效，因此只能使用通过时间的反向传播（Backpropagation Through Time）方法或随机梯度下降（SGD）。 另外，可参阅 Bill Willson 的张量积网络（Tensor Product Networks）：http://www.cse.unsw.edu.au/~billw/cs9444/tensor-stuff/tensor-intro-04.html递归神经网络 循环神经网络是递归网络的线性架构变体。 递归（recursion）可以促进分层特征空间中的分支，而且其所得到的网络架构可以在训练进行中模拟它。 其训练是通过子梯度方法（sub-gradient methods）使用随机梯度实现的。 R. Socher 等人 2011 年的论文《Parsing Natural Scenes and Natural Language with Recursive Neural Networks》中使用 R 语言对其进行了详细的描述，参阅：http://machinelearning.wustl.edu/mlpapers/paper_files/ICML2011Socher_125.pdf 神经历史压缩器 在 1991 年，Schmidhuber 首先报告了一种非常深度的学习器，其可以通过一种 RNN 层次的无监督预训练来在数百个神经层上执行功劳分配（credit assignment）。 每个 RNN 都是无监督训练的，可以预测下一个输入。然后只有产生错误的输入会被前馈，将新信息传送到该层次结构中的下一个 RNN，然后以更慢的、自组织的时间尺度进行处理。 事实表明不会有信息丢失，只是会有压缩。该 RNN stack 是数据的一个「深度生成模型（deep generative model）。这些数据可以根据其压缩形式重建。 参阅 J. Schmidhuber 等人 2014 年的论文《Deep Learning in Neural Networks: An Overview》：http://www2.econ.iastate.edu/tesfatsi/DeepLearningInNeuralNetworksOverview.JSchmidhuber2015.pdf当误差被反向传播通过大型结构时，非线性导数的极限（extremity）的计算会增长，会使功劳分配（credit assignment）困难甚至是不可能，使得反向传播失败。 长短期记忆网络 使用传统的通过时间的反向传播（BPTT）或实时循环学习（RTTL/Real Time Recurrent Learning），在时间中反向流动的误差信号往往会爆炸（explode）或消失（vanish） 反向传播误差的时间演化指数式地依赖于权重的大小。权重爆炸可能会导致权重振荡，而权重消失则可能导致学习弥合时间滞后并耗费过多时间或根本不工作。 LSTM 是一种全新的循环网络架构，可用一种合适的基于梯度的学习算法进行训练。- LSTM 是为克服误差反向流动问题（error back-flow problem）而设计的。它可以学习桥接超过 1000 步的时间间隔。- 在有噪声的、不可压缩的输入序列存在，而没有短时间滞后能力的损失时，这是真实的。LSTM 是为克服误差反向流动问题（error back-flow problem）而设计的。它可以学习桥接超过 1000 步的时间间隔。 通过一种有效的基于梯度的算法，误差反向流动问题可以克服，因为该算法让网络架构可以强迫常量（因此不会有爆炸或消失）误差流过特殊单元的内部状态。这些单元可以减少「输入权重冲突（Input Weight Conflict）」和「输出权重冲突（Output Weight Conflict）」的影响。 输入权重冲突：如果输入是非零的，则同样的输入权重必须被同时用于存储特定的输入并忽略其它输入，那么这就将会经常收到有冲突的权重更新信号。 这些信号将会试图使该权重参与存储输入并保护该输入。这种冲突会使学习变得困难，并且需要一个对背景更敏感的机制来通过输入权重控制「写入操作（write operations）」。 输出权重冲突：只要一个单元的输出是非零的，那么这个单元的输出连接的权重就将吸引在序列处理过程中生成的有冲突的权重更新信号。 这些信号将试图使正在输出的权重参与进来，获取存在在处理单元中信息，并且在不同的时间保护后续的单元免受正被馈送的单元的输出的干扰。 这些冲突并不特定于长期滞后（long-term lags），并且也可以同样影响到短期滞后（short-term lags）。值得注意的是，随着滞后的增长，存储的信息必须被保护起来免受干扰，尤其是在学习的高级阶段。 网络架构：不同类型的单元都可能传递关于网络当前状态的有用信息。比如说，一个输入门（输出门）可能会使用来自其它记忆单元（memory cell）的输入来决定是否存储（读取）其记忆单元中的特定信息。 记忆单元包含门（gate）。门特定于它们调解的连接。输入门是为了纠正输入权重冲突，而输出门是为了消除输出权重冲突。 门：具体来说，为了缓解输入和输出权重冲突以及干扰，我们引入了一个乘法输入门单元来保护存储的记忆内容免受不相关输入的干扰，还引入了一个乘法输出门单元来保护其它单元免受存储中当前不相关记忆内容的干扰。 LSTM 架构示例。这个 LSTM 网络带有 8 个输入单元、4 个输出单元和 2 个大小为 2 的记忆单元模块。in1 是指输入门，out1 是指输出门，cell1 = block1 是指 block 1 的第一个记忆单元。来自 1997 年的《Long Short-Term Memory》 因为处理元素的多样性和反馈连接的，LSTM 中的连接比多层感知器的连接复杂。 记忆单元模块：记忆单元共享同一个输入门和同一个输出门，构成一种名叫记忆单元模块（memory cell block）的结构。 记忆单元模块有利于信息存储；就像传统的神经网络一样，在单个单元内编码一个分布式输入可不是一件容易的事情。一个大小为 1 的记忆单元模块就是一个简单的记忆单元。 学习（Learning）：一种考虑了由输入和输出门导致的修改过的、乘法动态的实时循环学习（RTRL/Real Time Recurrent Learning）的变体被用于确保通过记忆单元误差的内部状态反向传播到达「记忆单元网络输入（memory cell net inputs）」的非衰减误差（non-decaying error）不会在时间中被进一步反向传播。 猜测（Guessing）：这种随机方法可以超越许多时间滞后算法。事实已经说明，之前的工作中所使用的许多长时间滞后任务可以通过简单的随机权重猜测得到比提出的算法更快的解决。 参见 S. Hochreiter 和 J. Schmidhuber《Long-Short Term Memory》：http://dl.acm.org/citation.cfm?id=1246450LSTM 循环神经网络最有意思的应用出现在语言处理领域。更全面的描述可参阅 Gers 的论文 ： F. Gers 和 J. Schmidhuber 2001 年的论文《LSTM Recurrent Networks Learn Simple Context Free and Context Sensitive Languages》：ftp://ftp.idsia.ch/pub/juergen/L-IEEE.pdf- F. Gers 2001 年的博士论文《Long Short-Term Memory in Recurrent Neural Networks》：http://www.felixgers.de/papers/phd.pdfF. Gers 2001 年的博士论文《Long Short-Term Memory in Recurrent Neural Networks》：http://www.felixgers.de/papers/phd.pdf LSTM 的局限性 LSTM 有效的截断版本无法轻松解决类似于「强延迟的异或（strongly delayed XOR）」这样的问题。 每个记忆单元模块都需要一个输入门和一个输出门。并不一定需要其它循环方法。 在记忆单元内穿过「常量误差传送带（Constant Error Carrousels）」的常量误差流可以得到与传统的前馈架构（会一次性获得整个输入串）一样的效果。 和其它前馈方法一样，LSTM 也有「regency」概念上的缺陷。如果需要精密的计数时间步骤，那么可能就需要额外的计数机制。 LSTM 的优点 该算法桥接长时间滞后的能力来自其架构的记忆单元中的常量误差反向传播。 LSTM 可以近似有噪声的问题域、分布式表征和连续值。 LSTM 可以很好地泛化其所考虑的问题域。这是很重要的，因为有的任务无法用已有的循环网络解决。 在问题域上对网络参数进行微调看起来是不必要的。 在每个权重和时间步的更新复杂度方面，LSTM 基本上就等于 BPTT。 LSTM 很强大，在机器翻译等领域实现了当前最佳的结果。 门控循环单元神经网络 门控循环单元神经网络已经在序列和时间数据上得到了成功的应用。 最适合语音识别、自然语言处理和机器翻译。与 LSTM 一起，它们在长序列问题领域表现优良。 门控（gating）被认为是在 LSTM 主题中，涉及到一个门控网络生成信号来控制当前输入和之前记忆发生作用的方式，以更新当前的激活，从而更新当前的网络状态。 门本身是自我加权的，会在整个学习阶段中根据一个算法有选择性地更新。 门网络会增加计算复杂度，从而会增加参数化（parameterization），进而引入额外的计算成本。 LSTM RNN 架构将简单 RNN 的计算用作内部记忆单元（状态）的中间候选项。门控循环单元（GRU）RNN 将 LSTM RNN 模型的门控信号减少到了 2 个。这两个门分别被称为更新门（update gate）和重置门（reset gate）。 GRU（和 LSTM）RNN 的门控机制和在参数化方面的简单 RNN 一样。对应这些门的权重也使用 BPTT 随机梯度下降来更新，因为其要试图最小化成本函数。 每个参数更新都将涉及到与整体网络的状态相关的信息。这可能会有不利影响。 门控的概念可使用三种新变体的门控机制来探索和扩展。 这三种门控变体为：GRU1（其中仅使用之前的隐藏状态和偏置来计算每个门——、GRU2（其中仅使用之前的隐藏状态来计算每个门—）和 GRU3（其中仅使用偏置来计算每个门）。我们可以观察到参数显著减少，其中 GRU3 的参数数量最小。 这三种变体和 GRU RNN 在手写数字的 MNIST 数据库和 IMDB 电影评论数据集上进行了基准测试。 从 MNIST 数据集生成了 2 个序列长度，而从 IMDB 数据集生成了 1 个序列长度。 这些门的主要驱动信号似乎是（循环）状态，因为其包含关于其它信号的基本信息。 随机梯度下降的使用隐含地携带了有关网络状态的信息。这可以解释仅在门信号中使用偏置的相对成功，因为其自适应更新携带了有关网络状态的信息。 门控变体可使用有限的拓扑结构评估来探索门控机制。 更多信息请参阅： R. Dey 和 F. M. Salem 2017 年的论文《Gate-Variants of Gated Recurrent Unit (GRU) Neural Networks》：https://arxiv.org/ftp/arxiv/papers/1701/1701.05923.pdf- J. Chung 等人 2014 年的论文《Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling》：https://pdfs.semanticscholar.org/2d9e/3f53fcdb548b0b3c4d4efb197f164fe0c381.pdfJ. Chung 等人 2014 年的论文《Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling》：https://pdfs.semanticscholar.org/2d9e/3f53fcdb548b0b3c4d4efb197f164fe0c381.pdf 神经图灵机 神经图灵机通过将神经网络与外部记忆资源耦合而对该神经网络的能力进行了延展——它们可以通过注意（attention）过程与外部记忆资源交互。参阅机器之心文章《神经图灵机深度讲解：从图灵机基本概念到可微分神经计算机》。 这种组合的系统类似于图灵机或冯·诺依曼结构，但它是端到端可微分的，使得其可以有效地使用梯度下降进行训练。 初步的结果表明神经图灵机可以根据输入和输出样本推理得到基本的算法，比如复制、排序和联想回忆（associative recall）。 RNN 相比于其它机器学习方法的突出之处在于其在长时间范围内学习和执行数据的复杂转换的能力。此外，我们都知道 RNN 是图灵完备的，因此其有能力模拟任意程序，只要连接方式合适即可。 标准 RNN 的能力被扩展以简化算法任务的解决方案。这种丰富性主要是通过一个巨大的可寻址的记忆实现的，所以通过类比于图灵的通过有线存储磁带实现的有限状态机（finite-state machine）的丰富性，其被命名为神经图灵机（NTM）。 和图灵机不同，神经图灵机是一种可微分的计算机，可以通过梯度下降训练，从而为学习程序提供了一种实用的机制。 神经图灵机架构。NTM 架构大体上如上所示。在每一个更新循环中，控制器网络接收一个来自外部环境的输入并给出一个输出作为响应。它也会通过一系列并行的读写头来读写一个记忆矩阵（memory matrix）。虚线是 NTM 回路与外部世界的分界线。来自 2014 年的《Neural Turing Machines》 关键在于，该架构的每个组件都是可微分的，使其可以直接使用梯度下降进行训练。这可以通过定义「模糊（blurry）」的读写操作来实现，其可以或多或少地与记忆中的所有元素进行交互（而非像普通的图灵机或数字计算机那样处理单个元素）。 更多信息请参阅： A. Graves 等人 2014 年的《Neural Turing Machines》：https://arxiv.org/pdf/1410.5401.pdf- R. Greve 等人 2016 年的《Evolving Neural Turing Machines for Reward-based Learning》：http://sebastianrisi.com/wp-content/uploads/greve_gecco16.pdfR. Greve 等人 2016 年的《Evolving Neural Turing Machines for Reward-based Learning》：http://sebastianrisi.com/wp-content/uploads/greve_gecco16.pdf NTM 实验 复制（copy）任务可以测试 NTM 是否可以存储和回调长序列的任意信息。向该网络提供一个随机二进制向量的输入序列，后面跟着一个分隔符。 该网络被训练用来复制 8 位的随机向量序列，其中序列长度是在 1 到 20 之间随机的。目标序列就仅仅是输入序列的副本而已（没有分隔符）。 重复复制任务是复制任务的扩展，即要求该网络输出被复制的序列给定的次数，然后给出一个序列终止标志。这个任务的主要目的是看 NTM 是否可以学习简单的嵌套函数。 该网络的输入是随机长度的随机二进制向量序列，后面跟着一个标量值，表示我们想要的副本的数量，其出现在一个单独的输入信道上。 联想记忆任务（associative recall tasks）涉及到组织间接产生的数据，即当一个数据项指向另一个项的时候。要创建一个项列表，使得查询其中一个项时需要该网络返回后续的项。 我们定义了一个二进制向量序列，通过分隔符对其左右进行了限制。在几个项被传播到该网络中后，通过展示随机项对该网络进行查询，看该网络是否可以产生下一个项。 动态 N-gram 任务测试的是 NTM 是否可以通过使用记忆作为可覆写的表格来快速适应新的预测分布；该网络可以使用这个表格来持续对转换统计保持计数，从而模拟传统的 N-gram 模型。 考虑在二进制序列上的所有可能的 6-gram 分布的集合。每个 6-gram 分布都可以被表达成一个有 32 个数字的表格，其基于所有可能长度的 5 个二进制历史指定了下一位（bit）为 1 的概率。通过使用当前的查找表绘制 200 个连续的位，会生成一个特定的训练序列。该网络对该序列进行观察，一次一位，然后被要求预测出下一位。 优先级排序任务测试的是 NTM 的排序能力。该网络的输入是一个随机二进制向量的序列，以及每个向量的一个标量优先级评分。该优先级是在 [-1, 1] 范围内均匀分布的。目标序列是根据它们的优先级排序后的二进制向量序列。 NTM 有 LSTM 的前馈结构作为它们的组件之一。 总结 你通过这篇文章了解了用于深度学习的循环神经网络。具体来说，你了解了： 用于深度学习的顶级循环神经网络的工作方式，其中包括 LSTM、GRU 和 NTM。- 顶级 RNN 与人工神经网络中更广泛的循环（recurrence）研究的相关性。- RNN 研究如何在一系列高难度问题上实现了当前最佳的表现。顶级 RNN 与人工神经网络中更广泛的循环（recurrence）研究的相关性。 转载来源：LSTM、GRU与神经图灵机：详解深度学习最热门的循环神经网络]]></content>
      <categories>
        <category>其它</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>文章</tag>
        <tag>R语言</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Error]]></title>
    <url>%2F2018%2F89e2c812%2F</url>
    <content type="text"><![CDATA[Error 转载来源：Error]]></content>
  </entry>
  <entry>
    <title><![CDATA[为什么“兴趣广泛”的通才更有可能获得成功？详解通才的7大优势]]></title>
    <url>%2F2018%2F1b54df58%2F</url>
    <content type="text"><![CDATA[为什么“兴趣广泛”的通才更有可能获得成功？详解通才的7大优势 转载来源：为什么“兴趣广泛”的通才更有可能获得成功？详解通才的7大优势]]></content>
  </entry>
  <entry>
    <title><![CDATA[为什么全球顶尖成功人士都会遵循“5小时法则”？]]></title>
    <url>%2F2018%2F5d969a54%2F</url>
    <content type="text"><![CDATA[全球最顶尖的5家公司的创始人 比尔·盖茨、史蒂夫·乔布斯、沃伦·巴菲特、杰夫·贝佐斯、拉里·佩奇，他们都是博学的通才，他们都有两种不同寻常的特质。在研究了白手起家的亿万富翁多年之后，我发现有两种特质在他们获得如今的财富、成功、影响力和名声方面发挥了关键作用。事实上，我自己也非常相信这两种特质，所以我在自己的生活中创办公司、成为一个更好的作家、做一个更好的丈夫、实现财务安全的过程中都会利用这两种特质。这些成功人士拥有的两种共同特质分别是： （1）他们都是如饥似渴的学习者。 （2）他们都是通才。 下面让分别阐述一下这两个特质，并分享一些简单的技巧，从而让你自己在日常生活中也能利用它们。 首先，这两种特质的定义。我把一个如饥似渴的学习者定义为一个遵循“5小时原则”的人，即每周至少花5个小时来学习。我将通才定义为一个能在至少三个不同领域里都能胜任的人，并将这几个领域的技能都整合到一个技能组合中，使他们成为所在领域内前1%的顶尖人才。如果你不断学习和模拟这两个特质，并且认真对待它们，我相信它们会对你的生活产生巨大的影响，并会加速你获得成功的脚步。当你变成一个如饥似渴的学习者时，那么你过去所学到的所有知识的价值就会呈现复合效应。当你成为一个通才时，你就能开发出综合技能的能力，并开发出一套独特的技能组合，这能够帮助你获得竞争优势。 根据比尔盖茨自己的估计，他每周都会读完一本书，这个习惯已经坚持了52年，其中很多书籍都是与软件或商业无关的书籍。此外，在他的整个职业生涯中，他每年还会有一个为期两周的阅读假期，这两周时间专门用来阅读。在1994年的接受《花花公子》的采访中，我们发现他已经把自己当成了一个博学的通才了： 花花公子：你不喜欢自己被称为是一名商人吗？- 盖茨：不喜欢。在我的所有思考时间中，我将10%的思考时间用于商业思考。商业并没有那么复杂，我不想将商人的身份放在我的名片上。- 花花公子：那么你会将什么写在你的名片上呢？- 盖茨：科学家。当我读到一些伟大的科学家的故事，比如克里克和沃森是如何发现DNA的时候，我就会非常兴奋。商业成功的故事无法让我获得同样的乐趣和快感。盖茨：不喜欢。在我的所有思考时间中，我将10%的思考时间用于商业思考。商业并没有那么复杂，我不想将商人的身份放在我的名片上。 盖茨：科学家。当我读到一些伟大的科学家的故事，比如克里克和沃森是如何发现DNA的时候，我就会非常兴奋。商业成功的故事无法让我获得同样的乐趣和快感。 盖茨将自己视为一位科学家，这是非常有意思的，因为他从大学辍学了，并在辍学后就一直扎根于软件行业。 有趣的是，埃隆·马斯克也不认为自己是个商人。在最近一次接受CBS的采访中，马斯克说他认为自己更像是一个设计师、工程师、技术专家，甚至是巫师。 这样的例子不胜枚举。众所周知，拉里·佩奇会花时间与谷歌的每一个人进行深度交流，从谷歌的门卫到核聚变科学家，并总是希望自己能从他们身上学到些什么。 沃伦·巴菲特是这样描述自己获得成功的关键的：“每天阅读500页书。知识的运作方式是：知识会慢慢累积呈复合效应，就像复利一样。” 杰夫·贝佐斯是通过实验进行大规模学习的方式来打造他的整个公司的，并且他在一生中都是一个如饥似渴的阅读者。 最后，史蒂夫·乔布斯将各种学科结合在一起，并将之视为苹果的竞争优势，他曾经这样说过：“单靠科技是远远不够的，必须要让科技与人文科学以及人性相结合，其成果必须能够让用户产生共鸣。” 当然，上面五家顶尖公司的创始人并不是唯一拥有这两种特质的成功人士。正如我之前写过的，如果我们把这份名单扩大到其他白手起家的亿万富翁，我们很快就会看到奥普拉·温弗瑞、雷·达利奥、大卫·鲁本斯坦、菲尔·耐特、霍华德·马克斯、马克·扎克伯格、埃隆·马斯克、查尔斯·科赫和其他很多拥有类似特质和习惯的成功人士。 为什么世界上最忙碌的人会把最宝贵的时间投入到学习与和他们所在领域看起来无关的知识上，比如核聚变、字体设计、科学家传记和医生回忆录? 他们每个人都掌管着一个由全世界成千上万最聪明的人组成的公司。他们把自己生活和公司业务中的几乎每一项任务都委派给了最优秀、最聪明的人去负责。那么他们为什么还要坚持学习大量知识呢？ 在写了几篇试图回答这些问题的文章之后，这就是我最终得出的结论：“在最高层次上，学习并不是一件你为工作做准备的事情，学习本身就是最重要的工作。它是你要打造的一个核心能力。这是一件你永远不能委派给别人去做的东西。这是关系到企业进步和长远成功的终极驱动力之一。“ 当我意识到这一点的时候，我在想：在如今这样一个日益复杂、快速变化、先进的知识经济时代，为什么大家没有在‘我们在一生中都应该成为贪婪的学习者和博学的通才’上达成共识并认为这是我们理所应当做的事情呢？为什么大部分普通人都将可以学习视为一种可做可不做的事情呢？ 我认为，这主要和我们在学校、大学和社会上被反复灌输的三个强有力信息有关，这些信息在过去可能是事实，但现在已经不再是了，它们现在已经变成三条谎言了。下面我们就来看看如何一一打破这三条谎言： （1）谎言1：学科是分类知识的最好方法。 （2）谎言2：大部分学习都发生在学校/大学。 （3）谎言3：你必须选择一个领域并且专攻这个领域。 这些观念是如此有害，它们摧毁了我们对学习和知识的直觉，最终阻碍我们创造我们想要的成功。如果我们能意识到它们，我们就能像世界上最成功的人所做的那样去纠正它们。 谎言1：学科是分类知识的最好方法。 我们的教育体系建立在这样一种模型上，即将知识分为不同学科——数学、阅读、历史和科学等。从幼儿园开始，我们接收到的信息就是，这些科目最好是能各自单独学习。 我们甚至将这些学科细分为更小的学习领域，例如，将经济学进一步细分为微观经济学和宏观经济学。这种将学科领域进行分解并分开教学的范例叫做简化论。尽管它仍然是我们社会的标准，但它实际上已经开始在一些更进步的国家发生变化了。 简化论能带来很多益处。在关系紧密领域内，想法能够得到迅速而高效地传播，因为每个人都属于同一种文化，都说着同样的语言。研究一个系统的各个部分要比研究一个复杂的整体系统要更为容易。这种范式在很多重大的发现中发挥了非常重要作用。 但简化论的一个关键缺点在于，不同领域之间的连接变得非常模糊，所带来的结果就是所谓的“负学习迁移”，即学习一件东西会使学习其他东西变得更加困难，因为我们所学到的概念与特定的研究领域是如此紧密地联系在一起。如果你在学习第二语言的时候陷入了困境，因为你所学的第二语言的语法、语序、时态或复数的规则与你的母语的规则不匹配，你就会经历负学习迁移。 简化论的另一个缺点是，在一个专业领域之外的人很难理解这个领域内发生的事情。不妨想象一下一个神经外科医生试图向一个平面设计师解释大脑手术的进展，会发生什么。每个领域都有各自的语言和文化，所以在一个领域里独特的见解并不适用于另一个领域，尽管不同领域内的见解经常可以而且应该是彼此适用的。这就导致了回音室的出现。 生物学家James Zull在他的书《The Art of the Changing Brain》里解释了为什么学习迁移如此复杂。“通常情况下，我们没有将一个主题和另一个主题连接起来的神经网络。它们是分开建立的，特别是如果我们在将知识分解成数学、语言、科学和社会科学等不同部分的标准课程中学习。” 因为没有人教我们去看所有这些知识的共同根源，所以我们看不到它们之间的内在联系。 埃隆·马斯克强烈地感到我们的教育体系无法教给孩子们学习所有这些学科知识的共同根源，他创建了自己的学校，并让自己的所有孩子都进入这个学校。 埃隆·马斯克在接受北京电视台采访时称，由于不喜欢他的孩子的学校，因此他便自己建了一所学校。这所学校被命名为Ad Astra，意思是“奔向星辰”。这所学校规模极小，相对比较隐秘。它没有自己的网站，也没有创建社交网帐号。曾经撰写过一篇关于洛杉矶私立学校的文章的克里斯蒂娜·西蒙（Christina Simon）深入了解了Ad Astra学校。她说她认识了一位与马斯克的孩子上同一所学校的孩子的母亲。这位母亲对西蒙说，相对比较新的Ad Astra学校规模很小，还处于测试阶段。这所学校目前只接纳了少数孩子，他们的父母亲都是SpaceX的员工。马斯克在接受采访时说，Ad Astra学校创立了一年多，现有14名学生。Ad Astra学校未区分年级，一年级和三年级的学生之间是没有明显区别的。他说：“让所有的孩子在同一时间通过相同的年级考试，就像装配线一样。” 他说：“有人喜欢英语或语言，有人喜欢数学，有人喜欢音乐。人各有志，各有所长。因此，根据他们的态度和能力来因材施教就显得非常重要了。”马斯克为他的孩子办了退学手续，为了创办Ad Astra学校甚至还挖走了一名教师。他说：“我认为这些事情都是应该做的，但我没有看到普通学校去做这些事。”马斯克认为，普通学校在教授学生如何解决问题上犯了一个根本性的错误。马斯克说：“教授解决问题的方法或者讲解问题本身而非解决问题的工具，这一点很重要。” 他说：“假设你想教别人引擎的工作原理，传统的做法是说，’我们将讲授有关螺丝刀和扳手的所有知识。’这是一种截然不同的教学方式。” 相反，马斯克认为直接给学生们提供一台引擎然后在学生们面前拆卸它，这种教授方式会更有意义。马斯克解释说：“我们如何将它分解开来？你需要一把螺丝刀，然后一件非常重要的事情随之发生了，那就是工具的关联性变得很明显了。” 多年来，我了解到有一种更深层的方法来分类知识，一种学习基本原理的方法，它适用于所有领域，并教授让人终生受用的技能。这些基本原理被称为思维模型。 让我们来看看一个思维模型，叫做“压力和恢复”。在锻炼中，压力和恢复的现象是锻炼使我们变得更强壮的原因所在：它暂时使我们的肌肉和心血管系统超过了它们现在的承受能力，并在恢复过程中重建自己。有了这种思维模式，我们就可以在其他领域寻找到这种思维模型。例如，它解释了为什么经历了某些类型的困难能够帮助我们变得更加强大。在心理学领域，这被称为创伤后成长。在社会心理学领域，这些痛苦的经历被称为多样化的经历。在成年人的发展中，他们被称为最优冲突。通过这些例子，我们可以看到同一个底层思维模型在不同的应用领域里是如何被赋予不同的名称的。 思维模型是将学科联系在一起的无形的思想网络。 这正是很多世界顶尖学习者和通才在如今的知识经济中获得成功的秘密所在。 真相：将知识按思维模型分类与将知识按学科分类是同样重要的，因为思维模型是将不同学科连接起来的底层基础。 谎言2：大部分学习发生在学校/大学。 “自学是唯一的教育形式。”——Mark Twain 我确信，教育最根本的问题之一就是学校与学习的归并融合。事实上，学校只是学习发生的一个环境。在我们的生活中，几乎所有的学习都发生在学校之外：在家里，在操场上，在运动场上，在旅行中，在我们读的书和我们喜欢的爱好里，特别是在我们的工作中。然而，我们却被训练成只将正规教育视为真正的教育。 在军事和执法部门，把在学校里学到的东西和现实世界里发生的事情混为一谈都被认为是“训练伤痕”。在《Algorithms To Live By》这本书中，引用一些有关这些伤痕的极端例子，显示它们会带来多严重的后果：在现实枪战中，警察只开枪两次就把枪放进枪套里（就像他们在训练中所做的那样)。在一个令人惊恐的例子中，一名警察从袭击者手中夺过枪，然后本能地将枪还给他，就像他在警察学院的训练过程中一次又一次做的那样。同样的道理，我们在学校里学习到的一些技能，这些技能要么无法转移到现实世界中，要么会阻碍我们在现实世界中的表现。 例如，相信我们中的大多数人都会认同，在课堂上，那些遵守纪律和规则的人会因此得到奖励。但在现实世界中，关键的领导特质包括冒险和原创思维，这两个特质都是与我们在课堂上学的东西相违背的。简而言之，我们接受的大部分正规教育都是为了将我们培养成追随者，而不是领导者。 下面我分享一下我是如何在我自己的生活中揭露“学校等于学习”这个谎言的。 我童年的大部分时间都是在学习中度过的，所以我的GPA绩点很高，也就顺理成章进入一所很好的大学。因为从小到大，大人们都一直给我灌输这样的理念：一所好大学是通往美好生活的门票，我对这一点也深信不疑。我的求学之路也非常顺利，我被理想中的学校录取了：沃顿商学院和纽约大学斯特恩商学院。刚开始的时候，我非常努力地完成每一项学习任务，并在每一项研究课题上都付出了大量心血。但后来我读到一份研究报告，这份报告显示，大多数美国总统、国会议员、参议员甚至大学校长毕业时的GPA绩点都很低。一项对700名百万富翁的调查显示，他们的平均GPA绩点是2.9。事实上，我所在大学的校长是以2.1的GPA绩点毕业的。此外，我了解到，我的大多数创业榜样都没能完成大学学业，即使他们完成了大学学业，他们也不会将其视为他们所获得的成功的重要因素。 “这是咋回事？我被骗了一辈子。我想成为一名企业家，但分数并不重要。”我暗自纳闷。 自那一天起，我就停止了为了分数而读书的日子。我的平均GPA绩点下降到2.9，当教授置仅仅为了让我不要闲下来而给我布置作业的时候，我就会直接跳过这些作业。相反，我设计了自己的学习书籍、要参加的会议，并旁听了我感兴趣的课程。幸运的是，我做了一个很好的赌注。当我面试实习的时候，只有一次面试是让我提供GPA绩点的。尽管我的GPA绩点很低，但我还是得到了实习机会。在工作场合，我从来没有被问过我是在哪所学校就读的。 这些年来，我的思维变得更加微妙。下面是我目前对正规教育的看法： （1）最具影响力的领导者、艺术家和科学家几乎都对学习有一种内在的热爱，那种近乎痴迷地热爱，这种痴迷贯穿了他们的一生。无论他们有多忙，他们都会抽出时间来学习。1991年，比尔·盖茨在接受采访中曾自豪地这样，虽然他经常工作到深夜，但深夜回到家后还是会花点时间用来阅读。当埃隆·马斯克想要雇佣世界上最聪明的人时，他更关心的是候选人的技能组合，而不是学位。 （2）中学和高等教育一般都没有鼓励学生自主学习或培养终身学习的习惯。事实上，为了考试或者是进入一所好的大学而学习往往会培养外在的动机，而这实际上会阻碍内在的动机。正规的教育通常不擅长向学生展示不同学科之间的联系，也不擅长教学生如何在现实世界中应用他们所学到的东西去得到他们想要的结果。 （3）正规教育的最重要成果是让学生热爱学习并拥有自主学习的能力。一个自主学习的学习者能够识别他们所面临的问题并对问题进行优先级划分，识别那些他们可以学习的用来解决这些问题的知识并对这些知识进行优先级划分，坚持每周至少学习5个小时的时间，并将他们学到的东西应用到现实世界的挑战中。一旦一个人爱上了学习，他们就会终身学习。当然，几十年终身学习的积累要比四年大学积累的东西要多得多。 （4）妖魔化正规教育并不是真正的解决之道。多年来，我已经在数百所高中和大学发表过演讲，演讲对象从最精英的人群到最弱势的人群都有，我的观点已经软化了。我有两个孩子在上小学。他们的老师改变了我们孩子的生活。我在教育系统中遇到过许多了不起的老师，他们提供了变革的经验。这些教师中，很多人收入严重不足、不受尊重，却是社会中最有价值的贡献者。在整个系统中，有一些机构也很了不起。立法者制定了更严格的考试规定，导致了为了考试而教学和学习的文化的出现，这仅仅是因为他们希望学校系统更加可靠。这是一个复杂而重要的挑战。 真相：在我们的生活中，大多数的学习都是在学校之外完成的，在获得成功的过程中，自主学习能力是比分数和学位更重要的因素。 谎言3：你必须选择一个领域并且专攻这个领域。 在亚当·斯密的代表作《国富论》这本书的第一页中，他以一个别针工厂为例，说明了专业化的力量。在这个工厂里，只有10名员工，每天却能生产出令人震惊的48000个别针，这就是分工的结果，每个人只专注于制作流程中的其中一个部分。史密斯估计，如果这10个工人中的每一个人都亲自参与整个制作流程中的每一步的话，那么这个工厂每天只能制作出200个别针。换句话说，专业化将他们的效率提高了240倍。 我们几乎所有人都被灌输这样的观念，要想在生活中出人头地，就必须专攻一个领域。当你看到上面的别针工厂的例子，这就不足为奇了。在工业时代，生产力是通过定量产出来衡量的。对于那些仍在制造业工作的人来说，这种模式仍然适用。 但我们大多数人现在都生活中知识经济中，在这个经济中，生产力不是用数量来衡量的，而是用创造性的产出来衡量的。产生创造性想法的最好方法之一就是学习和综合你所在领域内的其他人还不知道的有价值的技能和概念。在知识经济中，学习跨领域的不同兴趣和技能，然后将你的见解运用到你的核心专业里，换句话说，也就是成为一个现代的博学通才，这才能让你真正脱颖而出。 在《为什么“兴趣广泛”的通才更有可能获得成功？详解通才的7大优势》这篇文章中，我详细解释了为什么在现代知识经济的大环境中每个人都应该成为一个通才，并列出了通才的7大优势，这7大优势分别是：（1）通过融合两种及以上的技能能够让你成为一流的人才；（2）大多数创造性突破是通过非典型技能组合实现的；（3）学习并精通一项新技能比以往更加容易和迅速；（4）比以往更加容易开创一个新领域、新行业或者全新的技能组合；（5）它可以为你的未来职业发展保驾护航，让你的技能不会过时；（6）它可以帮你解决更为复杂的问题；（7）它可以帮你脱颖而出，在全球经济中有效竞争。 真相：专业化是工业经济的关键。在当前的知识经济中，学习掌握了至少三个领域的知识并能将它们整合到一个技能组合中的现代型通才将会有更大的优势，这能够使他们成为自己所在领域中前1%的顶尖人才。 需要记住的三大新真相 综上所述，我们过去的学习方式已经不再适用于快速变化的知识经济了。相反，要记住这些新的真相： （1）除了按照学科来分类知识外，按照思维模型来分类知识同样重要甚至更重要。因为思维模型是将不同学科连接起来的底层基础。 （2）大部分学习都是在学校之外完成的，在获得成功的过程中，自主学习能力是比成绩和学位更重要的因素。 （3）在当前的知识经济中，学习掌握了至少三个领域的知识并能将它们整合到一个技能组合中的现代型通才将会有更大的优势，这能够使他们成为自己所在领域中前1%的顶尖人才。 这就是为什么那些喜欢阅读和学习的通才以及那些研究思维模型的人都能变得如此成功的原因所在。这也解释了为什么世界上这么多顶尖CEO、亿万富翁、科学家和成功人士都具有这些共同的特质。 现在就下定决心，不要把所有的时间都花在一个狭窄领域的细节知识上，因为这只能让自己看不到世界其他地方、其它领域发生的事情，这将阻碍你快速适应新发展的能力。 相反，要在终身学习上进行投入，每周至少花五个小时在你的领域之外进行学习和探索，学习你的同事还不知道的技能和概念。此外，一定要学习思维模型，这些底层基础知识是所有领域知识的基础，而且基本不会随时时间的推移发生变化。将自己训练成为一名对思维模型有深入了解的自学成才型通才，做到这些将是在现代知识经济中获得成功的关键。 原文链接：https://medium.com/the-mission/the-founders-of-the-worlds-five-largest-companies-all-follow-the-5-hour-rule-and-they-re-9ca82e93f3fc 编译组出品。 转载来源：为什么全球顶尖成功人士都会遵循“5小时法则”？]]></content>
      <categories>
        <category>科技</category>
      </categories>
      <tags>
        <tag>成功的秘密</tag>
        <tag>比尔·盖茨</tag>
        <tag>伊隆·马斯克</tag>
        <tag>乔布斯</tag>
        <tag>巴菲特</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[第三方登录真的有那么便捷吗？]]></title>
    <url>%2F2018%2Fcb5648e3%2F</url>
    <content type="text"><![CDATA[打开新下载的 APP，小伙伴们会选择手机号码注册登录吗？ 我们越来越习惯不去记密码，而是点击跳转到微信、微博、QQ 等，再点击授权进入到 APP 中。主流 App 为了迎合用户的使用习惯，也会标配第三方登录。 非技术大神的你们难道就不好奇，为什么点击一下授权就完成了登录？如此神奇的功能背后的产品逻辑是什么？ 所谓的第三方登录，就是APP识别到用户将第三方的账号绑定到自己平台的 ID上直接完成登录的过程，简单来说，是指基于用户在第三方平台上已有的账号和密码来快速完成己方应用的登录或者注册的功能。常见的第三方登录平台，一般是已经拥有大量用户的平台，国内的就是各大厂：微信、微博、QQ 等，国外有 Facebook、Twitter。 了解了本质后，我们来看一下第三方登录为何如此受青睐。 APP 接入了第三方登录后，能快速获取到例如头像、昵称、性别、所在城市等用户信息，减少用户填写信息的步骤，简化注册流程，让用户能更快地使用产品，进而提高注册转化率。 而用户选择第三方登录，是为了更方便快捷，此外,和大厂有合作的 APP 也更值得信赖。 举个粟子，微信是最多用户选择的第三方接入应用，通过微信登录来注册一个APP ，只要点击“微信登录-跳转至微信页面-点击确认授权-返回APP ”四个步骤，不用记密码，不用收一大堆短信，在授权期间还不用频繁登录。 然而，第三方登录并不是百利而无一弊。 部分 APP 在用户确认授权后返回界面，仍然需要绑定用户手机号码才能完成账号注册；而且，如果用户用账号密码注册后再用第三方登录进入应用，很可能会发生多于一个账户的情况出现；再者，第三方登录会带来隐藏的个人信息或动态的泄露风险。 APP 商以为第三方登录有效降低用户的使用门槛，减少用户注册登录时长，降低安全风险。但实际上，近些年常用的第三方登录容易造成一人多号，导入的用户鱼龙混杂导致难以转化，用户账号管理上也更加混乱。 哇哩咧？找不到快捷又安全的登录方式提高注册转化率，辛辛苦苦引来的流量不能转化为真正的注册用户，身背几百万拉新任务的渠道宝宝们，KPI的鞭子在后面啪啪响，向 Boss 再借五百年也完不成任务啊！ 经过多方寻找和对比，渠道宝宝们终于找到了一款真正的一键登录，那就是中国移动推出的一键免密登录。为了不再被坑，我们来仔细了解下。 一键免密登录是中国移动旗下的移动认证面向 APP 推出的一项功能，它是基于运营商特有的网关取号、验证能力，能自动通过底层数据网络网关和短信网关直接识别本机号码，在不会泄漏用户信息的前提下，安全、快速地验证用户身份。用户无需经过繁琐的注册登录环节就能一键免密登录 APP，将用户用户体验做到了极致。 APP 接入此能力后，因为降低了用户的注册或登录成本，从而减少由于本地注册的繁琐性而带来的隐形用户流失，最终提高注册转化率；此外，该能力还能返回登录的本机号码，帮助 APP 建立用户与本机号码一对一的账号体系，便于精准营销。 这才是真正意义上的一键登录，APP 如果想一步到位实现注册登录的快捷性，移动认证能力无疑是最好的选择。不行，这么好的产品一定要推荐给Boss，KPI 完成指日可待。 中国移动 移动认证 第三方登录 安全验证 转载来源：第三方登录真的有那么便捷吗？]]></content>
      <categories>
        <category>数码</category>
      </categories>
      <tags>
        <tag>软件</tag>
        <tag>移动互联网</tag>
        <tag>中国移动</tag>
        <tag>运营商</tag>
        <tag>KPI</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[优质博文集锦-云栖社区-阿里云]]></title>
    <url>%2F2018%2F214a417c%2F</url>
    <content type="text"><![CDATA[优质博文集锦-云栖社区-阿里云 转载来源：优质博文集锦-云栖社区-阿里云]]></content>
  </entry>
  <entry>
    <title><![CDATA[微信红包订单存储架构变迁的最佳实践]]></title>
    <url>%2F2018%2F16838e0f%2F</url>
    <content type="text"><![CDATA[微信红包订单存储架构变迁的最佳实践 转载来源：微信红包订单存储架构变迁的最佳实践]]></content>
      <tags>
        <tag>CSDN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[首发：基于 Python3 的开源堡垒机 Jumpserver v1.0正式发布]]></title>
    <url>%2F2018%2Fc0f2fff6%2F</url>
    <content type="text"><![CDATA[Jumpserver是一款开源堡垒机产品(GPLv2 License)，使用 Python3 和 Django1.11 开发。自2014年诞生以来，经历了从 v0.1 到 v0.5， 5个版本迭代，今天我们正式发布一个里程碑版本 v1.0.0。Jumpserver Star数已超过4400个，安装也超过20000人次，感谢朋友们的支持和守候，我们会再接再厉砥砺前行，为中国开源做出一些贡献，改变世界从一点点开始。 1.0.0 **版本新增主要功能:** Windows支持 在既有Linux Web Terminal基础上整合 Apache Guacamole实现了RDP登录- 容器化部署 解决了之前版本持久化内容过多问题，支持容器化部署- 资产树 之前版本使用资产组来组织资产，这次实现了可嵌套的资产组，方便组织资产和以后的授权- 录像/命令存储支持OSS/S3和ES 命令和录像记录通常会限制堡垒机的并发，这里我们实现录像云存储和命令存储到ES集群- 分布式部署 一个中心节点部署Jumpserver API，提供核心API，各云上部署登录组件Coco，无并发限制，无性能瓶颈- 系统用户自动推送 授权后自动将系统用户推送到资产上，当然也支持手动推送- 标签管理 可以给资产打标签，方便过滤- 命令统计增加输出展示 之前版本只统计了输入命令，该版本输出同样进行了收集- Web Terminal改进 类似IDE形式的Web Terminal让你有放弃 xshell, secureCRT的冲动- 系统设置 有些配置直接在页面中设置即可，不用再修改配置文件- LDAP支持 支持了LDAP集中认证，方便企业统一认证容器化部署 解决了之前版本持久化内容过多问题，支持容器化部署 录像/命令存储支持OSS/S3和ES 命令和录像记录通常会限制堡垒机的并发，这里我们实现录像云存储和命令存储到ES集群 系统用户自动推送 授权后自动将系统用户推送到资产上，当然也支持手动推送 命令统计增加输出展示 之前版本只统计了输入命令，该版本输出同样进行了收集 系统设置 有些配置直接在页面中设置即可，不用再修改配置文件 下面秀一波图： Dashboard 用户列表 资产树列表 命令统计 在线会话 Web Terminal Linux Web Teminal Windows SSH登陆 更多图大家自己安装体验吧，感谢你的阅读，Jumpserver致力于打造更好用的混合云堡垒机系统，诚邀你的参与。 官网：http://www.jumpserver.org 文档：http://docs.jumpserver.org 项目：http://github.com/jumpserver/jumpserver 转载来源：首发：基于 Python3 的开源堡垒机 Jumpserver v1.0正式发布]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Apache</tag>
        <tag>Windows</tag>
        <tag>Dashboard</tag>
        <tag>IDE</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[从鸿茅药酒违法广告数2000+依然热销，谈消费者为什么对保健品痴迷？]]></title>
    <url>%2F2018%2Ff623961a%2F</url>
    <content type="text"><![CDATA[从鸿茅药酒违法广告数2000+依然热销，谈消费者为什么对保健品痴迷？ 转载来源：从鸿茅药酒违法广告数2000+依然热销，谈消费者为什么对保健品痴迷？]]></content>
      <tags>
        <tag>冀连梅药师</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[帐号已迁移]]></title>
    <url>%2F2018%2Fbf19b63e%2F</url>
    <content type="text"><![CDATA[帐号已迁移 转载来源：帐号已迁移]]></content>
  </entry>
  <entry>
    <title><![CDATA[重磅数据揭示中国经济未来，真相无比残酷！]]></title>
    <url>%2F2018%2Fbe1e1600%2F</url>
    <content type="text"><![CDATA[重磅数据揭示中国经济未来，真相无比残酷！ 转载来源：重磅数据揭示中国经济未来，真相无比残酷！]]></content>
      <tags>
        <tag>凤凰财经</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[净水器消费者报告：关于净水你需要知道的一切]]></title>
    <url>%2F2018%2Fa1b04fb1%2F</url>
    <content type="text"><![CDATA[净水器消费者报告：关于净水你需要知道的一切 转载来源：净水器消费者报告：关于净水你需要知道的一切]]></content>
      <tags>
        <tag>爱否科技</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度学习精要之CapsuleNets理论与实践（附Python代码）]]></title>
    <url>%2F2018%2Fe0aa8c36%2F</url>
    <content type="text"><![CDATA[摘要： 本文对胶囊网络进行了非技术性的简要概括，分析了其两个重要属性，之后针对MNIST手写体数据集上验证多层感知机、卷积神经网络以及胶囊网络的性能。 神经网络于上世纪50年代提出，直到最近十年里才得以发展迅速，正改变着我们世界的方方面面。从图像分类到自然语言处理，研究人员正在对不同领域建立深层神经网络模型并取得相关的突破性成果。但是随着深度学习的进一步发展，又面临着新的瓶颈——只对成熟网络模型进行加深加宽操作。直到最近，Hinton老爷子提出了新的概念——胶囊网络（Capsule Networks），它提高了传统方法的有效性和可理解性。 本文将讲解胶囊网络受欢迎的原因以及通过实际代码来加强和巩固对该概念的理解。 为什么胶囊网络受到这么多的关注？对于每种网络结构而言，一般用MINST手写体数据集验证其性能。对于识别数字手写体问题，即给定一个简单的灰度图，用户需要预测它所显示的数字。这是一个非结构化的数字图像识别问题，使用深度学习算法能够获得最佳性能。本文将以这个数据集测试三个深度学习模型，即：多层感知机（MLP）、卷积神经网络（CNN）以及胶囊网络（Capsule Networks）。 多层感知机（MLP） 使用Keras建立多层感知机模型，代码如下： 在经过15次迭代训练后，结果如下： 可以看到，该模型实在是简单！ 卷积神经网络（CNN） 卷积神经网络在深度学习领域应用十分广泛，表现优异。下面构建卷积神经网络模型，代码如下： 打印模型参数概要： 从上图可以发现，CNN比MLP模型更加复杂，下面看看其性能： 可以发现，CNN训练耗费的时间比较长，但其性能优异。 胶囊网络（Capsule Network） 胶囊网络的结构比CNN网络更加复杂，下面构建胶囊网络模型，代码如下： 打印模型参数概要： 该模型耗费时间比较长，训练一段时间后，得到如下结果： 可以发现，该网络比之前传统的网络模型效果更好，下图总结了三个实验结果： 这个实验也证明了胶囊网络值得我们深入的研究和讨论。 胶囊网络背后的概念为了理解胶囊网络的概念，本文将以猫的图片为例来说明胶囊网络的潜力，首先从一个问题开始——下图中的动物是什么？ 它是一只猫，你肯定猜对了吧！但是你是如何知道它是一只猫的呢？现在将这张图片进行分解： 情况1——简单图像 你是如何知道它是一只猫的呢？可能的方法是将其分解为单独的特征，如眼睛、鼻子、耳朵等。如下图所示： 因此，本质上是把高层次的特征分解为低层次的特征。比如定义为： P(脸) = P(鼻子) &amp; ( 2 x P(胡须) ) &amp; P(嘴巴) &amp; ( 2 x P(眼睛) ) &amp; ( 2 x P(耳朵) ) 其中，P(脸) 定义为图像中猫脸的存在。通过迭代，可以定义更多的低级别特性，如形状和边缘，以简化过程。 情况2——旋转图像 将图像旋转30度，如下图所示： 如果还是按照之前定义的相同特征，那么将无法识别出它是猫。这是因为底层特征的方向发生了改变，导致先前定义的特征也将发生变化。 综上，猫识别器可能看起来像这样： 更具体一点，表示为： P(脸) = ( P(鼻子) &amp; ( 2 x P(胡须) ) &amp; P(嘴巴) &amp; ( 2 x P(眼睛) ) &amp; ( 2 x P(耳朵) ) ) OR ( P(rotated_鼻子) &amp; ( 2 x P(rotated_胡须) ) &amp; P(rotated_嘴巴) &amp; ( 2 x P(rotated_眼睛) ) &amp; ( 2 x P(rotated_耳朵) ) ) 情况3——翻转图像 为了增加复杂性，下面是一个完全翻转的图像： 可能想到的方法是靠蛮力搜索低级别特征所有可能的旋转，但这种方法耗时耗力。因此，研究人员提出，包含低级别特征本身的附加属性，比如旋转角度。这样不仅可以检测特征是否存在，还可以检测其旋转是否存在，如下图所示： 更具体一点，表示为： P(脸) = [ P(鼻子), R(鼻子) ] &amp; [ P(胡须_1), R(胡须_1) ] &amp; [ P(胡须_2), R(胡须_2) ] &amp; [ P(嘴巴), R(嘴巴) ] &amp; … 其中，旋转特征用R()表示，这一特性也被称作旋转等价性。 从上述情况中可以看到，扩大想法之后能够捕捉更多低层次的特征，如尺度、厚度等，这将有助于我们更清楚地理解一个物体的形象。这就是胶囊网络在设计时设想的工作方式。 胶囊网络另外一个特点是动态路由，下面以猫狗分类问题讲解这个特点。 上面两只动物看起来非常相似，但存在一些差异。你可以从中发现哪只是狗吗？ 正如之前所做的那样，将定义图像中的特征以找出其中的差异。 如图所示，定义非常低级的面部特征，比如眼睛、耳朵等，并将其结合以找到一个脸。之后，将面部和身体特征结合来完成相应的任务——判断它是一只猫或狗。 现在假设有一个新的图像，以及提取的低层特征，需要根据以上信息判断出其类别。我们从中随机选取一个特征，比如眼睛，可以只根据它来判断其类别吗？ 答案是否定的，因为眼睛并不是一个区分因素。下一步是分析更多的特征，比如随机挑选的下一个特征是鼻子。 只有眼睛和鼻子特征并不能够完成分类任务，下一步获取所有特征，并将其结合以判断所属类别。如下图所示，通过组合眼睛、鼻子、耳朵和胡须这四个特征就能够判断其所属类别。基于以上过程，将在每个特征级别迭代地执行这一步骤，就可以将正确的信息路由到需要分类信息的特征检测器。 在胶囊构件中，当更高级的胶囊同意较低级的胶囊输入时，较低级的胶囊将其输入到更高级胶囊中，这就是动态路由算法的精髓。 胶囊网络相对于传统深度学习架构而言，在对数据方向和角度方面更鲁棒，甚至可以在相对较少的数据点上进行训练。胶囊网络存在的缺点是需要更多的训练时间和资源。 胶囊网络在MNIST数据集上的代码详解首先从识别数字手写体项目下载数据集，数字手写体识别问题主要是将给定的28x28大小的图片识别出其显示的数字。在开始运行代码之前，确保安装好Keras。 下面打开Jupyter Notebook软件，输入以下代码。首先导入所需的模块： 然后进行随机初始化： 下一步设置目录路径： 下面加载数据集，数据集是“.CSV”格式。 展示数据表示的数字： 现在将所有图像保存为Numpy数组： 这是一个典型的机器学习问题，将数据集分成7:3。其中70%作为训练集，30%作为验证集。 下面将分析三个不同深度学习模型对该数据的性能，分别是多层感知机、卷积神经网络以及胶囊网络。 1.多层感知机 定义一个三层神经网络，一个输入层、一个隐藏层以及一个输出层。输入和输出神经元的数目是固定的，输入为28x28图像，输出是代表类的10x1向量，隐层设置为50个神经元，并使用梯度下降算法训练。 打印模型参数概要： 在迭代15次之后，结果如下： Epoch 14/1534300/34300 [==============================] - 1s 41us/step - loss: 0.0597 - acc: 0.9834 - val_loss: 0.1227 - val_acc: 0.9635Epoch 15/1534300/34300 [==============================] - 1s 41us/step - loss: 0.0553 - acc: 0.9842 - val_loss: 0.1245 - val_acc: 0.9637 结果不错，但可以继续改进。 2.卷积神经网络 把图像转换成灰度图（2D），然后将其输入到CNN模型中： 下面定义CNN模型： 打印模型参数概要： 通过增加数据来调整进程： CNN模型的结果： 3.胶囊网络 建立胶囊网络模型，结构如图所示： 下面建立该模型，代码如下： 打印模型参数概要： 胶囊模型的结果： 为了便于总结分析，将以上三个实验的结构绘制出测试精度图： 从结果中可以看出，胶囊网络的精度优于CNN和MLP。 总结本文对胶囊网络进行了非技术性的简要概括，分析了其两个重要属性，之后针对MNIST手写体数据集上验证多层感知机、卷积神经网络以及胶囊网络的性能。 作者信息 Faizan Shaikh，数据科学，深度学习初学者。 本文由阿里云云栖社区组织翻译，文章原标题《Essentials of Deep Learning: Getting to know CapsuleNets (with Python codes)》，作者：Faizan Shaikh，译者：海棠，审阅：Uncle_LLD。 转载来源：深度学习精要之CapsuleNets理论与实践（附Python代码）]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>编程语言</tag>
        <tag>深度学习</tag>
        <tag>Python</tag>
        <tag>机器学习</tag>
        <tag>编译器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[特朗普为何执意要与中国打贸易战？-虎嗅网]]></title>
    <url>%2F2018%2F698d1f6a%2F</url>
    <content type="text"><![CDATA[虎嗅注：特朗普上台和“推特治国”标志着民粹主义在美国崛起。 而内部的民粹主义又往往表现为外部的经济民族主义和贸易保护主义。仔细分析民粹主义的几波浪潮不难发现，背后的原因实际上是社会贫富差距的扩大，少数人和企业掌握了绝大部分的财富。以美国为例，最富有的10%的人群，掌握了75%的资产。而大量的中产阶级没有享受到科技发展的果实。特朗普的对内对外的经济策略，正是迎合了这部分失意人群。而某种程度上，中国也正面临类似的困境。 本文转载自微信公众号“点拾投资”（ID：deepinsightapp），作者：朱昂，原标题为《吞噬一切的民粹主义，根源到底是什么？》。 过去一年，全球的民粹主义大规模抬头，以特朗普通过发推特影响全球股市的波动而达到了顶峰。 从定义上看，民粹主义指的是平民论者所拥护的政治和经济理念，这个理念拥护平民掌控政治，反对精英或贵族掌控政治。那么，每一次民粹主义崛起背后的原因是什么，其对全球和各国经济发展又会有什么影响？ 民粹主义创1930以来新高 其实许多人对于民粹主义的感受不会很深，在过去几十年中民粹主义的出现非常少，仅仅在某些发展中国家，比如委内瑞拉的查韦斯，菲律宾新任总统杜特尔特等。在发达国家中，很长一段时间民粹主义都消失了。 但是最近几年这个势头开始崛起，下图发现民粹主义已经达到了1930年的高度，那一次之后就引发了世界大战。这个图是基于所有发达国家政党中，民粹主义获得投票的比例。包括了美国、英国、日本、德国、法国等。当然数据统计口径会有些不同，1930年的情况不完全等同于现在。但是我们更多希望大家从这张图中，能够明白全球民粹主义在今天是多么的严重。 桥水在2017年3月的这份报告中，就精准预测到了民粹主义在未来一年对于全球经济的影响。 他们认为未来一年民粹主义对于经济影响会比传统的货币政策，财政政策更加重要，甚至将影响未来一年的全球关系。 通过对于10个国家，14次民粹主义事件的研究中，桥水也发现了一些共性，他们通过研究对于民粹主义有几个关键词定义：普通人掌权，攻击现有利益者，贫富差距拉大，人民对于政府的效率不满意，需要强大的个人来更好掌控政府，保护主义，国家主义，军事主义，企图影响甚至控制媒体。下图是1930年代左右，两次世界大战全球涌现的一批民粹主义领导人。当时的共性就是经济增长疲软，贫富差距扩大，债务比例较高，对于目前的政党和金融业进行攻击。 案例一：美国罗斯福总统 美国历史上有两位罗斯福总统，这位是30年代的Franklin D Roosevelt（后面一个是Teddy Roosevelt）。他并不完全算民粹主义，只能说半民粹。我们都知道1929年开始的大萧条，是美国历史上经济最糟糕的阶段，那时候其实和现在的美国经济有部分类似的地方。 1）债务达到了泡沫阶段，导致经济和市场见顶（1929和2007年） 2）利率水平接近0，带来经济衰退（1932年和2008年） 3）印钞带来的去杠杆（1933年和2009年） 4）股票市场经历一次反弹（1933年~1936年和2009年~2017年） 5）经济周期性复苏（1933年~1936年和2009年~2017年） 6）央行开始收紧，带来一轮自我循环的经济回落（1937年） 当时，美国经济特征就是贫富差距很大，和今天类似。当时全美最富有的前10%人群，拥有85%的资产，今天这个比例是75%，但也是创了过去50年的新高。当然，那时候的失业率远比今天高。 当时美国还有一个现象，就是移民人口比例非常高，这直接导致了后面有一段出台了禁止移民政策。当然，那时的美国移民，今天已经都是真正的美国人。他们并非有色人种移民，更多是来自欧洲的美国第一代移民。 案例二：意大利，墨索里尼 在第一次世界大战之后，意大利的经济严重下滑，通胀高企，失业率更高。而且一战之后大量的军人无所事事。当时许多人移民去了美国，但是后来美国的移民收紧，导致这些人更加没有去处了。在墨索里尼上台之前，四年内换了四个总理，国家主义情绪飙升，墨索里尼也是利用军队获得了最高权力。 墨索里尼的主要政策包括，控制私有企业，让国有经济占到经济的四分之三；贸易保护主义，提高很高的关税；公开禁止犹太人参与到经济中；控制媒体；强硬的外交政策；政府刺激经济。 案例三：德国，希特勒 可能是影响最大的民粹主义者，希特勒当政的背景也源于财富分配不均衡。在1920年代，德国最富有的前10%人口，拥有国家40%的收入。这个贫富差距，是德国历史上最高的一次。而且当时的失业率也达到了历史新高。当时德国的外交政策也很弱，在凡尔赛协议下，基本上被英国和法国压制，这让许多德国人忍受不了。纳粹的投票权越来越高。 于是，反对弱外交政策的希特勒获得选票越来越多。作为墨索里尼的学徒，曾经当过兵的希特勒获得呼声越来越高。 当年他给墨索里尼写过一封信，里面有一句话的原文是“What will rank Mussolini among the great men of this earth is his determination not to share Italy with the Marxists, but to destroy internationalism and save the fatherland from it.” 1932年4月的总统选举中，希特勒获得了37%的选票。纳粹获得选票的增量，在于那些之前不投票的选民，他们许多人是中低收入的工人，曾经的保守派，农民。在1930年代的那次选举中，新增的投票人数超过了400万，接近15%的增幅，他们都是来支持希特勒的。 希特勒上台后的主要政策就是中央集权主义。其中分为几个部分，第一控制了劳动力，劳工局；第二，工业企业的政府化；第三，大量印钞来创造需求；第四，限制汇率管制和价格控制。下图是德国股市在希特勒上任后的表现，股市基本上还是上涨的。 案例四：西班牙内战，弗朗西斯科·佛朗哥 佛朗哥出生于海军军官家庭。1928年任新成立的萨拉戈萨高等军事学院院长。1936年7月18日联合其他反动军官发动反政府武装叛乱，挑起西班牙内战。同年10月被推举为“国家元首”并任叛军总司令，晋大元帅。1937年4月成为长枪党党魁。由于得到德、意法西斯支持和英、法等国采取“不干涉”政策，以及军队武器装备和训练素质占优势，最终获得胜利。 1939年内战结束后，成为终身国家元首。第二次世界大战期间，取缔其他一切政党，实行法西斯独裁统治。名义上保持中立，但帮助希特勒侵略苏联。第二次世界大战后，他被各国孤立，但和美国保持亲密的盟友关系， 因为美国的援助，西班牙成为工业化的发达国家。1947年自任摄政王。1969年指定胡安·卡洛斯为王位继承人。1975年逝世于马德里。其死后胡安·卡洛斯登上王位，实行民主改革， 西班牙结束独裁统治。 再来看看西班牙经济在1920到1940年的情况。当时整个当权政府效率很低，政府的力量越来越小，军队的力量越来越大。到了1931年，整个国内的掌权者开始向左翼倾斜，在后面直接导致了西班牙的内战。在佛朗哥当权的时候，他也是实行集权统治，有很强的保护主义倾向，以及很强的国民主义政策。整个国民在经历了几次大战后，资产价格都大幅缩水，农业和工业经济同时萎缩，甚至出现了严重的自然灾害。下图是佛朗哥当政前，西班牙股票市场的走势。 案例五：日本，军事力量的崛起 在一战前的1912年，日本负有19亿日元债务，战后不但全部偿清，且进而成为拥有27.7亿日元的债权国。日本军国主义专靠战争掠夺而致富，这种暴发经济具有很大的虚弱性，潜伏着巨大的危机。虽然重工业为战争需求而畸形发展，但基础工业和轻工业相对落后。农村中仍处于寄生地主制统治之下，农业生产力很低，生产规模小，破产农民不断增加，城乡人民生活贫困。 1929年10月，自美国首先爆发的世界性经济危机，使尚未从金融危机中恢复过来的日本经济受到了新的冲击。 日本政府1929年11月宣布，自1930年1月11日开始实行“黄金解禁”，试图促进出口，振兴经济，同时实行通货紧缩政策，借以摆脱危机。 然而“黄金解禁”带来相反的结果，黄金外流1930年达27552万日元，1931年外流43310万日元。另一方面，通货紧缩又招致生产萎缩，原材料价格猛跌。据统计从1930年6月~1931年6月，原材料价格下跌21.6％，生产资料下跌29.3％，消费资料下跌16.7％。从股份市场看，以1924年1月的股票价格指数为100，则1929年6月为104，1930年6月下跌到74，同年10月进一步跌到63。 危机还进一步使日本金融业遭受打击。 据大藏省调查，全国普通银行774家之中，有58家被迫停业。许多中小资本被兼并或削弱，1930年~1931年，减削资本总额达73000万日元，被解散公司资金达104000万日元。大量工人失业，工资指数急剧下降，而一些大公司垄断组织资金膨胀，卡特尔和托拉斯进一步发展，危机还波及到日本的海外殖民地与半殖民地，满铁的收入也急剧减少，1931年还出现了创建以来的第一次赤字现象。 1919年，与意大利墨索里尼的法西斯组织出现的同时，日本法西斯的鼻祖北一辉，写出了《国家改造案原理大纲》（1923年发行时改名《日本改造法案大纲》），要求对日本实行法西斯主义的国家改造。翌年，北一辉与大川周明组建了日本第一个法西斯团体“犹存社”，该社以北一辉的《大纲》为指导纲领。他们的根本目标，是建立法西斯专制，要“基于天皇亲政的本义”，“打破以党利为主的国策下之政党政治陋习，以期亿兆一心，实现国民理想之皇国政治。下图是这个阶段日本股市的走势： 民粹主义抬头的共性：贫富差距加大 由于全球经济增速大幅度放缓，低利率伴随着相对的低通胀。各个央行都希望刺激消费，通过宽松的流动性拉高通胀。然而在低迷的实体经济增长背景下，通胀一直起不来，经济整体修复也是进二退一。导致央行必须不断保持宽松，来迎合“politically correct”（政治正确）。 过去的十年，我们也真正发现央行并不独立。宽松的货币背后是贫富差距拉大，越来越多的钱进入优质资产。拥有优质资产的人财富大规模提高。拥有优质资产的国家和其他国家拉开差距，拥有优质资产的公司也和普通公司拉开差距。这就是今天全球社会的根源。 我们先看下面这张图，就能认识到Trump为何有如此高支持率。 从2000年以来，美国家庭收入中位数水平是下滑的。美国家庭收入中，只有在最高收入前80%分位的人出现了增长。这背后就是美国的大部分中产并没有享受到过去十几年经济增长的果实。 相反，全球化带来的是他们工作被中国，被科技所替代。而金融，互联网领域中具有优质资产的人收入不断提高，特别是经历了金融危机后，美国家庭贫富差距加大。 在美国的同学也跟我说，原本那些拿着高工资的汽车生产线工人、邮递员、医院设备调试员等收入都出现了下滑。但是华尔街、硅谷最优秀的一批人收入大规模提高。上一轮互联网泡沫最优秀程序员的工资大概是25万美元，这一轮互联网泡沫大量程序员年收入已经超过了100万美元。而美国总统大选是一个popularity test（人气测试）。Trump的政治方案虽然听上去疯狂，但符合了大部分美国人的心意。利率不断降低所导致的贫富差距拉大，终于在今天的美国全面爆发。 2000年、2007年、2014年到今天美国家庭收入变化。50%分位家庭从2000年和2007年到今天的收入是下滑的。 在中国，类似的事情也在发生。持续宽松的信贷政策以及低利率带来了资产价格的泡沫。 过去几年最显著的就是北上广深房价上涨，中国楼市已经是目前人类最大的一次泡沫之一了。深圳的房价收入比高达38倍为全球最高，北京和上海也在30倍以上，排名第四和第五。 过去几个月，关于房价的研究和讨论远远超过股市。更可怕的是，在经济增速向下的大背景下，大部分人的收入已经难以高速增长。如此高的房价收入比，无法通过更高的收入增速来消化。而在这个过程中，显然拥有资产的富人财富继续增长，没有资产的穷人变得越来越难翻身。经济增速放缓，资产泡沫拉大贫富差距，社会流动固化，这也导致了中国目前一系列的问题。 公司贫富差距拉大 贫富差距的拉大不但发生在家庭，也同样发生在公司。低利率带来了更便宜的流动性，这些流动性不断涌向回报率更高的公司，最终导致优秀的公司获得的资源越来越大，大幅拉开和普通公司之间的距离。 今天，全球大约10%的上市公司掌握了80%的利润。收入规模在10亿美元以上的公司拥有全球60%的企业收入以及65%的市值。 大公司市值集中度提高在美国尤为明显。名义GDP中来自于前100大公司的占比从1994年的33%提高到了2013年的46%。另一边，美国上市公司数量从1997年的6797家大幅下滑到2013年的3485家。我们会看到一个现象，就是大公司越来越大。今天前100大公司的市值目前已经超过了16万亿美元，而这个数在2009年只有8.4万亿。我们会惊奇的看到类似于Facebook, 亚马逊这样的超级大盘股能够实现一年翻倍的行情，而许多中小市值的公司股价却下跌了70%以上。互联网被几大平台公司垄断后，创业反而越来越艰难。 最终特朗普是否会扭转这个趋势，带来一次财富重新分配？ 转载来源：特朗普为何执意要与中国打贸易战？-虎嗅网]]></content>
  </entry>
  <entry>
    <title><![CDATA[中兴禁运令有感]]></title>
    <url>%2F2018%2F48d10726%2F</url>
    <content type="text"><![CDATA[中兴禁运令有感 转载来源：中兴禁运令有感]]></content>
      <tags>
        <tag>Dribbler</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CNode：Node.js专业中文社区]]></title>
    <url>%2F2018%2Fff373b3b%2F</url>
    <content type="text"><![CDATA[CNode：Node.js专业中文社区 转载来源：CNode：Node.js专业中文社区]]></content>
  </entry>
  <entry>
    <title><![CDATA[我曾误删了公司的数据库，但还是活下来了]]></title>
    <url>%2F2018%2F71efaa3e%2F</url>
    <content type="text"><![CDATA[编者按：开发人员总以为自己误删了数据是天大的事情。的确如此，但是“罪不至死”。本文作者Zachary Kuhn在“Those two times where I clearly had no clue what I was doing as a developer”一文中分享了自己的亲身遭遇，并认为犯了错误不重要，重要的是要有所得。 上周我与同事们进行了一次关于职业生涯中搞砸了一些事情的简短谈话。这确实会沦为他人笑柄，却更给我们带来了珍贵的教训。重要的是，我们应该分享那些曾经的错误，这样其他人就可以从其中学习。下文是最近在我身上发生的例子。 为什么有如此多误删生产数据库的事情发生？ 几个月前，Reddit上有一篇文章，讲了一名初级开发人员在上班的第一天就删除了生产数据库的事。我们都很憷于读到这类犯了这类无法让人忘却的大错误的文章。因为我们离这些也不远，而大多数人都是“死里逃生”。 在我的第一份工作中，一位高级数据库管理员在上班第一天就误删了生产数据库。这类故事情节比比皆是。这个团队从一个星期的备份中恢复了他导致的错误，并让他继续工作。十年后，他们依然将其作为笑点。 今年早些时候，我被派去检查一个客户的生产数据上的问题。他们进行了小范围的非公开测试，结果网站上没有显示任何内容。我想查查是否是存在漏洞或是易损性问题导致了这一结果。 我通过了生产机器上的签名环节，然后打开了数据库。内容库（articles table）内空空如也。这证实了我们在网站上看到的情况是真实的。 用户库（users table）内依然有用户数据存在。真让人奇怪。所以情况是我们丢失了所有内容，但是至少测试用户的信息依然存在。我们给出的解释是这是一个测试行为，所以这些事情有可能发生。 接下来的几分钟一片混乱。我不记得自己做了什么。我不认为自己笨到在控制台上执行了删除用户库的操作。但是事实就是这么发生了，现在后台既没有了内容库，也没有了用户库。这真实下了我一大跳。 然后我的大脑就开始转动起来思考如何解决这个问题。我真的把用户库给删掉了吗？是的。我们存备份了吗？没有。我们应该如何告诉客户这个事情？不知道。 我犹记得自己走向项目经理那里，坐在她身边，向她解释了发生了什么事情时的场面。因为我们的内容库中没有内容，这就是为什么网站上空空如也的原因。同时，我还删除了用户库。他们现在需要重新邀请所有的用户，如果他们能够弄清楚谁是谁。 我回到了自己的办公室，垂头丧气。 不过，我还是没有接受这件事。我们一开始是如何失去这些东西的？ 我开始不停地往深处想。半是为了否认这件事，半是想要挽回面子。不久，我注意到了一些重要事情。 在服务器上还存在着其他5个数据库。其中一个数据库的名字和我刚才看到的数据库名字很像。 当我查看这个数据库的时候，发现所有的内容都在里面。用户库也安然无恙。结果证明，是一个配置变动无意中改变了生产设置，使站点指向了一个全新的数据库。我之前所看的用户信息是什么？种子数据。 真是谢天谢地。早上的神经紧张和胃酸让我觉得很不舒服，但是我们“恢复”了数据，并在坏消息传开之前找到了真正的问题。 从这件事中可以吸取很多教训。其中一点是关于最简单原则：我们总是在做的备份，也许是开发人员最有成效的挽救药。 继续前进但不要冲得过前 我最近犯的一个错误不太引人注目。事实上，这是一个经由小错误所引起的小错误最终导致了一场混乱的故事。 我们面临的是一个时间紧迫的项目。 在初次会议上，我们团队一致认为完成它会花费比预定时间多一倍的时间。这个最后期限一开始就对我们产生影响，让我宽松地通过了身份认证部分而留有更多时间去关注客户所实际关注的功能设计。 我只是在一个单一页面测试了身份验证测试，但是当时还不了解它们将如何被组合在一起。 把它单列出来是我做的一个错误决定。我忽略了一些重要事情： 用户在登陆之后会从cookie中加载内容，但是这个页面却试图在没有任何等待的情况下进行加载。根据事件的发生顺序，用户会得到带来服务器的反映，说其是未经授权的。1. 身份验证也未检查令牌是否过期。如果用户不经常访问这个网站。那么当其再一次访问时，网站需要用户登出再登入才会运行。1. 令牌应该基于每个请求进行更新，但是我从未花费时间去理解其发生前后的规则。所以，这又产生了一个时间问题。如果我们同时发送了几个请求，根据它们返回的顺序，用户会得到那个在后来的请求中无法使用的令牌。身份验证也未检查令牌是否过期。如果用户不经常访问这个网站。那么当其再一次访问时，网站需要用户登出再登入才会运行。 我们匆匆忙忙地赶着项目，却仍花费了比规定多一倍的时间。区别之处在于有更多的漏洞，并需要花更多时间去跟踪并修复这些漏洞。 这使我感到窘迫。之后因为整件事情变得比较糟糕哦而让我在公众场合感到羞愧。 我想说的是：在此之后，我花费了时间去学习认证程序。我现在了解了OAuth、JWT、刷新令牌和到期行为。我仔细研究了其他人所编写的身份验证代码。我能够在不同的语言和框架中建构身份验证程序。 将失败转化为未来的成功 这是我从那些表现糟糕的事情中所获得的经验。如果你愿意，那么几乎所有好的结果都会由此而来。 如果有人能从自己的错误中汲取教训，那么他就会比现在更优秀。我试着不去打击那些第一次犯错误的队友。他们通常都知道自己把事情搞的一团糟。 我也正尝试不对那些不断犯同样错误的人施加压力。他们仍然值得同情。 如果在错误中做到这4点，那么你就会不断成长： 嘲笑自己。1. 从中汲取经验教训。1. 改正错误。1. 分享自己的错误，让其他人也有所收获。从中汲取经验教训。 分享自己的错误，让其他人也有所收获。 最后，我想讲一个关于错误价值的轶事。20世纪初，IBM的首席执行官托马斯·J·沃森曾遇到过一名员工，这名员工的一系列糟糕决策让公司付出了巨大代价。当沃森被问到是否会解雇这名员工时，他回应道： “不，我刚在他身上花了60万美元的培训费。为什么要让别人白白捡去这个便宜？” 原文链接：https://medium.freecodecamp.org/the-times-ive-messed-up-as-a-developer-3c0bcaa1afd6 编译组出品。编辑：郝鹏程 转载来源：我曾误删了公司的数据库，但还是活下来了]]></content>
      <categories>
        <category>其它</category>
      </categories>
      <tags>
        <tag>DBA</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[腾讯入局在线文档领域，初创公司还有机会逆袭吗？]]></title>
    <url>%2F2018%2Fdcf1147a%2F</url>
    <content type="text"><![CDATA[摘要：4 月 18 日，腾讯正式发布在线文档工具「腾讯文档」，马化腾在朋友圈表示：这是一个意外惊喜，没有一丝丝的防备。 上周 4 月 12 日的媒体沟通会上，石墨文档正式对外公布，已于 2017 年完成近亿元的 B 轮融资，投资方为今日头条。4 月 18 日，腾讯也正式发布在线文档工具「腾讯文档」，马化腾在朋友圈表示：这是一个意外惊喜，没有一丝丝的防备。 腾讯文档入局，更「意外」的应该是刚刚站稳脚跟的同类产品。此前国内并没有成熟的在线文档产品，初创公司最大的优势之一就是出发早，而腾讯入局无疑会加速这一领域的竞争。 腾讯是带着资源、野心来的，在线协作工具需要的社交场景腾讯已经具备，各种资源、能力的接入也比创业团队更有优势。当然，有优势并不代表一定会成功。总体来看，在线文档在国内还处于起步阶段，腾讯这样的大玩家加入，更大的意义在于普及作用。 我们为什么要用在线文档？ 在线文档并不是一个新概念。国外的典型产品是 Google Docs，2007 年发布，至今已经有十年。2016 年，Google Docs 的活跃用户就超过了 3 亿，企业用户达 500 万，年收入超过 10 亿美元。在传统办公领域占据领导地位的微软，也在 2011 年推出了云端的 Office 365。从理论上讲，微软 Office 在全球的十几亿用户都是在线文档的潜在用户，市场潜力十分巨大。 在腾讯文档发布之前，国内并没有互联网巨头直接参与到在线文档的竞争，不过大家对这一市场已经早有关注。今日头条投资了石墨文档、金山 WPS 投资了一起写，就连微软也针对微信平台推出了 Office 365 微助理。 大家已经都意识到这是一片巨大的蓝海，但参与方式有所不同，这跟各家的基因有关。金山 WPS、微软 Office 本来就在做传统办公软件，Google 一直在布局自己的软件生态，腾讯则是以社交为切入点。 先不谈各自的目的，在线文档本身就是一种产品进化的产物。首先它是在线的，不需要额外下载软件，在浏览器就能使用，抛弃了本地文件，文件的管理更方便。第二，在线文档让协作和分享变得更自由，实时编辑、实时修改，减少了本地文件繁多造成的失误。第三，很重要的一点，在线文档比本地文件更安全，设备损坏、被偷，文件都不会丢失。尤其是涉及多人协作的时候，文档的查看、编辑权限都由用户自己控制。 石墨文档的创始人吴洁举过这样一个例子，现在任何一家企业员工离职 5 分钟之内就可以把文件资料全都拷走，而云端的文件批量下载时，是可以通过技术手段切断的，拷走的内容也可以打上水印。所以当企业员工统一用在线文档工具时，内部资料很安全，离职时也能回收，并且不会影响自己的个人资料。 腾讯做在线文档的优势 其实从目前的产品来看，腾讯文档和同类软件并没有太大区别，上面说的功能大家也几乎都有，那么腾讯做这件事的优势在哪里呢？ 石墨文档的吴洁曾提到，中国的创业公司做在线文档，是有一定技术壁垒的。国外把 Office 三件套做全的公司只有微软和谷歌，不算 PPT 的话还有 Quip，而 Quip 的创始人是 Facebook 的前 CTO，团队是硅谷最厉害的一批工程师。抛去这样的背景不讲，想做 Office 是很难的。在中国做在线文档，需要坚定的决心、长时间的投入，和关键的人才。 这些挑战对创业公司来说比较困难，而由腾讯来做，起步的优势、运营能力、未来的发展都有了很多的可能性。 腾讯文档的发布并不是临时起意，其实最早用户用 QQ 传文件，这就是一种办公的雏形。直到现在，QQ 上还有将近两亿的文档活跃用户，日均文件传输量超过 1.8 亿次。2017 年 4 月，腾讯推出主打办公的 TIM 客户端，进一步解决用户的办公问题，目前月活用户也已经突破两千万，内置的在线文档规模超过 1300 万。这次推出腾讯文档，腾讯有更大的野心，就是通过文档这样一个支点，去撬动办公背后一个足够支撑千亿市值的巨大市场。 腾讯的基因是社交，而在线文档又需要多人协同，所以微信、QQ 的关系链就成了腾讯文档最大的优势。腾讯文档的服务除了支持网页端、移动端两大平台，还和微信、QQ 全面打通。QQ 传输的文档可以一键转为在线文档进行编辑以及预览，微信上则有小程序的版本支持。比起其他同类产品的分享，腾讯文档不管是在查看、编辑体验，还是账号登录、分享上，都要便捷许多。 作为国内互联网的巨头，腾讯产品间的资源互补形成了一个庞大的生态，拿已经加入的翻译功能来说，接入的是「翻译君」的能力，类似的天气、地理等数据，都可以在闭环系统中提供。在线文档是核心，关系链是纽带。未来腾讯的 QQ 邮箱、多人语音都可以与在线文档产生融合，如果能达到这样的预期，腾讯完全可以自己搞出一套办公领域的产品矩阵。 对初创公司是好事还是坏事？ 对这个领域的初创公司来说，腾讯的入局是好事还是坏事？这很难说。正如前面提到的，两者目前在功能上并看不出太大区别，甚至石墨的某些细节功能、设计要做得更好。所以前期腾讯文档能带来的，一定是把整个在线文档的市场扩大，将更多潜在用户带入这个全新的领域。至于以后，与在线文档相近的笔记、协作类领域都诞生过独角兽，机会还是有的。只是腾讯文档一出现，初创公司的节奏就不得不加快了，整个产业会进入快速竞争的时期，玩家也会越来越多。 从目前国内的在线文档市场来看，现在各家抢的不是对方的市场，而是未开垦的潜在用户，腾讯十亿的用户无疑是个巨大的压力。据石墨文档的吴洁说，目前他们的盈利还主要靠在个人收费版和企业版上，内部也在推出 SDK 等一些商业化的产品。盈利对初创公司来说永远是个紧箍咒，相比之下腾讯文档少了很多束缚。 现在去谈谁是最后的赢家还很远，可以预见的是在线文档领域会涌入更多的玩家。好在在线文档还是个重体验的产品，所以虽然腾讯入局，机会也还是有的，未来这个领域会有更多的可能。 编辑：Rubberso ■ 转载来源：腾讯入局在线文档领域，初创公司还有机会逆袭吗？]]></content>
      <categories>
        <category>科技</category>
      </categories>
      <tags>
        <tag>创业</tag>
        <tag>Google</tag>
        <tag>移动互联网</tag>
        <tag>微软</tag>
        <tag>云计算</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[五大常见的MySQL高可用方案]]></title>
    <url>%2F2018%2F9957f042%2F</url>
    <content type="text"><![CDATA[1.概述我们在考虑MySQL数据库的高可用的架构时，主要要考虑如下几方面： 如果数据库发生了宕机或者意外中断等故障，能尽快恢复数据库的可用性，尽可能的减少停机时间，保证业务不会因为数据库的故障而中断。- 用作备份、只读副本等功能的非主节点的数据应该和主节点的数据实时或者最终保持一致。- 当业务发生数据库切换时，切换前后的数据库内容应当一致，不会因为数据缺失或者数据不一致而影响业务。用作备份、只读副本等功能的非主节点的数据应该和主节点的数据实时或者最终保持一致。 关于对高可用的分级在这里我们不做详细的讨论，这里只讨论常用高可用方案的优缺点以及高可用方案的选型。 2.高可用方案2.1.主从或主主半同步复制 使用双节点数据库，搭建单向或者双向的半同步复制。在5.7以后的版本中，由于lossless replication、logical多线程复制等一些列新特性的引入，使得MySQL原生半同步复制更加可靠。 常见架构如下： 通常会和proxy、keepalived等第三方软件同时使用，即可以用来监控数据库的健康，又可以执行一系列管理命令。如果主库发生故障，切换到备库后仍然可以继续使用数据库。 优点： 架构比较简单，使用原生半同步复制作为数据同步的依据；1. 双节点，没有主机宕机后的选主问题，直接切换即可；1. 双节点，需求资源少，部署简单；双节点，没有主机宕机后的选主问题，直接切换即可； 缺点： 完全依赖于半同步复制，如果半同步复制退化为异步复制，数据一致性无法得到保证；1. 需要额外考虑haproxy、keepalived的高可用机制。需要额外考虑haproxy、keepalived的高可用机制。 2.2.半同步复制优化 半同步复制机制是可靠的。如果半同步复制一直是生效的，那么便可以认为数据是一致的。但是由于网络波动等一些客观原因，导致半同步复制发生超时而切换为异步复制，那么这时便不能保证数据的一致性。所以尽可能的保证半同步复制，便可提高数据的一致性。 该方案同样使用双节点架构，但是在原有半同复制的基础上做了功能上的优化，使半同步复制的机制变得更加可靠。 可参考的优化方案如下： 2.2.1.双通道复制 半同步复制由于发生超时后，复制断开，当再次建立起复制时，同时建立两条通道，其中一条半同步复制通道从当前位置开始复制，保证从机知道当前主机执行的进度。另外一条异步复制通道开始追补从机落后的数据。当异步复制通道追赶到半同步复制的起始位置时，恢复半同步复制。 2.2.2.binlog文件服务器 搭建两条半同步复制通道，其中连接文件服务器的半同步通道正常情况下不启用，当主从的半同步复制发生网络问题退化后，启动与文件服务器的半同步复制通道。当主从半同步复制恢复后，关闭与文件服务器的半同步复制通道。 优点： 双节点，需求资源少，部署简单；1. 架构简单，没有选主的问题，直接切换即可;1. 相比于原生复制，优化后的半同步复制更能保证数据的一致性。架构简单，没有选主的问题，直接切换即可; 缺点： 需要修改内核源码或者使用mysql通信协议。需要对源码有一定的了解，并能做一定程度的二次开发。1. 依旧依赖于半同步复制，没有从根本上解决数据一致性问题。依旧依赖于半同步复制，没有从根本上解决数据一致性问题。 2.3.高可用架构优化 将双节点数据库扩展到多节点数据库，或者多节点数据库集群。可以根据自己的需要选择一主两从、一主多从或者多主多从的集群。 由于半同步复制，存在接收到一个从机的成功应答即认为半同步复制成功的特性，所以多从半同步复制的可靠性要优于单从半同步复制的可靠性。并且多节点同时宕机的几率也要小于单节点宕机的几率，所以多节点架构在一定程度上可以认为高可用性是好于双节点架构。 但是由于数据库数量较多，所以需要数据库管理软件来保证数据库的可维护性。可以选择MMM、MHA或者各个版本的proxy等等。常见方案如下： 2.3.1.MHA+多节点集群 MHA Manager会定时探测集群中的master节点，当master出现故障时，它可以自动将最新数据的slave提升为新的master，然后将所有其他的slave重新指向新的master，整个故障转移过程对应用程序完全透明。 MHA Node运行在每台MySQL服务器上，主要作用是切换时处理二进制日志，确保切换尽量少丢数据。 MHA也可以扩展到如下的多节点集群： 优点： 可以进行故障的自动检测和转移;1. 可扩展性较好，可以根据需要扩展MySQL的节点数量和结构;1. 相比于双节点的MySQL复制，三节点/多节点的MySQL发生不可用的概率更低可扩展性较好，可以根据需要扩展MySQL的节点数量和结构; 缺点： 至少需要三节点，相对于双节点需要更多的资源;1. 逻辑较为复杂，发生故障后排查问题，定位问题更加困难;1. 数据一致性仍然靠原生半同步复制保证，仍然存在数据不一致的风险;1. 可能因为网络分区发生脑裂现象;逻辑较为复杂，发生故障后排查问题，定位问题更加困难; 可能因为网络分区发生脑裂现象; 2.3.2.zookeeper+proxy Zookeeper使用分布式算法保证集群数据的一致性，使用zookeeper可以有效的保证proxy的高可用性，可以较好的避免网络分区现象的产生。 优点： 较好的保证了整个系统的高可用性，包括proxy、MySQL;1. 扩展性较好，可以扩展为大规模集群;扩展性较好，可以扩展为大规模集群; 缺点： 数据一致性仍然依赖于原生的mysql半同步复制;1. 引入zk，整个系统的逻辑变得更加复杂;引入zk，整个系统的逻辑变得更加复杂; 2.4.共享存储 共享存储实现了数据库服务器和存储设备的解耦，不同数据库之间的数据同步不再依赖于MySQL的原生复制功能，而是通过磁盘数据同步的手段，来保证数据的一致性。 2.4.1.SAN共享储存 SAN的概念是允许存储设备和处理器（服务器）之间建立直接的高速网络（与LAN相比）连接，通过这种连接实现数据的集中式存储。常用架构如下： 使用共享存储时，MySQL服务器能够正常挂载文件系统并操作，如果主库发生宕机，备库可以挂载相同的文件系统，保证主库和备库使用相同的数据。 优点： 两节点即可，部署简单，切换逻辑简单；1. 很好的保证数据的强一致性；1. 不会因为MySQL的逻辑错误发生数据不一致的情况；很好的保证数据的强一致性； 缺点： 需要考虑共享存储的高可用；1. 价格昂贵；价格昂贵； 2.4.2.DRBD磁盘复制 DRBD是一种基于软件、基于网络的块复制存储解决方案，主要用于对服务器之间的磁盘、分区、逻辑卷等进行数据镜像，当用户将数据写入本地磁盘时，还会将数据发送到网络中另一台主机的磁盘上，这样的本地主机(主节点)与远程主机(备节点)的数据就可以保证实时同步。常用架构如下： 当本地主机出现问题，远程主机上还保留着一份相同的数据，可以继续使用，保证了数据的安全。 DRBD是linux内核模块实现的快级别的同步复制技术，可以与SAN达到相同的共享存储效果。 优点： 两节点即可，部署简单，切换逻辑简单；1. 相比于SAN储存网络，价格低廉；1. 保证数据的强一致性；相比于SAN储存网络，价格低廉； 缺点： 对io性能影响较大；1. 从库不提供读操作；从库不提供读操作； 2.5.分布式协议 分布式协议可以很好解决数据一致性问题。比较常见的方案如下： 2.5.1.MySQL cluster MySQL cluster是官方集群的部署方案，通过使用NDB存储引擎实时备份冗余数据，实现数据库的高可用性和数据一致性。 优点： 全部使用官方组件，不依赖于第三方软件；1. 可以实现数据的强一致性；可以实现数据的强一致性； 缺点： 国内使用的较少；1. 配置较复杂，需要使用NDB储存引擎，与MySQL常规引擎存在一定差异；1. 至少三节点；配置较复杂，需要使用NDB储存引擎，与MySQL常规引擎存在一定差异； 2.5.2.Galera 基于Galera的MySQL高可用集群， 是多主数据同步的MySQL集群解决方案，使用简单，没有单点故障，可用性高。常见架构如下： 优点： 多主写入，无延迟复制，能保证数据强一致性；1. 有成熟的社区，有互联网公司在大规模的使用；1. 自动故障转移，自动添加、剔除节点；有成熟的社区，有互联网公司在大规模的使用； 缺点： 需要为原生MySQL节点打wsrep补丁1. 只支持innodb储存引擎1. 至少三节点；只支持innodb储存引擎 2.5.3.POAXS Paxos 算法解决的问题是一个分布式系统如何就某个值（决议）达成一致。这个算法被认为是同类算法中最有效的。Paxos与MySQL相结合可以实现在分布式的MySQL数据的强一致性。常见架构如下： 优点： 多主写入，无延迟复制，能保证数据强一致性；1. 有成熟理论基础；1. 自动故障转移，自动添加、剔除节点；有成熟理论基础； 缺点： 只支持innodb储存引擎1. 至少三节点；至少三节点； 3.总结随着人们对数据一致性的要求不断的提高，越来越多的方法被尝试用来解决分布式数据一致性的问题，如MySQL自身的优化、MySQL集群架构的优化、Paxos、Raft、2PC算法的引入等等。 而使用分布式算法用来解决MySQL数据库数据一致性的问题的方法，也越来越被人们所接受，一系列成熟的产品如PhxSQL、MariaDB Galera Cluster、Percona XtraDB Cluster等越来越多的被大规模使用。 随着官方MySQL Group Replication的GA，使用分布式协议来解决数据一致性问题已经成为了主流的方向。期望越来越多优秀的解决方案被提出，MySQL高可用问题可以被更好的解决。 参考文献 [2015 OTN]彭立勋-DoubleBinlog方案.pdf —————— 其他阅读推荐： 我如何使用 Django + Vue.js 快速构建项目 一起学 Node.js | 使用 Express + MongoDB 搭建多人博客 本文由『UCloud存储研发团队』提供。 关于作者：王松磊，现任职于UCloud，从事MySQL数据库内核研发工作。主要负责UCloud云数据库udb的内核故障排查工作以及数据库新特性的研发工作。 ———以下是活动的分割线——— 欢迎加入 UCloud开源作者交流群，遇见更多有意思的项目和作者。 添加UCloud运营小妹个人微信号：Surdur，备注：开源作者，我们将拉你进群！ 另，运营小妹也陪聊很专业哦：） 「UCloud机构号」将独家分享云计算领域的技术洞见、行业资讯以及一切你想知道的相关讯息。欢迎提问&amp;求关注 o(////▽////)q~ 以上。 转载来源：五大常见的MySQL高可用方案]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>软件</tag>
        <tag>技术</tag>
        <tag>Linux</tag>
        <tag>MySQL</tag>
        <tag>MariaDB</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[EOS DAWN-V3.0.0 智能合约开发之Hello World]]></title>
    <url>%2F2018%2Fb1580910%2F</url>
    <content type="text"><![CDATA[C、C++、Java还是任何其他语言，一般刚开始学习的时候，我们都会从HelloWorld开始，这篇文章主要讲解EOS DAWN-V3.0.0智能合约开发之Hello World。 1. 编写合约代码在桌面创建一个文件夹，比如：0418，用Atom打开0418文件夹。新建文件Hello.cpp文件，并将下面的源码拷贝到Hello.cpp文件中。 1#include &lt;eosiolib/eosio.hpp&gt;#include &lt;eosiolib/print.hpp&gt;// 视频网站：http://kongyixueyuan.com// 个人博客：http://liyuechun.org// 公众号：区块链部落// 进技术群，请加微信（kongyixueyuan）//用eosio命名空间using namespace eosio;//所有的智能合约都继承自contract类class Hello : public eosio::contract &#123; public: using contract::contract; /// @abi action void hi( account_name user ) &#123; print( &quot;Hello, &quot;, name&#123;user&#125; ); &#125;&#125;;EOSIO_ABI( Hello, (hi) ) 2. 生成.wast文件1liyuechun:Project yuechunli$ eosiocpp -o Hello.wast Hello.cppliyuechun:Project yuechunli$ lsHello.cppHello.wastliyuechun:Project yuechunli$ 3. 生成.abi文件1liyuechun:Project yuechunli$ eosiocpp -g Hello.abi Hello.cpp Generated Hello.abi ...liyuechun:Project yuechunli$ lsHello.abiHello.cppHello.wastliyuechun:Project yuechunli$ 1&#123; &quot;____comment&quot;: &quot;This file was generated by eosio-abigen. DO NOT EDIT - 2018-04-18T08:15:50&quot;, &quot;types&quot;: , &quot;structs&quot;: [&#123; &quot;name&quot;: &quot;hi&quot;, &quot;base&quot;: &quot;&quot;, &quot;fields&quot;: [&#123; &quot;name&quot;: &quot;user&quot;, &quot;type&quot;: &quot;account_name&quot; &#125; ] &#125; ], &quot;actions&quot;: [&#123; &quot;name&quot;: &quot;hi&quot;, &quot;type&quot;: &quot;hi&quot;, &quot;ricardian_contract&quot;: &quot;&quot; &#125; ], &quot;tables&quot;: , &quot;clauses&quot;: &#125; 4. 创建钱包账号4.1 创建钱包1liyuechun:Hello yuechunli$ cleos wallet createCreating wallet: defaultSave password to use in the future to unlock this wallet.Without password imported keys will not be retrievable.&quot;PW5J3rx7Bfg9zb8Kf2owTytccFyJqtDTrqnUX8iBRRUvbwM8RyzRL&quot; PW5J3rx7Bfg9zb8Kf2owTytccFyJqtDTrqnUX8iBRRUvbwM8RyzRL必须保存好，解锁钱包时需要使用到这个密码。 4.2 创建两组key1liyuechun:Hello yuechunli$ ./cleos create key-bash: ./cleos: No such file or directoryliyuechun:Hello yuechunli$ cleos create keyPrivate key: 5K7QdknUZsF9apdBhD8TDMZGJjw8zJ8esYwS173YyFRv2453Z9tPublic key: EOS5RU8VsYBLnN5snGeUKmt1sDDzpvQbGyW7LPP6qEryaFctYieCKliyuechun:Hello yuechunli$ cleos create keyPrivate key: 5J8kComGiQHZyNmH6VvkHgtFggeQemazLpihKR4QW75DNkWTVdAPublic key: EOS5fqiC3VFAJ1riMrKf8vzD28nqd4EpXvZGpXt6YewEBnH8DYinG 4.3 向钱包导入私钥1liyuechun:Hello yuechunli$ cleos wallet import 5K7QdknUZsF9apdBhD8TDMZGJjw8zJ8esYwS173YyFRv2453Z9timported private key for: EOS5RU8VsYBLnN5snGeUKmt1sDDzpvQbGyW7LPP6qEryaFctYieCKliyuechun:Hello yuechunli$ cleos wallet import 5J8kComGiQHZyNmH6VvkHgtFggeQemazLpihKR4QW75DNkWTVdAimported private key for: EOS5fqiC3VFAJ1riMrKf8vzD28nqd4EpXvZGpXt6YewEBnH8DYinG 4.4 创建账户1liyuechun:cleos yuechunli$ ./cleos create account eosio liyc111 EOS5RU8VsYBLnN5snGeUKmt1sDDzpvQbGyW7LPP6qEryaFctYieCK EOS5fqiC3VFAJ1riMrKf8vzD28nqd4EpXvZGpXt6YewEBnH8DYinG 5. 部署合约1liyuechun:build yuechunli$ cleos set contract liyc111 ./contracts/HelloReading WAST/WASM from ./contracts/Hello/Hello.wast...Assembling WASM...Publishing contract...executed transaction: 21d891e425f3d65852432e2b6a78146e2e2992a267c9f28c8ce56cd5dbea98f2 1632 bytes 2200576 cycles# eosio &lt;= eosio::setcode &#123;&quot;account&quot;:&quot;liyc111&quot;,&quot;vmtype&quot;:0,&quot;vmversion&quot;:0,&quot;code&quot;:&quot;0061736d0100000001370b60027f7e0060027e7e006001...# eosio &lt;= eosio::setabi &#123;&quot;account&quot;:&quot;liyc111&quot;,&quot;abi&quot;:&#123;&quot;types&quot;:,&quot;structs&quot;:[&#123;&quot;name&quot;:&quot;hi&quot;,&quot;base&quot;:&quot;&quot;,&quot;fields&quot;:[&#123;&quot;name&quot;:&quot;user&quot;,&quot;t...liyuechun:build yuechunli$ cleos get code liyc111code hash: e387951f9a18870f2c151fbceea5b279a3861bdabab58ea87a67296a8a6583d0liyuechun:build yuechunli$ 6. 执行合约6.1 解锁钱包PW5J3rx7Bfg9zb8Kf2owTytccFyJqtDTrqnUX8iBRRUvbwM8RyzRL是创建钱包是的密码。 1liyuechun:build yuechunli$ cleos wallet unlock --password PW5J3rx7Bfg9zb8Kf2owTytccFyJqtDTrqnUX8iBRRUvbwM8RyzRLUnlocked: default 6.2 执行合约1liyuechun:build yuechunli$ cleos push action liyc111 hi &apos;&#123;&quot;user&quot;:&quot;liyc1215&quot;&#125;&apos; -p liyc111executed transaction: 9abcaec2711ce31c693e5124af507f34aa666702bd5bb230ec31ddd6903248a8 232 bytes 102400 cycles# liyc111 &lt;= liyc111::hi &#123;&quot;user&quot;:&quot;liyc1215&quot;&#125;&gt;&gt; Hello, liyc1215liyuechun:build yuechunli$ 转载来源：EOS DAWN-V3.0.0 智能合约开发之Hello World]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>技术</tag>
        <tag>Java</tag>
        <tag>Bash</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据：美团酒旅实时数据规则引擎应用实践]]></title>
    <url>%2F2018%2Fcedb0633%2F</url>
    <content type="text"><![CDATA[背景 美团点评酒旅运营需求在离线场景下，已经得到了较为系统化的支持，通过对离线数据收集、挖掘，可对目标用户进行T+1触达，通过向目标用户发送Push等多种方式，在一定程度上提高转化率。但T+1本身的延迟性会导致用户在产生特定行为时不能被实时触达，无法充分发挥数据的价值，取得更优的运营效果。 在此背景下，运营业务需要着手挖掘用户行为实时数据，如实时浏览、下单、退款、搜索等，对满足运营需求用户进行实时触达，最大化运营活动效果。 业务场景 在运营实时触达需求中，存在如下具有代表性的业务场景： 用户在30分钟内发生A行为次数大于等于3次- 用户为美团酒店老客，即用户曾购买过美团酒店产品- 用户在A行为前24小时内未发生B行为- 用户在A行为后30分钟内未发生B行为（排除30分钟内用户自发产生B行为的影响，降低对结果造成的偏差） 本文以该典型实时运营场景为例，围绕如何设计出可支撑业务需求高效、稳定运行的系统进行展开。 早期方案 运营实时触达需求早期活动数量较少，我们通过为每个需求开发一套Storm拓扑相关代码、将运营活动规则硬编码这一“短平快”的方式，对运营实时触达需求进行快速支持，如图1所示： 图1 早期方案示意图 早期方案的问题 早期方案是一种Case By Case的解决方式，不能形成一个完整的系统。随着实时运营业务开展，相关运营活动数量激增，线上维护着多套相似代码，一方面破坏了DRY（Don’t Repeat Yourself）原则，另一方面线上维护成本也呈线性增长，系统逐渐无法支撑当前的需求。 挑战 为解决早期方案中出现的问题，对系统建设提出了以下挑战： 硬编码活动规则的方式产生了大量重复代码，开发成本较高，需求响应时间较长。- 业务规则修改困难，调整运营活动条件需要修改代码并重启线上拓扑。- 线上Storm拓扑较多，资源利用率、系统吞吐量低，统一维护成本较高。- 缺乏完善的监控报警机制，很难早于业务发现系统及数据中存在的稳定性问题。业务规则修改困难，调整运营活动条件需要修改代码并重启线上拓扑。 缺乏完善的监控报警机制，很难早于业务发现系统及数据中存在的稳定性问题。 针对以上挑战，结合业务规则特点，美团点评数据智能团队调研并设计了酒旅运营实时触达系统。 技术调研 规则引擎的必要性 提高灵活度需要从业务规则和系统代码解耦和入手，规则和代码耦合直接导致了重复代码增多、业务规则修改困难等问题。那如何将业务规则和系统代码解耦和呢？我们想到使用规则引擎解决这一问题。 规则引擎是处理复杂规则集合的引擎。通过输入一些基础事件，以推演或者归纳等方式，得到最终的执行结果。规则引擎的核心作用在于将复杂、易变的规则从系统中抽离出来，由灵活可变的规则来描述业务需求。由于很多业务场景，包括酒旅运营实时触达场景，规则处理的输入或触发条件是事件，且事件间有依赖或时序的关系，所以规则引擎经常和CEP（复合事件处理）结合起来使用。 CEP通过对多个简单事件进行组合分析、处理，利用事件的相互关系，找出有意义的事件，从而得出结论。文章最前面背景中提到的业务场景，通过多次规则处理，将单一事件组合成具有业务含义的复合事件，进而提高该类仅浏览未下单的用户的下单概率。可以看出，规则引擎及CEP可以满足业务场景的具体需求，将其引入可以提高系统面对需求变化的灵活度。 规则引擎调研 在设计规则引擎前，我们对业界已有的规则引擎，主要包括Esper和Drools，进行了调研。 Esper Esper设计目标为CEP的轻量级解决方案，可以方便的嵌入服务中，提供CEP功能。 优势 轻量级可嵌入开发，常用的CEP功能简单好用。- EPL语法与SQL类似，学习成本较低。EPL语法与SQL类似，学习成本较低。 劣势 单机全内存方案，需要整合其他分布式和存储。- 以内存实现时间窗功能，无法支持较长跨度的时间窗。- 无法有效支持定时触达（如用户在浏览发生后30分钟触达支付条件判断）。以内存实现时间窗功能，无法支持较长跨度的时间窗。 Drools Drools开始于规则引擎，后引入Drools Fusion模块提供CEP的功能。 优势 功能较为完善，具有如系统监控、操作平台等功能。劣势 学习曲线陡峭，其引入的DRL语言较复杂，独立的系统很难进行二次开发。- 以内存实现时间窗功能，无法支持较长跨度的时间窗。- 无法有效支持定时触达（如用户在浏览发生后30分钟触达支付条件判断）。以内存实现时间窗功能，无法支持较长跨度的时间窗。 由于业务规则对时间窗功能及定时触达功能有较强的依赖，综合以上两种规则引擎的优劣势，我们选用了相对SpEL更为轻量的表达式引擎Aviator，将流式数据处理及规则引擎集成至Storm中，由Storm保证系统在数据处理时的吞吐量，在系统处理资源出现瓶颈时，可在公司托管平台上调整Worker及Executor数量，降低系统水平扩展所需成本。 技术方案 确定引入规则引擎后，围绕规则引擎的设计开发成为了系统建设的主要着力点。通过使用实时数据仓库中的用户实时行为数据，按业务运营活动规则，组合成有意义的复合事件，交由下游运营业务系统对事件的主体，也就是用户进行触达。将系统抽象为以下功能模块，如图2所示： 图2 系统模块图 总体来看，系统组成模块及功能如下： 规则引擎：集成于Storm拓扑中，执行运营活动条件转换成为的具体规则，作出对应响应。- 时间窗模块：具有可选时间跨度的滑动时间窗功能，为规则判定提供时间窗因子。- 定时触达模块：设定规则判定的执行时间，达到设定时间后，执行后续规则。- 自定义函数：在Aviator表达式引擎基础函数之上，扩展规则引擎功能。- 报警模块：定时检查系统处理的消息量，出现异常时为负责人发送报警信息。- 规则配置控制台：提供配置页面，通过控制台新增场景及规则配置。- 配置加载模块：定时加载活动规则等配置信息，供规则引擎使用。时间窗模块：具有可选时间跨度的滑动时间窗功能，为规则判定提供时间窗因子。 自定义函数：在Aviator表达式引擎基础函数之上，扩展规则引擎功能。 规则配置控制台：提供配置页面，通过控制台新增场景及规则配置。 其中，规则引擎由核心组件构成的最小功能集及扩展组件提供的扩展功能组成。由于规则引擎解耦了业务规则和系统代码，使得实时数据在处理时变的抽象，对数据监控、报警提出了更高的要求。下面我们将从规则引擎核心组件、规则引擎扩展组件、监控与报警三个方面分别进行介绍。 规则引擎核心组件 规则引擎核心组件为构成规则引擎的最小集合，用以支持完成基础规则判断。 规则引擎核心流程 引入规则引擎后，业务需求被转换为具体场景和规则进行执行，如图3所示： 图3 规则引擎处理流程图 规则引擎在执行规则过程中，涉及以下数据模型： 场景：业务需求的抽象，一个业务需求对应一个场景，一个场景由若干规则组成。用不同的规则组成时序和依赖关系以实现完整的业务需求。- 规则：规则由规则条件及因子组成，由路由至所属场景的事件触发，规则由规则条件、因子及规则响应组成。- 规则条件：规则条件由因子构成，为一个布尔表达式。规则条件的执行结果直接决定是否执行规则响应。- 因子：因子是规则条件的基础组成部分，按不同来源，划分为基础因子、时间窗因子和第三方因子。基础因子来源于事件，时间窗因子来源于时间窗模块获取的时间窗数据，第三方因子来源于第三方服务，如用户画像服务等。- 规则响应：规则执行成功后的动作，如将复合事件下发给运营业务系统，或发送异步事件进行后续规则判断等。- 事件：事件为系统的基础数据单元，划分为同步事件和异步事件两种类型。同步事件按规则路由后，不调用定时触达模块，顺序执行；异步事件调用定时触达模块，延后执行。规则：规则由规则条件及因子组成，由路由至所属场景的事件触发，规则由规则条件、因子及规则响应组成。 因子：因子是规则条件的基础组成部分，按不同来源，划分为基础因子、时间窗因子和第三方因子。基础因子来源于事件，时间窗因子来源于时间窗模块获取的时间窗数据，第三方因子来源于第三方服务，如用户画像服务等。 事件：事件为系统的基础数据单元，划分为同步事件和异步事件两种类型。同步事件按规则路由后，不调用定时触达模块，顺序执行；异步事件调用定时触达模块，延后执行。 时间窗模块 时间窗模块是酒旅运营实时触达系统规则引擎中的重要构成部分，为规则引擎提供时间窗因子。时间窗因子可用于统计时间窗口内浏览行为发生的次数、查询首次下单时间等，表1中列举了在运营实时触达活动中需要支持的时间窗因子类型： |类型|示例|因子构成|——|count|近X分钟浏览POI大于Y次|count(timeWindow(event.id, event.userId, X 60))|distinct count|近X分钟浏览不同POI大于Y次|count(distinct(timeWindow(event.id, event.userId, X 60)))|first|近X天支付的首单酒店|first(timeWindow(event.id, event.userId, X 60))|last|近X天最后一次搜索的酒店|last(timeWindow(event.id, event.userId, X 60)) 表1 时间窗因子类型 根据时间窗因子类型可以看出，时间窗因子有以下特点： 时间窗存储中需要以List形式保存时间窗详情数据，以分别支持聚合及详情需求。1. 时间窗因子需要天粒度持久化，并支持EXPIRE。1. 时间窗因子应用场景多，是许多规则的重要组成因子，服务承受的压力较大，响应时间需要在ms级别。时间窗因子需要天粒度持久化，并支持EXPIRE。 对于以上特点，在评估使用场景和接入数据量级的基础上，我们选择公司基于Tair研发的KV的存储服务Cellar存储时间窗数据，经测试其在20K QPS请求下，TP99能保证在2ms左右，且存储方面性价比较高，可以满足系统需求。 在实际运营活动中，对时间窗内用户某种行为次数的判断往往在5次以内，结合此业务场景，同时为避免Value过大影响读写响应时间，在更新时间窗数据时设置阈值，对超出阈值部分进行截断。时间窗数据更新及截断流程如图4所示： 图4 时间窗数据更新示意图 文章最前面背景中提到的业务场景，在1. 用户在30分钟内发生A行为次数大于等于3次`3. 用户在A行为前24小时内未发生B行为4. 用户在A行为后30分钟内未发生B行为（排除30分钟内用户自发产生B行为的影响，降低对结果造成的偏差）`中，均使用了时间窗模块对滑动时间窗内的用户行为进行了统计，以时间窗因子作为规则执行判断的依据。 规则引擎扩展组件 规则引擎扩展组件在核心组件的基础上，增强规则引擎功能。 自定义函数 自定义函数可以扩充Aviator功能，规则引擎可通过自定义函数执行因子及规则条件，如调用用户画像等第三方服务。系统内为支持运营需求扩展的部分自定义函数如表2所示： |名称|示例|含义|——|equals|equals(message.orderType, 0)|判断订单类型是否为0|filter|filter(browseList, ‘source’, ‘dp’)|过滤点评侧浏览列表数据|poiPortrait|poiPortrait(message.poiId)|根据poiId获取商户画像数据，如商户星级属性|userPortrait|userPortrait(message.userId)|根据userId获取用户画像数据，如用户常住地城市、用户新老客属性|userBlackList|userBlackList(message.userId)|根据userId判断用户是否为黑名单用户 表2 自定义函数示例 文章最前面背景中提到的业务场景，在2. 用户为美团酒店老客，即用户曾购买过美团酒店产品中，判断用户是否为美团酒店老客，就用到了自定义函数，调用用户画像服务，通过用户画像标签进行判定。 定时触达模块 定时触达模块支持为规则设定定时执行时间，延后某些规则的执行以满足运营活动规则。文章最前面背景中提到的业务场景，在4. 用户在A行为后30分钟内未发生B行为（排除30分钟内用户自发产生B行为的影响，降低对结果造成的偏差）条件中，需要在A行为发生30分钟后，对用户是否发生B行为进行判定，以排除用户自发产生B行为对活动效果造成的影响。 定时触达模块涉及的数据流图如图5所示： 图5 定时触达模块数据流图 早期的业务需求对延迟时间要求较短，且活动总数量较小，通过维护纯内存DelayQueue的方式，支持定时触达需求。随着相关运营活动数量增多及定时触达时间的延长，纯内存方式对内存的占用量越来越大，且在系统重启后定时数据会全部丢失。在对解决方案进行优化时，了解到公司消息中间件组在Mafka消息队列中支持消息粒度延迟，非常贴合我们的使用场景，因此采用此特性，代替纯内存方式，实现定时触达模块。 监控与报警 对比离线数据，实时数据在使用过程中出现问题不易感知。由于系统针对的运营活动直接面向C端，在出现系统异常或数据质量异常时，如果没有及时发现，将会直接造成运营成本浪费，严重影响活动转化率等活动效果评估指标。针对系统稳定性问题，我们从监控与报警两个角度入手解决。 监控 利用公司数据平台现有产品，对系统处理的实时事件按其事件ID上报，以时间粒度聚合，数据上报后可实时查看各类事件量，通过消息量评估活动规则和活动效果是否正常，上报数据展示效果如图6所示： 图6 实时事件监控图 报警 监控只能作为Dashboard供展示及查看，无法实现自动化报警。由于用于监控所上报的聚合数据存储于时序数据库OpenTSDB中，我们基于OpenTSDB开放的HTTP API，定制报警模块，定时调度、拉取数据，对不同事件，按事件量级、活动重要性等指标，应用环比、绝对值等不同报警规则及阈值。超出设定阈值后，通过公司IM及时发送报警信息。如图7所示，该事件环比出现数据量级下降，收到报警后相关负责人可及时跟踪问题： 图7 报警信息示意图 总结与展望 酒旅运营实时触达系统已上线稳定运行一年多时间，是运营业务中十分重要的环节，起到承上启下的作用，在系统处理能力及对业务贡献方面取得了较好的效果： 平均日处理实时消息量近10亿。- 峰值事件QPS 1.4万。- 帮助酒店、旅游、大交通等业务线开展了丰富的运营活动。- 对转化率、GMV、拉新等指标促进显著。峰值事件QPS 1.4万。 对转化率、GMV、拉新等指标促进显著。 当前系统虽然已解决了业务需求，但仍存在一些实际痛点： 实时数据接入非自动化。- 规则引擎能力需要推广、泛化。- 场景及规则注册未对运营PM开放，只能由RD完成。规则引擎能力需要推广、泛化。 展望未来，在解决痛点方面我们还有很多路要走，未来会继续从技术及业务两方面入手，将系统建设的更加易用、高效。 作者简介 晓星，美团平台技术部－数据中心－数据智能组系统工程师，2014年毕业于北京理工大学，从事Java后台系统及数据服务建设。2017年加入美团点评，从事大数据处理相关工作。 伟彬，美团平台技术部－数据中心－数据智能组系统工程师，2015年毕业于大连理工大学，同年加入美团点评，专注于大数据处理技术与高并发服务。 招聘信息 美团平台技术部－数据中心－数据智能组长期招聘数据挖掘算法、大数据系统开发、Java后台开发方面的人才，有兴趣的同学可以发送简历到lishangqiang#meituan.com。 转载来源：大数据：美团酒旅实时数据规则引擎应用实践]]></content>
      <categories>
        <category>科技</category>
      </categories>
      <tags>
        <tag>移动互联网</tag>
        <tag>大数据</tag>
        <tag>美团网</tag>
        <tag>数据挖掘</tag>
        <tag>Storm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[算法与产品：抖音、快手的“气质”成因]]></title>
    <url>%2F2018%2F03a04b30%2F</url>
    <content type="text"><![CDATA[同一类型的两款产品，从功能上看，似乎没有明显差别，但为什么给人的“感觉”却是完全不同呢？ 算法快抖的视频内容分为推荐（发现）、附近（同城）和关注三个模块，这里主要说明推荐模块的算法机制。 视频与用户画像的匹配程度1. 热度（赞、评论、转发等）1. 发布时间根据用户数据和内容标签计算两者的匹配程度，是每个内容产品的算法核心，已经被总结很多次了，其理论大体一致，在本社区搜索“算法”关键词就能找到，这里就不再赘述了，下面主要介绍热度和发布时间两点。 打开你的抖音，你会看到系统已经为你推荐好了一系列内容，再仔细观察一下，你会发现这些视频的获赞数量基本都在50万以上，中位数大概在100万（刷多了会减少）。 而打开快手呢？ 你会发现视频获赞数量基本维持在1万到10万这个区间，有的甚至会出现几千几百，但很难找到过10万的视频。 出现这种差距，难道是因为快手用户少吗？ 显然不是，快手的用户已经是其他短视频产品的用户之和了。之所以出现这种状况，其实是因为快手算法特有的“热度权重“。 视频发布初期，随着其热度提高，曝光机会也会跟着提高，此时，“热度权重”起到“择优去劣”的作用。而在视频热度达到一定阈值后，它的曝光机会将不断降低，此时，“热度权重”起到“择新去旧”的作用（其实是为了给用户平等的展示机会，后面会讲到）。 与此同时，快抖对于“发布时间”的看法也是不一样的。 在抖音，你会发现很多视频其实几个月前就发布了（验证这一点，只需要在评论区不断下翻即可，可根据早期评论的发布时间进行推断）。因为用户一般不会在乎短视频是不是最新的，只要足够精彩即可。 而在快手，大部分视频都是近期发布的，再远的视频也是一个月内（在视频界面右下方有时间标注）。 那么，前面提到的“热度权重”和“发布时间权重”对于用户来讲又会有怎样的影响呢？ 首先，短视频的用户大体分两种：一种是“看视频”的看官，一种是“拍视频”的制作方。这里，我们把看官的注意力比作一个蛋糕，而制作方比作分蛋糕的人。 首先，我们来看分蛋糕的人。在抖音，分到大量蛋糕的用户还会继续加快分蛋糕的速度（高热度会不断提高曝光机会），头部用户集中了大量的用户注意力资源，这种中心化会让普通制作者、草根制作者难以被关注（这与微博颇为类似）。 而在快手，头部用户分到的蛋糕被设了上限（高热度和旧视频曝光机会会大大降低），因此会有更多的人分到蛋糕，这也体现了快手的理念——希望所有用户都能展示自我，任何一位普通用户都有被关注的权利。 对于看官来说，抖音给了他们大量的优质资源，这些都是经过大量用户检验，而放到推荐模块的内容池的视频。而快手只是经过初步检验就根据用户喜好开始推荐了，所以会有很多小众和“乡土”的内容。 抖音和快手，一个是精致的台上表演，一个是平凡的街边才艺。 相对来说，抖音是看官导向的，而快手则更偏向于制作方，尤其是草根用户。这也就不难理解为什么快手会沦落到被整改的尴尬境地，因为“三俗”生产者总能找到自己的市场。 产品除了算法，我认为以下两点也是快抖气质差异的诱因。 录制功能1. 交互设计 1. 先音乐后录制的妙处与其他短视频不同，抖音的录制功能别具一格，先选音乐再根据音乐录视频，而不只是充当背景音乐。 每当视频的动感与音频的调子相重合时，会大大刺激观众的感官，带来不一样的体验。同时，也产生了更多玩法，比如：对口型、拍同款等，增加了趣味性、可看性。因此，抖音会给人一种“酷炫”的感觉（但是拍摄门槛也抬高了）。 2. 不断上滑的“沉浸式体验”一打开抖音，便进入了播放界面，接着依靠上下滑动来更换视频，嗯 … 这种状态可以维持一个多小时 … 这种懒人式交互大大提升了用户粘性，不过也削弱了用户改变“状态”的意愿，即附近模块、关注模块的使用几率将会降低。由此，用户的注意力又被“粘”在了头部用户的优质内容上，中心化进一步加剧。 与之相反的，快手的推荐（发现）对用户并没有那么大的粘性，三个模块的交互方式相当，都是瀑布流布局，并且快手的启动页是用户上一次退出的页面。 比如：上一次在同城离开，下一次启动页会是同城模块，关注模块也是如此，这样，用户选择的自由度“无形”地增加了。同时，快手也因其同城和关注的高使用频率，而在社交属性上更胜一筹，而不仅仅是一个娱乐软件。 其实快抖的算法与交互设计是相辅相成的，抖音的算法决定了它的视频更加优质，因此不需要用户做太多的选择，适合逐个播放，也减少了用户操作负担和选择负担。而快手视频的优质密度没有那么大，需要用户选择播放，在操作上会繁琐一些。 回忆一下我们使用抖音的时候，是不是一般会看完整个视频再播放下一个，很少掠过。而使用快手的时候，我们通常要掠过几个，才能选出自己想看的视频。 因此，“滚动播放”更适合抖音，而瀑布流适用于快手（其实快手也可以尝试美拍的“瀑布流+滚动播放相关视频”的交互设计，或者抖音附近模块的“瀑布流+滚动播放下一个视频”的交互设计）。 结尾除了算法和产品设计，还有着其他因素导致快抖的风格差异，比如：运营和品牌公关。 不过从效果上看，算法和产品设计更像是产品的“基因”，从最开始就影响着“两个宝宝”的未来走向（抖音的精致范和快手的平民化）。 如何设计出我们想要的产品，让“宝宝”长成我们想要的样子，抖音和快手的实例值得我们借鉴。 本文由 @ 信管专业学生 原创发布于人人都是产品经理。未经许可，禁止转载 题图来自网络 转载来源：算法与产品：抖音、快手的“气质”成因]]></content>
      <categories>
        <category>其他</category>
      </categories>
      <tags>
        <tag>音乐</tag>
        <tag>软件</tag>
        <tag>蛋糕</tag>
        <tag>甜品</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[算法与产品：抖音、快手的“气质”成因]]></title>
    <url>%2F2018%2F974c8151%2F</url>
    <content type="text"><![CDATA[同一类型的两款产品，从功能上看，似乎没有明显差别，但为什么给人的“感觉”却是完全不同呢？ 算法快抖的视频内容分为推荐（发现）、附近（同城）和关注三个模块，这里主要说明推荐模块的算法机制。 视频与用户画像的匹配程度1. 热度（赞、评论、转发等）1. 发布时间根据用户数据和内容标签计算两者的匹配程度，是每个内容产品的算法核心，已经被总结很多次了，其理论大体一致，在本社区搜索“算法”关键词就能找到，这里就不再赘述了，下面主要介绍热度和发布时间两点。 打开你的抖音，你会看到系统已经为你推荐好了一系列内容，再仔细观察一下，你会发现这些视频的获赞数量基本都在50万以上，中位数大概在100万（刷多了会减少）。 而打开快手呢？ 你会发现视频获赞数量基本维持在1万到10万这个区间，有的甚至会出现几千几百，但很难找到过10万的视频。 出现这种差距，难道是因为快手用户少吗？ 显然不是，快手的用户已经是其他短视频产品的用户之和了。之所以出现这种状况，其实是因为快手算法特有的“热度权重“。 视频发布初期，随着其热度提高，曝光机会也会跟着提高，此时，“热度权重”起到“择优去劣”的作用。而在视频热度达到一定阈值后，它的曝光机会将不断降低，此时，“热度权重”起到“择新去旧”的作用（其实是为了给用户平等的展示机会，后面会讲到）。 与此同时，快抖对于“发布时间”的看法也是不一样的。 在抖音，你会发现很多视频其实几个月前就发布了（验证这一点，只需要在评论区不断下翻即可，可根据早期评论的发布时间进行推断）。因为用户一般不会在乎短视频是不是最新的，只要足够精彩即可。 而在快手，大部分视频都是近期发布的，再远的视频也是一个月内（在视频界面右下方有时间标注）。 那么，前面提到的“热度权重”和“发布时间权重”对于用户来讲又会有怎样的影响呢？ 首先，短视频的用户大体分两种：一种是“看视频”的看官，一种是“拍视频”的制作方。这里，我们把看官的注意力比作一个蛋糕，而制作方比作分蛋糕的人。 首先，我们来看分蛋糕的人。在抖音，分到大量蛋糕的用户还会继续加快分蛋糕的速度（高热度会不断提高曝光机会），头部用户集中了大量的用户注意力资源，这种中心化会让普通制作者、草根制作者难以被关注（这与微博颇为类似）。 而在快手，头部用户分到的蛋糕被设了上限（高热度和旧视频曝光机会会大大降低），因此会有更多的人分到蛋糕，这也体现了快手的理念——希望所有用户都能展示自我，任何一位普通用户都有被关注的权利。 对于看官来说，抖音给了他们大量的优质资源，这些都是经过大量用户检验，而放到推荐模块的内容池的视频。而快手只是经过初步检验就根据用户喜好开始推荐了，所以会有很多小众和“乡土”的内容。 抖音和快手，一个是精致的台上表演，一个是平凡的街边才艺。 相对来说，抖音是看官导向的，而快手则更偏向于制作方，尤其是草根用户。这也就不难理解为什么快手会沦落到被整改的尴尬境地，因为“三俗”生产者总能找到自己的市场。 产品除了算法，我认为以下两点也是快抖气质差异的诱因。 录制功能1. 交互设计 1. 先音乐后录制的妙处与其他短视频不同，抖音的录制功能别具一格，先选音乐再根据音乐录视频，而不只是充当背景音乐。 每当视频的动感与音频的调子相重合时，会大大刺激观众的感官，带来不一样的体验。同时，也产生了更多玩法，比如：对口型、拍同款等，增加了趣味性、可看性。因此，抖音会给人一种“酷炫”的感觉（但是拍摄门槛也抬高了）。 2. 不断上滑的“沉浸式体验”一打开抖音，便进入了播放界面，接着依靠上下滑动来更换视频，嗯 … 这种状态可以维持一个多小时 … 这种懒人式交互大大提升了用户粘性，不过也削弱了用户改变“状态”的意愿，即附近模块、关注模块的使用几率将会降低。由此，用户的注意力又被“粘”在了头部用户的优质内容上，中心化进一步加剧。 与之相反的，快手的推荐（发现）对用户并没有那么大的粘性，三个模块的交互方式相当，都是瀑布流布局，并且快手的启动页是用户上一次退出的页面。 比如：上一次在同城离开，下一次启动页会是同城模块，关注模块也是如此，这样，用户选择的自由度“无形”地增加了。同时，快手也因其同城和关注的高使用频率，而在社交属性上更胜一筹，而不仅仅是一个娱乐软件。 其实快抖的算法与交互设计是相辅相成的，抖音的算法决定了它的视频更加优质，因此不需要用户做太多的选择，适合逐个播放，也减少了用户操作负担和选择负担。而快手视频的优质密度没有那么大，需要用户选择播放，在操作上会繁琐一些。 回忆一下我们使用抖音的时候，是不是一般会看完整个视频再播放下一个，很少掠过。而使用快手的时候，我们通常要掠过几个，才能选出自己想看的视频。 因此，“滚动播放”更适合抖音，而瀑布流适用于快手（其实快手也可以尝试美拍的“瀑布流+滚动播放相关视频”的交互设计，或者抖音附近模块的“瀑布流+滚动播放下一个视频”的交互设计）。 结尾除了算法和产品设计，还有着其他因素导致快抖的风格差异，比如：运营和品牌公关。 不过从效果上看，算法和产品设计更像是产品的“基因”，从最开始就影响着“两个宝宝”的未来走向（抖音的精致范和快手的平民化）。 如何设计出我们想要的产品，让“宝宝”长成我们想要的样子，抖音和快手的实例值得我们借鉴。 本文由 &#64; 信管专业学生 原创发布于人人都是产品经理。未经许可，禁止转载 题图来自网络 转载来源：算法与产品：抖音、快手的“气质”成因]]></content>
      <categories>
        <category>其他</category>
      </categories>
      <tags>
        <tag>音乐</tag>
        <tag>软件</tag>
        <tag>蛋糕</tag>
        <tag>甜品</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一段关于国产芯片和操作系统的往事]]></title>
    <url>%2F2018%2Fedd542d4%2F</url>
    <content type="text"><![CDATA[一段关于国产芯片和操作系统的往事 转载来源：一段关于国产芯片和操作系统的往事]]></content>
      <tags>
        <tag>梁宁-闲花照水录</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[阿里巴巴参谋长曾鸣全面深入阐释：何谓互联网的本质？]]></title>
    <url>%2F2018%2Fe56af9ec%2F</url>
    <content type="text"><![CDATA[阿里巴巴参谋长曾鸣全面深入阐释：何谓互联网的本质？ 转载来源：阿里巴巴参谋长曾鸣全面深入阐释：何谓互联网的本质？]]></content>
      <tags>
        <tag>小象</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[美国再爆惊天大丑闻：窃取5000万Facebook用户数据，为了操控人心]]></title>
    <url>%2F2018%2F2e56afc8%2F</url>
    <content type="text"><![CDATA[美国再爆惊天大丑闻：窃取5000万Facebook用户数据，为了操控人心 转载来源：美国再爆惊天大丑闻：窃取5000万Facebook用户数据，为了操控人心]]></content>
      <tags>
        <tag>假装在纽约</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Youtube爆火视频 | 用TensorFlow+40行代码识别手写数字图像]]></title>
    <url>%2F2018%2F9e6156c3%2F</url>
    <content type="text"><![CDATA[Youtube爆火视频 | 用TensorFlow+40行代码识别手写数字图像 转载来源：Youtube爆火视频 | 用TensorFlow+40行代码识别手写数字图像]]></content>
      <tags>
        <tag>大数据文摘</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据科学家需要了解的5种聚类算法]]></title>
    <url>%2F2018%2F108a61a6%2F</url>
    <content type="text"><![CDATA[编者按：聚类是一种涉及数据点分组的机器学习技术。给定一组数据点，我们可以使用聚类算法将每个数据点到分类到图像中的特定组中。理论上，同一组中的数据点应具有相似的属性和特征，而不同组中的数据点的属性和特征则应高度不同。聚类是无监督学习的一种方法，是用于多领域统计数据分析的常用技术。 在数据科学中，我们可以通过聚类分析观察使用聚类算法后这些数据点分别落入了哪个组，并从中获得一些有价值的信息。那么今天，我们就跟着机器学习工程师George Seif来看看数据科学家需要掌握的5种实用聚类算法以及它们的优缺点。 K-Means聚类K-Means（k-平均或k-均值）可以称的上是知名度最高的一种聚类算法，它常出现在许多有关数据科学和机器学习的课程中。在代码中非常容易理解和实现！让我们来看下面这幅动图。 K-Means聚类 首先，我们确定要几个的聚类（cluster，也称簇），并为它们随机初始化一个各自的聚类质心点（cluster centroids），它在上图中被表示为“X”。要确定聚类的数量，我们可以先快速看一看已有的数据点，并从中分辨出一些独特的数据。1. 其次，我们计算每个数据点到质心的距离来进行分类，它跟哪个聚类的质心更近，它就被分类到该聚类。1. 需要注意的是，初始质心并不是真正的质心，质心应满足聚类里每个点到它的欧式距离平方和最小这个条件。因此根据这些被初步分类完毕的数据点，我们再重新计算每一聚类中所有向量的平均值，并确定出新的质心。1. 最后，重复上述步骤，进行一定次数的迭代，直到质心的位置不再发生太大变化。当然你也可以在第一步时多初始化几次，然后选取一个看起来更合理的点节约时间。其次，我们计算每个数据点到质心的距离来进行分类，它跟哪个聚类的质心更近，它就被分类到该聚类。 最后，重复上述步骤，进行一定次数的迭代，直到质心的位置不再发生太大变化。当然你也可以在第一步时多初始化几次，然后选取一个看起来更合理的点节约时间。 K-Means的优点是速度非常快，因为我们所做的只是计算数据点和质心点之间的距离，涉及到的计算量非常少！因此它的算法时间复杂度只有O(n)。 另一方面，K-Means有两个缺点。一是你必须一开始就决定数据集中包含多少个聚类。这个缺点并不总是微不足道的，理想情况下，我们的目标其实是用一种算法来分类这些数据，并从结果中观察出一些规律，而不是限制几个条件强行聚类。二是一开始质心点的选取是随机的，算法可能会初始化出差异巨大的点。这个缺点导致的结果是质心点的位置不可重复且缺乏一致性。 K-Medians是与K-Means相关的另一种聚类算法，不同之处在于它使用簇的中值向量来重新计算质心点。该方法对异常值不敏感（因为使用中值），但在较大数据集上运行时速度会慢很多，因为每次计算中值向量，我们都要重新排序。 Mean-Shift聚类Mean shift算法，又称均值漂移算法，这是一种基于核密度估计的爬山算法，可用于聚类、图像分割、跟踪等。它的工作原理基于质心，这意味着它的目标是定位每个簇/类的质心，即先算出当前点的偏移均值，将该点移动到此偏移均值，然后以此为新的起始点，继续移动，直到满足最终的条件（找出最密集的区域）。如果没理解，请看下图。 Mean-Shift聚类 为了理解均值漂移，我们可以像上图一样想象二维空间中的一组数据点，然后先随机选择一个点C，以它为圆心画一个半径为r的圆开始移动。之前提到了，这是个爬山算法，它的核函数会随着迭代次数增加逐渐向高密度区域靠近。1. 在每轮迭代中，算法会不断计算圆心到质心的偏移均值，然后整体向质心靠近。漂移圆圈内的密度与数据点数成正比。到达质心后，算法会更新质心位置，并继续让圆圈向更高密度的区域靠近。1. 当圆圈到达目标质心后，它发现自己无论朝哪个方向漂移都找不到更多的数据点，这时我们就认为它已经处于最密集的区域。1. 这时，算法满足了最终的条件，即退出。在每轮迭代中，算法会不断计算圆心到质心的偏移均值，然后整体向质心靠近。漂移圆圈内的密度与数据点数成正比。到达质心后，算法会更新质心位置，并继续让圆圈向更高密度的区域靠近。 这时，算法满足了最终的条件，即退出。 这里我们主要介绍了一个漂移圆圈的思路，如下图所示，其实Mean shift算法事实上存在多个圆形区域，图中黑点代表质心，而灰点则是数据点。 和K-Means算法相比，Mean-Shift不需要实现定义聚类数量，因为这些都可以在计算偏移均值时得出。这是一个巨大的优势。同时，算法推动聚类中心在向密度最大区域靠近的效果也非常令人满意，这一过程符合数据驱动型任务的需要，而且十分自然直观。如果要说Mean-Shift有什么缺点，那就是对高维球区域的半径r的定义，不同选择可能会产生高度不同的影响。 具有噪声的基于密度的聚类方法（DBSCAN）DBSCAN是一种基于密度的聚类算法，它与mean-shift类似，但又有一些显著优势。我们先来看看下面这幅花哨的图。 DBSCAN笑脸聚类 首先，DBSCAN算法会以任何尚未访问过的任意起始数据点为核心点，并对该核心点进行扩充。这时我们给定一个半径/距离ε，任何和核心点的距离小于ε的点都是它的相邻点。1. 如果核心点附近有足够数量的点，则开始聚类，且选中的核心点会成为该聚类的第一个点。如果附近的点不够，那算法会把它标记为噪声（之后这个噪声可能会成为簇中的一部分）。在这两种情形下，选中的点都会被标记为“已访问”。1. 一旦聚类开始，核心点的相邻点，或者说以该点出发的所有密度相连的数据点（注意是密度相连）会被划分进同一聚类。然后我们再把这些新点作为核心点，向周围拓展ε，并把符合条件的点继续纳入这个聚类中。1. 重复步骤2和3，直到附近没有可以扩充的数据点为止，即簇的ε邻域内所有点都已被标记为“已访问”。1. 一旦我们完成了这个集群，算法又会开始检索未访问过的点，并发现更多的聚类和噪声。一旦数据检索完毕，每个点都被标记为属于一个聚类或是噪声。如果核心点附近有足够数量的点，则开始聚类，且选中的核心点会成为该聚类的第一个点。如果附近的点不够，那算法会把它标记为噪声（之后这个噪声可能会成为簇中的一部分）。在这两种情形下，选中的点都会被标记为“已访问”。 重复步骤2和3，直到附近没有可以扩充的数据点为止，即簇的ε邻域内所有点都已被标记为“已访问”。 与其他聚类算法相比，DBSCAN有一些很大的优势。首先，它不需要输入要划分的聚类个数。其次，不像mean-shift，即使数据点非常不同，它也会将它们纳入聚类中，DBSCAN能将异常值识别为噪声，这就意味着它可以在需要时输入过滤噪声的参数。第三，它对聚类的形状没有偏倚，可以找到任意大小和形状的簇。 DBSCAN的主要缺点是，当聚类的密度不同时，DBSCAN的性能会不如其他算法。这是因为当密度变化时，用于识别邻近点的距离阈值ε和核心点的设置会随着聚类发生变化。而这在高维数据中会特别明显，因为届时我们会很难估计ε。 EM聚类 均值→质心，方差→椭圆聚类，权重→聚类大小。 K-Means算法的主要缺点之一是它直接用了距离质心的平均值。我们可以从下图中看出这样做为什么不好——图的左侧是两个半径不同的同心圆，K-Means没法处理这个问题，因为这些聚类的平均值非常接近；图的右侧是一个中心对称的非圆形分布，K-Means同样解决不了它，因为如果单纯依靠均值判断，算法无法捕捉更多特征。 K-Means的两个失败案例 高斯混合模型（GMM）比K-Means算法具有更好的灵活性。它是多个高斯分布函数的线性组合，理论上可以拟合出任意类型的分布，通常用于解决同一集合下的数据包含多个不同的分布的情况。对于GMM，我们假设数据点满足不同参数下的高斯分布——比起均值，这是一个限制较少的假设。我们用两个参数来描述聚类的形状：均值和标准差！以二维分布为例，标准差的存在允许聚类的形状可以是任何种类的椭圆形。因此这个算法的思想是：如果数据点符合某个高斯分布，那它就会被归类为那个聚类。 为了找到每个聚类的高斯参数，我们要用到一种名为期望最大化（EM）的优化算法。下图是高斯混合模型的聚类过程，那么，你知道怎么在其中运用EM算法吗？ 首先，我们确定聚类的数量（如K-Means），并随机初始化每个聚类的高斯分布参数。你也可以尝试通过快速查看数据来为初始参数提供更好的猜测，但从上图可以看出，这其实不是很必要，因为算法会很快进行优化。1. 其次，根据每个聚类的高斯分布，计算数据点属于特定聚类的概率。如果数据点越接近高斯质心，那它属于该聚类的概率就越高。这很直观，因为对于高斯分布，我们一般假设大部分数据更靠近聚类质心。1. 在这些概率的基础上，我们为高斯分布计算一组新的参数，使聚类内数据点的概率最大化。我们用数据点位置的加权和来计算这些新参数，其中权重就是数据点属于聚类的概率。为了可视化这个过程，我们可以看看上面的图片，特别是黄色的聚类。第一次迭代中，它是随机的，大多数黄点都集中在该聚类的右侧。当我们按概率计算加权和后，虽然聚类的中部出现一些点，但右侧的比重依然很高。随着迭代次数增加，黄点在聚类中的位置也完成了“右下→左下”的移动。因此，标准差的变化调整着聚类的形状，以使它能更适合数据点的分布。1. 迭代步骤2和步骤3，直至收敛。其次，根据每个聚类的高斯分布，计算数据点属于特定聚类的概率。如果数据点越接近高斯质心，那它属于该聚类的概率就越高。这很直观，因为对于高斯分布，我们一般假设大部分数据更靠近聚类质心。 迭代步骤2和步骤3，直至收敛。 论智注：对于上述第3步，请结合混合高斯模型定义公式理解。如果我们设K为模型的个数，πk为第k个高斯的权重，即第k个高斯的概率密度函数，其均值为μk，方差为σk。我们对此概率密度的估计就是要求πk、μk和σk各个变量。当求出的表达式后，求和式的各项的结果就分别代表样本x属于各个类的概率。在做参数估计的时候，常采用的方法是最大似然。——引自 姜文晖《聚类(1)——混合高斯模型》 GMM有两个关键优势。首先它比K-Means更灵活，由于标准差的引入，最后聚类的形状不再局限于圆形，它还可以是大小形状不一的椭圆形——K均值实际上是GMM的一个特例，其中每个聚类的协方差在所有维上都接近0。其次，权重的引入为同一点属于多个聚类找到了解决方案。如果一个数据点位于两个聚类的重叠区域，那我们就可以简单为它定义一个聚类，或者计算它属于X聚类的百分比是多少，属于Y聚类的百分比是多少。简而言之，GMM支持混合“成员”。 谈及缺点，和K-Means相比，GMM每一步迭代的计算量比较大。另外，它的求解办法基于EM算法，因此有可能陷入局部极值，需要经过多次迭代。 层次聚类层次聚类实际上可以被分为两类：自上而下和自下而上。其中自下而上算法（Bottom-up algorithms）首先会将每个数据点视为单个聚类，然后连续合并（或聚合）成对的聚类，直到所有聚类合并成包含所有数据点的单个聚类。它也因此会被称为hierarchical agglomerative clustering 或HAC。该算法的聚类可以被表示为一幅树状图，树根是最后收纳所有数据点的单个聚类，而树叶则是只包含一个样本的聚类。在讲解具体步骤前，我们先看看整个过程的图解。 层次聚类 首先，我们把每个数据点看作是一个聚类，即如果数据集中有X个数据点，它就有X个聚类。然后我们设置一个阈值作为衡量两个聚类间距离的指标：如果距离小于阈值，则合并；如果距离大于阈值，迭代终止。1. 在每轮迭代中，我们会把两个聚类合并成一个聚类。这里我们可以用到average linkage，它的思路是计算所有分属于两个目标聚类的数据点之间距离，然后求一个平均值。每次我们会根据设定的阈值选取平均距离最小的两个聚类，然后把它们合并起来，因为按照我们的标准，它们是最相似的。1. 重复步骤2，直到我们到达树根，即最后只有一个包含所有数据点的聚类。通过这种方式，我们可以选择要几个聚类，以及什么时候停止聚类。在每轮迭代中，我们会把两个聚类合并成一个聚类。这里我们可以用到average linkage，它的思路是计算所有分属于两个目标聚类的数据点之间距离，然后求一个平均值。每次我们会根据设定的阈值选取平均距离最小的两个聚类，然后把它们合并起来，因为按照我们的标准，它们是最相似的。 层次聚类不要求我们指定聚类的数量，由于这是一个构建树的过程，我们甚至可以选择那种聚类看起来更合适。另外，该算法对距离阈值的选择不敏感，无论怎么定，算法始终会倾向于给出更好地聚类结果，而不像其他算法很依赖参数。根据层次聚类的特点，我们可以看出它非常适合具有层次结构的数据，尤其是当你的目标是为数据恢复层次时。这一点是其他算法无法做到的。与K-Means和GMM的线性复杂性不同，层次聚类的优势是以较低的效率为代价，因为它具有O(n3)的时间复杂度。 结论以上就是数据科学家需要知道的5个聚类方法。在文章的最后，就让我们以一幅聚类图做结，直观展示这些算法和其他算法的完美表现！ 转载来源：数据科学家需要了解的5种聚类算法]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>卡尔·高斯</tag>
        <tag>技术</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[这十部美剧，代表了美剧在2016年的最高水准]]></title>
    <url>%2F2018%2Fe9c0a8b2%2F</url>
    <content type="text"><![CDATA[这十部美剧，代表了美剧在2016年的最高水准 转载来源：这十部美剧，代表了美剧在2016年的最高水准]]></content>
  </entry>
  <entry>
    <title><![CDATA[专访｜基于LSTM与TensorFlow Lite，kika输入法是如何造就的]]></title>
    <url>%2F2018%2F08688a4a%2F</url>
    <content type="text"><![CDATA[近日，机器之心采访了 kika 的高级技术总监黄康，他向我们讲述了 kika 开发输入法 AI 引擎（项目代号：Alps）所采用的深度学习模型以及在移动端轻量化部署遇到的各种挑战。本文从输入法与语言模型开始介绍了 kika Alps 项目的理论支持与实践挑战，并重点讨论了轻量化部署方法。 深度学习模型由于强大的表征能力在很多任务上都有非常优秀的表现，但也因为模型大小和计算量很难轻量化部署到移动端。这也是目前很多研发团队都在思考如何解决的难题。 一般在我们借助 TensorFlow、MXNet、和 Caffe2 等框架构建深度学习模型后，它在服务器训练与推断往往会有非常好的效果。但如果我们将模型部署到移动端，即使只执行推断过程，也会因为硬件条件和系统环境而遇到各种各样的问题。此外，目前关注于移动端的解决方案如 TensorFlow Mobile、TensorFlow Lite 等在一定程度上并不完善（TF Mobile 的内存管理与 TF Lite 的 Operators 的缺失），在实践中可能需要更多的修正与完善。 关注于输入法的 kika 成功地将基于循环神经网络的深度学习模型应用到安卓版的手机输入法引擎中，在克服工程化问题的情况下大大提升了输入体验：不仅使基于上下文的词预测更加准确，同时还使得词纠错功能更加强大。 在构建这样的输入法引擎过程中，kika 不仅需要考虑使用 LSTM 还是 GRU 来实现高效的语言模型，同时还需要探索如何使整个方案更轻量化以及如何快速的进行部署。本文首先介绍了输入法及 kika 所采用的语言模型，并在随后展示了 Android 移动端轻量化部署所遇到的工程化挑战。最后，本文介绍了 kika 压缩模型所采用的稀疏词表征方法与 K-means 参数量化方法，它们是轻量化部署深度学习模型的重要前提。 输入法与语言模型 输入法最重要的部分就是输入法引擎，kika 很多算法和项目都围绕它展开。一般而言，输入法引擎的输入包含两部分，即已经键入的词组和当前正在输入的词汇，前者可以视为上下文，而未完成的后者可称为键码。输入法引擎的输出是给定所有上下文和当前输入键码所『预测』的词，它也包含两部分，即当前输入词汇的补全和纠错。实现这样的功能也就是输入法最为核心的模块，kika 最开始是使用谷歌半开源的 LatinIME 来实现这样的功能，但这种基于 n-gram 的方法并不能实现顶尖的用户体验，因此经过研究与开发才有了现在基于循环神经网络（RNN）的解决方案。 输入法引擎这种给定上下文和当前键码以预测下一个词的方法其实可以看成语言建模，一般来说，语言模型旨在定义自然语言中「标记」的概率分布，这种标记可以是单词、字符甚至是字节。根据 kika 介绍，LatinIME 构建语言模型的方法是 n-gram，这种模型定义了一个条件概率分布，即给定前 n-1 个单词后第 n 个词的条件概率。因为假定当前词出现的概率只与前面 n-1 个词相关，那么 n-gram 可以使用这种条件概率的乘积来定义较长序列的概率分布： 虽然 n-gram 一直以来都是统计语言模型的核心模块，但它还是有很多局限性。对于输入法而言，较小的 n（二元语法与三元语法）不足以捕捉整个上下文信息来执行预测，而增大 n 又会使计算量成指数级增加。此外，kika 还希望引擎实现其它一些智能功能，例如根据上下文自动纠错或排序等。因此，kika 选择了更强大的循环神经网络构建语言模型。 为了构建强大的语言模型，kika 选择了长短期记忆单元（LSTM）作为网络的基础。LSTM 作为标准循环神经网络的变体在语言模型上有非常好的性能，它引入自循环的巧妙构想来更新「记忆」，若通过门控控制这样的自循环，那么累积的历史记忆或时间尺度就可以动态地改变。直观来说，LSTM 会通过门控选择需要保留的上下文信息或记忆，并用于预测当前输入的词。每当输入一个词，输入门会控制当前输入对最终预测有用的信息，而遗忘门会控制前面输入词对最终预测有用的信息，最后的输出门则会直接控制前两部分最终有效的信息。 kika 表明最开始 LSTM 只是用来实现标准的语言模型，它不会将正在输入的键码作为模型输入。这一套早期方案使用 LSTM 语言模型根据上下文预测当前词，而键码部分仍然使用传统基于 n-gram 和字典等的解决方案。后来 kika 使用一种新的策略将两部分结合在一起，因此模型不仅能接受上下文的输入，同时还能接受键码的输入。这种通用解决方案的架构如下图所示，它和一般的单词级语言模型和字符级语言模型都不一样，可以说是结合了两种架构。 如上图所示，首先 LSTM 会对前面输入的词进行建模，并输出对应的隐藏状态和记忆而作为后面字符级语言模型的先验输入。后面从 Start Flag 开始对键码实现字符级的建模而最终得出预测。 根据 kika 的解释，最后这种方案统一了两种输入。它的基本思想首先考虑到前面的 LSTM 语言模型除了要根据隐藏状态预测当前时间步的输出，同时还会向后传递这个隐藏状态。且 kika 表示若网络有良好的训练，那么这个隐藏状态是可以包含足够的语意信息，因此我们可以将它作为后面字符级 LSTM 网络的初始状态。这相当给循环神经网络一个初始量，然后再接受键码的输入而作出最终的词预测和词纠错等。 其实这里还有一个非常有意思的问题，即为什么 kika 会采用 LSTM 而不是 GRU。因为我们知道若需要将深度学习模型部署到移动端，那么我们要严格控制模型与计算量的大小，kika 所采用的稀疏词表征与参数量化等方法能有效控制模型大小，这一点将在后文展开。但 LSTM 的结构比 GRU 要复杂，门控也需要得更多，因此 LSTM 的参数会比 GRU 多，那么 kika 为什么不采用 GRU 控制参数数量？ kika 就这一点对机器之心做了详细的解答。黄康说：「在层数和单元数均一致的情况下，GRU 要比 LSTM 少一些参数和矩阵运算，因此，模型体积和训练速度方面都会有一定的优势。为了严谨的进行效果对比，我们做了两组实验。其中第一组是将 LSTM 和 GRU 的超参数设置一致，结果是： GRU 的效果明显差于 LSTM，同时，由于整体模型体积的主要贡献来源于前后两个巨大的词嵌入矩阵，模型体积方面的优势也不明显。」 但在同样超参数的情况下，GRU 的实际参数数量明显少于 LSTM。因此，kika 继续做了第二组实验，在保证基本一致的参数数量而放开网络架构约束的情况下，最后得到的结论是：LSTM 与 GRU 的模型大小基本一致，效果也基本一致，实际上，在 kika 的应用场景下，LSTM 的效果略好，但也仅仅是略好一点点。此外，由于 GRU 在当时也是比较新的结构，因此在体积和效果没有优势的情况下 kika 还是倾向于选择更温和的 LSTM，从而把主要精力用于模型结构的调整与参数调优方面。其实最近 kika 也在做一些网络架构和基本单元方面的调研，因为最近在 GRU 之后又出现了非常多训练简单且高效的单元。在 kika 当前的开发过程中也出现了一些场景更为复杂的 NLP/NLU 应用，因此也在考虑采用一些训练时间上更为友好的网络结果。对于如何选择网络结构，黄康表示：「我们内部有共识，考虑新结构有一个基本原则：我们会采用类似机器翻译的复杂任务去验证此种网络是否真实有效，才会考虑在工程上采用。」 总体而言，kika 花了很大一部分时间完成参数调优，因而能基于一体化的 LSTM 实现效果非常好的输入法引擎。当然只是构建模型还远远不够，将这种深度学习模型部署到移动端还面临着非常多的挑战，例如深度学习框架的选择和模型压缩的方法等等。 轻量化部署的工程挑战 在 kika，轻量化部署包括以下四个方面：模型压缩、快速的响应时间、较低的内存占用以及 较小的 so 库（shared object，共享库）大小等。除了优化模型的设计之外，压缩模型大小的方法主要是稀疏词表征与量化，这一部分我们将在后一部分展开讨论。 响应时间与内存是去年 kika 的工作重点，它主要是需要对 TensorFlow Mobile 和 Lite 做大量的修补。最后是动态链接库文件（.so），它定义了所有需要的运算和操作。因为整个输入法的核心代码是 C++ 完成的，而它在安卓设备中是以一个库（.so 文件）的形式存在的，它的大小直接影响了安装包的大小（由于 Android 加载 so 的机制，也会影响到内存的开销）。 针对响应时间与内存，kika 最开始是基于 TensorFlow Mobile 做一些修补和改进。黄康说：「TensorFlow Mobile 有一个非常好的优势，即它在底层使用了一套很成熟很稳定的矩阵运算库。因此，我们的主要精力放在 TensorFlow Mobile 在底层矩阵运算库之上的部分。它在矩阵运算库之间采用了很多封装与调用，但是没有考虑到很多实际工业化中遇到的问题，尤其是在内存保护这一块做得相当一般。」 TF Mobile 的内存管理与内存保护设计得并不完善，存在两个主要的问题：1. 内存保护机制不完善，在实际内存不足的情况（尤其对于一部分低端机型），容易引发内存非法操作。2. 内存大小控制机制存在明显的问题，例如模型本身在计算时只有 20MB，但加载到内存之后的运行时峰值可能会达到 40 到 70MB。据 kika 的数据，基于 TF Mobile 的解决方案大概有 1% 的场景（如游戏中调起输入法）由于内存大小限制的原因会加载不了深度学习模型，只能回退到非深度的解决方案。 2017 年 11 月，谷歌正式发布了 TensorFlow Lite，这对于移动端深度学习模型来说是非常重要的框架。在 TF Lite 开源后，kika 马上就进行了测试，并重点关注内存管理模块。黄康表示：「TF Lite 的内存管理上确实有非常大的改进，加载不了深度学习模型的场景会成百倍地减少。但它最大的问题就是 Operator 的不全，它基本上只定义了最基础的运算和操作。所以 kika 为了在内存上减少 20 多 MB 的开销，我们自行编写了大量的 Operator。但目前这个还是有一定的风险，因为如果我们修改了模型结构，那还是需要手写新的运算。」 工程化挑战最后一个问题就是动态链接库的大小，这一部分还会涉及到参数量化方法的实现，我们会在参数量化方法那边讨论。其实 TF Mobile 还有一个缺点，即它会将很多冗余的操作与运算都会打包到 .so 文件中，因此也就导致了动态链接库过大。kika 为了让 .so 文件尽可能小，开发了一套全新的工具，用于自动的判断到底哪些操作与运算的定义是模型实际需要的。 稀疏词表征 深度学习模型在输入法客户端部署的一个重要问题就是模型大小，我们需要将参数数量与计算量限制绝大部分移动设备可接受的范围内。kika 发现模型体积的主要矛盾体现在词嵌入矩阵中。因此，如果能够压缩词嵌入矩阵的大小，那么就能有效地控制模型大小。kika 采用了稀疏词表征的方法以压缩词嵌入矩阵的大小，从而大幅度减少 LSTM 语言模型的参数与计算量。 其实对于语言模型，甚至是自然语言处理而言，词嵌入是非常重要的成分，我们可以使用 300 到 500 维的向量表示词表中数以万计的词汇。这种方法不会像 one-hot 编码那样使用超高维的向量表示一个词，可以说词嵌入将词的表征由|V|维减少到几百维，其中|V|表示词汇数量。但是当我们使用词嵌入作为语言模型的输入时，我们会发现尽管每个词的维度只有 n，但需要|V|个向量，而 |V| 通常要比 n 高好几个量级。因此，稀疏词表征就尝试使用少量词向量（少于|V|）而表征 |V| 个词。 这种方法的直观概念即我们的词表可以分为常见词与非常见词，而一般单个词可以由多个词定义，因此非常见词可以使用多个常见词表示。根据这样的观点，我们可以使用一组常见词的词嵌入向量作为基础，再通过组合而表示所有词的词嵌入向量。因此，我们能使用少量词嵌入向量表示大量词汇。又因为每一个词的表征都只使用少量的常见词来定义，所以这种表示方法是非常稀疏的，这也就是稀疏词表征的直观概念。 若我们将词表 V 分割为两个子集 B 和 C，第一个子集 B 为基向量集，它包含了固定数量的常见词。而 C 包含了所有不常见的词，因此现在需要使用 B 的词嵌入向量以线性组合的方式编码 C 中的词。这一过程可通过最小化由基向量集学习重构的词表征和完整表征之间的距离而学习，一般来说整个过程可表示为： 使用全部词汇训练一个词嵌入矩阵。 按词频选取最常见的 |B| 个词嵌入向量，并组成过完备基矩阵。 非常见词的预测可表示为 B 中所有词嵌入向量的稀疏组合，即 其中 w hat 为预测的非常见词词向量、U 为常见词词向量，而 x 为稀疏矩阵。 最小化预测词向量和实际词向量间的距离来学习稀疏表征，即 其中第一项表示通过稀疏表示 x 预测的词向量与完整词向量（w）间的 L2 距离。后一项为 L1 正则化，它会将矩阵 x 中的元素推向 0，从而实现稀疏表示。 在 kika 的论文 Sparse Word Representation for RNN Language Models on Cellphones 中，他们使用了以下伪代码展示了稀疏表示的学习算法： 这个算法很大的特点是实现了一个二元搜索来确定α，因为我们不能直接控制稀疏矩阵 x 的稀疏程度，所以我们根据稀疏矩阵的非零元素数来控制α的变化。整个稀疏词表征算法需要输入过完备基矩阵 U（常见词）、完整词嵌入矩阵 w、稀疏程度 s 和作为终止条件的容忍度 tol。 其中 s 是非常重要的一个参数，它控制了一个词最多需要多少个过完备基向量表征。kika 表示：「s 是一种权衡，如果参数较大，那么压缩比就会很小，模型达不到预期效果。如果参数较小，那么重构的词表征就不能有效地表示所有词。」正因为需要进行精调来确定 s 及其它超参数，kika 表明总体模型调优时间是训练时间的 4 到 5 倍，所以整个稀疏词表征的训练过程还是比较挺长的。 如上算法所示，首先我们会确定α的搜索范围，然后取α的中间值并最小化损失函数而求得稀疏表示 x，并统计 x 中每一个列向量的非零元素数，它们代表了一个词需要多少个常见词表示。如果 k 大于 s，那么非零的元素就过多，我们需要加大 α 以增强 L1 正则化的效果。这样的二元搜索直到α的上下界距离小于参数 tol 才会终止，且一般迭代几次就能快速收敛到合适的 α 来控制 x 的稀疏性。在完成 x 的学习后，我们将每一列稀疏向量抽取为对应的索引与权重，索引代表使用哪些基向量或常见词，而权重代表它们定义某个词的重要性。 又因为前面的二元搜索将 k 限制为不大于 s，所以有可能 k 是小于 s 的，因此我们需要使用零将这些向量补全。经过上面的步骤，最终我们会产生包含 s 个元素的等长向量 indices 和 weights。储存这两种向量而不直接储存稀疏矩阵 x* 能节省很多空间，这对于减小安装包大小有非常重要的作用。 论文中给出的词嵌入恢复算法以一种串行密集运算的方式进行展示，这可以令读者清晰地理解重构过程： 若给定 U、indices 和 weights，一个词的词嵌入重构可直接利用索引取对应的基向量，并与对应的权重求加权和。这种线性组合非常简单且高效，也是线性代数中非常直观的表示方法。因为任何秩为 n 的矩阵都可以由 n 个线性不相关的向量或基表示出来，完整的词嵌入矩阵也就能由过完备基的线性组合表示。算法 1.2 最后返回的 v 就是我们线性组合多个常见词词嵌入而重构出来的完整词嵌入向量。 以上是 kika 采用的稀疏词表征方法，它可以有效减少模型参数和计算量，但我们还能进一步使用参数量化来压缩模型的存储大小。 量化 一般而言，应用的安装包大小对于用户体验非常重要，这一点对于移动端尤为突出。因此，我们可以使用参数量化的方法来减小安装包大小。kika 也曾尝试使用 TensorFlow 封装的压缩方法，但仍发现一些难以解决的问题，因此他们最终使用 k-means 方法重新构建参数量化而解决包体增大的问题。 kika 最开始尝试使用官方的 tf.quantize 执行参数量化，并用 tf.dequantize 恢复参数。这个方法非常迅速，基本上几十兆的模型只需要分钟级的时间就能完成压缩。但 kika 发现这种方法有一个非常大的问题，即如果当我们希望读取量化后的模型时，TensorFlow 会引入大量的 Operator，这势必会造成动态链接库（.so）的体积增大，因而会加大安装包的大小。因为动态链接库包含了所有 TF 定义的加法、减法、卷积和归一化等模型需要使用的运算，因此调用 TF 的量化方法同样会将相关的运算添加到动态链接库中。 根据 kika 的实验，使用 TF 官方的量化方法大概会使动态链接库增加 1 到 2 MB 的体积，对应的安装包大小也会增加这么多。由于这样的原因，kika 最后选择基于 k-means 的方法实现参数量化。简单而言，这个方法会先使用 k-means 将相似的向量聚类在一起，然后储存聚类中心，原参数矩阵就只需要存储聚类中心的索引就行了。kika 表明这种方法的有点在于不会额外增加动态链接库和安装包的大小。因此下面将简要介绍这种基于 k-means 的参数量化方法。 量化即从权重中归纳一些特征，这些特征会储存在码表（codebook）并以具体数值表示某一类权重，而原来的权重矩阵只需要存储索引来表示它们属于哪一类特征就行了，这种方法能很大程度上降低存储成本。 kika 使用的标量量化算法基本思路是，对于每一个 m×n 维的权重矩阵 W，首先将其转化为包含 m×n 个元素的向量 w。然后再对该权重向量的元素聚类为 k 个集群，这可借助经典的 k 均值聚类算法快速完成： 现在，我们只需储存 k 个聚类中心 c_j，而原权重矩阵只需要记录各自聚类中心的索引就行。在韩松 ICLR 2016 的最佳论文中，他用如下一张图非常形象地展示了量化的概念与过程。 如上所示权重矩阵的所有参数可以聚类为 4 个类别，不同的类别使用不同的颜色表示。上半部分的权重矩阵可以取聚类中心，并储存在 centroids 向量中，随后原来的权重矩阵只需要很少的空间储存对应的索引。下半部是韩松等研究者利用反向传播的梯度对当前 centroids 向量进行修正的过程。 稀疏词表征与参数量化是 kika 控制参数大小的主要方法，黄康表示：「实际上模型的大小可以分为两阶段，首先如果原模型是 40MB 的话，稀疏词表征可以将模型减少到 20MB 左右，这个大小是实际在内存中的大小。而进一步采用参数量化可以将大小压缩到 4MB 左右，它解决的问题是 APK 安装包大小，APK 大小也是非常重要的，毕竟作为输入法这样的应用，APK 的大小是非常重要的。不论使不使用参数量化，模型最终在计算上需要的内存就是稀疏词向量表征后的大小。」 最后两部分基本上就是 kika 解决模型大小的方案，它们令深度学习模型在实践中有了应用的可能。当然，要将深度学习模型嵌入输入法和移动端会有很多的挑战，仅仅控制模型大小是不够的，因此也就有了上文 kika 在内存大小、响应时间和动态链接库等方面的努力。 整个模型效果和工程化实践都是 kika 在过去 2 年来对输入法引擎的探索，未来还有很多优化与提升的方向，例如使用新型循环单元或新型强化学习来根据用户习惯调整输入法等。这些新功能与新方向将赋予输入法引擎更多的特性，也能适应性地为不同的用户提供最好的体验。 转载来源：专访｜基于LSTM与TensorFlow Lite，kika输入法是如何造就的]]></content>
      <categories>
        <category>军事</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>Word</tag>
        <tag>机器学习</tag>
        <tag>Google</tag>
        <tag>人工智能</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[IPFS星际文件系统初识 - 简书]]></title>
    <url>%2F2018%2F9bbe1052%2F</url>
    <content type="text"><![CDATA[IPFS星际文件系统初识 - 简书 转载来源：IPFS星际文件系统初识 - 简书]]></content>
  </entry>
  <entry>
    <title><![CDATA[Cocos引擎创始人王哲：区块链游戏，其实也没什么神秘的]]></title>
    <url>%2F2018%2Ff5e68156%2F</url>
    <content type="text"><![CDATA[文：王哲 在周六的 Cocos 区块链游戏开发者大会上，我和 INB 资本的合伙人尹健辉一起演讲和演示了基于 Cocos 引擎开发的区块链游戏的编译、发布、运行在 Cocos 公链上的整个过程，以及游戏道具脱离游戏后在链上存储、交易、然后从链上回到游戏里，把武器挂件装备上去、以及把炸弹消耗掉的整个过程。 由于这部分演讲是用对话形式开展的，速记稿也比较凌乱毕竟太多技术干货了，所以我打算直接用这篇长文，完整阐释我们做了什么、计划做什么、以及为什么要做这些事情的整个思路逻辑。这篇文章真的很长，请耐心看完。 一、为什么我们要着手区块链游戏的研究 在和健辉一起演示区块链之前，我的个人演讲的部分，解释了为什么 Cocos 要在今天开始着手研究区块链游戏的方向。我自己讲了大概 30~40 分钟，其实核心是围绕这张图来展开的： 目前手机原生游戏领域的多数从业者，都是 2012~2014 年才进入这个领域的。但是我们 2010 年就开始做 Cocos2d-x 了，在 2010 年 12 月 1 日发布了第一个版本 Cocos2d-x 0.7.0。当时我周围的人都很不理解，为什么不做端游呢？为什么不做页游呢？当时国内的市场环境，手游上就 iOS AppStore 可以达到每月 50 万的最高月流水，还不够端游月流水的一个零头；安卓上则完全没有任何游戏变现模式，没有内购计费没有广告变现，那么你们在瞎折腾啥呢？ 后来的历史，大家也都一起经历过了。这是 3 月 12 日上周一我做区块链游戏大会的 PPT 时，截取的几个国家游戏畅销榜排名，Cocos 游戏仍然占据了大多数。Cocos 引擎被喷了这么多年，事实证明，在中国市场上，仍然是市场占有率最高的手游引擎。网易的几款 3D 游戏，我们内部称为「Cocos+」，就是在 Cocos 的基础上，网易自研构造了整个 3D 引擎叠加上去。我半开玩笑地和网易的朋友说，你们大概用了 20% 的 Cocos 吧，那么就调成 20% 透明度？于是就有了下面这张图。网易的技术路线，也说明了 Cocos 在 3D 的潜力，只要我们假以时日和研发投入，希望 Cocos 引擎也能做到像《楚留香》那样级别的 3D 能力，而且不是属于某家游戏公司的，而是属于全行业的。 但是原生手游领域，在 2017 年开始就已经进入我称为「生态稳定」的状态了。「生态稳定」是中性词，说难听点是「生态凋零」，大厂占据了绝对优势地位，这个领域即使越来越赚钱，也和大多数中小 CP 和创业者没有任何关系，基金和风险投资也不再愿意投资 CP 了。在 2017 年大家看不到太多机会之后，很多人离开了游戏行业，改行去做 O2O、直播、大数据、人工智能等新兴领域了。但还是有很多和我一样，就是热爱游戏行业，一辈子只认做游戏的朋友仍在坚守。直到 2017 年 12 月 28 日微信发布小游戏，2018 年 3 月 15 日 Facebook 宣布开放 Instant Games 平台，都把 HTML5 小游戏推向了风口浪尖，大家才看到新的机会。之前有提前做 HTML5 技术布局的游戏公司，都从中获得了很大的增益。Cocos 引擎也在微信和 Facebook 首发小游戏里得到了新一轮的爆发，占据了绝对优势的市场份额。 微信小游戏在上周已经可以注册和调试了，并要求大家准备各种资质材料，相信距离完全开放、可自由发布已经不远了。Facebook Instant Games 则在发布的当天就立刻完全开放，而在平台开放后仅仅过了 7 小时，就有一家国内游戏公司用 Cocos 引擎发布上去 8 款游戏。 所以，现在回头来看，我们在 2012 年开始布局和探索 HTML5 游戏开发技术，和大家介绍用 JavaScript 语言开发游戏，是完全正确的。 今天对于 Cocos 开始探索区块链游戏的各种技术，社区论坛里有不少质疑和嘲笑的声音。但是回顾前面几年的历史，我在 2012 年开始介绍用 JavaScript 来开发原生游戏，用 JSB （JavaScript Bindings）技术打包到原生平台、而且还可以发布到 HTML5 环境的时候，当时有多少人反对、嘲笑、觉得我们不务正业呢？甚至到我们在 2016 年 3 月推出 Cocos Creator，以 JavaScript 为第一优先的开发语言时，仍然有至少 50%~60% 的开发者跳出来骂。当时很多人喷的是，Cocos 团队不务正业，我只关心我的 C++ 和 Lua，我不需要 JavaScript。今天小游戏的机会来了，你还能那么确信 Cocos 团队在几年前投入 HTML5 技术研发是不务正业吗？ 我们每次在一个技术爆发、未到成熟的时候，就立刻提前布局一些新技术的研发投入，其实就是为了在老技术老平台「生态稳定」的时候，可以给诸位开发者提供「多一些选择」。回到第一张图，原生手游领域我们 2010 年开始研究，2012 年爆发；这时候我们不等原生手游发展到生态稳定的阶段，就立刻投入 HTML5 技术的研发，即使被人喷、被人认为不务正业；6 年后 HTML5 小游戏爆发证明了我们的技术前瞻性，但同时也我们不会等到 HTML5 进入生态稳定，就需要立刻布局新的技术平台了。从目前的情况来看，区块链游戏的确是一种很有潜力的新技术，值得我们去研究。 很多开发者看到这里就会问了，OK，你说服我了，我也支持 Cocos 研究新技术，但是已有的原生和 H5 引擎是否会受影响？我在这里统一答复：不受影响。因为我甚至不太想把区块链研发团队建在厦门，和引擎团队放在一起。主要原因是厦门基本就没有区块链技术高手。目前可演示的 Cocos 区块链项目几个研发人员在成都，我最终会在北京和成都中间做出一个选择，毕竟这两个城市人才比较多，目前暂时倾向于成都。但同时，厦门的 Cocos Creator / Cocos2d-x 引擎团队在春节后也大举招人。这里顺便打个广告，对自己的引擎开发技术有自信，希望一起做一个全球一流引擎、一起做一家伟大公司的朋友，可以在微信上、或者通过微信公众号联系我，我们招2D/3D/编辑器技术高手，工作地点厦门。 在周六的会上，我也展示了 Cocos Creator 2.0 的性能飙升，我们只是用了在 3D 研发上得到的部分成果，一个 2D/3D 通用的渲染器，把 Cocos 原来的底层换掉，然后就得到了这样的性能提高。Creator 2.0 目前已经出到 Alpha-5 版本了，而且这次拉了腾讯互娱的大神们进来趟坑。填完坑之后，应该在 4~5 月份，就可以发至少 beta 甚至 RC 版给大家用了。 会上我也演示了 Cocos 3D 目前的工作流，以及一款可以部分客户端 2D、部分客户端 3D 的麻将游戏。我们希望通过这种技术方案，让开发者们可以平滑过渡到 3D 方案上，甚至因为是 H5/热更新的缘故，可以做灰度发布和 A/B test。具体演示过程大家可以去看这周稍后放出的会议录像。 好了，讲完了我们为什么要进入区块链游戏领域的逻辑，以及让大家放心我们对原生/H5的投入，和 Creator 2.0 版本即将带给大家性能上质的飞跃。 下面进入区块链的内容了。 二、区块链黑话翻译 几个月前，我刚开始看区块链相关资料的时候，也是极度懵逼的。后来发现，链圈自己发明了很多行业术语，或者说「黑话」，其实和手游圈一样的。外行人看手游圈，我们也是一堆黑话比如：SDK、API、次留、七留、人均阿普（ARPU）、付费阿普（ARPPU）、LTV、CPA、CPI、CPC、填充率、CPM 和 eCPM 还讲的是完全不同的东西……。类似地，搞清楚链圈的这些术语之后就容易理解多了。 所以在我和健辉一起完成演讲之前，我坚持要求把很多链圈的行业术语「翻译」成了游戏圈的术语，当然还有一些无法翻译的，我只能在这里简单科普一下，链圈的极客们就不用看了，主要给游戏圈的 Cocos 开发者们看的： 游戏运行在区块链环境上 —— 目前阶段，一般指的是游戏接入了区块链 SDK，在金币内购和道具生成/交易/兑换的时候通过区块链 SDK 调用任一区块链节点提供的API。最理想状态是游戏绝大多数逻辑、甚至全部逻辑都由智能合约构成，完全在智能合约虚拟机中执行。- 区块链虚拟机、智能合约虚拟机 —— 区块链 SDK 上绑了 JavaScript 、 Lua 脚本或 Solidity 脚本的执行环境。JSVM、Lua VM 的确是翻译成虚拟机没错。比较有意思的是，现在的区块链系统中，通常是数个节点共同执行并见证一份智能合约。区块链虚拟机、智能合约虚拟机 —— 区块链 SDK 上绑了 JavaScript 、 Lua 脚本或 Solidity 脚本的执行环境。JSVM、Lua VM 的确是翻译成虚拟机没错。比较有意思的是，现在的区块链系统中，通常是数个节点共同执行并见证一份智能合约。 区块链浏览器 —— 这个模糊的命名是被我吐槽最久的。其实它不是一个架设在区块链上的网页浏览器，而只是网站上登录后，类似「我的账户」里面可以看到「我的交易记录」以及每次交易记录区块描述的这么一堆网页。这应该是直译 Block Chain Expoloer 的锅。区块链浏览器通常还带有合约浏览、共识过程监督、出块记录、理事会等功能，这就看具体的设计了。- Token —— Token 是一种权益证明，并不是区块链特有，例如Q币也算是一种 token，在中心化系统中也可以发行和流通。从技术上来说，区块链和 token 是可以完全分开的。区块链系统中的权益证明，能通过加密算法和分布式账本标明资产的唯一性、确定真伪、并通过共识算法进行流通。一般 token 会被通俗理解为数字币。比特币、以太币、以及在以太坊上用各种智能合约生成的数字币都叫 TToken。实际上 token 也可以用来证明你拥有某种游戏道具，比如屠龙刀、裁决的拥有权，这就是虚拟资产的持有证明了，而不是纯粹的币。- TPS —— Trade Per Second，每秒能交易多少次，这是区块链主要性能指标。比特币大概 7 TPS，以太坊 25 TPS，而石墨烯技术的链理论上可以达到 10 万~100 万 TPS 的峰值吞吐性能。- 石墨烯技术和 DPOS 共识方案 —— 咱们行业多数游戏都是专制独裁式的。我游戏厂商今天要做个运营活动、发个道具，直接由游戏厂商说了算；比特币则类似全员民主投票制，稳，但是效率真是太低了；而所谓的石墨烯技术和 DPOS 共识就是类似咱们国家的民主集中制，大家先投票选出几个代表，然后由这些代表负责后续的日常投票就行了。这种方法在民主的低效和专制独裁的风险之间，取得了一个很好的平衡。Cocos 链就是采用这种技术的，否则每秒不到 30 次交易的那些公链，怎么可能支撑商业化游戏的运营呢。更多的链圈黑话，大家自己去网上搜索学习吧。我就不多写了。 正式开始之前，我得强调一下，我们是链圈的，不是币圈的。区块链底层技术可以拿来讨论，发币炒币这种事情就不用讨论了。我本人连股票都不炒的，比特币和以太坊钱包都还没开。花那个时间不如撸代码和打游戏有意思。 有媒体或开发者用「区块链引擎」来描述发布会后的 Cocos。这样的解读是不对的，游戏引擎就是游戏引擎，我们只是可以让开发者能更方便地接入区块链 SDK 而已。这和引擎提供广告、提供统计等服务的本质是一样的，甚至和引擎支持微信小游戏平台、支持 Facebook Instant Games 平台都是类似的。Unity 的 70% 收入来自于他们自家广告平台，人家也不会称自己是「广告引擎」嘛。正确表述是，「今天 Cocos 让游戏可以通过引擎快速接入我们的区块链 SDK 了」。 Cocos 区块链项目的名字叫 Project BCX，BCX 的全称是 Block Chain Expedition。BCX 在我们内部一般读为 /biks/。Expedition —— 让我们去远征吧，很可能是像 HTML5 技术一样，三到五年的一趟远征，最后带回来给开发者多一种新平台的选择。 三、区块链游戏的四阶段发展路线设想，以及我们的对应实现方案 首先，我们认为区块链游戏的发展，会分为四个阶段： 第一阶段：使用 token 作为游戏金币的结算 这一阶段的区块链游戏使用 token 作为游戏金币产出的结算。 一些项目的数字币基于以太坊的 ERC20 标准进行制作，基于 ERC20 协议发行的 token 很容易交换和兼容可用于不同的项目和平台， Token 的持有人可以完全控制资产并且跟踪到任何地址任何数量，其流通路径可在区块链浏览器中查询。 Candy.one 的游戏平台是这一阶段的代表。会前宣传说我们会演示 Cocos 游戏接入 Candy.one 平台，其实我们在现场根本就没有演示，因为这一阶段是在是太容易了。对于引擎而言，就和接入一个微信支付宝 SDK 一样简单。 真的是很简单。我们一款已经开发好的 H5 游戏，加入 Libs 里面两个混淆过的 JS 文件，调用这个区块链 SDK 的 API，构建，搞定。什么区块链容器、区块链虚拟机，我们游戏圈的人不用理会这些概念，就是接入一个 SDK，2 天时间搞定。 这个阶段会遇到的局限是：token 是同质化的，只能用来表达积分、金币这样的纯粹一些数字，无法表达不同的道具。比如说，你在游戏里打出了第一个钻石、然后第二个钻石，就可以记录为你有 2 个钻石，这个叫「同质化」；但是如果你有了第一只以太猫、第二只以太猫，两只猫的毛色、眼睛、形态都不一样，这样你就无法在链上记录说你有「以太猫x2」了，第一只和第二只以太猫需要分开记录，这叫「非同质化」。所以我们需要立刻进入第二阶段。 第二阶段：游戏金币和道具的去中介化、去代理交易 以太坊的 ERC721 Token，就是一种「非同质」 token 的标准范例。而之前大火的「以太猫」，就是非同质 token 的代表性应用。所以，不论是游戏中的道具、装备、玩家账号，你手里有把风之力、有把蛋刀、或者裁决、，都可以用非同质 token 来表达。这个 token 代表了你对这件道具的所有权，可以脱离游戏去买卖交易。 该阶段解决了玩家之间道具脱离游戏在链上交易、甚至是不同游戏里交易的问题。由于这个过程是去中介的，理论上到达该阶段之后，玩家就不需要像 5173、交易猫这样的道具交易平台了。你不再需要把账号密码给到一个交易平台上的代理人，那个代理人起到平台信誉担保的作用，一边收了买家的钱，把钱给到你，然后再一边把装备和账号给到买家，中间赚取差价。 我们的 Project BCX 正处于这个阶段，会稍微往前一点儿有个第三阶段的雏形。周六我和 INB 资本合伙人尹健辉在演示中，重点演示了我们已实现该阶段的功能。 健辉在这个地方有个很特别的设计是，交易的「原子化」实现。虽然「付钱」和「得到道具」是两件事情、属于两笔不同的记账，但是 Project BCX 里面的规则是把这两笔记账绑定绑定到一起，成为一个「原子」，如果你付了钱没有得到道具，或者得到了道具但是付款失败，这次交易的「原子」就会整体失败、整个回撤。而区块链的去中介去代理化，就是说只有你自己能对自己的道具和金币进行操作，在原子化交易规则之下和去中心挂化挂单、撮合系统的帮助之下，你在链上的交易里，再也不需要有道具交易平台那些代理人的帮助了。 但第二阶段的缺点是，游戏运行规则仍然是在链之外的。虽然道具的产量、流通是透明的，但产出规则仍然是可以被游戏厂商暗箱操作的。今天做个运营活动，明天出个新boss，结果玩家花了大量时间、或投入大量充值购买的装备道具被快速贬值。玩家和游戏厂商的价值严重背离了。我们不少游戏人，天天想着怎么洗用户，用户又不傻，很快就跑到王者荣耀、吃鸡游戏、TapTap 等「不会被洗」的地方去了。 第三阶段：关键规则上链运行 如何才能保证我作为玩家不会被反复洗呢？不会说得到一件橙装之后，下个月的运营活动里就被快速贬值呢？我们设想了第三阶段应该实现的功能，就是比如高级道具的掉落、金币产出规则等上链运行，游戏运营方将这些关键规则在区块链上以智能合约的形式实现，在区块链浏览器的支持下，规则对玩家是公开、透明的。 之前有提到，Project BCX 稍微有了第三阶段的雏形。我们设想并实现了一个很有趣的功能，称之为「铁匠铺」。铁匠铺应该是一个由游戏运营方、游戏玩家代表共同成立的治理委员会，关键规则在治理委员会讨论投票通过后，所有关键道具都只能通过铁匠铺来生成。未经过委员会讨论通过的情况下，游戏运营方是无法单方面生成各种道具装备的。 所以： 铁匠铺是具有道具、装备制作权限的账号和一组合约- 铁匠铺是独立于游戏的道具产出点- 铁匠铺的道具，具有限量性或唯一性- 铁匠铺由游戏厂商、玩家、玩家公会等构成的治理委员会管理铁匠铺是独立于游戏的道具产出点 铁匠铺由游戏厂商、玩家、玩家公会等构成的治理委员会管理 其实进一步想，这么设计之下，游戏的运营和收入方式会有很大的改动，游戏本身也会变得有趣：比如说在同样的三国世界观下，我们可以在《三国志》里面获得一把青龙偃月刀，记录到链上，然后到《真三国无双》里面割草，接着到《街机三国》里面把曹操砍了。这些游戏一开始可能是同家游戏厂商做的不同游戏，但最后有可能演变成不同游戏厂商做的不同游戏，但都接入同样的链、同样的世界观、同样的道具产出和交易规则、被一个世界观治理委员会管理。多家不同游戏厂商和玩家一起讨论、制定、公开青龙偃月刀的产出规则，而游戏运营的收入，就不能通过「洗用户」来维系了，而可能是通过提供区块链记账的算力来收取交易手续费。表面上看因为不能洗大R而短期收入降低了，但是游戏的生命周期则可能拉得很长，游戏厂商的关注点不应该是付费点设计和运营活动了，而是专注于有趣好玩的游戏内容生产，回归游戏的本质。 其实这一点我们还没完全思考得特别清晰，但直觉上这是一个正确的方向，让玩家和游戏运营方的利益一致化，实现游戏的长期运营和收入。如果我们今天不想做出一些改变，只是靠不停滚服洗玩家的话，最后玩家即使不玩你区块链游戏和铁匠铺规则，也会跑去玩那些大厂的公平竞技游戏了。 我和健辉在周六的演示里，演示了「只有铁匠铺账号才能创建道具」的功能，但对于游戏运营方和玩家、和玩家公会如何成立一个治理委员会来决定这些道具产出规则，则没有想得太明白，毕竟这已经不是技术的范畴了。 第四阶段：游戏整体上链运行 这个阶段的脑洞就比较大了。我们设想，行业的最终形态可能是游戏整体上链运行，游戏的全部逻辑代码都在链环境中执行，并由去中心化的区块链网络承载和存储数据，在这个场景下需要可信、高效、无延迟的运行容器与轻量级的节点，用于游戏的运行。但是，哈哈，目前业界尚无决定性的技术方案，大家各种链的性能和算力显然都扛不住啊。也许某天会有哪个顶尖聪明的程序员提出解决方案，这只能等了。 四、Project BCX 希望解决的问题，和技术特点 总体来讲，我们在区块链游戏方向的探索，希望能解决以下技术问题： 1、合适的区块容量和出块时间。容量太大么出块时间慢，容量太小么，怎么记录各种道具的一大堆数据。石墨烯方案的最大区块大小为 2M /块，对于记录游戏道具，这显然不够用。我们具体定多少呢，还在测，反正中间找到个平衡点。另外，我们也已经做了一些改进； 2、提供自定义数据结构存储。你这游戏是把风之力，那个游戏是把屠龙刀，每个游戏记录自家道具的数据结构肯定是不一样的，我们也不应该强求大家一样。所以块里面应该能支持自定义数据结构的存储； 3、提供带有区块链操作接口的多平台游戏运行环境。这句话翻译过来就是，区块链 SDK 应该能跨 iOS, Android, H5…… 这句基本是废话，本来就应该如此； 4、提供用户道具交易的 token 交换原子操作。这点上面已经讲解过了，因为没有中间代理商了，必须一手交钱一手交货，两个行为合并为一个原子。我们已经实现该功能； 5、去中介交易的实现和一个道具交易市场的范例。恩，这点我们也已经做完了； 6、提供完整的钱包和区块链浏览器。同上，开发完了，会上有演示； 7、支持同质和非同质 token 的跨链承兑网关。同质和非同质，前面已经讲解过了。什么叫「跨链承兑网关」呢？你看，Cocos 最早就是靠跨手机平台起家的，然后有 AnySDK 和 SDKBOX 帮助大家快速跨各种支付 SDK 广告 SDK。到了区块链游戏的时代，必然是各种公链各种币满天飞，这时候就需要引擎能帮助开发者能把游戏快速接入各种链各种币，反正多一种币的支付，就多一群玩家，玩家可不能因为他只有 a 币没有 b 币而流失； 8、二级资产的发行和交易能力。Project BCX 的设计是，CP 可以在 Cocos 公链上发行自己的二级数字币，游戏厂商或者我们前面提到的铁匠铺管委会，可以自由交易这些数字资产； 9、高速合约虚拟机。这个是和快速共识设计配套的，我们需要能快速地执行合约代码，不然玩家多了肯定扛不住。速度越快，可以上链的游戏内容就越多，对玩家利益就越有保障。 五、篇末 谢谢各位有耐心看到这里。我很久没有写过这么长的文章了，还得尽量通俗不晦涩。整体而言，Cocos 在区块链上的研究，是直奔「让玩家和游戏厂商利益一致化」「游戏厂商能专注生产好玩有趣的内容，而不是天天盘算怎么设计付费坑怎么洗用户」而去的。我前面也说过，大家天天洗用户洗大 R 的结果，就是玩家干脆跑到腾讯网易做的 MOBA、吃鸡等公平竞技游戏里去了，而这种公平竞技游戏，都需要高 DAU 堆起来的。在手机游戏流量红利结束之后，想要有 DAU 支撑，你要么得有超级流量平台、最好还是超级社交平台，要么得有大笔市场费用做投放买流量，这两件事情和大多数游戏公司已经没什么关系了。今天的 H5 小游戏，或者叫「手机页游」固然开放了新的流量获取方式，但可能两三年之后又会进入「生态稳定」的阶段，大家又必须去找新的突破口和新流量来源。今天 Cocos 投入区块链技术的研究，和我们早在 2010 年开始手游原生平台，2012 年开始 HTML5 技术一样，未雨绸缪，希望在手机页游/小游戏的流量红利结束之后，能给各位开发者提供多一种可选的方案。 最后我再强调一下，我们对区块链领域的投入研究，是和早几年的 Cocos VR 一样放在体外，并不影响 Cocos 作为游戏引擎本身的研发投入。引擎团队今年仍然在融资和扩张。应用了我们部分 3D 研发成果的 Cocos2d-x 4.0 将在 Q3 发布，将实现渲染多通道支持、2D 材质系统和 2D 光照，而且实现了下一步可适配 iOS Metal 的渲染架构；而性能大幅提升的 Cocos Creator v2.0 目前已经进展到 alpha-5 版本，计划在 Q2 发布，敬请期待。 不论是在手机原生，还是 H5 小游戏，或者在未来可能存在机会的区块链游戏上，『让游戏开发更简单』，既是对各位开发者的承诺，也是我们始终不变的愿景。 转载来源：Cocos引擎创始人王哲：区块链游戏，其实也没什么神秘的]]></content>
      <categories>
        <category>游戏</category>
      </categories>
      <tags>
        <tag>编程语言</tag>
        <tag>JavaScript</tag>
        <tag>小游戏</tag>
        <tag>HTML5</tag>
        <tag>TPS游戏</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python爬虫框架Scrapy：爬取校花网]]></title>
    <url>%2F2018%2Fea9d394a%2F</url>
    <content type="text"><![CDATA[以校花网为例进行爬取，校花网：http://www.xiaohuar.com/，让你体验爬取校花的成就感。 想爬哪就爬哪 Scrapy，Python开发的一个快速,高层次的屏幕抓取和web抓取框架，用于抓取web站点并从页面中提取结构化的数据 Scrapy吸引人的地方在于它是一个框架，任何人都可以根据需求方便的修改。它也提供了多种类型爬虫的基类，如BaseSpider、sitemap爬虫等 整体架构大致 Scrapy运行 基本使用 1、创建项目 运行命令: 1 scrapy startproject p1（your_project_name） 2.自动创建目录的结果： 文件说明：- scrapy.cfg 项目的配置信息，主要为Scrapy命令行工具提供一个基础的配置信息。（真正爬虫相关的配置信息在settings.py文件中）- items.py 设置数据存储模板，用于结构化数据，如：Django的Model- pipelines 数据处理行为，如：一般结构化的数据持久化- settings.py 配置文件，如：递归的层数、并发数，延迟下载等- spiders 爬虫目录，如：创建文件，编写爬虫规则注意：一般创建爬虫文件时，以网站域名命名 编写爬虫 备注：- 1.爬虫文件需要定义一个类，并继承scrapy.spiders.Spider- 2.必须定义name，即爬虫名，如果没有name，会报错。因为源码中是这样定义的： 3.编写函数parse，这里需要注意的是，该函数名不能改变，因为Scrapy源码中默认callback函数的函数名就是parse；- 4.定义需要爬取的url，放在列表中，因为可以爬取多个url，Scrapy源码是一个For循环，从上到下爬取这些url，使用生成器迭代将url发送给下载器下载url的html。源码截图： 4、运行 5.scrapy查询语法： 6.scrapy查询语法中的正则： 7、格式化处理 各位同学注意啦！！ 想获取更多视频或者有任何学习问题 欢迎加入Python交流群 626062078 转载来源：Python爬虫框架Scrapy：爬取校花网]]></content>
      <categories>
        <category>科技</category>
      </categories>
      <tags>
        <tag>编程语言</tag>
        <tag>Python</tag>
        <tag>HTML</tag>
        <tag>Scrapy</tag>
        <tag>网络爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[瑜伽，被“异化”了还是被“误读”了？]]></title>
    <url>%2F2018%2F13985a81%2F</url>
    <content type="text"><![CDATA[瑜伽，似乎是一个永远被关注的话题。 自从瑜伽进入中国以来，围绕瑜伽究竟是健身还是伤身的话题，就一直没有停止过。爱她的，趋之若鹜，恨她的，嗤之以鼻。用冰火两重天来形容大家对瑜伽的看法，一点都不为过。 这不，刚刚生完宝宝的曹蕊，看着身上的赘肉正在发愁，朋友告诉她练练瑜伽吧，不仅减肥而且塑身，最关键的是还能提高气质。曹蕊心动了，正要去健身房报名时，突然看到了“无数中国女人，正被瑜伽毁掉身体……”以及“辟谣:瑜伽正在毁掉中国女人?信息不实!”正反两篇文章。曹蕊瞬间懵圈了“什么情况？瑜伽到底是好呢还是不好呢？” 瑜伽一词由印度梵语而来，有“结合”“和谐”的含义。是一种达到身体、心灵与精神和谐统一的运动形式。 在印度，瑜伽被认为是一门以感觉语言科学体系为基础理论的学修科。而在国内，瑜伽则是一种健身手段。 “瑜伽在我国被异化了。”中华医学会运动医疗分会候任主委陈世益教授一针见血地指出，原本一项很好的运动，现在不仅被注入了太多的商业气息，而且被当成了时尚。 尽管都叫瑜伽，但此瑜伽非彼瑜伽 据了解，这项源自古印度的修行方式，目前已经成为一个价值百亿的庞大全球产业。仅在美国，就有1500万人在有规律地练习瑜伽。自1985年，瑜伽正式进入我国以来。有统计显示，目前90%以上的健身房都开设有瑜伽课程，学员已经覆盖到不同年龄阶段的人群，从老到少，无一不对瑜伽狂热。 因为瑜伽动作相对柔和，有助身心放松，对现代人减压非常有效等一系列好处，瑜伽的拥趸越来越多。但与此同时，针对瑜伽的争议也一直就没有停止过。 近几年，有多篇文章和相关专家纷纷指出，“瑜伽病”已成新病种，主要以肌肉韧带拉伤、软骨撕裂为主，还有关节错位、脊椎错位以及关节炎症、神经痛等。 “所谓疾病，在医学上是有严格的定义的。”小汤山医院康复科主任武亮对记者表示，现在报道称的所谓“瑜伽病”，无非是瑜伽练习中所产生的一些可能造成伤害的综合体现，基本上属于局部发生的问题。这些既不是练习瑜伽后必然导致的情况，也不是全身性的问题。如果说成“瑜伽病”，则会让人产生很大的误解。“尽管，没有“瑜伽病”这个病种。但是瑜伽伤害还是有可能的，我们应该尽量力去避免。” 武亮直言，“瑜伽病”源于练习不当。练习瑜伽时应该充分考虑自己的柔韧、平衡和力量素质，一定要遵循量力而行的运动原则。 从运动生理学和解剖学来说，人体的每一个关节都有自己的活动范围，长期超过这一范围的高强度活动，往往会给关节带来近期或远期的损害。 “人体的脊柱结构也是如此，不适当的过度弯曲和牵拉，会造成脊椎损伤，严重的甚至会造成脊椎骨折、滑脱，椎间盘突出，伤及脊髓和神经。”火箭军总医院骨科主治医生朱泽兴博士透露，很多运动损伤为积累损伤，时间长、起病隐蔽，待发现时往往为时已晚。 优胜美地瑜伽的创始人郝宇晖女士客观地分析指出，近年来，消费市场升级，国人越来越注重身心的和谐健康，开始关注瑜伽领域，而目前中国瑜伽市场中瑜伽教学带来的瑜伽伤害问题层出不穷，这些业态已经偏离了古老瑜伽体系的修习和哲学内核。“因为中国的瑜伽市场很大，经营者的准入门槛低，所以对于瑜伽概念和练习的误区是非常深的。要想避免瑜伽伤害，应该从培养正确练习瑜伽的意识开始。” 瑜伽能减肥、塑身，那是你想多了 瑜伽运动之所以“大热”，跟很多人为它所赋予的减肥、塑形、排毒、美容等“附加值”不无关系。 “的确，很多练瑜伽的人都被美轮美奂的照片所吸引，但并不是每个人都能达到照片里的效果。”武亮不无幽默地说，如果你指望着通过瑜伽进行减肥或塑身，只能说你的期待有些跑偏了。 陈世益也持相同的态度。“目前，没有太多证据表明练瑜伽可以减肥和塑形。” 有研究表明，根据不同的瑜伽类型，每小时的卡路里消耗在50-150Kcal，而消耗一公斤脂肪的热量是7716大卡，因此要减去一公斤脂肪，需要通宵练瑜伽约三天三夜。瑜伽对减肥的作用是忽略不计的。也许你会说某些瑜伽实际上强度也很大，好吧，确实有些瑜伽比其他瑜伽更耗费体能，但每小时也不会超过200大卡的，对减肥来说是不值一提。 瑜伽圈有个著名的自黑笑话，说的就是一个胖子想练瑜伽减肥，最后变成了一个柔软的胖子。 其实，瑜伽既不能减脂，同样也不能增肌。 所谓“塑形”，其实就是增肌+减脂。减少全身赘肉，增肌目标肌肉，已达到臀部挺翘性感。因此，女性想要身材凹凸有致，就得同时增肌和减脂，而想要增肌，则需要做肌肉产生位移的等张收缩，而瑜伽这种“静力性收缩”，对肌肥大的刺激效果是忽略不计的。 虽然瑜伽训练本身对增肌效果并不明显，但如果作为一种辅助训练，极大地增加身体各关节柔韧性后，运动轨迹被大大加长了，在力量训练中，更长的运动轨迹就意味着更充分的肌肉刺激和更好的肌肉增长效果，这种效果是显而易见且绝对有效的，理论上的增肌效果可以提升10%-20%。可以肯定地说，练习瑜伽可以很好地增强身体协调性以及平衡能力。 对此，陈世益呼吁，要回归到理性上来，不要把太多想法寄予到瑜伽上，否则受伤的就是你。 你不是她，不要追求无谓的极致 刚从领导岗位退下来的辛女士，在朋友的推荐下报了瑜伽班。 作为新学员，60岁的辛女士总觉得跟不上节奏，争强好胜一辈子的她，为了不落别人的后面在家偷偷苦练。一天，辛女士正在做下腰的动作时，突然听到“咔嗒”一声清脆的响声，回过身来就隐隐觉得腰不舒服。开始她并没当回事，只是做了热敷，贴了膏药，但疼痛始终没有得到缓解。 直到实在疼得受不了了，辛女士才来医院检查。结果发现腰椎发生压缩性骨折，需要接受微创手术，对发生骨折的腰椎进行固定填充。 “在门诊，因为练瑜伽受伤的病人时不时地就能遇到。”朱泽兴已是见怪不怪，普通的肌肉拉伤居多，髋部及腰部受伤的也不少见。 之所以会出现这种情况，是因为成人的关节、韧带、肌肉、神经血管，尤其是脊髓都已经定型，过度使用就会造成损伤。特别是处于退行性病变高发期的中老年人，大都存在关节的老化问题，体质较为脆弱，如果锻炼过程中稍不注意，颈椎、骨骼就很容易受伤。 “不当运动导致出现问题的病人很多，每周都能碰到一两例。”武亮指出，运动伤的发生，大多是因为患者没有进行热身准备，缺乏专业性的指导，更重要的是对不良运动没有良好的防范意识和警觉意识。 瑜伽很多动作都是身体的极致伸展，但需要提醒的是，每个人身体条件不同，练习的程度也应有所不同，练习时一定要尊重自我的感觉，不要盲目攀比，要始终根据自己的实际能力来做瑜伽的动作。切勿盲目追求一些高难度的体位动作，以免伤害自己。任何运动都有风险，绝不能因噎废食。只要你做到量力而行，瑜伽是很安全的运动。 郝宇晖也认为，太多练习者不顾自己的自身条件去一味地效仿别人他就肯定会受伤。如果练习者的腰椎不好，还做大量的前屈体式；如果颈椎不好，却偏要做头倒立；如果髋骨很紧，依然执着地做莲花盘……自然就会受伤，其实瑜伽就是看自己、做自己能够做到的练习。“瑜伽最终是为了提高心理和精神需求达到身体心灵和谐联结的本源。” 郝宇晖再三强调。 “瑜伽伤害并不是不可避免的，”在浙江大学在读哲学博士，资深瑜伽教师闻风看来，经营者不应将瑜伽神化，任何运动如果处理不当都会导致伤害的发生。一方面，习练者要有自我保护的意识；另一方面，瑜伽教练必须对自己的学员负责任。毕竟，瑜伽体式练习的最核心要素，是通过呼吸培养清晰的觉知，获得稳定的心智，回归当下，以脱离焦虑和压力，堪破世间幻象得到真知。“瑜伽练习是我们每个人自己的事，它无竞争、不攀比。” 转载来源：瑜伽，被“异化”了还是被“误读”了？]]></content>
      <categories>
        <category>健康</category>
      </categories>
      <tags>
        <tag>美体</tag>
        <tag>健身</tag>
        <tag>减肥</tag>
        <tag>骨折</tag>
        <tag>健康</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Lady Gaga为何零出场费演出？天价广告位销路如何？超级碗的这些事你需要知道]]></title>
    <url>%2F2018%2F33817910%2F</url>
    <content type="text"><![CDATA[Lady Gaga为何零出场费演出？天价广告位销路如何？超级碗的这些事你需要知道 转载来源：Lady Gaga为何零出场费演出？天价广告位销路如何？超级碗的这些事你需要知道]]></content>
      <tags>
        <tag>虎嗅网</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[被点赞就能挣钱？基于区块链的社交媒体平台 Steemit - CSDN博客]]></title>
    <url>%2F2018%2Ff48380b8%2F</url>
    <content type="text"><![CDATA[从PC时代到移动互联网时代，社群媒体平台的需求一直都存在，从QQ到微信，我们在这些平台上撰写了多少文章、挥洒了多少青春岁月，同时也被把隐私卖给了平台、注意力也被广告所攫获？ 不知你是否曾经想过，自己在社群媒体平台上发布的内容（文章、相片、影片）能值多少钱呢？ 80.22*7.42 = 595.2324, 也就是这篇文章价值将近600块钱 Steemit背后到底运作原理到底是什么呢？下面让我们来为你揭开他的神秘面纱 发表优质文章：文章质量越高、收到的赞数越高，作者得到的奖励就越高1. 挖掘优质文章：越早在优质文章刚发表的时候透过点赞、留言来支持，也能获得奖励1. 持有Steem Power：类似股权分红（下面详解）1. 透过外部交易所购买Steam代币 直接奖励内容生成者，省去平台中介费1. 无广告，把使用者的注意力留给优质内容1. 没人能透过算法操纵、屏蔽平台上的文章 可转换成SP或者SMD (立即生效) 还本：不论Steem市场价格如何波动，1SMD永远等价于1美金的Steem假设今天1 Steem = 0.5美金1 SMD 可跟平台兑换 2 Steem- 1 SMD 可跟平台兑换 100 Steem 可以用赞同/反对来决定每个文章的收益 可转换成Steem (需等13周，每周等额到帐) 把Steem转为SP的过程称为Power Up- 把SP转为Steam的过程称为Power Down 以Steem为核心，这三种代币的转换关系图如下 90% 依比例分配给SP持有者- 10% 进入贡献奖金池，再分成三份75% 发帖/回覆/评论创作者- 15% 投票者(根据SP计算)- 10% 证人(记帐/打包区块者) 在撰写文章介面的右下角有个Rewards，有三种不同的奖励方式可以选择，默认是Default (50%/50%) Power UP 100% - 将文章奖励 100% 存成 SP1. Default (50%/50%) - 将文章奖励的50%存成 SP，另外 50% 透过 STEEM &amp; SBD 组合形式发放（根据市场行情，可能是一种或者两种组合）1. Decline Payout： 文章不要奖励，对应文章的所有奖励会回归奖池 SP多的人点赞影响力更大，所有文章根据点赞者的SP加总计算后得出分数，共同瓜分奖金池1. 你可以获得别人给你帖子回覆、评论时获得的奖励的 50% 否决票：如果被网友发现恶意刷赞行为，可以给予否决票。但为了避免否决票被滥用，每个人都有个信誉值，这个分数必须透过发帖、评论、日积月累才能逐步提升的，信誉值低对信誉值高的人的否决票是不起作用的1. 投票力：每个人的投票力是有限的，每次投票都会消耗，随著投票力下降，点赞者带来的影响力也会下降，每天恢复20%的投票力1. 延迟奖励：所有投票会被延迟24小时后才会计算奖励，如果有人在短时间内投票作弊，仍然会发现并予以否决 共识机制：DPOS (委托权益证明) 买卖平台：Bittrex, Poloniex, Livecoin 竞争对手：YOYOW, Yours, Facebook, YouTube, Instagram, Twitter Steemit新人直通车 https://steemit.com/cn/@tumutanzi/5ndvpm-steemit 30分钟全面搞懂Steem https://steemit.com/steem/@gaoduzhu/steem STEEM的基本介紹及資料整理 https://steemit.com/cn/@htliao/steem 艾伯特 http://www.aibbt.com/a/14027.html 转载来源：被点赞就能挣钱？基于区块链的社交媒体平台 Steemit - CSDN博客]]></content>
  </entry>
  <entry>
    <title><![CDATA[为什么“兴趣广泛”的通才更有可能获得成功？详解通才的7大优势]]></title>
    <url>%2F2018%2F8e15e6e3%2F</url>
    <content type="text"><![CDATA[为什么“兴趣广泛”的通才更有可能获得成功？详解通才的7大优势 转载来源：为什么“兴趣广泛”的通才更有可能获得成功？详解通才的7大优势]]></content>
  </entry>
  <entry>
    <title><![CDATA[扒一扒装修行业的骗局（附实用装修建议）]]></title>
    <url>%2F2018%2F7a75ebea%2F</url>
    <content type="text"><![CDATA[扒一扒装修行业的骗局（附实用装修建议） 转载来源：扒一扒装修行业的骗局（附实用装修建议）]]></content>
      <tags>
        <tag>缓缓说</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[自动驾驶安全员究竟都干什么？我今天去百度Apollo体验了一天]]></title>
    <url>%2F2018%2Feae7da24%2F</url>
    <content type="text"><![CDATA[自动驾驶安全如今又一次引发了社会关注，随着自动驾驶技术的发展，越来越多的公司将自家的自动驾驶汽车从实验室搬到了公共道路上。 （今天我到百度公司当一天“自动驾驶汽车安全员”） 自动驾驶安全员作为自动驾驶汽车上最关键的监察角色，有着十分重要的地位。可真正的自动驾驶安全员的日常生活究竟是什么样的？我们在中国最成体系和规模的自动驾驶研发团队百度 Apollo 待了一天。 这是一份具有“人道主义”的高尚工作 “我之前是个军人，曾经获得汽车大比武三等功，与自动驾驶汽车相处我认为是一个逐渐相互信任的过程。”——百度自动驾驶汽车安全员靳师傅。 （靳师傅参与 Apollo 自动驾驶汽车项目已经500多天了） “刚开始那会儿甭提多晕了，研发上车没20分钟就得吐。”靳师傅打趣到，百度算是最早开始研发自动驾驶并准备上路测试的公司。 在项目开始测试时，自动驾驶汽车就像一个孩子，在街上慢慢地走着，看到路口小心翼翼地打转向。而安全员就像个大家长，关键时刻拉一把。随着技术的进步，百度自动驾驶汽车需要的介入越来越少，如今安全员与自动驾驶汽车实现了某种默契， 作为自动驾驶汽车第一个乘客，汽车安全员的“车品”很重要。 百度自动驾驶汽车能做到如此自然的体感也是拜他们所赐。“佛系”的驾驶习惯更容易让自动驾驶汽车体验像有司机驾驶的。在自动驾驶汽车上路前，安全员却又是自动驾驶汽车研发时的最后屏障，“责任感”是安全员一直强调的理念。 既是自动驾驶汽车第一名乘客， 又是最后一道屏障 在百度自动驾驶部门中有一个不成文的守则，不管人还是机器，在职业上做必须做到100%尽责，百度作为站在世界头部的自动驾驶研发公司，从规范、布局和远景上都有相当全面的考虑，安全员除了驾驶水平过硬外，还要从技术上尽可能杜绝安全漏洞，把问题反馈给研发测试团队。 这时，安全员将不再是汽车的家长，而是最大的产品经理和“产品体验官”，所以你会经常看到汽车安全员指责研发慢，或用小白板给产品经理上课的一幕。 （北京共发布了五张自动驾驶测试实验用临时牌照，百度 Apollo 有五张） 前不久，北京市发布了首批自动驾驶测试实验用临时牌照，该临时牌照公分为T1-T5五个级别，百度拿到的五张T3牌照是国内目前最高级别。 安全员将自动驾驶汽车人为介入的原因划分为四大模块，即策略缺陷、系统故障、规则政策、人为原因。任何人为的记录都会按照动机、路面状况等原因进行归纳总结，再反馈给研发和产品。他既为自己的生命负责也为汽车负责，更为整个公共道路的行人、车辆负责。 成为一名汽车安全员需要经历非常严格的考验，根据百度相关人员的介绍，大概要分为五个阶段： 1、司机考察期； 2、开环数据采集司机； 3、开环研发车司机； 4、闭环研发车司机； 5、自动驾驶汽车安全员。 司机入职，除了法律法规测试之外还要经过一定公里数的上路测试。司机行车时不仅要严格遵守法律法规不能出错，还要具备紧急停车、紧急避让等高难度车技。我们采访到一位曾担任某世界顶尖车企的测试师，在他看来，百度自动驾驶安全员的考核相当全面且职业。 当司机通过初轮考察后便成为开环数据采集车或者开环研发车的实习司机，这些测试司机们要学习自动驾驶相关知识，协助研发、产品进行相关数据的收集和测试。 当成为闭环研发司机阶段时，证明司机已经具备了稳定的驾驶技术和相当的自动驾驶业务能力，可进一步深入接触自动驾驶汽车的测试，在这个阶段司机逐渐会将控制方向盘的权限交给车辆本身，成为一名观察者。这一阶段的司机要适应既控制车还要需要时刻关注周边的道路情况这一全新的状态。 最后成为一名合格的、了解相关自动驾驶参数且能独立上路的百度自动驾驶安全员。 说安全员不累，那是假的 “你觉得做自动驾驶安全员，累么？” “说不累那是假的，这行业跟普通司机很不一样，你需要时刻注意车辆周边的路况，尤其是过路口或者车多的时候。” 百度自动驾驶安全员李师傅说到。 自动驾驶安全员按照当日任务和路况一天要跑60-200公里不等，在一天的采访中发现他们基本上大部分的时间都在车上工作。在工作时，安全员一方面需要集中精神观察测试车辆附近的交通路况，还要将脚放在刹车上，提防车辆自己的“小脾气”。每次出现人工介入时，安全员都要按照严格的流程记录介入的细节和时间。 在体验时，我发现这辆自动驾驶汽车行车体验已经无限接近人类驾驶的车辆，而在此之前，这些安全员可有点受罪。 项目起步时，自动驾驶汽车行车体验很不稳定，经常会出现突然刹车，打转向等危险操作。一旦车辆出现失控情况，安全员必须时刻准备着握紧方向盘、及时将方向调正。随着研发越发深入，行车体验逐渐变好，安全员又将车辆关注的焦点由车转移到周边的人和车上。 虽然自动驾驶汽车已经发展许久，但是公共道路上的“围观群众”都对这个顶着奇怪东西的改装车有着强烈的好奇心和“误解”。在公共道路上测试自动驾驶汽车时，经常会受到其他“不明真相”群众的误解和调戏。这些司机发现这是一辆自动驾驶汽车时，总想试试这辆车的“真功夫”。 “要么离得特别近想看看里面什么样，要么就想别咱们。虽然挺无奈的，但毕竟咱们这行太前沿了，可以理解。” 李师傅说。 我不禁想起上个月 Uber 自动驾驶汽车的事故，根据录像显示，车辆在自动驾驶模式中直接撞向了横穿马路的行人，车里的安全员看起来注意力并不是那么集中。我向百度的安全员求证后得知，长时间的工作很容易进入一种疲劳的状态，注意力及体力都有所下降。 对此，百度会给安全员充足的空间去自我调整和休息，不允许安全员在行驶时双手离开方向盘。随着设备集成度越来越高，安全员的辅助监测设备也减少到一个屏幕。百度自动驾驶官方人员表示，将会进一步优化自动驾驶安全员的行车体验，同时也为安全员上了高额保险。 （百度 Apollo 自动驾驶测试车） 这是一个全新的新旧观念碰撞的职业 虽然自动驾驶安全员目前只出现于自动驾驶行业的特殊工种，但它一定会在不远的未来高度爆发。随着自动驾驶行业的稳步推进，关于“自动驾驶技术的量产”议题也提上了日程。 在可预见的两年内，封闭道路的自动驾驶汽车一定会实现普及，此时安全员将成为一个关乎乘客性命、车辆安全的重要角色。职业的标准化也将随着行业的普及形成一个具象形态。安全员具体的职能一方面离不开百度、谷歌这些头部队伍对自动驾驶上路的思考，另一方面还需要安全员从事人员自身的责任感和热情。 从我这一天与安全员的体验与沟通中，我发现安全员本身都很积极拥抱自动驾驶技术，对工作有着极大热情。自动驾驶汽车就像他们的孩子一样，一点点的成长和优化都能传达到安全员的体验中。也正是这些前线的安全员不断付出和努力，才让自动驾驶汽车愈发有人性。 转载来源：自动驾驶安全员究竟都干什么？我今天去百度Apollo体验了一天]]></content>
      <categories>
        <category>汽车</category>
      </categories>
      <tags>
        <tag>Google</tag>
        <tag>Uber</tag>
        <tag>汽车用品</tag>
        <tag>交通</tag>
        <tag>汽车产业</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【应用】利用IPFS构建自己的去中心化分布式Wiki系统 – ipfs]]></title>
    <url>%2F2018%2Fd8daf94a%2F</url>
    <content type="text"><![CDATA[【应用】利用IPFS构建自己的去中心化分布式Wiki系统 – ipfs 转载来源：【应用】利用IPFS构建自己的去中心化分布式Wiki系统 – ipfs]]></content>
  </entry>
  <entry>
    <title><![CDATA[三四线的房子，都被这些人买走了（100城数据分析）]]></title>
    <url>%2F2018%2F8f1a9488%2F</url>
    <content type="text"><![CDATA[来源：华尔街见闻（ID：wallstreetcn），更多精彩资讯请登陆wallstreetcn.com，或下载华尔街见闻APP，文中观点不构成投资建议。 2017年以来，三四线城市楼市迎来了一轮普遍可见的成交热销和价格上涨。 三四线楼市那么热，房子都被哪些人买走了？这些购买者是投资还是自住？他们是当地人还是外地人？ 近期，克而瑞研究中心（CRIC）调研100座三四线城市，对购房人群个体特征和购房偏好进行了多角度研究，见闻君为大家带来了脱水版报告。（文中数据及图片均来自 CRIC） 01 购房者来源：本地人为主 从购房客户来源来看，三四线城市一般还是以本地客户为主，本地客户平均占比超过八成，外地客户接近20%，比一二线典型城市高出9个百分点。 从区域来看，华南的三四线城市吸引的外地跨市购房投资客相对较多，外地客户占比最多，达到近三成，比全国均值高出近一成。这一方面是因为华南民间投资氛围更加浓厚，购房溢出需求更为强烈，另一方面也是因为华南区划内城市相对密集，广东、广西三线城市距离核心城市距离更近，核心城市需求外溢的距离成本相对较低。 外地购房客户占比最高的五个城市为三亚、佛山、东莞、鄂州和廊坊，三亚的外地客户占比高达5成，其余四市的外来客户占比也都在4成以上。 其中，值得一提的是湖北省鄂州市，外地客群占比达到63%，主要来自武汉。2017年鄂州房价涨幅高达31%，主要原因是顺丰机场建设利好。 除三亚等少数旅游城市以外，大多数三四线城市的外地客源还是来自于房价水平更高的一二线城市，具体购房动机方面不一而足，或是出于返乡置业需要，或是因为核心城市边界扩张、或是为了资产配置的投资性需求需要。 02 购房者年龄：35岁以上近6成 25岁至50岁的客户是三四线购房者的主力，占比最高的客户年龄段为35-40岁，占整体购房者数量的22%。 正是有更多外来成年客户的涌入，三四线购房者平均年龄更大一些， 35岁以上购房者占比达到了57%，比一二线高出3个百分点。 分区域来看，华北的购房客群年轻化较为明显，西南、东北区域客户整体购房年龄偏大。 03 购房谁做主：以男性为主转向夫妻 购房决策正在从以男性决策为主逐渐向夫妻共同决定转变，占比均为34%。 按各区域表现来看，华北男性主导占比最大，达到4成；而女性在购房决策中的占比在各区域之间差异不大。 04 购房动机：刚需客群仍是主流 在购房用途方面，整体来看仍然以首次置业、首次改善为主，基本符合目前市场“房住不炒”的主旋律，但投资需求的占比也在逐渐加大。 从各区看，华东、华南的客户投资意识显然领先于全国其他区域，西北区域市场发展相对缓慢，投资型购房的占比较低，首次置业及首次改善的客户占比高达8成。 从住宅拥有情况来看，只拥有1套房的客户占比还是最高的，占总购房客户的39%，拥有1-2套房产是常态，说明更多人仍处于刚需或者置换的阶段。 拥有房屋套数达到四套及以上客户占比最高的五个城市为遵义、北海、三亚、揭阳和莱芜。 05 资金来源：首套贷款近半 2017年的购房客户中，背负首套房贷的客户占比最高，有49%的客户使用首套贷款，其次为使用二套贷款的客户占比为31%，而全款购房的客户则较少，仅占总客户的2成左右。 二套贷款使用占比较高的区域为华东、西南和华南，均达到了35%及以上。使用二套贷款的客户越多，说明该区域客户在购房市场上更加活跃，投资性购房行为也更多。 在2017年三四线城市市场热度提高，表现优异，除了政策推动以外，三四线城市积极推行的货币化安置也起到了较大作用，甚至有3%的项目中有40%以上客户为货币化安置客户。 06 购房偏好：90-120㎡、50-100万最受欢迎 从面积来看，三四线城市的居民在购房上依旧以首购和改善类刚性需求为主， 90-120平方米的产品依旧是各区域购房者的最爱，占总比的33%。 从价格来看，各个区域三四线城市的购房者的主要偏好仍是50-100万类型的产品，其中以西北地区和华中地区为最，占比分别为51%和50%。 从总体上看，三四线城市的购房者对三房类型的产品购买意愿最为强烈，近半成（占比为48.61%）的购房者都对三房类产品感兴趣。 ▼ 欢迎转发和点赞支持见闻君 转载来源：三四线的房子，都被这些人买走了（100城数据分析）]]></content>
      <categories>
        <category>房产</category>
      </categories>
      <tags>
        <tag>投资</tag>
        <tag>购房</tag>
        <tag>CRIC</tag>
        <tag>房产</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[区块链的那些事，你知道和不知道的都在这里！-云栖社区]]></title>
    <url>%2F2018%2Fbcf5d490%2F</url>
    <content type="text"><![CDATA[区块链的那些事，你知道和不知道的都在这里！-云栖社区 转载来源：区块链的那些事，你知道和不知道的都在这里！-云栖社区]]></content>
  </entry>
  <entry>
    <title><![CDATA[textgenrnn：只需几行代码即可训练文本生成网络]]></title>
    <url>%2F2018%2F22dc3d88%2F</url>
    <content type="text"><![CDATA[本文是一个 GitHub 项目，介绍了 textgenrnn，一个基于 Keras/TensorFlow 的 Python 3 模块。只需几行代码即可训练文本生成网络。 项目地址：https://github.com/minimaxir/textgenrnn?reddit=1 通过简简单单的几行代码，使用预训练神经网络生成文本，或者在任意文本数据集上训练你自己的任意规模和复杂度的文本生成神经网络。 textgenrnn 是一个基于 Keras/TensorFlow 的 Python 3 模块，用于创建 char-rnn，具有许多很酷炫的特性： 它是一个使用注意力权重（attention-weighting）和跳跃嵌入（skip-embedding）等先进技术的现代神经网络架构，用于加速训练并提升模型质量。- 能够在字符层级和词层级上进行训练和预测。- 能够设置 RNN 的大小、层数，以及是否使用双向 RNN。- 能够对任何通用的输入文本文件进行训练。- 能够在 GPU 上训练模型，然后在 CPU 上使用这些模型。- 在 GPU 上训练时能够使用强大的 CuDNN 实现 RNN，这比标准的 LSTM 实现大大加速了训练时间。- 能够使用语境标签训练模型，能够更快地学习并在某些情况下产生更好的结果。能够在字符层级和词层级上进行训练和预测。 能够对任何通用的输入文本文件进行训练。 在 GPU 上训练时能够使用强大的 CuDNN 实现 RNN，这比标准的 LSTM 实现大大加速了训练时间。 你可以使用 textgenrnn，并且在该 Colaboratory Notebook（https://drive.google.com/file/d/1mMKGnVxirJnqDViH7BDJxFqWrsXlPSoK/view?usp=sharing）中免费使用 GPU 训练任意文本文件。 示例 [Spoiler] Anyone else find this post and their person that was a little more than I really like the Star Wars in the fire or health and posting a personal house of the 2016 Letter for the game in a report of my backyard. 该模型可以很容易地在新的文本上进行训练，甚至可以在仅仅输入一次数据之后生成合适的文本。 Project State Project Firefox 这个模型的权重比较小（占磁盘上 2 MB 的空间），它们可以很容易地被保存并加载到新的 textgenrnn 实例中。因此，你可以使用经过数百次数据输入训练的模型。（实际上，textgenrnn 的学习能力过于强大了，以至于你必须大大提高温度（Temperature）来得到有创造性的输出。） Why we got money “regular alter”Urburg to Firefox acquires Nelf Multi ShamnKubernetes by Google’s Bern 您还可以训练一个支持词级别嵌入和双向 RNN 层的新模型。 使用方法 textgenrnn 可以通过 pip 从 pypi（https://pypi.python.org/pypi/textgenrnn）中安装： 你可以在该 Jupyter Notebook（https://github.com/minimaxir/textgenrnn/blob/master/docs/textgenrnn-demo.ipynb）中查看常见的功能和配置选项的演示案例。- /datasets 包含用于训练 textgenrnn 的 Hacker News 和 Reddit data 示例数据集。- /weights 包含在上述的数据集上进一步预训练的模型，它可以被加载到 textgenrnn 中。- /output 包含从上述预训练模型中生成文本的示例。/datasets 包含用于训练 textgenrnn 的 Hacker News 和 Reddit data 示例数据集。 /output 包含从上述预训练模型中生成文本的示例。 神经网络架构及实现 textgenrnn 基于 Andrej Karpathy 的 char-rnn 项目（https://github.com/karpathy/char-rnn），并且融入了一些最新的优化，如处理非常小的文本序列的能力。 本文涉及到的预训练模型遵循 DeepMoji 的神经网络架构（https://github.com/bfelbo/DeepMoji/blob/master/deepmoji/model_def.py）的启发。对于默认的模型，textgenrnn 接受最多 40 个字符的输入，它将每个字符转换为 100 维的字符嵌入向量，并将这些向量输入到一个包含 128 个神经元的长短期记忆（LSTM）循环层中。接着，这些输出被传输至另一个包含 128 个神经元的 LSTM 中。以上所有三层都被输入到一个注意力层中，用来给最重要的时序特征赋权，并且将它们取平均（由于嵌入层和第一个 LSTM 层是通过跳跃连接与注意力层相连的，因此模型的更新可以更容易地向后传播并且防止梯度消失）。该输出被映射到最多 394 个不同字符的概率分布上，这些字符是序列中的下一个字符，包括大写字母、小写字母、标点符号和表情。（如果在新的数据集上训练一个新模型，可以配置所有上面提到的数值参数。） 或者，如果可以获得每个文本文档的语境标签，则可以在语境模式下训练模型。在这种模式下，模型会学习给定语境的文本，这样循环层就会学习到非语境化的语言。前面提到的只包含文本的路径可以借助非语境化层提升性能；总之，这比单纯使用文本训练的模型训练速度更快，且具备更好的定量和定性的模型性能。 软件包包含的模型权重是基于（通过 BigQuery）在 Reddit 上提交的成千上万的文本文档训练的，它们来自各种各样的 subreddit 板块。此外，该网络还采用了上文提到的非语境方法，从而提高训练的性能，同时减少作者的偏见。 当使用 textgenrnn 在新的文本数据集上对模型进行微调时，所有的层都会被重新训练。然而，由于原始的预训练网络最初具备鲁棒性强得多的「知识」，新的 textgenrnn 最终能够训练地更快、更准确，并且可以学习原始数据集中未出现的新关系。（例如：预训练的字符嵌入包含所有可能的现代互联网语法类型中的字符语境。） 此外，重新训练是通过基于动量的优化器和线性衰减的学习率实现的，这两种方法都可以防止梯度爆炸，并且大大降低模型在长时间训练后发散的可能性。 注意事项 即使使用经过严格训练的神经网络，你也不能每次都能得到高质量的文本。这就是使用神经网络文本生成的博文（http://aiweirdness.com/post/170685749687/candy-heart-messages-written-by-a-neural-network）或推文（https://twitter.com/botnikstudios/status/955870327652970496）通常生成大量文本，然后挑选出最好的那些再进行编辑的主要原因。 不同的数据集得到的结果差异很大。因为预训练的神经网络相对来说较小，因此它不能像上述博客展示的 RNN 那样存储大量的数据。为了获得最佳结果，请使用至少包含 2000-5000 个文档的数据集。如果数据集较小，你需要在调用训练方法和／或从头开始训练一个新模型时，通过调高 num_epochs 参数来对模型进行更长时间的训练。即便如此，目前也没有一个判断模型」好坏」的启发式方法。 你并不一定需要用 GPU 重新训练 textgenrnn，但是在 CPU 上训练花费的时间较长。如果你使用 GPU 训练，我建议你增加 batch_size 参数，获得更好的硬件利用率。 未来计划 更多正式文档；- 一个使用 tensorflow.js 的基于 web 的实现（由于网络规模小，效果特别好）；- 一种将注意力层输出可视化的方法，以查看神经网络是如何「学习」的；- 有监督的文本生成模式：允许模型显示 top n 选项，并且由用户选择生成的下一个字符/单词（https://fivethirtyeight.com/features/some-like-it-bot/）；- 一个允许将模型架构用于聊天机器人对话的模式（也许可以作为单独的项目发布）；- 对语境进行更深入的探索（语境位置 + 允许多个语境标签）；- 一个更大的预训练网络，它能容纳更长的字符序列和对语言的更深入理解，生成更好的语句；- 层次化的作用于词级别模型的 softmax 激活函数（Keras 对此有很好的支持）；- 在 Volta／TPU 上进行超高速训练的 FP16 浮点运算（Keras 对此有很好的支持）。一个使用 tensorflow.js 的基于 web 的实现（由于网络规模小，效果特别好）； 有监督的文本生成模式：允许模型显示 top n 选项，并且由用户选择生成的下一个字符/单词（https://fivethirtyeight.com/features/some-like-it-bot/）； 对语境进行更深入的探索（语境位置 + 允许多个语境标签）； 层次化的作用于词级别模型的 softmax 激活函数（Keras 对此有很好的支持）； 使用 textgenrnn 的项目 Tweet Generator：训练一个为任意数量的 Twitter 用户生成推文而优化的神经网络。 转载来源：textgenrnn：只需几行代码即可训练文本生成网络]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>编程语言</tag>
        <tag>GitHub</tag>
        <tag>Python</tag>
        <tag>机器学习</tag>
        <tag>GPU</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[简述表征句子的3种无监督深度学习方法]]></title>
    <url>%2F2018%2F7849c7a2%2F</url>
    <content type="text"><![CDATA[本文介绍了三种用于表征句子的无监督深度学习方法：自编码器、语言模型和 Skip-Thought 向量模型，并与基线模型 Average Word2Vec 进行了对比。 近年来，由于用连续向量表示词语（而不是用稀疏的 one-hot 编码向量（Word2Vec））技术的发展，自然语言处理领域的性能获得了重大提升。 Word2Vec 示例 尽管 Word2Vec 性能不错，并且创建了很不错的语义，例如 King - Man + Woman = Queen，但是我们有时候并不在意单词的表征，而是句子的表征。 本文将介绍几个用于句子表征的无监督深度学习方法，并分享相关代码。我们将展示这些方法在特定文本分类任务中作为预处理步骤的效果。 分类任务 用来展示不同句子表征方法的数据基于从万维网抓取的 10000 篇新闻类文章。分类任务是将每篇文章归类为 10 个可能的主题之一（数据具备主题标签，所以这是一个有监督的任务）。为了便于演示，我会使用一个 logistic 回归模型，每次使用不同的预处理表征方法处理文章标题。 基线模型——Average Word2Vec 我们从一个简单的基线模型开始。我们会通过对标题单词的 Word2Vec 表征求平均来表征文章标题。正如之前提及的，Word2Vec 是一种将单词表征为向量的机器学习方法。Word2Vec 模型是通过使用浅层神经网络来预测与目标词接近的单词来训练的。你可以阅读更多内容来了解这个算法是如何运行的：http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/。 我们可以使用 Gensim 训练我们自己的 Word2Vec 模型，但是在这个例子中我们会使用一个 Google 预训练 Word2Vec 模型，它基于 Google 的新闻数据而建立。在将每一个单词表征为向量后，我们会将一个句子（文章标题）表征为其单词（向量）的均值，然后运行 logistic 回归对文章进行分类。 我们的基线 average Word2Vec 模型达到了 68% 的准确率。这很不错了，那么让我们来看一看能不能做得更好。 average Word2Vec 方法有两个弱点：它是词袋模型（bag-of-words model），与单词顺序无关，所有单词都具备相同的权重。为了进行句子表征，我们将在下面的方法中使用 RNN 架构解决这些问题。 自编码器 自编码器是一种无监督深度学习模型，它试图将自己的输入复制到输出。自编码器的技巧在于中间隐藏层的维度要低于输入数据的维度。所以这种神经网络必须以一种聪明、紧凑的方式来表征输入，以完成成功的重建。在很多情况下，使用自编码器进行特征提取被证明是非常有效的。 我们的自编码器是一个简单的序列到序列结构，由一个输入层、一个嵌入层、一个 LSTM 层，以及一个 softmax 层组成。整个结构的输入和输出都是标题，我们将使用 LSTM 的输出来表征标题。在得到自编码器的表征之后，我们将使用 logistics 回归来预测类别。为了得到更多的数据，我们会使用文章中所有句子来训练自编码器，而不是仅仅使用文章标题。 我们实现了 60% 的准确率，比基线模型要差一些。我们可能通过优化超参数、增加训练 epoch 数量或者在更多的数据上训练模型，来改进该分数。 语言模型 我们的第二个方法是训练语言模型来表征句子。语言模型描述的是某种语言中一段文本存在的概率。例如，「我喜欢吃香蕉」（I like eating bananas）这个句子会比「我喜欢吃卷积」（I like eating convolutions）这个句子具备更高的存在概率。我们通过分割 n 个单词组成的窗口以及预测文本中的下一个单词来训练语言模型。你可以在这里了解到更多基于 RNN 的语言模型的内容：http://karpathy.github.io/2015/05/21/rnn-effectiveness/。通过构建语言模型，我们理解了「新闻英语」（journalistic English）是如何建立的，并且模型应该聚焦于重要的单词及其表征。 我们的架构和自编码器的架构是类似的，但是我们只预测一个单词，而不是一个单词序列。输入将包含由新闻文章中的 20 个单词组成的窗口，标签是第 21 个单词。在训练完语言模型之后，我们将从 LSTM 的输出隐藏状态中得到标题表征，然后运行 logistics 回归模型来预测类别。 这一次我们得到了 72% 的准确率，要比基线模型好一些，那我们能否让它变得更好呢？ Skip-Thought 向量模型 在 2015 年关于 skip-thought 的论文《Skip-Thought Vectors》中，作者从语言模型中获得了同样的直觉知识。然而，在 skip-thought 中，我们并没有预测下一个单词，而是预测之前和之后的句子。这给模型关于句子的更多语境，所以，我们可以构建更好的句子表征。您可以阅读这篇博客（https://medium.com/@sanyamagarwal/my-thoughts-on-skip-thoughts-a3e773605efa），了解关于这个模型的更多信息。 skip-thought 论文中的例子（https://arxiv.org/abs/1506.06726） 我们将构造一个类似于自编码器的序列到序列结构，但是它与自编码器有两个主要的区别。第一，我们有两个 LSTM 输出层：一个用于之前的句子，一个用于下一个句子；第二，我们会在输出 LSTM 中使用教师强迫（teacher forcing）。这意味着我们不仅仅给输出 LSTM 提供了之前的隐藏状态，还提供了实际的前一个单词（可在上图和输出最后一行中查看输入）。 这一次我们达到了 74% 的准确率。这是目前得到的最佳准确率。 总结 本文中，我们介绍了三个使用 RNN 创建句子向量表征的无监督方法，并且在解决一个监督任务的过程中展现了它们的效率。自编码器的结果比我们的基线模型要差一些（这可能是因为所用的数据集相对较小的缘故）。skip-thought 向量模型语言模型都利用语境来预测句子表征，并得到了最佳结果。 能够提升我们所展示的方法性能的可用方法有：调节超参数、训练更多 epoch 次数、使用预训练嵌入矩阵、改变神经网络架构等等。理论上，这些高级的调节工作或许能够在一定程度上改变结果。但是，我认为每一个预处理方法的基本直觉知识都能使用上述分享示例实现。 转载来源：简述表征句子的3种无监督深度学习方法]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>Word</tag>
        <tag>机器学习</tag>
        <tag>Google</tag>
        <tag>镜音双子</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一段关于国产芯片和操作系统的往事]]></title>
    <url>%2F2018%2F752ebcba%2F</url>
    <content type="text"><![CDATA[一段关于国产芯片和操作系统的往事 转载来源：一段关于国产芯片和操作系统的往事]]></content>
      <tags>
        <tag>梁宁-闲花照水录</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[您要找的页面不存在 - 简书]]></title>
    <url>%2F2018%2F7ee06b97%2F</url>
    <content type="text"><![CDATA[您要找的页面不存在 - 简书 转载来源：您要找的页面不存在 - 简书]]></content>
  </entry>
  <entry>
    <title><![CDATA[任志强最新演讲万字文章：房地产可能的发展趋势（含48页PPT）]]></title>
    <url>%2F2018%2F781da653%2F</url>
    <content type="text"><![CDATA[任志强最新演讲万字文章：房地产可能的发展趋势（含48页PPT） 转载来源：任志强最新演讲万字文章：房地产可能的发展趋势（含48页PPT）]]></content>
  </entry>
  <entry>
    <title><![CDATA[收割者已从房市股市退出，正在寻找下一片韭菜地]]></title>
    <url>%2F2018%2Fe1d370a5%2F</url>
    <content type="text"><![CDATA[收割者已从房市股市退出，正在寻找下一片韭菜地 转载来源：收割者已从房市股市退出，正在寻找下一片韭菜地]]></content>
      <tags>
        <tag>凤凰财经</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[OAuth 2.0 认证的原理与实践]]></title>
    <url>%2F2018%2Ff3555418%2F</url>
    <content type="text"><![CDATA[更多深度文章，请关注云计算频道：https://yq.aliyun.com/cloud 原文链接：https://yq.aliyun.com/articles/72652 使用 OAuth 2.0 认证的的好处是显然易见的。你只需要用同一个账号密码，就能在各个网站进行访问，而免去了在每个网站都进行注册的繁琐过程。 本文将介绍 OAuth 2.0 的原理，并基于 Spring Security 和 GitHub 账号，来演示 OAuth 2.0 的认证的过程。 什么是 OAuth 2.0 OAuth 2.0 的规范可以参考 ： RFC 6749 OAuth 是一个开放标准，允许用户让第三方应用访问该用户在某一网站上存储的私密的资源（如照片，视频，联系人列表），而无需将用户名和密码提供给第三方应用。目前，OAuth 的最新版本为 2.0 OAuth 允许用户提供一个令牌，而不是用户名和密码来访问他们存放在特定服务提供者的数据。每一个令牌授权一个特定的网站（例如，视频编辑网站)在特定的时段（例如，接下来的2小时内）内访问特定的资源（例如仅仅是某一相册中的视频）。这样，OAuth 允许用户授权第三方网站访问他们存储在另外的服务提供者上的信息，而不需要分享他们的访问许可或他们数据的所有内容。 OAuth 2.0 的核心概念 OAuth 2.0 主要有4类角色： resource owner：资源所有者，指终端的“用户”（user）- resource server：资源服务器，即服务提供商存放受保护资源。访问这些资源，需要获得访问令牌（access token）。它与认证服务器，可以是同一台服务器，也可以是不同的服务器。如果，我们访问新浪博客网站，那么如果使用新浪博客的账号来登录新浪博客网站，那么新浪博客的资源和新浪博客的认证都是同一家，可以认为是同一个服务器。如果，我们是新浪博客账号去登录了知乎，那么显然知乎的资源和新浪的认证不是一个服务器。- client：客户端，代表向受保护资源进行资源请求的第三方应用程序。- authorization server： 授权服务器， 在验证资源所有者并获得授权成功后，将发放访问令牌给客户端。 ## OAuth 2.0 的认证流程resource server：资源服务器，即服务提供商存放受保护资源。访问这些资源，需要获得访问令牌（access token）。它与认证服务器，可以是同一台服务器，也可以是不同的服务器。如果，我们访问新浪博客网站，那么如果使用新浪博客的账号来登录新浪博客网站，那么新浪博客的资源和新浪博客的认证都是同一家，可以认为是同一个服务器。如果，我们是新浪博客账号去登录了知乎，那么显然知乎的资源和新浪的认证不是一个服务器。 authorization server： 授权服务器， 在验证资源所有者并获得授权成功后，将发放访问令牌给客户端。 ## OAuth 2.0 的认证流程 认证流程如下： （A）用户打开客户端以后，客户端请求资源所有者（用户）的授权。- （B）用户同意给予客户端授权。- （C）客户端使用上一步获得的授权，向认证服务器申请访问令牌。- （D）认证服务器对客户端进行认证以后，确认无误，同意发放访问令牌。- （E）客户端使用访问令牌，向资源服务器申请获取资源。- （F）资源服务器确认令牌无误，同意向客户端开放资源。（B）用户同意给予客户端授权。 （D）认证服务器对客户端进行认证以后，确认无误，同意发放访问令牌。 （F）资源服务器确认令牌无误，同意向客户端开放资源。 其中，用户授权有四种模式： 授权码模式（authorization code）- 简化模式（implicit）- 密码模式（resource owner password credentials）- 客户端模式（client credentials）简化模式（implicit） 客户端模式（client credentials） 实践 OAuth 2.0 Talk is cheap！下面将演示代码。 本例子将通过 Gradle、Spring Boot、Spring Security、 Thymeleaf、等技术来实现一个client 以及 resource server，并 通过 GitHub来给我们的应用授权。 依赖 本项目基于Gralde 来管理依赖，读者可以自行改成 Maven 的方式： 配置 项目的核心配置如下： 包括了作为一个client 所需要大部分参数。其中 clientId 、 clientSecret 是在 GitHub 注册一个应用时生成的。如果读者不想注册应用，则可以直接用上面的配置即可。 如果要注册，则文章最后有注册流程。 项目安全的配置 安全配置上需要加上@EnableWebSecurity 、 @EnableOAuth2Client注解，来启用Web 安全认证记忆，表明这是一个OAuth 2.0 客户端 ： 使用 Spring Security，我们需要继承 org.springframework.security.config.annotation.web.configuration.WebSecurityConfigurerAdapter并重写以下 configure 方法： 上面的配置是设置了一些过过滤策略，除了静态资源以及不需要授权的页面，我们允许访问，其他的资源，都是需要授权访问。 其中，我们也设置了一个过滤器 ssoFilter，用于在 BasicAuthenticationFilter 之前进行拦截。如果拦截道的是/login，就是访问认证服务器。 资源服务器 我们写了两个控制器来提供相应的资源。 MainController.java 在index 页面，将如认证成功，将会显示一些认证信息。 UserController.java 是用来模拟用户管理的相关资源。 前端页面 页面，我主要是采用 Thymeleaf 以及Bootstrap 来编写的。 首页用于现实用户的基本信息。 用户管理界面显示用户的列表： 运行效果 这个是没有授权访问首页： 当我们点击登录，会重定向到 GitHub，登录界面并进行授权： 这个是授权后的首页： 授权后就能够进入用户管理界面： 注册GitHub 应用 如果需要注册，请看下面的流程，来生成 Client ID 和 Client Secret 访问https://github.com/settings/applications/new 注册应用，生成 客户端 id 和 密码。比如： 客户端 id 和 密码写入程序配置即可。 源码 《Spring Security 教程》：https://github.com/waylau/spring-security-tutorial 转载来源：OAuth 2.0 认证的原理与实践]]></content>
      <categories>
        <category>科技</category>
      </categories>
      <tags>
        <tag>GitHub</tag>
        <tag>科技</tag>
        <tag>文章</tag>
        <tag>Gradle</tag>
        <tag>新浪</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[当美团滴滴激战正酣，他却和VIPKID深谈了一次，未来百亿美元教育公司长啥样？]]></title>
    <url>%2F2018%2Ffa8a7c50%2F</url>
    <content type="text"><![CDATA[2018年4月，零界·新经济100人2018年CEO峰会在北京举行，讨论关于中国的创业者和投资者们都面临时代的变革，技术、资本、用户的变化将加速各领域的融合和统一，文本为VIPKID创始人及CEO米雯娟，新经济100人创始人兼CEO李志刚一场有关新教育改变未来的对话。 李志刚：这次跟我聊的嘉宾，热爱教育，年龄比我小10岁，做教育已经二十年了。2014年春节后，我第一次遇到她，她刚刚创办她的公司，才两个员工。从2013年到2015年，我大概看了一百家在线教育公司，认识她1年8个月后，2015年10月，我选择第一家在线教育公司做，就是VIPKID创始人米雯娟。 米雯娟：我先问你两个问题，第一个是过去你采访了两百多家公司，教育类占5%，为什么？ 李志刚：首先从个人角度来说，我是自学成才的，对教育充满渴望，我父亲也是一位中学语文老师，从小受教育的熏陶。 第二个原因是在商言商，中国有13亿人口，中国传统文化注重教育，注重出人头地、望子成龙。教育是很大的行业，现金流非常好，所以我一直关注这个领域。 李志刚米雯娟：的确，教育在全球是6万亿美元的行业。2014年，这么多互联网教育创业的公司，你为什么选择了VIPKID？当时我们的团队非常惊讶和惊喜。 李志刚：2014年，很多在线教育公司融了1亿美元，比VIPKID高的有一二十家，为什么选择你？我觉得核心原因有两个，第一是赛道的选择。教育行业我认为只有在大的IP里才会出现百亿美金公司的机会，第一是K12，第二是英语。同时后者还伴随着全球化的激励。第二是对人的选择。刚才我说过了，我认识你1年8个月才做，之前也见面过几次。其实，2015年10月我做VIPKID的文章，我对小米的评价是：专注、执行力强、自带鸡血。 米雯娟：您还预测了我们会做中文项目，我们真做了Lingo Bus。 李志刚：是的，当时我们边走路边去中关村地铁，我觉得为什么会做中文，因为中国经济在2028年左右会超过美国，成为全球第一大经济实体，一定会伴随着一个重要的历史机遇，是什么呢？像两三百年前伴随的是英文的全球化，未来世界各地和中国做生意，首先要了解中国，了解中文。这对教育公司来讲是一个机会，让全球少年学中文，这是我三年前建立的逻辑。 最终，这件事是你干起来的。短短三年时间，你为什么做成了这么一家公司？你经历了哪些重要时刻？ 米雯娟：过去三年是VIPKID推向市场做产品增长、国际化发展的三年。在这之前我们花了一年半时间来打磨产品。 我坚信小朋友在线上学习没问题的，孩子们享受探索的天性和在线教育的特点可以很好地结合在一起。三年前我们推向市场的时候，少儿英语在线的比例基本等于0%，到现在所有人想做在线少儿英语的时候，比例已经超过10%。未来三五年线上比例会达到百分之三五十。 当时和你聊的时候，你还觉得我们是第一个吃螃蟹的人：8岁以下的孩子怎么在线学习？所以在找老师的时候，我们确保找到全球最好的老师。当时还有一个更便捷的方式是在国内找外教。但对VIPKID来说，我们选择在国外寻找外教，而且坚持只找最优质的北美老师。 米雯娟李志刚：4万名北美老师，老师是很个性化的，从公司角度来看，希望变成标准化的，你怎么做？ 米雯娟：个性化和标准化是完全协同的。我们的标准化体现在知识体系上，教学要求上。我们要求老师一定要采取10种方式来鼓励孩子学习；缺席七堂课，账号就被关掉了。 李志刚：未来是五万，甚至5年后是十万老师，人很难管理，更何况是中外文化差异，如何来管理10万名老师？ 米雯娟：特别简单，以学生为中心，以拼搏者为本。拼搏者包括老师。我们所有问题是以学生为中心，然后我们去服务好我们的老师。举个例子，有一次，我们告诉老师，希望他们圣诞节也来上课，我们会安排一些激励。但是激励规则没有讲清楚，当时老师们认为我应该有这个激励。当时，我们面临的损失是100万美金。 公司很小，这笔钱很多。当时团队讨论完之后，决定这笔损失我们认，因为是我们没讲清楚。 有的老师就留言说，没想VIPKID能够这样做，我要在你们这里教学一辈子。这种信任感，是共享经济中非常重要的要点。 李志刚：昨天谈到重公司，第一是供应链，在你这里就是老师。第二是劳动密集型，正好你也有。第三点，技术驱动。你在现在、未来，有没有在这方面做一些工作？ 米雯娟：VIPKID每个月积攒压缩后有超过100TB的数据，利用大数据、人工智能等技术，我们已经初步建立起了个性化学习路径，在教、学、评、练、测环节形成了“数据+算法+场景”的闭环。一方面我们持续赋能教师，从教师管理、教学辅助、教学内容三方面，帮助老师提升教学效能。另一方面，为孩子提供个性化的学习路径，学习系统可以利用人脸识别技术实时反馈学生的上课数据，如孩子发音、孩子情绪等，量化孩子的上课表现，从而及时给予老师教学指导意见，为学生打造真正个性化的、高效的学习课堂。在“测”和“评”上，我们同样做了很多工作帮助小朋友提升学习效果。 我们每个月新增数据非常大，这是新教育的特点，传统教育是出版社出版教材，一年培训一次老师，搜集意见再改进。我们可以做到以周来迭代内容，随时有几万个老师给予反馈。 米雯娟：你对教育未来发展趋势怎么看？ 李志刚：在中国首先还是K12，另外一块是素质教育。未来中产家庭越来越多，除了成绩以外，会关注未来孩子探索更多的世界。另外就是技术驱动教育，通过技术给学生提供更好的学习教育。 米雯娟：构建全球云端的K12大课堂，让最合适的老师帮助到地球每个角落的小朋友学习成长。未来我们最大的梦想不是做一个成功的线上儿童教育机构，更重要的是站在这样一个历史时间点，一个是互联网教育被初步认证，孩子们特别喜欢，另一个是中国发展走向全球，是不是有一个国际化的中国教育品牌去帮助孩子们去学习？我觉得对于教育来讲，在技术非常多的介入之前，始终没有解决的问题是教育的均衡和普惠。 均衡讲的是资源的连接，一个在北京非常优秀的语文老师，有没有可能帮助在伦敦定居的小朋友学中文。那同样为什么不能够让我们在山区的小朋友们，都能够接受非常高质量的英文教育。 所以在均衡这件事情来讲，我们其实做了一些事情，做乡村英语教育，我们在2017年就做了100家学校，2018年我们要做1000家学校，进入到学校里面，给孩子们开设免费的在线外教课。将来我们做1万家。 李志刚：以五年时间为基础，VIPKID有五万的老师群体，未来可能是10万。我认为，肯定需要技术驱动，老师的业务模式标准化。同时，通过技术捕捉学生信息，他的表情、他的态度、他的效果，给予更好的学习效率。这应该是VIPKID五年内应该干的。 五到十年后，如果说我对VIPKID有什么期望的话，它没有国界，不仅是教育，还包括文化、语言、人文的交流，是一个用科技支撑起来的教育、人文、科技类的公司。 本文转自新经济100人。文章为作者独立观点，不代表芥末堆立场。 转载来源：当美团滴滴激战正酣，他却和VIPKID深谈了一次，未来百亿美元教育公司长啥样？]]></content>
      <categories>
        <category>科技</category>
      </categories>
      <tags>
        <tag>经济</tag>
        <tag>在线教育</tag>
        <tag>移动互联网</tag>
        <tag>美团网</tag>
        <tag>滴滴打车</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[千人千面智能淘宝店铺背后的算法研究登陆人工智能顶级会议AAAI 2017]]></title>
    <url>%2F2018%2F4ed85306%2F</url>
    <content type="text"><![CDATA[千人千面智能淘宝店铺背后的算法研究登陆人工智能顶级会议AAAI 2017 转载来源：千人千面智能淘宝店铺背后的算法研究登陆人工智能顶级会议AAAI 2017]]></content>
      <tags>
        <tag>阿里技术</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度学习精要之CapsuleNets理论与实践（附Python代码）]]></title>
    <url>%2F2018%2F352ec6fd%2F</url>
    <content type="text"><![CDATA[摘要： 本文对胶囊网络进行了非技术性的简要概括，分析了其两个重要属性，之后针对MNIST手写体数据集上验证多层感知机、卷积神经网络以及胶囊网络的性能。 神经网络于上世纪50年代提出，直到最近十年里才得以发展迅速，正改变着我们世界的方方面面。从图像分类到自然语言处理，研究人员正在对不同领域建立深层神经网络模型并取得相关的突破性成果。但是随着深度学习的进一步发展，又面临着新的瓶颈——只对成熟网络模型进行加深加宽操作。直到最近，Hinton老爷子提出了新的概念——胶囊网络（Capsule Networks），它提高了传统方法的有效性和可理解性。 本文将讲解胶囊网络受欢迎的原因以及通过实际代码来加强和巩固对该概念的理解。 为什么胶囊网络受到这么多的关注？对于每种网络结构而言，一般用MINST手写体数据集验证其性能。对于识别数字手写体问题，即给定一个简单的灰度图，用户需要预测它所显示的数字。这是一个非结构化的数字图像识别问题，使用深度学习算法能够获得最佳性能。本文将以这个数据集测试三个深度学习模型，即：多层感知机（MLP）、卷积神经网络（CNN）以及胶囊网络（Capsule Networks）。 多层感知机（MLP） 使用Keras建立多层感知机模型，代码如下： 在经过15次迭代训练后，结果如下： 可以看到，该模型实在是简单！ 卷积神经网络（CNN） 卷积神经网络在深度学习领域应用十分广泛，表现优异。下面构建卷积神经网络模型，代码如下： 打印模型参数概要： 从上图可以发现，CNN比MLP模型更加复杂，下面看看其性能： 可以发现，CNN训练耗费的时间比较长，但其性能优异。 胶囊网络（Capsule Network） 胶囊网络的结构比CNN网络更加复杂，下面构建胶囊网络模型，代码如下： 转载来源：深度学习精要之CapsuleNets理论与实践（附Python代码）]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>编程语言</tag>
        <tag>深度学习</tag>
        <tag>Python</tag>
        <tag>机器学习</tag>
        <tag>编译器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于Sup-Separable Convolution的Seq2Seq模型 SliceNet原理解析]]></title>
    <url>%2F2018%2Fda2030de%2F</url>
    <content type="text"><![CDATA[基于Sup-Separable Convolution的Seq2Seq模型 SliceNet原理解析 转载来源：基于Sup-Separable Convolution的Seq2Seq模型 SliceNet原理解析]]></content>
  </entry>
  <entry>
    <title><![CDATA[从TPP破产到一带一路崛起，中国下了很大一盘棋！]]></title>
    <url>%2F2018%2F6f2a99aa%2F</url>
    <content type="text"><![CDATA[从TPP破产到一带一路崛起，中国下了很大一盘棋！ 转载来源：从TPP破产到一带一路崛起，中国下了很大一盘棋！]]></content>
      <tags>
        <tag>凤凰财经</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[知识｜从Github上看深度学习和自然语言处理有趣的项目]]></title>
    <url>%2F2018%2F895b955b%2F</url>
    <content type="text"><![CDATA[知识｜从Github上看深度学习和自然语言处理有趣的项目 转载来源：知识｜从Github上看深度学习和自然语言处理有趣的项目]]></content>
  </entry>
  <entry>
    <title><![CDATA[腾讯入局在线文档领域，初创公司还有机会逆袭吗？]]></title>
    <url>%2F2018%2F6bd1d4a4%2F</url>
    <content type="text"><![CDATA[摘要：4 月 18 日，腾讯正式发布在线文档工具「腾讯文档」，马化腾在朋友圈表示：这是一个意外惊喜，没有一丝丝的防备。 上周 4 月 12 日的媒体沟通会上，石墨文档正式对外公布，已于 2017 年完成近亿元的 B 轮融资，投资方为今日头条。4 月 18 日，腾讯也正式发布在线文档工具「腾讯文档」，马化腾在朋友圈表示：这是一个意外惊喜，没有一丝丝的防备。 腾讯文档入局，更「意外」的应该是刚刚站稳脚跟的同类产品。此前国内并没有成熟的在线文档产品，初创公司最大的优势之一就是出发早，而腾讯入局无疑会加速这一领域的竞争。 腾讯是带着资源、野心来的，在线协作工具需要的社交场景腾讯已经具备，各种资源、能力的接入也比创业团队更有优势。当然，有优势并不代表一定会成功。总体来看，在线文档在国内还处于起步阶段，腾讯这样的大玩家加入，更大的意义在于普及作用。 我们为什么要用在线文档？ 在线文档并不是一个新概念。国外的典型产品是 Google Docs，2007 年发布，至今已经有十年。2016 年，Google Docs 的活跃用户就超过了 3 亿，企业用户达 500 万，年收入超过 10 亿美元。在传统办公领域占据领导地位的微软，也在 2011 年推出了云端的 Office 365。从理论上讲，微软 Office 在全球的十几亿用户都是在线文档的潜在用户，市场潜力十分巨大。 在腾讯文档发布之前，国内并没有互联网巨头直接参与到在线文档的竞争，不过大家对这一市场已经早有关注。今日头条投资了石墨文档、金山 WPS 投资了一起写，就连微软也针对微信平台推出了 Office 365 微助理。 大家已经都意识到这是一片巨大的蓝海，但参与方式有所不同，这跟各家的基因有关。金山 WPS、微软 Office 本来就在做传统办公软件，Google 一直在布局自己的软件生态，腾讯则是以社交为切入点。 先不谈各自的目的，在线文档本身就是一种产品进化的产物。首先它是在线的，不需要额外下载软件，在浏览器就能使用，抛弃了本地文件，文件的管理更方便。第二，在线文档让协作和分享变得更自由，实时编辑、实时修改，减少了本地文件繁多造成的失误。第三，很重要的一点，在线文档比本地文件更安全，设备损坏、被偷，文件都不会丢失。尤其是涉及多人协作的时候，文档的查看、编辑权限都由用户自己控制。 石墨文档的创始人吴洁举过这样一个例子，现在任何一家企业员工离职 5 分钟之内就可以把文件资料全都拷走，而云端的文件批量下载时，是可以通过技术手段切断的，拷走的内容也可以打上水印。所以当企业员工统一用在线文档工具时，内部资料很安全，离职时也能回收，并且不会影响自己的个人资料。 腾讯做在线文档的优势 其实从目前的产品来看，腾讯文档和同类软件并没有太大区别，上面说的功能大家也几乎都有，那么腾讯做这件事的优势在哪里呢？ 石墨文档的吴洁曾提到，中国的创业公司做在线文档，是有一定技术壁垒的。国外把 Office 三件套做全的公司只有微软和谷歌，不算 PPT 的话还有 Quip，而 Quip 的创始人是 Facebook 的前 CTO，团队是硅谷最厉害的一批工程师。抛去这样的背景不讲，想做 Office 是很难的。在中国做在线文档，需要坚定的决心、长时间的投入，和关键的人才。 这些挑战对创业公司来说比较困难，而由腾讯来做，起步的优势、运营能力、未来的发展都有了很多的可能性。 腾讯文档的发布并不是临时起意，其实最早用户用 QQ 传文件，这就是一种办公的雏形。直到现在，QQ 上还有将近两亿的文档活跃用户，日均文件传输量超过 1.8 亿次。2017 年 4 月，腾讯推出主打办公的 TIM 客户端，进一步解决用户的办公问题，目前月活用户也已经突破两千万，内置的在线文档规模超过 1300 万。这次推出腾讯文档，腾讯有更大的野心，就是通过文档这样一个支点，去撬动办公背后一个足够支撑千亿市值的巨大市场。 腾讯的基因是社交，而在线文档又需要多人协同，所以微信、QQ 的关系链就成了腾讯文档最大的优势。腾讯文档的服务除了支持网页端、移动端两大平台，还和微信、QQ 全面打通。QQ 传输的文档可以一键转为在线文档进行编辑以及预览，微信上则有小程序的版本支持。比起其他同类产品的分享，腾讯文档不管是在查看、编辑体验，还是账号登录、分享上，都要便捷许多。 作为国内互联网的巨头，腾讯产品间的资源互补形成了一个庞大的生态，拿已经加入的翻译功能来说，接入的是「翻译君」的能力，类似的天气、地理等数据，都可以在闭环系统中提供。在线文档是核心，关系链是纽带。未来腾讯的 QQ 邮箱、多人语音都可以与在线文档产生融合，如果能达到这样的预期，腾讯完全可以自己搞出一套办公领域的产品矩阵。 对初创公司是好事还是坏事？ 对这个领域的初创公司来说，腾讯的入局是好事还是坏事？这很难说。正如前面提到的，两者目前在功能上并看不出太大区别，甚至石墨的某些细节功能、设计要做得更好。所以前期腾讯文档能带来的，一定是把整个在线文档的市场扩大，将更多潜在用户带入这个全新的领域。至于以后，与在线文档相近的笔记、协作类领域都诞生过独角兽，机会还是有的。只是腾讯文档一出现，初创公司的节奏就不得不加快了，整个产业会进入快速竞争的时期，玩家也会越来越多。 从目前国内的在线文档市场来看，现在各家抢的不是对方的市场，而是未开垦的潜在用户，腾讯十亿的用户无疑是个巨大的压力。据石墨文档的吴洁说，目前他们的盈利还主要靠在个人收费版和企业版上，内部也在推出 SDK 等一些商业化的产品。盈利对初创公司来说永远是个紧箍咒，相比之下腾讯文档少了很多束缚。 现在去谈谁是最后的赢家还很远，可以预见的是在线文档领域会涌入更多的玩家。好在在线文档还是个重体验的产品，所以虽然腾讯入局，机会也还是有的，未来这个领域会有更多的可能。 编辑：Rubberso ■ 转载来源：腾讯入局在线文档领域，初创公司还有机会逆袭吗？]]></content>
      <categories>
        <category>科技</category>
      </categories>
      <tags>
        <tag>创业</tag>
        <tag>Google</tag>
        <tag>移动互联网</tag>
        <tag>微软</tag>
        <tag>云计算</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kaggle百万美元大赛优胜者：用CNN识别CT图像检测肺癌]]></title>
    <url>%2F2018%2F37cd7d9a%2F</url>
    <content type="text"><![CDATA[王小新 编译自GitHub 量子位 出品 | 公众号 QbitAI 今年，Kaggle网站举办了一场用肺部CT图像进行肺癌检测的比赛Data Science Bowl 2017，提供百万美元奖金池。美国国家癌症研究所为比赛提供了高分辨率的肺部CT图像，在比赛中，参赛者根据给定的一组病人肺部CT三维图像，预测癌症风险。 Julian de Wit和Daniel Hammack合作完成的解决方案获得了比赛的第二名。Wit最近写了一篇博客来介绍他们的方案。他们通过3D卷积神经网络，来构建结节探测器，预测患癌可能性。Wit在64位的Windows10系统下，结合TensorFlow 0.12.0和Keras库实现该网络模型。 BTW，第2名的奖金是20万美元。 以下内容编译自Wit的文章： 初步了解和制定计划在决定参赛之前，我观看了一个Bram van Ginneken关于肺部CT图像的介绍视频，有了基本的了解。我试图直接观察一些CT扫描样本，发现这是一个很难的问题，难度与大海捞针相当。视频中提到，图像的信噪比大约为1:1000。论坛中的一些讨论也提到，神经网络不能直接从这些原始图像中学习到有用信息。目前只有1300个训练样本及对应的癌症标签，这与网络提取出的图像实际特征相差甚远。 我们希望获得更高信噪比的训练集，或是找到标签和图像特征之间更直接的关系，来训练神经网络。幸运的是，比赛组织者指出，可以借鉴一个先前举办的比赛LUNA16。在LUNA16数据集中，医生为800多个病人CT图像中精心标记了1000多个肺结节。当然，LUNA16比赛也提供没有标记结节的数据集。 因此，你可以从整体CT图像中的标记周围裁剪出小型3D图像块，最终可以用更小的3D图像块与结节标记直接对应。结节大小是癌症的一个影响因素，数据集也说明了结节的大小，所以我认为这是一个有用的信息。 图1：方法网络示意图 我还注意到LUNA16数据集是由另一个公开数据集LIDC-IDRI转化过来的。在原始数据集中，医生不仅要检测结节，而且还评估了结节的恶性程度和其他指标。我们发现，恶性程度是评估患癌风险的最佳指标，也是神经网络可以学习的。 最终的计划方案是训练一个神经网络来检测结节，并评估结节的恶性程度。在预测时，网络通过滑动窗口来遍历整体CT图像，分别判断每个滑动窗口的区域包含恶性信息的可能性。最后基于这种信息和其他特征，估计该患者发展成癌症的可能性。 数据预处理和创建训练集在预处理中，要使扫描图像的尺度尽可能一致。我首先重新缩放了CT图像，使每个像素点只表示1x1x1毫米的体积。我们也尝试了一些其他尺度的实验，最终确定了采用1毫米的尺度，这能很好地平衡计算精度和计算负荷。对于CT图像，像素强度可以用Hounsfield来表示，一般叫做亨氏单位。论坛里提到，要尽量降低像素强度，即最大化hounsfield值，然后归一化处理。同时还要确保所有CT扫描都具有相同的方向，因为CT图像旋转超过45度，意味着在图像采集过程中出现错误。 极大部分关于结节检测的文献都是先从CT扫描图像中分离出肺组织。然而目前没有合适的分割方法，能够很好地处理隐藏在肺组织边缘周围的结节和肿块。在CT图像中，这些区域会直接被删除，更不用说使用结节探测器进行类型判定。我想要训练一个U-net网络，来更好地分割肺部。与传统的分割方法相比，U-net网络能够更有效地解决实际的图像分割问题，在同为Kaggle举办的卫星图像分割比赛中被广泛地使用。当我观察这些CT图像时，我认为可以通过肺组织的边缘，构建框架来找到肺结节。这么做可能是有用的，最后我决定对原始图像进行训练和预测。在调整训练数据后，该网络效果不错，似乎没有负面影响。 在这个竞赛中，给定了训练数据，可能没有很大的发挥空间。然而处理训练集是必须的，但不是最重要的那部分工作。我使用样本中的标签，自动生成训练集的标签，也采用主动学习方法，添加部分人工标记。以下是带有标记的不同数据集。 表1：标记后的训练集 LIDC数据集中被正面标记的数量是LUNA16数据集样本数的五倍。因为这些标记是4名医生的综合注释，所以一个结节可能被标记了4次。但LUNA16也忽略了不到3名医生标注的结节。我决定在训练集中保留这些被忽视的结节，因为他们也提供了宝贵的结节恶性信息。 LUNA16 v2数据集的标签是直接从LUNA16传来，一般是多个结节检测系统错误标出的假阳性结节。要注意的是，部分结节是上面提到的不到3名医生标记的结节。我保留了这些结节标记，是为了平衡那些可疑的假阳性结节。 为了得到肺部轮廓，我需要得到非肺组织的底片。我使用了论坛中提到的简单肺分割算法，在分割掩码边缘周围进行采样标注，从而分割得到肺部组织。 在进行第一轮训练之后，我在LUNA16数据集上进行结节预测，得到了所有假阳性结节，也并入LUNA16 v2数据集中。 随着比赛的进行，我想建立第二个模型。对于这个模型，我做了放射科医生的工作，在NDSB数据集上训练网络。我手动地从癌症样本中选择明显的阳性结节，并从非癌症样本中选择假阳性结节，用这些数据训练了第二个模型。我希望效果不错，但我是一个不合格的放射科医生，实际上第二个模型比无手动标注的模型要糟糕得多。但是结合这两个模型，这个得到的融合模型比单独的模型效果更好，所以我保留了第二个模型。 我简单地建立了一个结节观察器，来调试所有的标记。在观察时，我注意到医生忽略了一些大于3cm的大结节。在LIDC数据集的说明文档中，我发现医生被要求忽略掉大于3cm的结节。我担心这些被忽视的结节区域会迷惑分类器，故删除了相重叠的底片。下面是肺部CT图像的一些截图。 图2：图像中的标记。 左上：LUNA16 v2数据，右上：非肺组织的边缘， 左下：假阳性的区域，右下：被移除的无标注区域。 3D卷积神经网络的训练方法和网络结构在一个高质量的训练集下，我们仍需要多次调整来有效地训练神经网络。数据集的两类样本量严重不平衡，正反两类样本量的比为5000：500000，且正面例子的大小和形状有很大差异。我曾考虑使用U-net网络，但2D U-net不能完全利用结节本身的三维结构信息，3D U-net网络的训练过程非常缓慢而且不灵活。我不使用U-net的主要原因是不需要建立细粒度的概率图，而只是一个粗略的检测器。在CT图像的滑动窗口中，建立小型的3D Convnet，这更加轻便和灵活。 我的第一个目标是训练一个可作为基础的结节检测器。我先要对正面例子进行过采样，将正反两类的样本比上调到1:20。为了提高模型的泛化能力，我尝试了一些图像增强操作，但是只有一些无损的操作是有用的。最后我用了大量的转化操作和所有3D翻转操作。 设计好分类器后，我想训练一个用于估计恶化程度的回归模型。将肿瘤恶化程度分为从1（很可能不是恶性）到5（很可能是恶性的）。为了强调肿瘤的恶性程度，我将标签平方，范围扩大为从1到25。最开始，我考虑了分阶段的一种方法，用第一个网络来分类节点，然后训练另一个网络估计结节的恶化程度。为了缩短计算时间，我尝试只用一个网络，以多任务学习的方法，同时进行训练这两个任务。当编程实现后，我发现这个方法简单快速，网络的效果也很好。 通常，神经网络的结构是比赛和案例研究中最重要的成果之一。对于这场比赛，我在神经网络的结构上花费的时间相对较少，因为已经有很多优秀网络可供参考。刚开始我使用了一些简单的VGG网络和Resnet网络的相似结构，但是它们的性能大致相同。然后我尝试用一个预训练好的C3D网络，原有的网络权重根本没有帮助，但直接初始化权重后，这种网络结构的效果很好。基于C3D网络进行若干次调整后，我得到最终的分类评估网络。 我首先调整了输入大小，设置为32x32x32 mm。这看起来可能太小，但是在后续的网络层中加入一些技巧，发现这种维度的实际效果很好。这个想法是保持一切轻量化，并在比赛结束后再建立一个更大输入维度的网络。但是由于Daniel的网络输入是64x64x64 mm，我决定保持目前的输入大小，使网络的输出互补。接下来我立即对z轴进行平均池化操作，使得每个体素表示2mm的区域。这进一步减少了网络的参数量，并没有影响到精度，因为在大多数扫描中，z轴会比x轴和y轴更粗糙。最后，我在网络顶部引入了64个节点的全连接层。这里，我们不是直接预测恶性肿瘤，而是通过训练图片的中级特征，输出结节的恶化程度。 表2：3D Convnet的网络结构示意图 有趣的插曲：“奇怪组织”检测器看着论坛的帖子，我发现所有的团队都在做类似的工作，我也在寻找一个能直接上手的方法。在观察CT扫描图像时，我发现了一些其他的事情。与LUNA16数据集一样，大部分的工作集中在识别肺结节上。然而，当癌症发展时，它们转变成肺肿块或更复杂的组织。并且我注意到，当扫描图像中有很多“奇怪组织”时，它发展为癌症的概率更大。此外，在很多CT图像中，我的结节探测器没有发现任何结节，这造成了一些很不好的假阴性现象。 在训练集中有10例存在上述现象，其中的5例为癌症病例。为了解决这些严重的假阴性，在扫描时，需要检测获得奇怪组织的数量。很幸运，在LUNA16数据集上包含了很多这样的样本，所以我很快对数据集进行标记并训练了一个U-net网络。加入奇怪组织检测器后，效果不错，我因此提高了本地CV值和LB上的排名。因为它对于不同的模型提升不同，很难评定实际效果，但我认为它大约提升了0.002-0.005。说实话，我认为这种改进是一个创新性的补充。以下是一些包含有奇怪组织的样本。 图3：带有奇怪组织的CT图像样本。 肺气肿基本上是由吸烟导致的，我也试图建立一个肺气肿检测器。论坛上的医生都说，当肺气肿存在时，患有癌症的概率升高。有一些简单的算法公布了如何评估CT扫描中肺气肿区域的数量，设置hounsfield单位为950，来扫描CT图像。然而，我应用这种方法后，发现效果并不好。然后我标记了一些例子来训练一个U-net，发现效果不错，但是我的本地CV值没有丝毫提升。我的猜测是，因为肺部出现问题，进行扫描得到数据集中的许多病例，因此很多肺气肿样本没有看做是肺结节和癌症病例。我不能确定这个想法的正确性。 最后一步：癌症预测训练好网络后，下一步是让神经网络检测结节并估计其恶化程度。我建立的CT结节观察器很容易查看网络结果。我觉得神经网络的效果很好：它检测到了许多我完全忽视的结节，而我只看到很少量的假阳性结节。但是还存在一个严重的问题：由于它错过了一些非常大的明显结节，所以影响了对于假阴性的得分，有时使LogLoss升高了3.00。作为尝试，我试图对CT图像进行了两次降采样，看看检测器是否会检测大结节。值得注意的是，它的效果非常好。因此，我调整了网络结构，让网络预测3个尺度，分别为1，1.5和2.0。我觉得值得花这么多的时候来改善这方面的性能。 图4：在缩放1x的左图中，没有很好地检测到大结节；但是在2x放大的右图时，效果较好。矩形的大小表示坚持到的恶性肿瘤。 鉴于这些数据和一些其他特征，我想训练一个以梯度推进的分类器来预测一年内癌症的发病率，这是比较容易实现的。但是问题在于，排行榜的得分是基于给定的200名患者，这里面意外地包含了大量异常患者。经过一些调整后，我通过交叉验证得到了本地的平均值为0.39-0.40，而排行榜得分在0.44和0.47之间变化。此时很难将排行榜得分与本地CV值相关联。提高了本地CV值可能导致LB评分的降低，反之亦然。 我花了很多时间来研究本地CV值和LB评分的关系。我没有成功，所以我只使用能同时改进CV值和LB排名的技巧和特征。这是一场两阶段的比赛，而且与实际的训练集相比，第二阶段的数据存在与LB数据集更相似的可能。在这个地方，很多队伍也只能碰运气，结果显示排行榜上的很多队伍模型处于过拟合状态。最后我只使用7个特征来训练梯度推进器，分别是3个尺度下的最大恶性结节及其Z轴的位置和样本中奇怪组织的数量。 我也融合了两个模型来提高效果。第一个模型是在完整的LUNA16数据集上训练的。第二次，我试图从NDSB训练集中选择明显的阳性样本和假阳性样本，应用主动学习来训练。由于我不是放射科医师，所以我为了保险起见，只选择癌症病例的阳性例子和非癌症病例的阴性例子。我做错了，因为第二个模型比没有额外标注的LUNA16模型更糟糕。通过平均两个模型的输出，对LB排名有了很好的推动作用，并且显著提高了本地CV值。 与Daniel合作在进行机器学习比赛时，将不同角度的解决方案组合在一起往往是个好主意。我和Daniel在以前的医疗比赛中一起合作过，知道他是一个非常聪明的人，且他的参赛思路一般和我不同。他是从研究的角度来看问题，而我一般以工程的角度来看问题。当我们合作时，我们确信结合两者互补的方法，能有一个很棒的解决方案。 我们这次组队，一开始就发现两个人对LIDC数据集中的恶性信息有完全相同的观察角度，解决方案也很相似，感到有点失望。不过幸运的是，剩余部分的设计方法完全不同，结合后显著改进了LB排名和本地CV值。下面列举出一些主要的区别。 表3：Julian和Daniel之间设计方法的差异 强强联合是一个很好的选择。虽然我因为LB得分感到担忧，但Daniel觉得应该主要关注本地CV值。所以最终我减少研究本地CV值和LB的匹配关系，并着重于改进本地CV值。在最后的排行榜上，证明这是一个很好的决定，因为在最后，第二阶段的排行榜与本地CV值相当匹配，我们获得了比赛的第二名。尽管有许多队伍，在第一阶段取得了很好的排行榜得分，后来被证明模型过拟合。 总结与感想我们在观察网络对CT图像的结节检测时，模型效果很好。第一阶段，logloss为0.43，公开排行榜的ROC准确率为0.85，第二阶段，logloss为0.40，私人数据集的ROC准确率更高。这让我很兴奋，因为在这个数据集上，我们的模型已经是一位训练有素的放射科医生了。 对于放射科医师来说，这个自动结节检测的模型可能很有帮助，因为在实际判断中，部分结节容易被忽视。模型对肿瘤恶化程度的评估效果也很好，但训练样本量只有1000个，所以应该有很大的改进空间。 在Daniel和我合作的解决方案中，应用了相当多的工程办法，许多步骤和决定是基于经验和直觉来确定的。我们没有足够的时间来准确地验证所有方法的效果。下面提出进一步研究的一些建议。 1. 建立放射科医师基准线。根据一个放射学家在这个数据集上的具体表现，建立一个具有参考意义的基准。 2. 对NDSB数据集的恶性肿瘤标注。在这场比赛中，训练样本只有约1000个结节。输入更多精确标记的例子，肯定进一步提升算法准确度。 3. 尝试更多不同的神经网络结构。我花了很少时间来选择效果最佳的网络结构，可能会错过一些效果更好的结构。 相关资源这次比赛的Kaggle地址（含说明、数据集等）：https://www.kaggle.com/c/data-science-bowl-2017/ 文中提到的另一个比赛LUNA16：https://luna16.grand-challenge.org/ 该项目的完整程序请查看GitHub链接：https://github.com/juliandewit/kaggle_ndsb2017 Dan Hammack也公布了他的代码：https://github.com/dhammack/DSB2017/ （完） 招聘 量子位正在招募编辑记者、运营、产品等岗位，工作地点在北京中关村。相关细节，请在公众号对话界面，回复：“招聘”。 One More Thing… 今天AI界还有哪些事值得关注？在量子位（QbitAI）公众号会话界面回复“今天”，看我们全网搜罗的AI行业和研究动态。笔芯~ 转载来源：Kaggle百万美元大赛优胜者：用CNN识别CT图像检测肺癌]]></content>
      <categories>
        <category>科技</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>肺癌</tag>
        <tag>癌症</tag>
        <tag>Kaggle</tag>
        <tag>肺气肿</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[乐视是庞氏骗局？贾跃亭说：说这话的人不是黑手就是SB；孙宏斌说：老贾都用自己的钱，哪有这种骗局？]]></title>
    <url>%2F2018%2Ff89931ec%2F</url>
    <content type="text"><![CDATA[乐视是庞氏骗局？贾跃亭说：说这话的人不是黑手就是SB；孙宏斌说：老贾都用自己的钱，哪有这种骗局？ 转载来源：乐视是庞氏骗局？贾跃亭说：说这话的人不是黑手就是SB；孙宏斌说：老贾都用自己的钱，哪有这种骗局？]]></content>
      <tags>
        <tag>虎嗅网</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[张一鸣：为什么BAT挖不走我们的人才？]]></title>
    <url>%2F2018%2Fb961fd4b%2F</url>
    <content type="text"><![CDATA[张一鸣：为什么BAT挖不走我们的人才？ 转载来源：张一鸣：为什么BAT挖不走我们的人才？]]></content>
      <tags>
        <tag>正和岛</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[解析Google最新“移动实时视频分割”技术]]></title>
    <url>%2F2018%2Fdc3b54ed%2F</url>
    <content type="text"><![CDATA[视频分割是一种广泛使用的技术，电影导演和视频内容创作者可以使用这种技术将场景的前景与背景分离，然后将它们作为两个不同的视觉层处理。 通过修改或替换背景，创作者可以表达特定的情绪，将他们自己放在有趣的位置或强化消息的冲击力。不过，这个操作一直以来都是一个相当耗时的手动过程（例如，艺术家需要对每一帧进行转描），或者需要利用带绿幕的摄影棚环境来实时移除背景（这种技术称为色差抠像）。为了让用户能够在取景器中创造这种效果，我们设计了一种适合手机的新技术。 我们高兴地为 YouTube 应用带来精确、实时的设备上移动视频分割，将这项技术集成到”短片故事中”。”短片故事”目前仍处于有限测试阶段，它是 YouTube 推出的一种新型轻量化视频格式，专门为 YouTube 创作者设计。我们的新分割技术让创作者可以替换和修改背景，无需专业设备即可轻松地提高视频的制作价值。 YouTube 短片故事中的神经网络视频分割 为此，我们通过机器学习利用卷积神经网络来完成语义分割任务。特别是，在考虑以下要求和约束的基础上，我们设计了一个适合手机的网络架构和训练过程： 移动解决方案应当轻量化，并且运行速度至少要比现有的最先进照片分割模型快 10-30 倍。对于实时推理，此类模型需要达到每秒 30 帧的速度。- 视频模型应利用时间冗余度（相邻的帧看起来相似）并具备时间一致性（相邻的帧得到相似的结果）。- 优质的分割结果需要优质的标注。视频模型应利用时间冗余度（相邻的帧看起来相似）并具备时间一致性（相邻的帧得到相似的结果）。 数据集我们标注了数以万计捕捉各种前景姿态和背景环境的图像，以便为我们的机器学习管道提供优质数据。标注包括头发、眼镜、脖子、皮肤和嘴唇等前景元素的准确像素位置，一般背景标签的交叉验证结果可以达到人类标注质量 98% 的交集并集比例 (IOU or Jaccard index)。 我们的数据集中一个使用九个标签仔细标注的示例图像 - 前景元素叠加到图像上 网络输入我们具体的分割任务是计算一个二进制蒙版，将视频每个输入帧（三个通道，RGB）的前景与背景分离。在所有帧中实现计算蒙版的时间一致性非常关键。目前的方法使用 LSTM 或 GRU 来实现一致性，但是对于手机上的实时应用来说，这些方法的计算开销过高。因此，我们首先将前一帧的计算蒙版作为先验知识，将它用作第四个通道与当前的 RGB 输入帧串联，以实现时间一致性，如下图所示： 原始帧（左侧）分成三个颜色通道，并与上一个蒙版（中间）串联。这将用作我们神经网络的输入来预测当前帧（右侧）的蒙版 训练过程在视频分割中，我们既需要实现帧间的时间连续性，同时还应考虑时间不连续性，例如其他人突然闯入相机视野。为了训练我们的模型可靠地处理这些用例，我们以多种方式转换每个照片的标注真实值并将它作为前一帧的蒙版： 清空前一个蒙版 - 训练神经网络正确处理场景中的第一帧和新对象。这将模拟有人出现在相机帧中的情况。- 仿射转换的真实蒙版 - 小型转换可以训练网络传播到前一帧的蒙版并进行调整。大型转换则训练网络理解不合适的蒙版并舍弃它们。- 转换后的图像 - 我们对原始图像进行薄板样条平滑，以便仿真快速的相机移动和旋转。仿射转换的真实蒙版 - 小型转换可以训练网络传播到前一帧的蒙版并进行调整。大型转换则训练网络理解不合适的蒙版并舍弃它们。 网络架构利用修正的输入/输出，我们构建了一个标准的沙漏型分割网络架构，并进行了以下改进： 1. 我们使用具有步幅为4 或更多的大卷积内核来检测高分辨率 RGB 输入帧上的对象特征。具有少量通道的层（如 RGB 输入）的卷积开销相对较低，因此，使用大内核几乎不会影响计算开销。 2. 为了提高速度，我们使用较大步幅激进地进行下采样，并结合短路连接（skip connections，例如 U-Net）在上采样期间恢复低级别特征。对于我们的分割模型，与使用无短路连接相比，这种技术将 IOU 大幅提升了 5%。 带跳过连接的沙漏型分割网络 3. 为了进一步提高速度，我们优化了默认的 ResNet 瓶颈。在这篇论文中，作者将网络中间的通道压缩了四倍（例如，使用 64 个不同的卷积内核将 256 个通道缩减为 64 个）。不过，我们注意到可以更激进地压缩 16 倍或 32 倍，并且质量没有明显下降。 ResNet 瓶颈与较大的压缩系数 4. 为了优化和提高边缘的准确性，我们在网络顶层添加了多个 DenseNet 层，其全分辨率与神经抠图相似。这种技术将整体模型质量稍微提高了 0.5% IOU，但是分割的感知质量显著提升。 以上这些修改的最终结果是，我们的网络可以在移动设备上以相当快的速度运行。在保证高准确率（在验证数据集上实现 94.8% 的 IOU）的基础上，它在 iPhone 7 上可以达到 100+ FPS 的速度，而在 Pixel 2 上则可以达到 40+ FPS 的速度，从而为 YouTube 短片故事带来各种平滑的运行和自适应效果。 我们的近期目标是通过在 YouTube “短片故事”中进行有限的分阶段发布，在第一组效果中测试我们的技术。随着我们不断改进分割技术并扩展到更多标签，我们计划将它与 Google 更广泛的增强现实服务集成。 转载来源：解析Google最新“移动实时视频分割”技术]]></content>
      <categories>
        <category>其它</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>Google</tag>
        <tag>YouTube</tag>
        <tag>iPhone</tag>
        <tag>相机</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[李彦宏给百度的AI战略开了一剂猛药 拉来陆奇能带来什么？]]></title>
    <url>%2F2018%2F521f3b6d%2F</url>
    <content type="text"><![CDATA[李彦宏给百度的AI战略开了一剂猛药 拉来陆奇能带来什么？ 转载来源：李彦宏给百度的AI战略开了一剂猛药 拉来陆奇能带来什么？]]></content>
      <tags>
        <tag>腾讯科技</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一文简述ResNet及其多种变体]]></title>
    <url>%2F2018%2F4d745c83%2F</url>
    <content type="text"><![CDATA[本文主要介绍了 ResNet 架构，简要阐述了其近期成功的原因，并介绍了一些有趣的 ResNet 变体。 在 AlexNet [1] 取得 LSVRC 2012 分类竞赛冠军之后，深度残差网络（Residual Network, 下文简写为 ResNet）[2] 可以说是过去几年中计算机视觉和深度学习领域最具开创性的工作。ResNet 使训练数百甚至数千层成为可能，且在这种情况下仍能展现出优越的性能。 因其强大的表征能力，除图像分类以外，包括目标检测和人脸识别在内的许多计算机视觉应用都得到了性能提升。 自从 2015 年 ResNet 让人们刮目相看，研究界的许多人在深入探索所其成功的秘密，许多文献中对该模型做了一些改进。本文分为两部分，第一部分为不熟悉 ResNet 的人提供一些背景知识，第二部分将介绍我最近阅读的一些论文，关于 ResNet 的不同变体和对 ResNet 架构的理解。 重新审视 ResNet 根据泛逼近定理（universal approximation theorem），只要给定足够的容量，单层的前馈网络也足以表示任何函数。但是，该层可能非常庞大，网络和数据易出现过拟合。因此，研究界普遍认为网络架构需要更多层。 自 AlexNet 以来，最先进的 CNN 架构已经越来越深。AlexNet 只有 5 个卷积层，而之后的 VGG 网络 [3] 和 GoogleNet（代号 Inception_v1）[4] 分别有 19 层和 22 层。 但是，网络的深度提升不能通过层与层的简单堆叠来实现。由于臭名昭著的梯度消失问题，深层网络很难训练。因为梯度反向传播到前面的层，重复相乘可能使梯度无穷小。结果就是，随着网络的层数更深，其性能趋于饱和，甚至开始迅速下降。 增加网络深度导致性能下降 在 ResNet 出现之前有几种方法来应对梯度消失问题，例如 [4] 在中间层添加了一个辅助损失作为额外的监督，但其中没有一种方法真正解决了这个问题。 ResNet 的核心思想是引入一个所谓的「恒等快捷连接」（identity shortcut connection），直接跳过一个或多个层，如下图所示： 残差块 ResNet 架构 [2] 的作者认为，堆叠层不应降低网络性能，因为我们可以简单地在当前网络上堆叠恒等映射（该层不做任何事情），得到的架构将执行相同的操作。这表明较深的模型所产生的训练误差不应该比较浅的模型高。他们假设让堆叠层适应残差映射比使它们直接适应所需的底层映射要容易一些。上图中的残差块明确表明，它可以做到这一点。 事实上，ResNet 并不是第一个利用快捷连接的模型，Highway Networks [5] 就引入了门控快捷连接。这些参数化的门控制流经捷径（shortcut）的信息量。类似的想法可以在长短期记忆网络（LSTM）[6] 单元中找到，它使用参数化的遗忘门控制流向下一个时间步的信息量。ResNet 可以被认为是 Highway Network 的一种特殊情况。 然而，实验结果表明 Highway Network 的性能并不比 ResNet 好，这有点奇怪。Highway Network 的解空间包含 ResNet，因此它的性能至少应该和 ResNet 一样好。这表明，保持这些「梯度高速路」（gradient highway）的畅通比获取更大的解空间更为重要。 按照这种思路，[2] 的作者改进了残差块，并提出了一种残差块的预激活变体 [7]，梯度可以在该模型中畅通无阻地通过快速连接到达之前的任意一层。事实上，使用 [2] 中的原始残差块训练一个 1202 层的 ResNet，其性能比 110 层的模型要差。 残差块的变体 [7] 的作者在其论文中通过实验表明，他们可以训练出 1001 层的深度 ResNet，且性能超越较浅层的模型。他们的训练成果卓有成效，因而 ResNet 迅速成为多种计算机视觉任务中最流行的网络架构之一。 ResNet 的最新变体以及解读 随着 ResNet 在研究界的不断普及，关于其架构的研究也在不断深入。本节首先介绍几种基于 ResNet 的新架构，然后介绍一篇论文，从 ResNet 作为小型网络集合的角度进行解读。 ResNeXt Xie et al. [8] 提出 ResNet 的一种变体 ResNeXt，它具备以下构建块： 左：[2] 中 ResNet 的构建块；右：ResNeXt 的构建块，基数=32 ResNext 看起来和 [4] 中的 Inception 模块非常相似，它们都遵循了「分割-转换-合并」的范式。不过在 ResNext 中，不同路径的输出通过相加合并，而在 [4] 中它们是深度级联（depth concatenated）的。另外一个区别是，[4] 中的每一个路径互不相同（1x1、3x3 和 5x5 卷积），而在 ResNeXt 架构中，所有的路径都遵循相同的拓扑结构。 作者在论文中引入了一个叫作「基数」（cardinality）的超参数，指独立路径的数量，这提供了一种调整模型容量的新思路。实验表明，通过扩大基数值（而不是深度或宽度），准确率得到了高效提升。作者表示，与 Inception 相比，这个全新的架构更容易适应新的数据集或任务，因为它只有一个简单的范式和一个需要调整的超参数，而 Inception 需要调整很多超参数（比如每个路径的卷积层内核大小）。 这个全新的结构有三种等价形式： 在实际操作中，「分割-变换-合并」范式通常通过「逐点分组卷积层」来完成，这个卷积层将输入的特征映射分成几组，并分别执行正常的卷积操作，其输出被深度级联，然后馈送到一个 1x1 卷积层中。 密集连接卷积神经网络 Huang 等人在论文 [9] 中提出一种新架构 DenseNet，进一步利用快捷连接，将所有层直接连接在一起。在这种新型架构中，每层的输入由所有之前层的特征映射组成，其输出将传输给每个后续层。这些特征映射通过深度级联聚合。 除了解决梯度消失问题，[8] 的作者称这个架构还支持特征重用，使得网络具备更高的参数效率。一个简单的解释是，在论文 [2] 和论文 [7] 中，恒等映射的输出被添加到下一个模块，如果两个层的特征映射有着非常不同的分布，那么这可能会阻碍信息流。因此，级联特征映射可以保留所有特征映射并增加输出的方差，从而促进特征重用。 遵循该范式，我们知道第 l 层将具有 k （l-1）+ k_0 个输入特征映射，其中 k_0 是输入图像的通道数目。作者使用一个叫作「增长率」的超参数 (k) 防止网络过宽，他们还用了一个 11 的卷积瓶颈层，在昂贵的 3*3 卷积前减少特征映射的数量。整体架构如下表所示： 用于 ImageNet 的 DenseNet 架构 深度随机的深度网络 尽管 ResNet 的强大性能在很多应用中已经得到了证实，但它存在一个显著缺点：深层网络通常需要进行数周的训练时间。因此，把它应用在实际场景的成本非常高。为了解决这个问题，G. Huang 等作者在论文 [10] 中引入了一种反直觉的方法，即在训练过程中随机丢弃一些层，测试中使用完整的网络. 作者使用残差块作为他们网络的构建块。因此在训练期间，当特定的残差块被启用，它的输入就会同时流经恒等快捷连接和权重层；否则，就只流过恒等快捷连接。训练时，每层都有一个「生存概率」，每层都有可能被随机丢弃。在测试时间内，所有的块都保持被激活状态，并根据其生存概率进行重新校准。 从形式上来看，H_l 是第 l 个残差块的输出结果，f_l 是由第 l 个残差块的加权映射所决定的映射，b_l 是一个伯努利随机变量（用 1 或 0 反映该块是否被激活）。在训练中： 当 b_l=1 时，该块为正常的残差块；当 b_l=0 时，上述公式为： 既然我们已经知道了 H_(l-1) 是 ReLU 的输出，而且这个输出结果已经是非负的，所以上述方程可简化为将输入传递到下一层的 identity 层： 令 p_l 表示是第 l 层在训练中的生存概率，在测试过程中，我们得到： 作者将线性衰减规律应用于每一层的生存概率，他们表示，由于较早的层提取的低级特征会被后面的层使用，所以不应频繁丢弃较早的层。这样，规则就变成： 其中 L 表示块的总数，因此 p_L 就是最后一个残差块的生存概率，在整个实验中 p_L 恒为 0.5。请注意，在该设置中，输入被视为第一层 (l=0)，所以第一层永远不会被丢弃。随机深度训练的整体框架如下图所示： 训练过程中，每一层都有一个生存概率 与 Dropout [11] 类似，训练随机深度的深度网络可被视为训练许多较小 ResNet 的集合。不同之处在于，上述方法随机丢弃一个层，而 Dropout 在训练中只丢弃一层中的部分隐藏单元。 实验表明，同样是训练一个 110 层的 ResNet，随机深度训练出的网络比固定深度的性能要好，同时大大减少了训练时间。这意味着 ResNet 中的一些层（路径）可能是冗余的。 作为小型网络集合的 ResNet [10] 提出一种反直觉的方法，即在训练中随机丢弃网络层，并在测试中使用完整的网络。[14] 介绍了一种更加反直觉的方法：我们实际上可以删除已训练 ResNet 的部分层，但仍然保持相对不错的性能。[14] 还用同样的方式移除 VGG 网络的部分层，其性能显著降低，这使得 ResNet 架构更加有趣。 [14] 首先介绍了一个 ResNet 的分解图来使讨论更加清晰。在我们展开网络架构之后，很明显发现，一个有着 i 个残差块的 ResNet 架构有 2**i 个不同路径（因为每个残差块提供两个独立路径）。 根据上述发现，显然移除 ResNet 架构中的部分层对其性能影响不大，因为架构具备许多独立有效的路径，在移除了部分层之后大部分路径仍然保持完整无损。相反，VGG 网络只有一条有效路径，因此移除一个层会对该层的唯一路径产生影响。（如 [14] 中的实验所揭示的。） 作者的另一个实验表明，ResNet 中不同路径的集合有类似集成的行为。他们在测试时删除不同数量的层，测试网络性能与删除层的数量是否平滑相关。结果表明，网络行为确实类似集成，如下图所示： 当被删除的层数增加时，误差值随之增长 最终，作者研究了 ResNet 中路径的特征： 很明显，路径的可能长度分布遵循二项分布，如下图 (a) 所示。大多数路径流经 19 到 35 个残差块。 为了研究路径长度与经过路径的梯度大小之间的关系，得到长度为 k 的路径的梯度大小，作者首先向网络输入了一批数据，并随机采样 k 个残差块。当梯度被反向传播时，它们在采样残差块中仅通过权重层进行传播。(b) 表明随着路径长度的增加，梯度大小迅速下降。 现在将每个路径长度的频率与其期望的梯度大小相乘，以了解每个长度的路径在训练中起到多大作用，如图 (c) 所示。令人惊讶的是，大多数贡献来自于长度为 9 到 18 的路径，但它们只占所有路径的一小部分，如 (a) 所示。这是一个非常有趣的发现，它表明 ResNet 并没有解决长路径的梯度消失问题，而是通过缩短有效路径的长度训练非常深层的 ResNet 网络。 结论 本文主要介绍了 ResNet 架构，简要阐述了其近期成功的原因，并介绍了几篇论文，它们叙述了一些有趣的 ResNet 变体，或提供了富有洞察力的解释。希望这篇文章有助于大家理解这项开创性的工作。 本文所有的图表均来自于参考文献中的原始论文。 References: [1]. A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classification with deep convolutional neural networks. In Advances in neural information processing systems,pages1097–1105,2012. [2]. K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. arXiv preprint arXiv:1512.03385,2015. [3]. K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556,2014. [4]. C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich. Going deeper with convolutions. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,pages 1–9,2015. [5]. R. Srivastava, K. Greff and J. Schmidhuber. Training Very Deep Networks. arXiv preprint arXiv:1507.06228v2,2015. [6]. S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural Comput., 9(8):1735–1780, Nov. 1997. [7]. K. He, X. Zhang, S. Ren, and J. Sun. Identity Mappings in Deep Residual Networks. arXiv preprint arXiv:1603.05027v3,2016. [8]. S. Xie, R. Girshick, P. Dollar, Z. Tu and K. He. Aggregated Residual Transformations for Deep Neural Networks. arXiv preprint arXiv:1611.05431v1,2016. [9]. G. Huang, Z. Liu, K. Q. Weinberger and L. Maaten. Densely Connected Convolutional Networks. arXiv:1608.06993v3,2016. [10]. G. Huang, Y. Sun, Z. Liu, D. Sedra and K. Q. Weinberger. Deep Networks with Stochastic Depth. arXiv:1603.09382v3,2016. [11]. N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever and R. Salakhutdinov. Dropout: A Simple Way to Prevent Neural Networks from Overfitting. The Journal of Machine Learning Research 15(1) (2014) 1929–1958. [12]. A. Veit, M. Wilber and S. Belongie. Residual Networks Behave Like Ensembles of Relatively Shallow Networks. arXiv:1605.06431v2,2016. 转载来源：一文简述ResNet及其多种变体]]></content>
      <categories>
        <category>其它</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>机器学习</tag>
        <tag>人工智能</tag>
        <tag>Pages</tag>
        <tag>盗梦空间</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大神带你分分钟超越最好结果——基于分布式CPU计算的Deeplearning4j迁移学习应用实例]]></title>
    <url>%2F2018%2Fce524a22%2F</url>
    <content type="text"><![CDATA[更多深度文章，请关注：https://yq.aliyun.com/cloud 2016年，欧莱礼媒体公司首席数据科学家罗瑞卡宣称：“2017年将是数据科学和大数据圈参与AI技术合作的一年。”在2017年之前，对基于GPU的深度学习已经渗透到大学和研究机构，但基于CPU分散式深度学习开始在不同的公司和领域得到广泛采用。虽然GPU提供了顶级的数字计算性能，但CPU也在变得更加高效，并且现有的大部分硬件已经有大量可用的CPU计算能力。另外GPU的价格比CPU的价格要相对而言贵好多，相信大家最近一阵也发现显卡的价格暴涨，这源于数字货币比特币的暴涨，而比特币是通过电脑计算得到，计算能力越强，其每天的计算量也就越多，相当于每天“挖矿”的量。涉及深度学习的研究员都应该了解一个事实，基于GPU跑一个网络和基于CPU跑同一个网络，二者的仿真速度可以达到20倍左右的差距。因此，基于CPU的分散式深度学习也会成为后续研究的一个方向。而开源工具Deeplearning4j的出现将快速深度学习扩展到Hadoop堆栈，这将是未来几年影响深度学习的主要催化剂。本文将详细介绍如何使用开源工具——Apache Spark、Apache Hadoop和Deeplearning4j（DL4J），再加上商用硬件（Commodity Hardware，便宜、被广泛使用、容易被买到），能够使用有限的训练集在图像识别任务上获得最先进的结果。 Deeplearning4j：JVM的深度学习工具集 Deeplearning4j是许多开源深度学习工具包之一，创建于2014年。DL4J集成了Hadoop和Spark，设计用于运行在分布式GPU和CPU上的商用环境。它由总部位于旧金山的商用智能和企业软件公司Skymind牵头开发。团队成员包括数据专家、深度学习专家、Java系统工程师和具有一定感知力的机器人。虽然deeplearning4j是为JVM构建的，但它使用高性能原生线性代数库Nd4j，可以对CPU或GPU进行大量优化的计算。另外使用Java编写的DL4J API对于熟悉Java虚拟机（JVM）的Java和Scala开发人员特别有吸引力。此外，Spark模型的并行训练能力使得我们轻松利用现有的群集资源来加快训练时间，而不会牺牲精度。基于Caltech-256图像数据集的对象分类 本文介绍如何使用Apache Spark、Apache Hadoop和deeplearning4j来解决图像分类问题。简单来说，就是通过构建一个卷积神经网络来对Caltech-256数据集中的图像进行分类。在Caltech-256数据集中，实际上有257个对象类别，每类数量大概是80到800个图像，该数据集总共30,607个图像。值得注意的是，该数据集上目前最先进的分类精度在72 - 75％范围内。下面我将带领大家使用DL4J和Spark轻松超越这个结果。 小数据上的有效深度学习 目前，卷积网络可以有几亿个参数，比如在大型视觉识别挑战“ImageNet”中表现最佳的神经网络之一，有1.4亿个参数需要训练！这些网络不仅需要大量的计算和存储资源（即使是使用一组GPU，也可能需要几周时间才能完成计算），而且还需要大量数据。而Caltech-256只有30000多张图像，在这个数据集上训练这样一个复杂的模型是不现实的，因为没有足够的样本来充分学习这么多参数。相反，可以采用一种迁移学习的方法来实现。简单来说，就是将已学到的知识应用到其它领域，使其能够更好地完成新领域的学习。这是因为卷积神经网络在对图像数据集进行训练时往往会学习非常普遍的特征，因此这种类型的特征学习通常对其他图像数据集也是通用的。例如，在ImageNet上训练的网络可能已经学会了如何识别形状、面部特征、图案、文本等，这无疑对于Caltech-256数据集是有用的。 加载预训练的模型 下面讲解如何使用训练好的模型来完成自己的任务，以下示例使用VGG16 模型，该模型夺得了2014 ImageNet竞赛中的亚军（网络结构及训练好的参数已公开）。由于使用了不同的图像数据集，所以需要对VGG16模型进行微小修改以适用于Caltech-256数据集预测任务。该模型具有约1.4亿个参数，大约占用500 MB空间。 首先，获取DL4J可以理解和使用的VGG16型号的版本。事实证明，这种东西是建立在DL4J的API中的，它可以通过几行Scala代码完成。 该模型采用的格式易于DL4J使用，使用内置的模型进行检查。 上面代码显示VGG16网络的结构及参数，ConvolutionLayer表示卷积层、SubsamplingLayer表示采样层、DenseLayer表示全连接层。下图简明扼要的展示了该网络结构： VGG16具有13个卷积层，中间间隔放置最大池化层以收缩图像，降低计算复杂度。卷积层中的权重实际上是过滤器，可以学习从图像中挑选出视觉特征，当使用最大池化层时，它们会“收缩”图像，这意味着后来的卷积层中的滤波器实际上提取更加抽象的特征。这样，卷积层的输出是输入图像的抽象的视觉特征，如“这个图像中有脸吗？”还是“有日落？”卷积层的输出被馈送到连续的三个全连接层，全连接层能够学习这些视觉特征与输出之间的非线性关系。 另外卷积网络的关键性质之一是允许我们进行迁移学习——可以通过已经训练好的VGG16网络传递新的图像数据，并获取每个图像的特征。一旦提取了这些特征，就只需要送人最后的预测网络就可以完成相应的任务，这在计算和复杂度上都是非常容易解决的问题。 使用VGG16进行图像特征化 数据集可以从Caltech-256 网站下载，拆分为三个数据集，分别为训练/验证/测试数据集，并存储在HDFS中。一旦完成该步骤，下一步就是将整个图像数据集传递到网络的所有卷积层和第一个全连接层，并将该输出保存到HDFS。 样做的原因是是因为卷积网络中的大多数内存占用和耗时计算都是发生在卷积层中，VGG16中的大多数参数（权重）调用发生在全连接层。迁移学习利用预先训练的卷积层来获取关于新输入图像的特征，这意味着只有原始模型的一小部分——全连接层被重新训练。其余的参数是静态不变的。通过这种操作，迁移学习可以节省大量的训练时间和计算量。 首先提取用于特征化步骤的网络部分，Deeplearning4j具有内置的迁移学习API可用于此任务。即拆分VGG16模型，在拆分之前和之后获取整个图层列表，代码如下。 现在使用org.deeplearning4j.nn迁移学习包来提取全连接“fc2”层之前（包括“fc2”层）的网络模型，如下图所示：垂线左边部分。 接下来是读取数据库中的图像文件。在这种情况下，这些文件被单独保存到HDFS作为JPEG文件。图像被组织成子目录，其中每个子目录包含属于特定类的一组图像。首先通过使用sc.binaryFiles 加载存储在HDFS中的图像，并使用DataVec库（DL4J的ETL库）中的图像处理工具将它们转换为INDArrays，这是DL4J处理的本机张量表示（此处为完整代码）。最后，使用上图中的冻结网络部分对输入图像进行特征提取，本质上是将它们传递到VGG16模型中的预测层前。 经过上述操作后，得到一个保存到HDFS中新的数据集。接下来可以开始构建使用这种特征化数据的传输学习模型，从而大大减少训练时间和计算复杂度。在上述示例中，得到的新数据集由30607个长度为4096的向量组成（这是由于VGG16模型中的全连接层“f2”维度为4096）。 替换VGG16的预测层 VGG16模型是在ImageNet数据集上进行训练的，而ImageNet数据集具有1000种不同对象类别。在典型的图像分类神经网络中，输出层的最后一层使用其输入来为数据集中的每个对象生成概率（哪一类的概率大就判断为哪一类）。因此，该输入可以被认为是关于图像的抽象视觉特征，提供关于其包含的对象的有用信息。直观地说，上述步骤生成的新数据集于Caltech-256数据集中识别对象应该是有用的。因此，定义一个新的模型，“f2”层前的模型不变，只是替换VGG16模型的最后一层预测层，将维度从原先的1000变成257，正好对应Caltech256数据集的257个类别。 直观图如下，可以看到只是改变了预测层的维度： 该模型现在已准备好使用DL4J进行大量计算，而且还使用Spark进行规模化。简单来说是切分大规模的数据集，然后将分片交给spark群集中的每个工作核心上运行SGD，最后使用Spark RDD聚合操作对每个核心上学习的不同模型进行平均，实现分布式训练。 现在针对具体的迭代次数训练SparkComputationGraph，并监控一些训练统计数据以跟踪进度。 最后，通过spark提交训练工作，然后使用DL4J webui监控进度并诊断问题。下图绘制的是模型得分与迭代次数的关系，注意到分数是minibatch的负对数似然率，分数越小，效果越好。 这次将学习率调低后，该模型似乎能比Imagenet模型能更快地学习，因为这次使用的特征比ImageNet概率更具预测性。 由于训练准确率为88.8％，但验证准确率仅为76.3％，从结果上看该模型似乎已经过拟合了。为了确保模型不会过拟合到验证集，在测试集上评估该模型。 虽然准确率有所降低，但是使用基于现有Hadoop集群和商用CPU的简单深度学习架构仍然打破了该数据集的最好结果！虽然这可能不是一个突破性的成就，但这仍然是一个令人兴奋的结果。 结论 虽然deeplearning4j只是许多深度学习可用的工具之一，但它具有本机ApacheSpark集成，并且采用Java编写，使其特别适合整个Hadoop生态系统。由于现有的企业数据已经通过Hadoop进行了大量访问，而且在Spark上进行处理，所以deeplearning4j的定位是花费更少的时间部署和减少开销，从而企业公司可以立即开始从深度学习中提取数据。它利用ND4J进行大量计算，这是一种高度优化的库，可与商用CPU配合使用，但在需要性能提升时也支持GPU。Deeplearning4j提供了一个全功能的深度学习库，具有从采集到部署的工具，可 用于各种任务，如图像/视频识别，音频处理等。 作者信息 Nisha Muktewar，数据科学家，目前就职于Cloudera的数据科学团队，专注于专业服务、售前工作。 Seth Hendrickson，以前是电气工程师，现在是数据科学家和软件工程师，研究方向是分布式机器学习。 本文由北邮@爱可可-爱生活老师推荐，阿里云云栖社区组织翻译。 文章原标题《Deep learning on Apache Spark and Apache Hadoop withDeeplearning4j | Cloudera Engineering Blog》，作者：Nisha Muktewar、Seth Hendrickson，译者：海棠，审阅： 附件为原文的pdf 文章为简译，更为详细的内容，请查看原文 转载来源：大神带你分分钟超越最好结果——基于分布式CPU计算的Deeplearning4j迁移学习应用实例]]></content>
      <categories>
        <category>科技</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>Hadoop</tag>
        <tag>CPU</tag>
        <tag>Spark</tag>
        <tag>加州理工学院</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[专访｜基于LSTM与TensorFlow Lite，kika输入法是如何造就的]]></title>
    <url>%2F2018%2F0aeec824%2F</url>
    <content type="text"><![CDATA[近日，机器之心采访了 kika 的高级技术总监黄康，他向我们讲述了 kika 开发输入法 AI 引擎（项目代号：Alps）所采用的深度学习模型以及在移动端轻量化部署遇到的各种挑战。本文从输入法与语言模型开始介绍了 kika Alps 项目的理论支持与实践挑战，并重点讨论了轻量化部署方法。 深度学习模型由于强大的表征能力在很多任务上都有非常优秀的表现，但也因为模型大小和计算量很难轻量化部署到移动端。这也是目前很多研发团队都在思考如何解决的难题。 一般在我们借助 TensorFlow、MXNet、和 Caffe2 等框架构建深度学习模型后，它在服务器训练与推断往往会有非常好的效果。但如果我们将模型部署到移动端，即使只执行推断过程，也会因为硬件条件和系统环境而遇到各种各样的问题。此外，目前关注于移动端的解决方案如 TensorFlow Mobile、TensorFlow Lite 等在一定程度上并不完善（TF Mobile 的内存管理与 TF Lite 的 Operators 的缺失），在实践中可能需要更多的修正与完善。 关注于输入法的 kika 成功地将基于循环神经网络的深度学习模型应用到安卓版的手机输入法引擎中，在克服工程化问题的情况下大大提升了输入体验：不仅使基于上下文的词预测更加准确，同时还使得词纠错功能更加强大。 在构建这样的输入法引擎过程中，kika 不仅需要考虑使用 LSTM 还是 GRU 来实现高效的语言模型，同时还需要探索如何使整个方案更轻量化以及如何快速的进行部署。本文首先介绍了输入法及 kika 所采用的语言模型，并在随后展示了 Android 移动端轻量化部署所遇到的工程化挑战。最后，本文介绍了 kika 压缩模型所采用的稀疏词表征方法与 K-means 参数量化方法，它们是轻量化部署深度学习模型的重要前提。 输入法与语言模型 输入法最重要的部分就是输入法引擎，kika 很多算法和项目都围绕它展开。一般而言，输入法引擎的输入包含两部分，即已经键入的词组和当前正在输入的词汇，前者可以视为上下文，而未完成的后者可称为键码。输入法引擎的输出是给定所有上下文和当前输入键码所『预测』的词，它也包含两部分，即当前输入词汇的补全和纠错。实现这样的功能也就是输入法最为核心的模块，kika 最开始是使用谷歌半开源的 LatinIME 来实现这样的功能，但这种基于 n-gram 的方法并不能实现顶尖的用户体验，因此经过研究与开发才有了现在基于循环神经网络（RNN）的解决方案。 输入法引擎这种给定上下文和当前键码以预测下一个词的方法其实可以看成语言建模，一般来说，语言模型旨在定义自然语言中「标记」的概率分布，这种标记可以是单词、字符甚至是字节。根据 kika 介绍，LatinIME 构建语言模型的方法是 n-gram，这种模型定义了一个条件概率分布，即给定前 n-1 个单词后第 n 个词的条件概率。因为假定当前词出现的概率只与前面 n-1 个词相关，那么 n-gram 可以使用这种条件概率的乘积来定义较长序列的概率分布： 虽然 n-gram 一直以来都是统计语言模型的核心模块，但它还是有很多局限性。对于输入法而言，较小的 n（二元语法与三元语法）不足以捕捉整个上下文信息来执行预测，而增大 n 又会使计算量成指数级增加。此外，kika 还希望引擎实现其它一些智能功能，例如根据上下文自动纠错或排序等。因此，kika 选择了更强大的循环神经网络构建语言模型。 为了构建强大的语言模型，kika 选择了长短期记忆单元（LSTM）作为网络的基础。LSTM 作为标准循环神经网络的变体在语言模型上有非常好的性能，它引入自循环的巧妙构想来更新「记忆」，若通过门控控制这样的自循环，那么累积的历史记忆或时间尺度就可以动态地改变。直观来说，LSTM 会通过门控选择需要保留的上下文信息或记忆，并用于预测当前输入的词。每当输入一个词，输入门会控制当前输入对最终预测有用的信息，而遗忘门会控制前面输入词对最终预测有用的信息，最后的输出门则会直接控制前两部分最终有效的信息。 kika 表明最开始 LSTM 只是用来实现标准的语言模型，它不会将正在输入的键码作为模型输入。这一套早期方案使用 LSTM 语言模型根据上下文预测当前词，而键码部分仍然使用传统基于 n-gram 和字典等的解决方案。后来 kika 使用一种新的策略将两部分结合在一起，因此模型不仅能接受上下文的输入，同时还能接受键码的输入。这种通用解决方案的架构如下图所示，它和一般的单词级语言模型和字符级语言模型都不一样，可以说是结合了两种架构。 如上图所示，首先 LSTM 会对前面输入的词进行建模，并输出对应的隐藏状态和记忆而作为后面字符级语言模型的先验输入。后面从 Start Flag 开始对键码实现字符级的建模而最终得出预测。 根据 kika 的解释，最后这种方案统一了两种输入。它的基本思想首先考虑到前面的 LSTM 语言模型除了要根据隐藏状态预测当前时间步的输出，同时还会向后传递这个隐藏状态。且 kika 表示若网络有良好的训练，那么这个隐藏状态是可以包含足够的语意信息，因此我们可以将它作为后面字符级 LSTM 网络的初始状态。这相当给循环神经网络一个初始量，然后再接受键码的输入而作出最终的词预测和词纠错等。 其实这里还有一个非常有意思的问题，即为什么 kika 会采用 LSTM 而不是 GRU。因为我们知道若需要将深度学习模型部署到移动端，那么我们要严格控制模型与计算量的大小，kika 所采用的稀疏词表征与参数量化等方法能有效控制模型大小，这一点将在后文展开。但 LSTM 的结构比 GRU 要复杂，门控也需要得更多，因此 LSTM 的参数会比 GRU 多，那么 kika 为什么不采用 GRU 控制参数数量？ kika 就这一点对机器之心做了详细的解答。黄康说：「在层数和单元数均一致的情况下，GRU 要比 LSTM 少一些参数和矩阵运算，因此，模型体积和训练速度方面都会有一定的优势。为了严谨的进行效果对比，我们做了两组实验。其中第一组是将 LSTM 和 GRU 的超参数设置一致，结果是： GRU 的效果明显差于 LSTM，同时，由于整体模型体积的主要贡献来源于前后两个巨大的词嵌入矩阵，模型体积方面的优势也不明显。」 但在同样超参数的情况下，GRU 的实际参数数量明显少于 LSTM。因此，kika 继续做了第二组实验，在保证基本一致的参数数量而放开网络架构约束的情况下，最后得到的结论是：LSTM 与 GRU 的模型大小基本一致，效果也基本一致，实际上，在 kika 的应用场景下，LSTM 的效果略好，但也仅仅是略好一点点。此外，由于 GRU 在当时也是比较新的结构，因此在体积和效果没有优势的情况下 kika 还是倾向于选择更温和的 LSTM，从而把主要精力用于模型结构的调整与参数调优方面。其实最近 kika 也在做一些网络架构和基本单元方面的调研，因为最近在 GRU 之后又出现了非常多训练简单且高效的单元。在 kika 当前的开发过程中也出现了一些场景更为复杂的 NLP/NLU 应用，因此也在考虑采用一些训练时间上更为友好的网络结果。对于如何选择网络结构，黄康表示：「我们内部有共识，考虑新结构有一个基本原则：我们会采用类似机器翻译的复杂任务去验证此种网络是否真实有效，才会考虑在工程上采用。」 总体而言，kika 花了很大一部分时间完成参数调优，因而能基于一体化的 LSTM 实现效果非常好的输入法引擎。当然只是构建模型还远远不够，将这种深度学习模型部署到移动端还面临着非常多的挑战，例如深度学习框架的选择和模型压缩的方法等等。 轻量化部署的工程挑战 在 kika，轻量化部署包括以下四个方面：模型压缩、快速的响应时间、较低的内存占用以及 较小的 so 库（shared object，共享库）大小等。除了优化模型的设计之外，压缩模型大小的方法主要是稀疏词表征与量化，这一部分我们将在后一部分展开讨论。 响应时间与内存是去年 kika 的工作重点，它主要是需要对 TensorFlow Mobile 和 Lite 做大量的修补。最后是动态链接库文件（.so），它定义了所有需要的运算和操作。因为整个输入法的核心代码是 C++ 完成的，而它在安卓设备中是以一个库（.so 文件）的形式存在的，它的大小直接影响了安装包的大小（由于 Android 加载 so 的机制，也会影响到内存的开销）。 针对响应时间与内存，kika 最开始是基于 TensorFlow Mobile 做一些修补和改进。黄康说：「TensorFlow Mobile 有一个非常好的优势，即它在底层使用了一套很成熟很稳定的矩阵运算库。因此，我们的主要精力放在 TensorFlow Mobile 在底层矩阵运算库之上的部分。它在矩阵运算库之间采用了很多封装与调用，但是没有考虑到很多实际工业化中遇到的问题，尤其是在内存保护这一块做得相当一般。」 TF Mobile 的内存管理与内存保护设计得并不完善，存在两个主要的问题：1. 内存保护机制不完善，在实际内存不足的情况（尤其对于一部分低端机型），容易引发内存非法操作。2. 内存大小控制机制存在明显的问题，例如模型本身在计算时只有 20MB，但加载到内存之后的运行时峰值可能会达到 40 到 70MB。据 kika 的数据，基于 TF Mobile 的解决方案大概有 1% 的场景（如游戏中调起输入法）由于内存大小限制的原因会加载不了深度学习模型，只能回退到非深度的解决方案。 2017 年 11 月，谷歌正式发布了 TensorFlow Lite，这对于移动端深度学习模型来说是非常重要的框架。在 TF Lite 开源后，kika 马上就进行了测试，并重点关注内存管理模块。黄康表示：「TF Lite 的内存管理上确实有非常大的改进，加载不了深度学习模型的场景会成百倍地减少。但它最大的问题就是 Operator 的不全，它基本上只定义了最基础的运算和操作。所以 kika 为了在内存上减少 20 多 MB 的开销，我们自行编写了大量的 Operator。但目前这个还是有一定的风险，因为如果我们修改了模型结构，那还是需要手写新的运算。」 工程化挑战最后一个问题就是动态链接库的大小，这一部分还会涉及到参数量化方法的实现，我们会在参数量化方法那边讨论。其实 TF Mobile 还有一个缺点，即它会将很多冗余的操作与运算都会打包到 .so 文件中，因此也就导致了动态链接库过大。kika 为了让 .so 文件尽可能小，开发了一套全新的工具，用于自动的判断到底哪些操作与运算的定义是模型实际需要的。 稀疏词表征 深度学习模型在输入法客户端部署的一个重要问题就是模型大小，我们需要将参数数量与计算量限制绝大部分移动设备可接受的范围内。kika 发现模型体积的主要矛盾体现在词嵌入矩阵中。因此，如果能够压缩词嵌入矩阵的大小，那么就能有效地控制模型大小。kika 采用了稀疏词表征的方法以压缩词嵌入矩阵的大小，从而大幅度减少 LSTM 语言模型的参数与计算量。 其实对于语言模型，甚至是自然语言处理而言，词嵌入是非常重要的成分，我们可以使用 300 到 500 维的向量表示词表中数以万计的词汇。这种方法不会像 one-hot 编码那样使用超高维的向量表示一个词，可以说词嵌入将词的表征由|V|维减少到几百维，其中|V|表示词汇数量。但是当我们使用词嵌入作为语言模型的输入时，我们会发现尽管每个词的维度只有 n，但需要|V|个向量，而 |V| 通常要比 n 高好几个量级。因此，稀疏词表征就尝试使用少量词向量（少于|V|）而表征 |V| 个词。 这种方法的直观概念即我们的词表可以分为常见词与非常见词，而一般单个词可以由多个词定义，因此非常见词可以使用多个常见词表示。根据这样的观点，我们可以使用一组常见词的词嵌入向量作为基础，再通过组合而表示所有词的词嵌入向量。因此，我们能使用少量词嵌入向量表示大量词汇。又因为每一个词的表征都只使用少量的常见词来定义，所以这种表示方法是非常稀疏的，这也就是稀疏词表征的直观概念。 若我们将词表 V 分割为两个子集 B 和 C，第一个子集 B 为基向量集，它包含了固定数量的常见词。而 C 包含了所有不常见的词，因此现在需要使用 B 的词嵌入向量以线性组合的方式编码 C 中的词。这一过程可通过最小化由基向量集学习重构的词表征和完整表征之间的距离而学习，一般来说整个过程可表示为： 使用全部词汇训练一个词嵌入矩阵。 按词频选取最常见的 |B| 个词嵌入向量，并组成过完备基矩阵。 非常见词的预测可表示为 B 中所有词嵌入向量的稀疏组合，即 其中 w hat 为预测的非常见词词向量、U 为常见词词向量，而 x 为稀疏矩阵。 最小化预测词向量和实际词向量间的距离来学习稀疏表征，即 其中第一项表示通过稀疏表示 x 预测的词向量与完整词向量（w）间的 L2 距离。后一项为 L1 正则化，它会将矩阵 x 中的元素推向 0，从而实现稀疏表示。 在 kika 的论文 Sparse Word Representation for RNN Language Models on Cellphones 中，他们使用了以下伪代码展示了稀疏表示的学习算法： 这个算法很大的特点是实现了一个二元搜索来确定α，因为我们不能直接控制稀疏矩阵 x 的稀疏程度，所以我们根据稀疏矩阵的非零元素数来控制α的变化。整个稀疏词表征算法需要输入过完备基矩阵 U（常见词）、完整词嵌入矩阵 w、稀疏程度 s 和作为终止条件的容忍度 tol。 其中 s 是非常重要的一个参数，它控制了一个词最多需要多少个过完备基向量表征。kika 表示：「s 是一种权衡，如果参数较大，那么压缩比就会很小，模型达不到预期效果。如果参数较小，那么重构的词表征就不能有效地表示所有词。」正因为需要进行精调来确定 s 及其它超参数，kika 表明总体模型调优时间是训练时间的 4 到 5 倍，所以整个稀疏词表征的训练过程还是比较挺长的。 如上算法所示，首先我们会确定α的搜索范围，然后取α的中间值并最小化损失函数而求得稀疏表示 x，并统计 x 中每一个列向量的非零元素数，它们代表了一个词需要多少个常见词表示。如果 k 大于 s，那么非零的元素就过多，我们需要加大 α 以增强 L1 正则化的效果。这样的二元搜索直到α的上下界距离小于参数 tol 才会终止，且一般迭代几次就能快速收敛到合适的 α 来控制 x 的稀疏性。在完成 x 的学习后，我们将每一列稀疏向量抽取为对应的索引与权重，索引代表使用哪些基向量或常见词，而权重代表它们定义某个词的重要性。 又因为前面的二元搜索将 k 限制为不大于 s，所以有可能 k 是小于 s 的，因此我们需要使用零将这些向量补全。经过上面的步骤，最终我们会产生包含 s 个元素的等长向量 indices 和 weights。储存这两种向量而不直接储存稀疏矩阵 x* 能节省很多空间，这对于减小安装包大小有非常重要的作用。 论文中给出的词嵌入恢复算法以一种串行密集运算的方式进行展示，这可以令读者清晰地理解重构过程： 若给定 U、indices 和 weights，一个词的词嵌入重构可直接利用索引取对应的基向量，并与对应的权重求加权和。这种线性组合非常简单且高效，也是线性代数中非常直观的表示方法。因为任何秩为 n 的矩阵都可以由 n 个线性不相关的向量或基表示出来，完整的词嵌入矩阵也就能由过完备基的线性组合表示。算法 1.2 最后返回的 v 就是我们线性组合多个常见词词嵌入而重构出来的完整词嵌入向量。 以上是 kika 采用的稀疏词表征方法，它可以有效减少模型参数和计算量，但我们还能进一步使用参数量化来压缩模型的存储大小。 量化 一般而言，应用的安装包大小对于用户体验非常重要，这一点对于移动端尤为突出。因此，我们可以使用参数量化的方法来减小安装包大小。kika 也曾尝试使用 TensorFlow 封装的压缩方法，但仍发现一些难以解决的问题，因此他们最终使用 k-means 方法重新构建参数量化而解决包体增大的问题。 kika 最开始尝试使用官方的 tf.quantize 执行参数量化，并用 tf.dequantize 恢复参数。这个方法非常迅速，基本上几十兆的模型只需要分钟级的时间就能完成压缩。但 kika 发现这种方法有一个非常大的问题，即如果当我们希望读取量化后的模型时，TensorFlow 会引入大量的 Operator，这势必会造成动态链接库（.so）的体积增大，因而会加大安装包的大小。因为动态链接库包含了所有 TF 定义的加法、减法、卷积和归一化等模型需要使用的运算，因此调用 TF 的量化方法同样会将相关的运算添加到动态链接库中。 根据 kika 的实验，使用 TF 官方的量化方法大概会使动态链接库增加 1 到 2 MB 的体积，对应的安装包大小也会增加这么多。由于这样的原因，kika 最后选择基于 k-means 的方法实现参数量化。简单而言，这个方法会先使用 k-means 将相似的向量聚类在一起，然后储存聚类中心，原参数矩阵就只需要存储聚类中心的索引就行了。kika 表明这种方法的有点在于不会额外增加动态链接库和安装包的大小。因此下面将简要介绍这种基于 k-means 的参数量化方法。 量化即从权重中归纳一些特征，这些特征会储存在码表（codebook）并以具体数值表示某一类权重，而原来的权重矩阵只需要存储索引来表示它们属于哪一类特征就行了，这种方法能很大程度上降低存储成本。 kika 使用的标量量化算法基本思路是，对于每一个 m×n 维的权重矩阵 W，首先将其转化为包含 m×n 个元素的向量 w。然后再对该权重向量的元素聚类为 k 个集群，这可借助经典的 k 均值聚类算法快速完成： 现在，我们只需储存 k 个聚类中心 c_j，而原权重矩阵只需要记录各自聚类中心的索引就行。在韩松 ICLR 2016 的最佳论文中，他用如下一张图非常形象地展示了量化的概念与过程。 如上所示权重矩阵的所有参数可以聚类为 4 个类别，不同的类别使用不同的颜色表示。上半部分的权重矩阵可以取聚类中心，并储存在 centroids 向量中，随后原来的权重矩阵只需要很少的空间储存对应的索引。下半部是韩松等研究者利用反向传播的梯度对当前 centroids 向量进行修正的过程。 稀疏词表征与参数量化是 kika 控制参数大小的主要方法，黄康表示：「实际上模型的大小可以分为两阶段，首先如果原模型是 40MB 的话，稀疏词表征可以将模型减少到 20MB 左右，这个大小是实际在内存中的大小。而进一步采用参数量化可以将大小压缩到 4MB 左右，它解决的问题是 APK 安装包大小，APK 大小也是非常重要的，毕竟作为输入法这样的应用，APK 的大小是非常重要的。不论使不使用参数量化，模型最终在计算上需要的内存就是稀疏词向量表征后的大小。」 最后两部分基本上就是 kika 解决模型大小的方案，它们令深度学习模型在实践中有了应用的可能。当然，要将深度学习模型嵌入输入法和移动端会有很多的挑战，仅仅控制模型大小是不够的，因此也就有了上文 kika 在内存大小、响应时间和动态链接库等方面的努力。 整个模型效果和工程化实践都是 kika 在过去 2 年来对输入法引擎的探索，未来还有很多优化与提升的方向，例如使用新型循环单元或新型强化学习来根据用户习惯调整输入法等。这些新功能与新方向将赋予输入法引擎更多的特性，也能适应性地为不同的用户提供最好的体验。 转载来源：专访｜基于LSTM与TensorFlow Lite，kika输入法是如何造就的]]></content>
      <categories>
        <category>军事</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>Word</tag>
        <tag>机器学习</tag>
        <tag>Google</tag>
        <tag>人工智能</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[自动驾驶安全员究竟都干什么？我今天去百度Apollo体验了一天]]></title>
    <url>%2F2018%2F363fb4a9%2F</url>
    <content type="text"><![CDATA[自动驾驶安全员究竟都干什么？我今天去百度Apollo体验了一天 转载来源：自动驾驶安全员究竟都干什么？我今天去百度Apollo体验了一天]]></content>
  </entry>
  <entry>
    <title><![CDATA[Google收购Kaggle！拿下最大机器学习及数据竞赛平台]]></title>
    <url>%2F2018%2F85d36b57%2F</url>
    <content type="text"><![CDATA[李林 舒石 编译整理 量子位·QbitAI 出品 全球最大的机器学习及数据科学竞赛平台Kaggle，即将被Google收入囊中。来自TechCrunch的消息透露，虽然这项收购的细节尚未披露，但基本确定会在明天召开的Google旧金山Cloud Next大会上对外宣布。 至于这两家对收购传闻的态度，Google依然是“不对传闻置评”，TechCrunch电话联系了Kaggle联合创始人、CEO Anthony Goldbloom，他的反应是“拒绝否认这项收购（declined to deny that the acquisition is happening）”。 Kaggle是2010年由Goldbloom和Ben Hamner联合创立的，现在平台上大约有100万名数据科学家，基本上可以说是举办数据科学和机器学习竞赛的不二之选。 月初，Google也用上了这个平台，他们在Kaggle上举办的YouTube视频分类比赛《Google Cloud &amp; YouTube-8M Video Understanding Challenge》依然在进行中，总奖金额10万美元，有235支队伍参与。这个比赛的目标，是更好的用算法对视频进行等级分类。 这个比赛所用的数据集，就是YouTube发布的700多万部YouTube视频，平均每个视频已经打上3.4个标签。比赛和Google云计算平台也有着密切的关联。 目前在Kaggle上奖金最高的比赛是《Data Science Bowl 2017》，总奖金额100万美元，目前有1377支队伍参与角逐。今年的主题是如何通过大数据和人工智能的方式，可以更早的对美国肺癌患者进行确诊以及展开干预。 比赛的数据集，是美国国家癌症研究所提供的数千组高分辨率肺部扫描数据。 Kaggle上参赛最多的挑战是《Titanic: Machine Learning from Disaster》。这个项目要求参赛者使用机器学习的工具，对泰坦尼克号上的乘客船员进行生存几率预测。其实这是Kaggle的一个入门挑战，适用于新手或者刚入门的数据科学家。 虽然Kaggle所处的行业竞争也很激烈，DrivenData、TopCoder、HackerRank等对手虎视眈眈，Kaggle依然借先发优势和对细分领域的专注保持着领先地位。 Google这次收购看中的可能是Kaggle的用户群体而非技术。这次收购，可以说是买下了最大、最活跃的数据科学家社区，Google能够借此提升这个人群的关注度。通过TensorFlow等开源项目，Google在做的也是这样的事情。 从TechCrunch获得的消息来看，Google会在收购后维持Kaggle平台的运营，保持原有品牌。 和其他竞赛平台一样，Kaggle上也有求职公告板，不知道Google对这一部分打算如何处置。 Kaggle自2010年成立以来，总共融资1250万美元（数据来自Crunchbase，另一家创投数据库PitchBook显示是1275万美元），投资方包括Index Ventures，SV Angel，Max Levchin，Naval Ravikant，Google首席经济学家Hal Varian，Khosla Ventures和尤里·米尔纳。 今天AI还搞了哪些大新闻？ 在量子位（QbitAI）公众号会话界面回复“今天”，看我们全网搜罗的AI新鲜资讯。比心❤~ 转载来源：Google收购Kaggle！拿下最大机器学习及数据竞赛平台]]></content>
      <categories>
        <category>科技</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>Google</tag>
        <tag>人工智能</tag>
        <tag>Kaggle</tag>
        <tag>YouTube</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[6个最好的HTML框架，用于开发一个伟大的移动UI]]></title>
    <url>%2F2018%2Fe80b4f02%2F</url>
    <content type="text"><![CDATA[6个最好的HTML框架，用于开发一个伟大的移动UI 转载来源：6个最好的HTML框架，用于开发一个伟大的移动UI]]></content>
  </entry>
  <entry>
    <title><![CDATA[C/C++扩展之Python直接调用讯飞语言]]></title>
    <url>%2F2018%2F3592bb46%2F</url>
    <content type="text"><![CDATA[Python作为一门脚本语言有着非常好的易用性，但是很多人会病垢或纠结它的性能，不过Python与C/C++有着很好的沟通，很多对性能要求高的算法都可以用C/C++实现后供Python调用。 Python通过C/C++进行扩展有很多方法： 直接调用动态库.so- CPython使用C语言API编写模块include- 使用boost_python封装C++类- 使用SWIG扩展PythonCPython使用C语言API编写模块include 使用SWIG扩展Python 以Python使用科大讯飞语言识别的Linux接口来介绍Python如何调用动态库.so文件。 1. Linux下生成动态库.so文件 以下是c实现的一个简单函数(mylib.c)： extern &quot;C&quot; {&lt;br&gt;int sum(int a, int b) {&lt;br&gt;return a+b;&lt;br&gt; }&lt;br&gt;} 在shell中执行如下命令就会得到mylib.so动态库： g++ -fPIC -shared -o libmylib.so mylib.c 2. 使用Python调用动态库 #! /usr/bin/env python&lt;br&gt;&lt;br&gt;import ctypes&lt;br&gt;import os&lt;br&gt;&lt;br&gt;mylib = ctypes.CDLL(os.getcwd() + &#39;/libmylib.so&#39;)&lt;br&gt;print mylib.sum(2,6) 3. Python调用科大讯飞语音识别API 加载动态库： xflib = ctypes.cdll.LoadLibrary(&#39;msc/libmsc.so&#39;) 在Python里面调用C函数时主要是注意参数的类型。语音识别的接口如下：接口函数： const char* MSPAPI QISRSessionBegin( const char* grammarList, const char*&lt;br&gt;params, int* errorCode ) 返回的是一个char指针作为sessionID以供后续接口使用，在Python里面要用ctypes.c_voidp类型： ret = ctypes.c_int()&lt;br&gt;sessionId = ctypes.c_voidp()&lt;br&gt;sessionId = xflib.QISRSessionBegin(None, param1, ret) 调用其它接口函数时，还可能用到如下ctypes的类型和接口： ctypes.create_string_buffer()- ctypes.addressof()- ctypes.byref()- ctypes.string_at()- ctypes.c_char_p()- ctypes.c_uint()ctypes.addressof() ctypes.string_at() ctypes.c_uint() 关于ctypes的更详细说明可以参考官方文档 转载来源：C/C++扩展之Python直接调用讯飞语言]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>编程语言</tag>
        <tag>Python</tag>
        <tag>机器学习</tag>
        <tag>C语言</tag>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[利用Python进行深度学习的完整入门指南(附资源)]]></title>
    <url>%2F2018%2Fe10cc535%2F</url>
    <content type="text"><![CDATA[利用Python进行深度学习的完整入门指南(附资源) 转载来源：利用Python进行深度学习的完整入门指南(附资源)]]></content>
      <tags>
        <tag>大数据文摘</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[头条 | 万亿汽车后服务，你关注的创业经、血泪史和商业逻辑都在这里了]]></title>
    <url>%2F2018%2Fd11319ad%2F</url>
    <content type="text"><![CDATA[头条 | 万亿汽车后服务，你关注的创业经、血泪史和商业逻辑都在这里了 转载来源：头条 | 万亿汽车后服务，你关注的创业经、血泪史和商业逻辑都在这里了]]></content>
      <tags>
        <tag>虎嗅网</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[代币创建教程 » 论坛 » EthFans | 以太坊爱好者]]></title>
    <url>%2F2018%2F5803123d%2F</url>
    <content type="text"><![CDATA[代币创建教程 » 论坛 » EthFans | 以太坊爱好者 转载来源：代币创建教程 » 论坛 » EthFans | 以太坊爱好者]]></content>
  </entry>
  <entry>
    <title><![CDATA[对以太坊公有链上金融项目的开发赞助计划 » 论坛 » EthFans | 以太坊爱好者]]></title>
    <url>%2F2018%2Fc9c4f25a%2F</url>
    <content type="text"><![CDATA[对以太坊公有链上金融项目的开发赞助计划 » 论坛 » EthFans | 以太坊爱好者 转载来源：对以太坊公有链上金融项目的开发赞助计划 » 论坛 » EthFans | 以太坊爱好者]]></content>
  </entry>
  <entry>
    <title><![CDATA[日刷抖音三百条，让我重新认识今日头条]]></title>
    <url>%2F2018%2F78797b59%2F</url>
    <content type="text"><![CDATA[当我打开抖音，不知不觉中两个小时不见了，而在我刷了上百条抖音后，我重新认知了今日头条。 抖音成了香饽饽，相比较于快手在三四线城市的肆虐，抖音成为了一二线城市年轻用户的最爱。 这个现象让人疯狂，一二线白领、大学生，这批要求最多。眼光最高、套路最多、识别套路能力最强，多少个app都无法讨好的人群，被抖音收割了。 出于对于这件事情的兴趣，我也玩起了抖音，并且在15s的世界里被割下了大片时间，时间都去哪儿呢？ 音乐是抖音的切口，也是激发用户创造力的伴奏如果有人问什么是抖音，我们该如何在二十秒内准确描述它？ 抖音在对外的宣传资料里这样描述自己： 抖音短视频社区以音乐为切入点，搭配舞蹈、跑酷、表演等内容的创意表达形式，为用户创造丰富多样的玩法，让用户轻松快速地创作独特有张力的短视频，并在抖音社区与众多用户互动。 从这个定义里可以明确地得知几个信息点： 抖音是一个短视频，而且是以音乐作为创作基础的短视频。通过实际上手发现，15s的短视频决定了音乐必须实现定制。所以抖音购买音乐版权，创办音乐创作大赛，这一切目的的背后都是给用户提供更多的创作工具。 用户根据音乐这个创作工具需要自发地创作，这决定了这个产品一开始需要栽培的种子用户是富有想象力、创造力的人群。 轻松快速地制作短视频，意味着抖音必须提供足够多的创作工具，循环、慢拍、快进这些拍摄功能都是基本的。 在抖音社区与其他用户互动，社区意味着这是一个大舞台，但想要互动意味着需要连接多方，多方在这里可以简单理解为内容制作者和用户双方。连接必然涉及到推送机制，解决方案是今日头条的技术分发，同时结合常见的Follow机制。 从这个定义里，自然能够理解抖音做很多事情的必然性。 必然性之一：把生产工具打磨到足够好。 2017年3月前，抖音都没有大规模地进行宣传。这个时间段抖音在疯狂地迭代产品，也就是要完成工具的基本建设。 而今日头条在这一点上已经有类似经验，在今日头条早期，不断迭代头条号的后台编辑器，为内容创作者提供更好用的创作工具，这已经是这家公司打磨一款产品的基因。 必然性之二：和专业音乐人合作，定制短音乐。 音乐制作是一个门槛高的行业，而想要再制作15s的短视频配乐，同时这个音乐能够激发用户创造力，更是一件对专业人颇具挑战的任务。 在抖音的官网上，现在正在举行的抖音音乐计划，看起来就是这个需求的落地方案之一，而且从目前的方向来看，这件事情将会有很强的持续性。 必然性之三：通过运营降低生产成本，通过技术分发让用户获得“崇拜”。 在完成了工具的建设以后，急需要破除的一个门槛就是让用户学会使用这款工具，这和office套装才来到中国时一样，很多人都在学习如何使用word。 因此运营端必须首先面对这个现实：即使是15s短视频，只要涉及到拍摄，短视频都是一个门槛颇高的行业。 因此运营做的第一件事情就是出教程，教育和辅助用户如何拍摄出令人喜爱的抖音小视频？ 在这个环节，抖音的运营需要有很强的培训力和把控力，一边出教程，一边期待用户在这些基本功能上有新的创意发挥。就像学生时代老师教会了遣词造句的规则后，期待学生能够写出令人眼前一亮的语句。 把控力则体现在：面对每一支可能引发大规模模仿的音乐小视频，如何把控整体的调性？不能管理太多继而挫伤用户的创造性，又不能让用户过于狂热的创造性伤害到其他人。 当用户的创造力得到释放以后，从用户端来说，只有一种反馈能够促进它们继续沉迷在这个app中。 用户需要有奖励，对于内容创作者的奖励，曾经在今日头条上展现为现金，在抖音上则是“崇拜”。 因为所有人都需要钱，但是年轻人更需要“崇拜”。 在官方运营的推力下，聪明的用户将快速上手软件，并且有机会和官方一起去定义这款app的未来想象力。 究竟什么样的视频内容是既适合这个平台，又能得到官方认可并且大规模推荐的？ 这个答案一开始很难有定则，一定都是用户一开始百花齐放式创作，然后官方摸索着筛选出最适合抖音调性的内容风格，同时也让其他风格的内容给予一定量野蛮生长的空间。这些和app一起成长的抖音初期用户，一边享受着抖音带来的快乐，一边享受着其他粉丝赋予他们的“崇拜”。 品牌认知让有才华的人得到足够多的赞赏。 抖音的slogan为“让崇拜从这里开始”。我觉得这句slogan没有说完，完整的话术应该是让有才华的年轻人从这里开始享受崇拜。 这句话和今日头条提出的slogan背后含义如出一辙：你创作的，就是头条。 如果说今日头条的头条号平台是释放文字创作者的创造力的话，那么抖音短视频就是为了释放擅长视频拍摄者的创造力。 文字创作平台一开始就是写了无数年文章的中年人的天下，拍摄短视频这项技能，从一开始就掌握在伴随智能手机成长的年轻一代手里。在用手机创作短视频领域，中年人和年轻人并没有因为年龄的差距而有明显的经验差异性，相反年轻人在新事物上还存在更大的创造力。 打开抖音创作，不会有相对年龄比较大的人去选择一首欧美风格的音乐伴奏跳舞一曲。抖音的音乐创作工具，一开始就为了年轻人喜好的音乐风格去设计。 毕竟，音乐很容易切中很多年龄层，但创造力更大概率在年轻人群体手中，那抖音短视频是如何利用年轻人的创造力传播的呢？ 抖音宣传物料非常年轻人 品牌推广初期年轻人喜爱的明星，我们都聊一聊；年轻人喜欢的节目，我们都露露脸。 在所有分析抖音品牌推广的文章里，都会提到小悦悦，我不知道小悦悦的。微博发布是Marketing 推动，还是小悦悦自发的结果。 但是小悦悦实在是一个很好的推广开口，我们只需要仔细想想：小悦悦在现在年轻人中是个什么认知就可以理解，小悦悦的表情包存在于多少年轻人的手机里。 自从小悦悦发布以后，抖音开始窜进一大批年轻人的视野里。接下来的抖音名画H5、中国有嘻哈、快乐大本营、天天向上，包括举办线下活动idou夜，无一不是在疯狂推动这款产品在年轻人中的流行，最终这款产品彻底收服年轻人。 未来业务想象力音乐会吸引更多的声音，短视频也只是一种形式。 如果说从信息形式的角度想业务金字塔，音乐的底层是声音，对于声音这一信息形式的使用，是抖音往下拓根基的一个业务方向。 而我们也在目前的一些热门视频里，看到了基于不同场景带来的声音形式。往上则是音乐产业的想象力，我们以后能看到15s的完整音乐吗？甚至15s的音乐mv，非常期待音乐产业的创造力。 如果从另一个角度——短视频来分析。 短视频自身承载的绝对不仅仅是音乐，而抖音热门里也的确存在不少纯画面、生活中的短小纪录片优质内容，这也是短视频自身存在的多元空间。 从不同角度出发，向上和向下去拓展业务想象力，也给抖音带来了更大的商业合作空间。 想象一下声音是一门多大的生意？画面又是一门多大的生意呢？即使不去拓展这些业务空间，至少在未来推广商，从声音、画面这些角度有了新的合作点。 未来业务的挑战娱乐不俗气？年轻人群体是焦点，但不是世界的全部。 从运营的角度来看：内容生产、内容分发、用户沉淀、转化消费是UGC内容型工具发展的四个阶段。 在内容生产和内容分发的阶段，抖音比今日头条早期更早更快地迎来了人群的爆发，接下来在用户沉淀和转化消费上，如何制定运营策略？是抖音整个运营策略中下一步需要面临的挑战，优秀的运营端的摸索也可能去反哺商业化策略。 从市场角度来看：抖音极具爆发力的发展带来了一个互联网奇迹以外，也带来了很大的挑战。即使把15岁至30岁的年轻人全部收割，这个群体的数量也就约为2.8亿（2013年中国人口数据），这个数量和今日头条自身和快手对比也是不够的。 最大的挑战同时会出现在这群年轻人手里，极具创造力的年轻人必然会释放出强大的传播转化，一旦有更多的非年轻群体用户，开始将这个平台视为另一个快手来娱乐。 那么，在同一块场地，如何平衡打篮球的年轻人和跳广场舞的老年人满足自己的消费需求？这将成为后期内容定调的一个难题。 而且这种趋势已经开始有了萌芽，从最近首页信息流刷出来的抖音内容来看，明显有比年前的内容掉了一个档次。 抖音甩狗头 重新认知今日头条去中心化的背后是给予平等的机会。 抖音的成功既是短视频趋势的，又有音乐这一天才想法的切入点，结合自身团队的能力，能发展到今天，有很多是值得所有人向抖音团队学习的。 同时抖音的发展同时让我重新认知了今日头条，因为今日头条的去中心化的技术分发机制。 我们回过头来再仔细看今日头条对自己的介绍： 《今日头条》是一款基于数据挖掘的推荐引擎产品，它为用户推荐有价值的、个性化的信息，提供连接人与信息的新型服务。 今日头条的本质是：人与信息的连接服务，依靠的是数据挖掘，提供的是个性化有价值的信息。 这句话决定了今日头条没有版面推荐机制、去中心化，给予每个人都可能上热门的机会。每个人创造的自己擅长的内容，将推给和自己有相同喜好的群体。 这一切都来源于去中心化的分发技术。 在阶层流动越来越固化的今天，给予每个人平等的机会这件事儿原本应该是一个顶层设计，但是由于各种原因，这件事儿如果在互联网上最终依靠技术来解决。 其实这才回到了互联网一开始给人无数信心的年代——这个世界是平的。 门户时代的版面位置、头版头条这种代表着少数人控制话语权的时代，已经一去不复返了，留给无数远离北上广的有创造力有才华的年轻人一个念头——即使丝毫不认识互联网企业里的任何一个员工，一个有创造力的年轻人都将在这个平台获得更多的曝光和收入。 年轻人在这里创造，并且在这里收获崇拜，这是抖音。 有才华的人在这里创造内容，并且在这里收获更多的物质和精神满足，这是今日头条。 作者：生椒牛肉，微信：361793303， 微博：@生椒牛肉 链接：https://www.jianshu.com/p/7794f94a8fa7 本文由 @生椒牛肉 授权发布于人人都是产品经理，未经作者许可，禁止转载 转载来源：日刷抖音三百条，让我重新认识今日头条]]></content>
      <categories>
        <category>其他</category>
      </categories>
      <tags>
        <tag>软件</tag>
        <tag>移动互联网</tag>
        <tag>今日头条</tag>
        <tag>产品运营</tag>
        <tag>智能手机</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[优质博文集锦-云栖社区-阿里云]]></title>
    <url>%2F2018%2Ffbafdf9e%2F</url>
    <content type="text"><![CDATA[优质博文集锦-云栖社区-阿里云 转载来源：优质博文集锦-云栖社区-阿里云]]></content>
  </entry>
  <entry>
    <title><![CDATA[如何判断一个人是杰出的聪明人还是平庸的普通人？]]></title>
    <url>%2F2018%2F39e0aa89%2F</url>
    <content type="text"><![CDATA[如何判断一个人是杰出的聪明人还是平庸的普通人？ 转载来源：如何判断一个人是杰出的聪明人还是平庸的普通人？]]></content>
  </entry>
  <entry>
    <title><![CDATA[四十岁了，你居然还在编程？]]></title>
    <url>%2F2018%2F9dda8f83%2F</url>
    <content type="text"><![CDATA[四十岁了，你居然还在编程？ 转载来源：四十岁了，你居然还在编程？]]></content>
      <tags>
        <tag>MacTalk</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何用Python写一个贪吃蛇AI@慕课网 原创_慕课网_手记]]></title>
    <url>%2F2018%2Fabcdaffa%2F</url>
    <content type="text"><![CDATA[如何用Python写一个贪吃蛇AI@慕课网 原创慕课网手记 转载来源：如何用Python写一个贪吃蛇AI@慕课网 原创慕课网手记]]></content>
  </entry>
  <entry>
    <title><![CDATA[WEGENE基因检测广告片，确实有趣！]]></title>
    <url>%2F2018%2Fcae20e2a%2F</url>
    <content type="text"><![CDATA[WEGENE基因检测广告片，确实有趣！ 转载来源：WEGENE基因检测广告片，确实有趣！]]></content>
  </entry>
  <entry>
    <title><![CDATA[简述表征句子的3种无监督深度学习方法]]></title>
    <url>%2F2018%2F9a9ba65a%2F</url>
    <content type="text"><![CDATA[本文介绍了三种用于表征句子的无监督深度学习方法：自编码器、语言模型和 Skip-Thought 向量模型，并与基线模型 Average Word2Vec 进行了对比。 本文介绍了三种用于表征句子的无监督深度学习方法：自编码器、语言模型和 Skip-Thought 向量模型，并与基线模型 Average Word2Vec 进行了对比。 近年来，由于用连续向量表示词语（而不是用稀疏的 one-hot 编码向量（Word2Vec））技术的发展，自然语言处理领域的性能获得了重大提升。 Word2Vec 示例 尽管 Word2Vec 性能不错，并且创建了很不错的语义，例如 King - Man + Woman = Queen，但是我们有时候并不在意单词的表征，而是句子的表征。 本文将介绍几个用于句子表征的无监督深度学习方法，并分享相关代码。我们将展示这些方法在特定文本分类任务中作为预处理步骤的效果。 分类任务 用来展示不同句子表征方法的数据基于从万维网抓取的 10000 篇新闻类文章。分类任务是将每篇文章归类为 10 个可能的主题之一（数据具备主题标签，所以这是一个有监督的任务）。为了便于演示，我会使用一个 logistic 回归模型，每次使用不同的预处理表征方法处理文章标题。 基线模型——Average Word2Vec 我们从一个简单的基线模型开始。我们会通过对标题单词的 Word2Vec 表征求平均来表征文章标题。正如之前提及的，Word2Vec 是一种将单词表征为向量的机器学习方法。Word2Vec 模型是通过使用浅层神经网络来预测与目标词接近的单词来训练的。你可以阅读更多内容来了解这个算法是如何运行的：http&#58;//mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/。 我们可以使用 Gensim 训练我们自己的 Word2Vec 模型，但是在这个例子中我们会使用一个 Google 预训练 Word2Vec 模型，它基于 Google 的新闻数据而建立。在将每一个单词表征为向量后，我们会将一个句子（文章标题）表征为其单词（向量）的均值，然后运行 logistic 回归对文章进行分类。 我们的基线 average Word2Vec 模型达到了 68% 的准确率。这很不错了，那么让我们来看一看能不能做得更好。 average Word2Vec 方法有两个弱点：它是词袋模型（bag-of-words model），与单词顺序无关，所有单词都具备相同的权重。为了进行句子表征，我们将在下面的方法中使用 RNN 架构解决这些问题。 自编码器 自编码器是一种无监督深度学习模型，它试图将自己的输入复制到输出。自编码器的技巧在于中间隐藏层的维度要低于输入数据的维度。所以这种神经网络必须以一种聪明、紧凑的方式来表征输入，以完成成功的重建。在很多情况下，使用自编码器进行特征提取被证明是非常有效的。 我们的自编码器是一个简单的序列到序列结构，由一个输入层、一个嵌入层、一个 LSTM 层，以及一个 softmax 层组成。整个结构的输入和输出都是标题，我们将使用 LSTM 的输出来表征标题。在得到自编码器的表征之后，我们将使用 logistics 回归来预测类别。为了得到更多的数据，我们会使用文章中所有句子来训练自编码器，而不是仅仅使用文章标题。 我们实现了 60% 的准确率，比基线模型要差一些。我们可能通过优化超参数、增加训练 epoch 数量或者在更多的数据上训练模型，来改进该分数。 语言模型 我们的第二个方法是训练语言模型来表征句子。语言模型描述的是某种语言中一段文本存在的概率。例如，「我喜欢吃香蕉」（I like eating bananas）这个句子会比「我喜欢吃卷积」（I like eating convolutions）这个句子具备更高的存在概率。我们通过分割 n 个单词组成的窗口以及预测文本中的下一个单词来训练语言模型。你可以在这里了解到更多基于 RNN 的语言模型的内容：http&#58;//karpathy.github.io/2015/05/21/rnn-effectiveness/。通过构建语言模型，我们理解了「新闻英语」（journalistic English）是如何建立的，并且模型应该聚焦于重要的单词及其表征。 我们的架构和自编码器的架构是类似的，但是我们只预测一个单词，而不是一个单词序列。输入将包含由新闻文章中的 20 个单词组成的窗口，标签是第 21 个单词。在训练完语言模型之后，我们将从 LSTM 的输出隐藏状态中得到标题表征，然后运行 logistics 回归模型来预测类别。 这一次我们得到了 72% 的准确率，要比基线模型好一些，那我们能否让它变得更好呢？ Skip-Thought 向量模型 在 2015 年关于 skip-thought 的论文《Skip-Thought Vectors》中，作者从语言模型中获得了同样的直觉知识。然而，在 skip-thought 中，我们并没有预测下一个单词，而是预测之前和之后的句子。这给模型关于句子的更多语境，所以，我们可以构建更好的句子表征。您可以阅读这篇博客（https&#58;//medium.com/&#64;sanyamagarwal/my-thoughts-on-skip-thoughts-a3e773605efa），了解关于这个模型的更多信息。 skip-thought 论文中的例子（https&#58;//arxiv.org/abs/1506.06726） 我们将构造一个类似于自编码器的序列到序列结构，但是它与自编码器有两个主要的区别。第一，我们有两个 LSTM 输出层：一个用于之前的句子，一个用于下一个句子；第二，我们会在输出 LSTM 中使用教师强迫（teacher forcing）。这意味着我们不仅仅给输出 LSTM 提供了之前的隐藏状态，还提供了实际的前一个单词（可在上图和输出最后一行中查看输入）。 这一次我们达到了 74% 的准确率。这是目前得到的最佳准确率。 总结 本文中，我们介绍了三个使用 RNN 创建句子向量表征的无监督方法，并且在解决一个监督任务的过程中展现了它们的效率。自编码器的结果比我们的基线模型要差一些（这可能是因为所用的数据集相对较小的缘故）。skip-thought 向量模型语言模型都利用语境来预测句子表征，并得到了最佳结果。 能够提升我们所展示的方法性能的可用方法有：调节超参数、训练更多 epoch 次数、使用预训练嵌入矩阵、改变神经网络架构等等。理论上，这些高级的调节工作或许能够在一定程度上改变结果。但是，我认为每一个预处理方法的基本直觉知识都能使用上述分享示例实现。 转载来源：简述表征句子的3种无监督深度学习方法]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>Word</tag>
        <tag>机器学习</tag>
        <tag>Google</tag>
        <tag>镜音双子</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[textgenrnn：只需几行代码即可训练文本生成网络]]></title>
    <url>%2F2018%2F0ee30940%2F</url>
    <content type="text"><![CDATA[本文是一个GitHub项目，介绍了textgenrnn，一个基于Keras/TensorFlow的Python3模块。 本文是一个 GitHub 项目，介绍了 textgenrnn，一个基于 Keras/TensorFlow 的 Python 3 模块。只需几行代码即可训练文本生成网络。 项目地址：https&#58;//github.com/minimaxir/textgenrnn?reddit=1 通过简简单单的几行代码，使用预训练神经网络生成文本，或者在任意文本数据集上训练你自己的任意规模和复杂度的文本生成神经网络。 textgenrnn 是一个基于 Keras/TensorFlow 的 Python 3 模块，用于创建 char-rnn，具有许多很酷炫的特性： 它是一个使用注意力权重（attention-weighting）和跳跃嵌入（skip-embedding）等先进技术的现代神经网络架构，用于加速训练并提升模型质量。- 能够在字符层级和词层级上进行训练和预测。- 能够设置 RNN 的大小、层数，以及是否使用双向 RNN。- 能够对任何通用的输入文本文件进行训练。- 能够在 GPU 上训练模型，然后在 CPU 上使用这些模型。- 在 GPU 上训练时能够使用强大的 CuDNN 实现 RNN，这比标准的 LSTM 实现大大加速了训练时间。- 能够使用语境标签训练模型，能够更快地学习并在某些情况下产生更好的结果。能够在字符层级和词层级上进行训练和预测。 能够对任何通用的输入文本文件进行训练。 在 GPU 上训练时能够使用强大的 CuDNN 实现 RNN，这比标准的 LSTM 实现大大加速了训练时间。 你可以使用 textgenrnn，并且在该 Colaboratory Notebook（https&#58;//drive.google.com/file/d/1mMKGnVxirJnqDViH7BDJxFqWrsXlPSoK/view?usp=sharing）中免费使用 GPU 训练任意文本文件。 示例 &#91;Spoiler&#93; Anyone else find this post and their person that was a little more than I really like the Star Wars in the fire or health and posting a personal house of the 2016 Letter for the game in a report of my backyard. 该模型可以很容易地在新的文本上进行训练，甚至可以在仅仅输入一次数据之后生成合适的文本。 Project State Project Firefox 这个模型的权重比较小（占磁盘上 2 MB 的空间），它们可以很容易地被保存并加载到新的 textgenrnn 实例中。因此，你可以使用经过数百次数据输入训练的模型。（实际上，textgenrnn 的学习能力过于强大了，以至于你必须大大提高温度（Temperature）来得到有创造性的输出。） Why we got money “regular alter”Urburg to Firefox acquires Nelf Multi ShamnKubernetes by Google’s Bern 您还可以训练一个支持词级别嵌入和双向 RNN 层的新模型。 使用方法 textgenrnn 可以通过 pip 从 pypi（https&#58;//pypi.python.org/pypi/textgenrnn）中安装： 你可以在该 Jupyter Notebook（https&#58;//github.com/minimaxir/textgenrnn/blob/master/docs/textgenrnn-demo.ipynb）中查看常见的功能和配置选项的演示案例。- /datasets 包含用于训练 textgenrnn 的 Hacker News 和 Reddit data 示例数据集。- /weights 包含在上述的数据集上进一步预训练的模型，它可以被加载到 textgenrnn 中。- /output 包含从上述预训练模型中生成文本的示例。/datasets 包含用于训练 textgenrnn 的 Hacker News 和 Reddit data 示例数据集。 /output 包含从上述预训练模型中生成文本的示例。 神经网络架构及实现 textgenrnn 基于 Andrej Karpathy 的 char-rnn 项目（https&#58;//github.com/karpathy/char-rnn），并且融入了一些最新的优化，如处理非常小的文本序列的能力。 本文涉及到的预训练模型遵循 DeepMoji 的神经网络架构（https&#58;//github.com/bfelbo/DeepMoji/blob/master/deepmoji/model_def.py）的启发。对于默认的模型，textgenrnn 接受最多 40 个字符的输入，它将每个字符转换为 100 维的字符嵌入向量，并将这些向量输入到一个包含 128 个神经元的长短期记忆（LSTM）循环层中。接着，这些输出被传输至另一个包含 128 个神经元的 LSTM 中。以上所有三层都被输入到一个注意力层中，用来给最重要的时序特征赋权，并且将它们取平均（由于嵌入层和第一个 LSTM 层是通过跳跃连接与注意力层相连的，因此模型的更新可以更容易地向后传播并且防止梯度消失）。该输出被映射到最多 394 个不同字符的概率分布上，这些字符是序列中的下一个字符，包括大写字母、小写字母、标点符号和表情。（如果在新的数据集上训练一个新模型，可以配置所有上面提到的数值参数。） 或者，如果可以获得每个文本文档的语境标签，则可以在语境模式下训练模型。在这种模式下，模型会学习给定语境的文本，这样循环层就会学习到非语境化的语言。前面提到的只包含文本的路径可以借助非语境化层提升性能；总之，这比单纯使用文本训练的模型训练速度更快，且具备更好的定量和定性的模型性能。 软件包包含的模型权重是基于（通过 BigQuery）在 Reddit 上提交的成千上万的文本文档训练的，它们来自各种各样的 subreddit 板块。此外，该网络还采用了上文提到的非语境方法，从而提高训练的性能，同时减少作者的偏见。 当使用 textgenrnn 在新的文本数据集上对模型进行微调时，所有的层都会被重新训练。然而，由于原始的预训练网络最初具备鲁棒性强得多的「知识」，新的 textgenrnn 最终能够训练地更快、更准确，并且可以学习原始数据集中未出现的新关系。（例如：预训练的字符嵌入包含所有可能的现代互联网语法类型中的字符语境。） 此外，重新训练是通过基于动量的优化器和线性衰减的学习率实现的，这两种方法都可以防止梯度爆炸，并且大大降低模型在长时间训练后发散的可能性。 注意事项 即使使用经过严格训练的神经网络，你也不能每次都能得到高质量的文本。这就是使用神经网络文本生成的博文（http&#58;//aiweirdness.com/post/170685749687/candy-heart-messages-written-by-a-neural-network）或推文（https&#58;//twitter.com/botnikstudios/status/955870327652970496）通常生成大量文本，然后挑选出最好的那些再进行编辑的主要原因。 不同的数据集得到的结果差异很大。因为预训练的神经网络相对来说较小，因此它不能像上述博客展示的 RNN 那样存储大量的数据。为了获得最佳结果，请使用至少包含 2000-5000 个文档的数据集。如果数据集较小，你需要在调用训练方法和／或从头开始训练一个新模型时，通过调高 num_epochs 参数来对模型进行更长时间的训练。即便如此，目前也没有一个判断模型」好坏」的启发式方法。 你并不一定需要用 GPU 重新训练 textgenrnn，但是在 CPU 上训练花费的时间较长。如果你使用 GPU 训练，我建议你增加 batch_size 参数，获得更好的硬件利用率。 未来计划 更多正式文档；- 一个使用 tensorflow.js 的基于 web 的实现（由于网络规模小，效果特别好）；- 一种将注意力层输出可视化的方法，以查看神经网络是如何「学习」的；- 有监督的文本生成模式：允许模型显示 top n 选项，并且由用户选择生成的下一个字符/单词（https&#58;//fivethirtyeight.com/features/some-like-it-bot/）；- 一个允许将模型架构用于聊天机器人对话的模式（也许可以作为单独的项目发布）；- 对语境进行更深入的探索（语境位置 + 允许多个语境标签）；- 一个更大的预训练网络，它能容纳更长的字符序列和对语言的更深入理解，生成更好的语句；- 层次化的作用于词级别模型的 softmax 激活函数（Keras 对此有很好的支持）；- 在 Volta／TPU 上进行超高速训练的 FP16 浮点运算（Keras 对此有很好的支持）。一个使用 tensorflow.js 的基于 web 的实现（由于网络规模小，效果特别好）； 有监督的文本生成模式：允许模型显示 top n 选项，并且由用户选择生成的下一个字符/单词（https&#58;//fivethirtyeight.com/features/some-like-it-bot/）； 对语境进行更深入的探索（语境位置 + 允许多个语境标签）； 层次化的作用于词级别模型的 softmax 激活函数（Keras 对此有很好的支持）； 使用 textgenrnn 的项目 Tweet Generator：训练一个为任意数量的 Twitter 用户生成推文而优化的神经网络。 转载来源：textgenrnn：只需几行代码即可训练文本生成网络]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>编程语言</tag>
        <tag>GitHub</tag>
        <tag>Python</tag>
        <tag>机器学习</tag>
        <tag>GPU</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何让机器理解汉字一笔一画的奥秘？]]></title>
    <url>%2F2018%2F7ac50893%2F</url>
    <content type="text"><![CDATA[阿里工程师设计了一种全新的中文词向量算法。 转载来源：如何让机器理解汉字一笔一画的奥秘？]]></content>
      <tags>
        <tag>阿里技术</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[今日头条“改名”了，其实Google也是这么做的]]></title>
    <url>%2F2018%2F8255c6c5%2F</url>
    <content type="text"><![CDATA[“今日头条”向后退了，“字节跳动”向前来了。4月24日首届数字中国建设峰会上，创办了日活跃用户超过1.4亿人的阅读工具——今日头条的张一鸣。 转载来源：今日头条“改名”了，其实Google也是这么做的]]></content>
  </entry>
  <entry>
    <title><![CDATA[相比芯片，我们更该在意深度学习框架的中国化]]></title>
    <url>%2F2018%2F5850525e%2F</url>
    <content type="text"><![CDATA[摘要： 存在感，是一个个脚印踏出来的。 这两天美国宣布对中兴进行封锁，可谓在科技圈掀起了从上到下的一股龙卷风。 4月16日美国商务部发布命令，禁止美国企业向中兴通讯销售元器件，时间长达7年。假如这一纸禁令真正生效，意味着中兴通讯旗下全产业链所依靠的芯片等核心元件失去获取渠道，基本意味着庞大的中兴通讯将面临无法继续经营的最坏可能。 中兴通讯股价随之快速下跌，已相当于两个跌停板，… 摘要： 存在感，是一个个脚印踏出来的。 这两天美国宣布对中兴进行封锁，可谓在科技圈掀起了从上到下的一股龙卷风。 4月16日美国商务部发布命令，禁止美国企业向中兴通讯销售元器件，时间长达7年。假如这一纸禁令真正生效，意味着中兴通讯旗下全产业链所依靠的芯片等核心元件失去获取渠道，基本意味着庞大的中兴通讯将面临无法继续经营的最坏可能。 中兴通讯股价随之快速下跌，已相当于两个跌停板，其美国供应商的股价在“禁令”发布后也遭遇不同程度的下跌。 19日，中国商务部新闻发言人高峰在新闻发布会上表示，美方行径引起了市场对于美国贸易和投资环境的普遍担忧。 随着中美贸易战延伸到科技领域，似乎一夜间我们又回忆起了缺失核心研发能力的恐惧。“缺芯之痛”转瞬弥漫在舆论氛围里，甚至关于“中国科技到底行不行”的讨论又一次尘嚣直上。 当然了，出问题就无限夸大其实并没有太大意义。客观来说，美国的芯片禁令无法持久，毕竟这背后涉及的美国本土产业链、利益群落与工作岗位非常庞大。一旦接连失去中国大客户，美国引以为傲的科技产业本身就会撑不住。 另外我们需要认清的是，半导体行业今天的国际格局并不是几个月，甚至几年内造成的。而是整个半导体工业时代发展和遗留下来的产物。中国以及全世界更多市场的计算机、工业、通用电子系统上本土芯片占有率都是零。 事实上，中兴这样的中国企业，在技术上的锐意进取是有目共睹的。但芯片研发制造能力的国际垄断，是几十年积累下来的产业现实，是大量的科技因素与市场封锁、贸易规则制定综合得出的结论。是一两家公司，甚至是集合整个科技集群都难以改变的。 换句话说，中国科技公司面临可能出现的困境，除了加强研发投入也没有什么别的办法。过去我们无法改写，但今天我们能改变的东西，叫做未来。 就像几十年前的半导体技术是核心中的核心。今天提起能改变未来世界的技术，AI绝对是当仁不让。尤其值得注意的是，这个刚刚兴起、充满变化的技术领域里，中美之间的价值突围和技术博弈其实更加激烈。 AI的国家战略重要性是母庸质疑的，而针对产业链上游核心技术的争夺正在逐步呈现白热化。假如我们希望若干年后整个产业界，甚至整个国家不会再因为一纸封锁领而恐慌，那么AI这个战场的基础设施，才是真的不容有失。 或许对于大部分人来说，AI的底层之争在今天还有些陌生，但它确实很可能像曾经的半导体一样，产生新时代的世界科技格局垄断效应。比如说每个AI开发者都会用到，所有AI应用产生的基础——深度学习开发框架。 在这个普通人很陌生的领域，中国科技公司和万千开发者，正在一点点刷新着中国科技的存在感。 被忽略的深度学习框架 芯片为什么重要？原因在于它是一切运算的基础，是最后端的的东西，没有它一切硬件都玩不转。所以当垄断形成，就能对其他经济体的科技发展形成底层制约。 同样的道理，在AI时代也体现在开发框架这件事上。我们知道，AI开发者不能每开发一个模型就从最底层重新来过，所以想要进行算法训练、模型开发、应用部署，都必须在一定的开发平台上来完成。AI发展到今天，这个平台的角色主要依靠大学和企业提供的深度学习框架来扮演。 在中国，深度学习框架相对来说是一个科研和开发领域的事情，但在美国，产业界围绕开发框架的争夺战早已经火星四溢。 比如说，很多美国媒体都认为，谷歌今天在云计算、硬件、语音助手、AI教学等业务中，全都展现出“TensorFlow First”的特点，用尽各种办法将开发者引导至自己的开发平台上，并且坚决不兼容其他开发框架。 而Facebook、微软则对TensorFlow的封闭深恶痛绝，形成了以caffe、Python结盟形式的“反谷歌联盟”，希望以兼容性和社群开放等优势，打破谷歌一骑绝尘的战略格局。 对开发框架的重视，隐藏着科技企业和背后国家经济体对AI未来的押注。试想无数应用都在自己的平台上进行开发，那么所有数据、算法创新和模型训练过程就都留在了平台当中。企业和平台收获的，是作为地基的产业地位。而国家经济收获的，是可以从源头上控制其他经济体AI应用的“上游效应”。 幸运的是，已经吃够了“下游之苦”的中国，在深度学习框架这件事上并没有落后。 为了解决当时主流开发框架仅支持但GPU应用，无法进行大规模数据处理的问题。百度从2013年就开始研发自己的深度学习框架PaddlePaddle，经过长期内部应用后，在2016年正式将其进行开源。 这也让百度成为继谷歌、Facebook、IBM之后，全球第四家、中国第一家开源深度学习开发框架的科技公司，之后BAT相继推出深度学习开发框架，在这场基础设施争夺中，中国产业壁垒的高度已和过去不再相同。 可能出现的中美AI对决中，开发框架或是轴心武器 就目前中国AI的整体行业氛围而言，似乎普遍更关注AI“用”的一面，容易忽略在应用之前的开发与创新，以及为创新提供的基础设施，是整个AI商业想象力的原点。 事实上，假如我们将中美两国看做处于竞比关系的两个AI技术群落，那么深度学习框架的质量和接受度，很可能会影响到整个产业竞赛的进程甚至结果。 或许可以从三个角度，来看为什么中国一定要有自己的深度学习框架，以及中国开发者为何更应该支持“国货”。 1、中国AI无法离开中文：我们知道，AI的一个重要领域是语言与对话的交互。那么未来在中国市场应用的，必然是基于中文的AI开发。但在NLP与语音交互、神经网络翻译等技术上，国外主流开发框架很少有中文数据集，也缺乏在中文领域的技术探索。如今来看，开发者想要开展这方面的工作，几乎必须依赖PaddlePaddle这样的国产框架所提供的开发基础和数据集、文档。 2、产业链的安全风险：去年，谷歌的TensorFlow曾经被爆出重大安全漏洞。虽然没有造成实质影响，但当时专家评估，类似的漏洞完全可以影响甚至摧毁所有基于该平台开发出的AI模型。要知道AI大量涉及安防、识别、城市交通、公共服务等国家事务核心领域应用，这些应用如果在国外框架中开发运行，那么安全风险不言而喻。 3、产业应用需求不同：相比于美国，中国对AI开发这件事的需求其实有很大不同。比如说传统企业多、开发者的应用需求大、商业期待迫切、开发人才处在发展阶段。那么相比于前沿探索类的开发，中国开发者更需要在开发框架提供高效、灵活的开发方案，以及快速部署、弹性学习的能力。这些因素当然是远在天边的欧美开发平台不会考虑的。只有深谙中国开发者需求和中国AI市场生态环境的企业，才可能进行针对性价值提供。 不难看出，基于应用性、安全性和中文开发的必然性，中国的深度学习开发框架之争都是注定要发生的。 别是一风景 在科技领域，“理性支持国货”显得尤为重要。毕竟不能为一些大局层面的考虑，牺牲货真价实的成本与效率。 好在“国框”其实在很多层面上已经足可与欧美一较高下，比如Caffe 创始人贾杨清在评价百度的PaddlePaddle时，也认同其在简洁、灵活、快速等领域功力不俗，并且解决了Caffe早期的不少遗留问题。 今天的“国框”已经不能说是为了爱国而爱国的强硬选择。在很多层面上，AI开发的自主、自有、自生态，已经可以在中国这个世界第二大AI技术实体与市场独立完成。 或许对于中美贸易战，以及可能的科技禁运等情况，我们还是有些过于敏感了。 诚然，硬件和底层技术有差距，是必须要承认的事实；但在新的领域，在争夺未来的原点上，中国科技工作者、开发者以及无数企业，一直都没有停止奔跑。中美之间的差距，今天也在以肉眼可见的速度缩小。 对待中美科技之争，更合理的方式或许是承认差距的同时，认清很多关键领域的国家自信。 更多核心领域达成技术突破；开发者、资本和平台有效组织产业聚合；营造更好的创新土壤，那么中国终有一天不会再被人牵着鼻子走。 星河流转之后，或许别是一风景。 转载来源：相比芯片，我们更该在意深度学习框架的中国化]]></content>
  </entry>
  <entry>
    <title><![CDATA[特朗普考虑启动紧急法案 扩大限制中方敏感领域投资权力]]></title>
    <url>%2F2018%2F9409b59a%2F</url>
    <content type="text"><![CDATA[紧急法案赋予总统广泛限制交易的权力 ，例如可以限制中国资金投资某些特定产业。 因此，美国政府可能双管其下，最后可能同时使用紧急的行政法限制和CFIUS新法来规范中资对美投资。 （特朗普 资料图） 《财经》记者 蔡婷贻/文 郝洲/编辑 4月22日，中国商务部公开就美国财政部长姆努钦考虑来华磋商表示欢迎，为两国近几个月来因为贸易问题引发的各种交锋带来些微缓和的希望… &gt; 紧急法案赋予总统广泛限制交易的权力 ，例如可以限制中国资金投资某些特定产业。 因此，美国政府可能双管其下，最后可能同时使用紧急的行政法限制和CFIUS新法来规范中资对美投资。 （特朗普 资料图） 《财经》记者 蔡婷贻/文 郝洲/编辑**** 4月22日，中国商务部公开就美国财政部长姆努钦考虑来华磋商表示欢迎，为两国近几个月来因为贸易问题引发的各种交锋带来些微缓和的希望。 美国对与中国展开的谈判以及可能达成的协议表示“谨慎乐观”，姆努钦指出。 不过，缓和极可能是表面迹象。就在数日前，美国商务部4月16日宣布美国企业至2025年为止，不可对中兴出口软件、技术和货品。接着，美国财政部助理部长塔博特（Heath Tarbert）于19日确认，该部门正研究是否为限制中国对美国特定高科技产业的投资和并购启动1977年通过的《国际紧急经济权力法案》(International Emergency Economic Powers Act，下称紧急法案) 。 美国限制外国投资的机构为跨部门的美国外国投资委员会（Committee on Foreign Investment in the United States，简称CFIUS）。塔博特称，根据特朗普的命令，在CFIUS之外，财政部还下设一个专门的办公室处理限制中国投资的事务。 美国国会也正推动修改法律，寻求从严规范外国资金对美国敏感企业的投资。但由于修改法律旷日费时，财政部考虑使用行政法直接阻止中资在美国的并购或投资。 紧急法案主要在美国面临“异乎寻常的严重威胁”时，对政权、个人和特定组织实施制裁。根据前例，美国使用该法案对其他国家**进行经济制裁时多出于政治考量，如伊朗和苏丹，而且制裁对象扩大到包括考虑与这些国家进行**商业往来的美国企业 。 特朗普上台后，曾于2017年8月25日对委内瑞拉动用紧急法案实施金融制裁，禁止美国金融机构参与委内瑞拉政府和国有的委内瑞拉石油公司新的债务和股权交易，禁止美方机构参与委内瑞拉公共部门现已发行的部分债券交易等。 企业并购行业的资深律师戴维斯（Christian Davis）对《财经》指出， CFIUS立法需要在参众两院通过后，特朗普才能签署成为法案；但即便如此，到时候CFIUS可能都不包括特朗普政府希望对中国投资使用的限制。而紧急法赋予总统广泛限制交易的权力 ，例如可以限制中国资金投资某些特定产业。 因此，美国政府可能双管其下，最后可能同时使用紧急的行政法限制和CFIUS新法来规范中资对美投资。 推动CFIUS修法和研究启动紧急是因应301知识产权调查结果的两个独立机制。紧急法案一旦启用，将赋予特朗普全面限制中国在半导体、机器人等敏感领域投资的权力。 美国国会研究（国会的智库）亚洲贸易和金融专家莫里森（Wayne Morrison） 对《财经》指出，特朗普政府目前采取的是“大棒”策略。主要目标包括防止中国企业通过并购美国公司在特定产业取得领先地位，实现“中国制造2025”的目标；同时也希望迫使中国对美国企业做出互惠对待，同等程度地开放市场 。 特朗普政府自3月22日在白宫宣布将对部分中国产品征收关税后，贸易争端引发的紧张就不断上升，各方对争端升级为贸易战的担忧也持续加深。 代表美国主要企业利益的美国贸易全国委员会就对《财经》指出，他们了解特朗普政府正考虑把相应投资限制当作一个选项，但是“我们看不出来这个政府的策略是什么…这怎么解决问题？这个政府要中方采取什么行动来改善知识产权保护和终止强迫技术转让？我们希望（美国）政府能说清楚。最终，美国企业寻求的是在中国和国内同业者在各方面享有平等的机会。” 由于这一法案从未在贸易纠纷时使用，部分贸易法专家质疑特朗普政府是否扩大解释了该法案中的“紧急状态”。 不过，彼得森国际经济研究所高级研究员赫夫鲍尔指出，法院从未质疑过总统宣称的“国家紧急状态”，因此引用此法案在法理上应不会受到挑战。 由于特朗普在竞选时做出各种强硬的贸易承诺，赫夫鲍尔于2016年就罗列出美国法律赋予总统在贸易政策上的行政特权；除了紧急法案外，《对敌贸易法》 （Tading with the Enemy Act）也赋予总统广泛权力，此法案允许总统监控所有国际贸易和金融活动，甚至冻结或没收外国资产，对象不限于敌对国家。1962年通过的《贸易拓展法》（Trade Expansion Act）赋予总统为巩固国家安全而提高关税的权力；另外，1974年的《贸易法》下，总统有权以处理国际逆差为由，将关税提高15%，为期150天。 戴维斯指出，在财政部公布紧急的具体实施方案前，对哪些投资的具体限制不得而知，但任何方面的限制都会使中国企业投资美国更加困难，特别是和“中国制造2025”相关的投资将可能是该法案限制的主要对象。 美国财政部将在5月底向特朗普提交新的投资限制计划。莫里森担忧的指出，“这个政府采取的策略是正确的吗？当两个对手实力悬殊时，边缘政策能奏效，但是中美对峙并不符合这种情况。中国也可以像美国一样，对商业往来采取各种限制。” 转载来源：特朗普考虑启动紧急法案 扩大限制中方敏感领域投资权力]]></content>
  </entry>
  <entry>
    <title><![CDATA[施一公：科研水平低下，所有精英都想干金融，是中国潜伏的最大危机]]></title>
    <url>%2F2018%2F379c86b3%2F</url>
    <content type="text"><![CDATA[作者：施一公，1967年5月5日出生于河南郑州，1989年毕业于清华大学，1995年在美国约翰霍普金斯大学获博士学位。中国科学院院士、结构生物学家、清华大学教授 。曾获第二届“未来科学大奖”之“生命科学奖”。 现任中国科学技术协会第九届全国委员会副主席 ，西湖大学校长 。 中兴事件注定将成为中国崛起路上的一个标志性事件。虽然自家的头牌“高科技公司”一打就趴下，但它能带给我们疼痛和清醒。西… 作者：施一公，1967年5月5日出生于河南郑州，1989年毕业于清华大学，1995年在美国约翰霍普金斯大学获博士学位。中国科学院院士、结构生物学家、清华大学教授 。曾获第二届“未来科学大奖”之“生命科学奖”。 现任中国科学技术协会第九届全国委员会副主席 ，西湖大学校长 。 中兴事件注定将成为中国崛起路上的一个标志性事件。虽然自家的头牌“高科技公司”一打就趴下，但它能带给我们疼痛和清醒。西湖大学校长施一公曾发表演讲：当所有精英都想干金融，这个国家一定出了大问题！以下为施一公演讲摘录： 如今我们的GDP已经全球第二， 但是看技术革新和基础研究的创新能力， 作为一个国家我们排在20名开外。 我不知道在座的哪一位可以心安理得地面对这个数字。 我们有14亿人口， 我们号称重视教育、重视科技、重视人才。 我们的科技实力、创新能力、科技质量在世界上排在20名开外。 有的人或许会怀疑， 会说我们都上天揽月、下海捉鳖了， 怎么可能创新不够， 我们都高铁遍布祖国大地了， 怎么可能科技实力排在20名开外。 我想说的是，你看到的指标和现象， 这是经济实力决定的， 不是科技实力决定的。 我们占的是什么优势， 我们占的是经济体量的优势。 请大家别忘了1900年我们签订《辛丑条约》赔款九亿八千万白银的时候， 中国的GDP也是世界第一， 但大不代表强， 这是我们面临的一个沉重的现实。 我在海外的时候， 只要有人说我的祖国的坏话， 我会拼命去争论， 因为我觉得我很爱国。 四月份， 我在瑞典皇家科学院年会上领奖， 晚宴时，与一位瑞典的知名教授聊天， 谈到中国的科技发展， 他很不屑一顾， 我觉得很委屈、很愤懑， 但是我轻描淡写地说了一句：不管怎么说，我们国家登月已经实现了，你们在哪儿？ 但他回敬了一句，让我说不出话。 他说：施教授，如果我们有你们中国的经济体量，我们能把五百个人送到月球上并安全回来。 我们对国家的科技实力和现状应该有一个清醒的认识， 中国的大学很有意思， 比如我曾经所在的清华大学， 学生从入学开始， 就要接受“就业引导教育”。 堂堂清华大学， 都要引导学生去就业， 都让学生脑子里时时刻刻有一根弦叫就业， 我觉得非常不可思议。 研究型大学从来不以就业为导向， 从来不该在大学里谈就业。 就业只是一个出口， 大学办好了自然会就业， 怎么能以就业为目的来办大学。 就业是一个经济问题， 中国经济达到一定程度就会提供多少就业， 跟大学没有直接关系。 让学生进去后就想就业， 会造成什么结果？ 就是大家拼命往挣钱多的领域去钻。 清华70%至80%的高考状元去哪儿了？ 去了经济管理学院。 连我最好的学生， 我最想培养的学生都告诉我说， 老板我想去金融公司。 当这个国家所有的精英都想往金融上转的时候， 我认为这个国家出了大问题。 管理学在清华、在北大、在整个中国都很热， 这是违背教育规律的一件事情。 每个学校都用就业这个指标考核领导， 这对大学有严重干扰。 我们的大学现在基础研究能力太差， 转化不出来，不是缺乏转化， 是没有可以转化的东西。 你们认为我们的文化鼓励创新吗？ 我觉得不鼓励， 我们的文化鼓励枪打出头鸟， 当有人在出头的时候， 我觉得很多人在看笑话。 当一个人想创新的时候， 同样有这个问题。 什么是创新，创新就是做少数， 就是有争议。 科学与民主是两个概念， 科学从来不看少数服从多数， 在科学上的创新是需要勇气的。 我们有1400万中小学教师， 我们虽然口口声声希望孩子培养创新、独立思考的思维，但我们的老师真的希望孩子们多提一些比较尖锐的问题吗？ 这和我们的部分文化， 师道尊严又是矛盾的， 所以我们在创新的路上的确还背负了沉重的文化枷锁。 转载来源：施一公：科研水平低下，所有精英都想干金融，是中国潜伏的最大危机]]></content>
  </entry>
  <entry>
    <title><![CDATA[专家：建议中国搞x86与Intel竞争的，都是在忽悠国家的钱]]></title>
    <url>%2F2018%2Fc7d8c52d%2F</url>
    <content type="text"><![CDATA[新智元专栏 作者：筋斗云 【新智元导读】为美国制裁中兴事件，对国内集成电路产业影响多大？芯片的基本生态是谁先做出一个超出同行的东西，大家都会自动地转入这个生态。即便ARM这样成功的公司背后，是无数产业链公司艰辛的活着。本文作者有多年芯片从业经验，他认为：所有建议中国搞x86的，与Intel竞争的，在行业内看来，都是忽悠国家/VC 钱的。 中兴事件对集成电路有多大的影响… 新智元专栏 作者：筋斗云 【新智元导读】为美国制裁中兴事件，对国内集成电路产业影响多大？芯片的基本生态是谁先做出一个超出同行的东西，大家都会自动地转入这个生态。即便ARM这样成功的公司背后，是无数产业链公司艰辛的活着。本文作者有多年芯片从业经验，他认为：所有建议中国搞x86的，与Intel竞争的，在行业内看来，都是忽悠国家/VC 钱的。 中兴事件对集成电路有多大的影响？ 作为行业内的人来说，基本没有影响。 这个行业产业链和生态最重要，因为硅片的NRE成本高，单价成本低。所以这个生态基本是谁先做出一个超出同行的东西，大家都会自动地转入这个生态。 这个类似于从A城市修了一条道到B城市。修道路成本极高，过路费的价格不高，而且过路费还在不断下降中。 现在你自己要重新修一条路，你的道路比别人窄，收费更贵，人家又修了B到C。你自己跑自己的路还比对手的路又慢又贵，你怎么进行竞争？ Intel和思科在当年占了硅谷一半以上的利润，无数的VC想再做个Intel出来，没有一个成功的。ARM现在成功背后，是无数类似的竞争公司的艰辛活着。欧洲、日本搞了多年，没搞出一个新的Intel。美国搞了多年，没搞出一个新的ARM。（最近RISC-V又在搞） 所有建议中国搞x86，与Intel竞争的，在行业内看来，都是忽悠国家/VC 钱的。 那么正确的道路是什么呢？弯道超车，大家一起竞争造新路。 路是无限的，而钱是有限的。如果我们把钱拿去重新造CPU，让美帝在新路上独家制造，收未来的过路费，我觉得这才是美帝最得意的阴谋了。（估计川普没这么聪明） 所以，我们不应该花大钱去造x86 CPU这样行为，不要去重新造一条同样的路。而是要往前一起造新路，因为这个时候你能拉到客户收过路费。 最近的新路有： 1、5G芯片。国家应该大力支持这方面的资金投入，特别是对企业的研究经费，如果能够对失败/成功项目的科研经费进行100%补贴。企业会大大增加新品研发的。 2、AI芯片。这个Google很奇特地没有加入战场，我们还有一定时间的余地。Google修了一条高速，开发了TPU，但是它只是自己用，不销售，也不收“过路费”。（其实这是AI公司头顶的一把剑，等到做出来了，谷歌可能又开源或者开卖芯片。） 3、数字货币。目前数字货币芯片已经占到台积电的10%，我预估未来还会上升。这是一个新品，而中国在此领先的。可惜去年9月的政策等在削弱这个领先。三星等在大幅度追赶。 4、低功耗GPU芯片。这个GPU不是为了显卡，而是为了类似VR/AR的新应用。 5、ARM服务器、RISC-V等，建议国家让企业多看着先预研，如果起来了，中国就自然地用新品替换了x86服务器等了。 数字货币最好的办法是监管和准入证，监管类似期货，普通投资者自然排除在外。目前这种扑灭的模式，如果遇到数字货币类似PC/通讯/手机这样大产业，我们会又一次上演拱手让人的悲剧。（我们数字货币软件已经如此了，数字货币金融创新基本全在美日了。就算矿机这个领域，三星也进入，三星有fab，极大优势。） 到底什么是新路，旧路还有多少油水，值不值得投入。这个每年看看fab的流片大致就知道了，其实并不是那么难理解的。 超越，说起来很简单，但是要考虑一个行业的特点，就不是那么容易了。很好的事是，集成电路是一个飞速发展的行业，旧路的大小基本定了，最发展的大多是新路。华为等在手机芯片上的切入，就是这种新路上的突破。比特大陆的崛起，也是因为它一直走的是条新路。趁着美日韩等还没注意时，就先修好了一条高速路。 中国在新路上多投入，别制造障碍，随着新路的繁荣，旧路过路费慢慢占比就少了。 那种别人有我一定要有的思维模式，会导致我们在旧路上投入过高，反而是歧路。 那些认为投入就能产出的，我建议先不计一切代价，先搞个国足世界杯冠军，因为显然这个要更容易些。 转载来源：专家：建议中国搞x86与Intel竞争的，都是在忽悠国家的钱]]></content>
  </entry>
  <entry>
    <title><![CDATA[巨头正在将互联网变成一个“伪市场”]]></title>
    <url>%2F2018%2F2f318e45%2F</url>
    <content type="text"><![CDATA[Fog Creek Software CEO Anil Dash提出，互联网用了一代的时间已经从免费开放的新市场变成了创建一系列利用社会的伪市场，而大多数媒体或政客甚至都没有注意到这一点。 编者按：互联网一开始是承载着我们开放自由的梦想而崛起的。但当巨头垄断了少数的平台时，他们慢慢开始作弊让天平向自己倾斜。Fog Creek Software CEO Anil Dash提出，互联网用了一代的时间已经从免费开放的新市场变成了创建一系列利用社会的伪市场，而大多数媒体或政客甚至都没有注意到这一点。我们作为消费者如何才能避免长期利益受损呢？答案是最简单也是最困难的。 1、开放的互联网市场 美国文化热衷用理想的竞争性自由市场作为包治百病的解决方案。尽管其自诩的自由市场对关心患癌症的婴儿并没有动机，一个完善的市场当然可以是看看哪一家提供商提供了一卷手纸或者一斤苹果最便宜的价格的很好手段。 鉴于这种文化嗜好，在web早期建立新市场是大家一开始比较喜欢做的事情之一。也许eBay就是典型例子；任何人（好吧，几乎任何人）都可以把自己的陶瓷雕像放到eBay上面去卖并且参与到一个相对公平的市场里面。在市场的一头，一群雕像的狂热粉丝正在踊跃地寻找最好的交易，在另一头，一群雕像供应商则要在价格、质量和服务上展开竞争。在中间，一个中立的市场通过即时更新的信息帮助将买家和卖家连接起来。每个人都很满意！ 后来，卖家可以为自己的产品购买在eBay搜索结果上的首选广告位置，而一些产品目录开始被批发供应商统治，但这仍然是一个相对开放的系统。每个人基本上都是满意的！ 在eBay推出后不久，Google也作为一种内容市场的形式面世了，其PageRank显然决定了哪些页面会出现在我们的搜索结果里面，其依据是导入链接的数量。一头是读者，另一头是出版商，中间是Google利用神秘但在一定程度上仍然可以理解的算法来建立一个几乎每个人都觉得自己能参与进来的市场。 可是不久之前，那些排名机制开始被spammer污染，因为在搜索结果中排名更高突然有了货币化价值，而制作垃圾链接的费用要比支付给Google的广告产品便宜。这时候开放市场应该怎么做呢？ 2、作弊市场的崛起 早期开放数字化市场不可避免的自动化对赌无意间催生了下一个时代：作弊市场。Google对恶意的搜索引擎优化技巧感到担忧，总是不断调整自己的算法，意味着只有那些能够不断调整自身技术跟上这场新的军备竞赛的web出版商才能发展下去。仅仅几年之后，这变成了一种富者恒富的经济，刺激了每一家较小规模的出版商采用少数标准化的出版工具来跟上Google的要求。只有最大的内容提供商才能付得起开发自己的工具同时还能遵循Google永远在变的算法要求。 问题不可避免变成了在最有价值市场成为最显著的那个。最终，在类似旅游这样有利可图的垂直市场，Google开始优先展示自己的订票工具而不是第三方订票网站的结果，其想法是他们的体验要好于第三方令人困惑且不一致的结果。没错，但对于Google来说赚钱也太方便了，现在他们已经开始从那些链接上赚到更多的钱了。 这就是web上一个微妙但极其重要的模式的开端：用户体验的短期改进帮助一家统治性的技术公司在远期占领一个传统的市场。 Amazon经历了类似的过程，他们后来开始试图影响结果，比如在产品搜索中优先展示自己的产品，哪怕他们的东西并不是最便宜的。我们目睹了一种迅速转变，那些经营着之前还是开放的市场的公司开始赋予自身不公平的优势，而市场的其他卖家根本没法抵消这种优势。 动图由Rob Weychert/ProPublica绘制 https&#58;//www.propublica.org/article/amazon-says-it-puts-customers-first-but-its-pricing-algorithm-doesnt 那些经营着之前还是开放的市场的公司开始赋予自身不公平的优势，而市场的其他卖家根本没法抵消这种优势。 这种像作弊市场的转变在应用商店中又得到了更淋漓尽致的体现，像苹果和Google这样的主流玩家可以选择那些app成为精选或者予以推销，同时防止任何会取代或威胁其市场统治力的app的建立。即便一个app取得了成功，应用商店也会推销广告支持的模式，让app创建者依赖于该公司的平台进行分发，而不是让app直接从用户获得收入。 但即便是在今天的作弊市场新玩家仍然有一些竞争的手段。你可以帆布一款新的照片共享app，并且理论上可以跟Instagram或者Snapchat在苹果的应用商店上同台竞技。一位普通的买家可以在Amazon的网站上面搜索“床单”，然后预期得到一系列可以购买的床单列表，其中既有来自独立制造商的，也有来自Amazon自己的Pinzon品牌。即便这些市场是扭曲的，它们仍然还是市场，是市场就会有机会。 当然这不是说这些系统就是公平的：大公司可以选择哪些玩家进入市场参与竞争，而网络不平等的问题意味着有足够特权成为早期采用者的人或者公司会获得不公平优势。但即便存在这些不公平待遇，我们仍然可以应付过去，新产品或者竞争对手有时候仍然能崭露头角。 这就是过去10年大部分时间的现状。但下一波技术创新者的崛起将令“市场”的定义甚至更加扭曲，扭曲到其实已经不算市场的程度。 3、现在：伪市场 Uber的承诺很简单：你用他们的app打车，独立司机池里面的一位司机同意载你，然后每个人都很满意。在他们的设想中，自己是一个中立的市场，把客户和服务提供商连接到一起——有点像eBay！ 不过跟eBay上的竞争性卖家不一样的是，Uber司机没法自己定价。实际上，Uber可以单方面（而且经常）改变价格。而在挑选司机这件事情上乘客无法做出知情选择：匹配乘客与司机的算法是不透明的——无论是对乘客还是对司机来说都是如此。实际上，正如Data &amp; Society的研究所表明那样，Uber有时候会在自己的app里面故意显示“幽灵”车给用户看，有意制造一个虚假的市场。 这个“市场”似乎有一些十分恶心和怪异的特质。 消费者无法信任被提供的信息来做出购买决定。1. 单个不透明的算法定义了哪一位买家与哪一位卖家匹配。1. 卖家对自己身的定价或者利润空间没有控制权。1. 监管者看到了真正的短期消费者利益，但并没有意识到会带来的长期伤害。单个不透明的算法定义了哪一位买家与哪一位卖家匹配。 监管者看到了真正的短期消费者利益，但并没有意识到会带来的长期伤害。 按照任何合理的定义来看，这些根本就不是市场。也许有人会把Uber称为是“伪市场”。尽管如此，通过很有心机地把系统内的司机称为是“创业者”并且采用真正市场的说法，Uber已经受到了社区和政策制定者的欢迎，就好像他们建立了一个新市场一样。这对于政策、监管甚至人权都具有重大影响。比方说，我们可以由衷地赞美Uber让非洲裔美国乘客更可靠更容易地打到车，但如果其对司机冥顽不化的偏见模式在Uber时代再度抬头的话，监管这些滥用行为就会变得更加困难，因为Uber通常并不遵循与监管有拍照的司机相同的政策。 这些伪市场模式还掩盖了补贴的模式，比如Uber目前的运营其实是投资者提供了补贴，每年高达20亿美元。这种成本很快就会转嫁到消费者头上，只要Uber成功取代了传统的士的话。 《金融时报》非常明确地指出了这种经济布局的潜在意图： 所有这一切都等同于一种从劳动阶级向都市精英的经济转移，受益的只有一家特定企业而不是别人。这实在是太疯狂了。 这些新的伪市场只有在对监管者和媒体使用障眼法的时候才会看起来像是真正的市场，因为后者对高科技解决方案的热枕是没有边际的，而他们对互联网上的市场的理解仍然停留在20年前的早期eBay时代。 伪市场不仅产生在传统产品和服务上——也来自于内容和出版的世界。出版商日益被激励去使用像Facebook的Instant Articles这样的平台以及Google的AMP这样的格式。就像Uber临时补贴更便宜的价格以及对打车服务更广泛的访问一样，这些新的出版格式也的确为消费者提供了一些短期的好处，其形式是更快的加载时间以及更干净的阅读体验。 但Facebook和Google提供更快速阅读体验的技术机制正好顺带取代了大多数的第三方广告平台——那些不是由Facebook和Google本身提供的平台。使用这些新的分发渠道的Facebook出版商被激励去使用Facebook的广告平台，其支付率和利润空间最终将随时可变。就像Uber在取代受监管的士期间补贴费用一样，Facebook也在他们取代第三方广告网络期间补贴出版商广告费。 除了让出版商在收入上对这两家技术巨头更加依赖以外，还有用于发现内容的算法问题。几乎使用Facebook的每个人都已经意识到其用于展示内容的算法是不透明的，无论是对出版商还是读者来说都是如此。因此，出版商可用来确保读者看到自己内容的可理解的技巧越来越少——而用Instant Articles格式发布是少数已知有效的办法之一。这正好又要求出版商把稀缺资源投入到支持Facebook格式上面，其结果是该出版商变得愈发依赖于Facebook进行分发。 那么：也就是说读者和出版商都不知道为什么Facebook会把一篇特别的故事展示到新闻流里面。而媒体监管者和政策制定者有没有办法对加载更快的故事的短期好处。 内容的伪市场看起来是这样的： 读者无法信任被提供的信息来做出决定。 单个的不透明的算法定义了哪一位读者跟哪一家出版商匹配。 出版商对自己的广告费或者利润空间没有控制权。 监管者看到的是对读者真正的短期利益，但并未意识到这会带来的长期伤害。 4、市场之后：自驱动新闻 不过请等一下，情况会变得更加糟糕。接下来我们会取代市场上的卖家。 现在共享乘车或者内容出版的局面是朝着一个由一或两家私有企业玩家控制的锁定系统快速迈进。但即便在这些伪市场里面，目前也还有多家提供商在生态体系内提供自己的服务。这些提供商是那些Uber司机或者Facebook出版商，这些提供商是值得赞美的，正是这些独立的创业者令平台欣欣向荣。 但Uber已经明确宣布了自己的路线图：自动驾驶汽车。备受称颂的独立司机创业者将会被全自动服务提供商尽快取代，而且那些新的自动驾驶汽车不仅没有要付费的司机，而且它们还是Uber所有的。当这一转变在未来10年变成现实时，整个独立承包商的市场将会被取代，这正好是社会保障体系被拆解的时点。与此同时，不同的政客一直都在把这些“零工经济”说成是未来的工作形态。 不过无人车是很难做成的。制造一个可以在城市里穿梭将乘客安全可靠送达目的地的机器人是一个极其困难的问题，需要很长时间才能做好。 相比之下，自驱动新闻的障碍又是什么呢？我们已经看到很多新闻消费者对安全可靠地将精确新闻送到自己手上并不感兴趣。这种情况下成功会容易得多：机器人出版商只需要提供情感上足够吸引人的内容来赢得读者的阅读即可。如果内容出版商或者分发商不关心故事正确与否的话这件事甚至还要容易。 还有要记住的是，Facebook往往会对那些利用自身新平台功能的出版商，但一旦那些出版商对他们形成依赖之后补贴就停止了。出版商已经在努力挣扎于媒体业总体的经济状况；Facebook的出价他们感觉是无法拒绝的。 那么我们该怎么办？ 在这些公司开发这些功能的大多数人目的并不是要破坏市场。在Uber和Facebook这样的公司的编码者和设计师意图通常都是好的，是真心为用户着想的。从目前看来，他们甚至都没有错；能够轻易打到车或者迅速读到文章是真正有好处的。但大多数技术员工，包括最大型技术公司里面的那些技术员工，他们对自己公司所有者和投资者激进的政治和社会议程都一无所知。 更糟的是，我们已经丧失了辨别能力，看不到给某些用户带来短期好处的补贴背后是难以维系投资模式，会给社会带来恐怖的长期后果。我们已经被风投注入到市场的资本所带来的暂时激励给迷住了，尽管我们知道这样的市场将会被技术变革和自动化所重塑。唯一能指望或者防止这些颠覆的社会力量是政策制订者，但是这帮人往往对高科技的工作机制又不甚了了，同时又极力想要自己身上蒙上一股“高科技”的光环，因为后者就是美国的世俗宗教。 我们加大对这些问题的发声力度是必不可少的，也许最有效的行动就是教育当选官员正在发生的改变。这个东西很复杂，而教育所有的议员为什么这些新的高科技app带来的改变从长期来看未必就是对我们的社区最好的是需要时间的。 但我们仍有时间让事情重回正轨。我们不可避免要被迫将我们开放的市场交给新的由1或2个技术巨头公司统治的伪市场。也许我们能够做的唯一一件最大的事情既是最难又是最容易的一件：我们可以改变自己的行为。马上看看你手机上的app吧。当每个人的手机上跑的都是跟你一样的app时，你确定会对即将发生的事情感到舒服吗？ 原文链接：https&#58;//medium.com/humane-tech/tech-and-the-fake-market-tactic-8bd386e3d382 编译组出品。编辑：郝鹏程。 转载来源：巨头正在将互联网变成一个“伪市场”]]></content>
      <categories>
        <category>其它</category>
      </categories>
      <tags>
        <tag>软件</tag>
        <tag>Google</tag>
        <tag>eBay</tag>
        <tag>亚马逊公司</tag>
        <tag>苹果公司</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[碎片信息太多怎么办？这几款收集神器来帮你]]></title>
    <url>%2F2018%2F6c693166%2F</url>
    <content type="text"><![CDATA[而在手机端上就是各种复制，「方片收集」中的“历史剪切板”上就会出现复制过的信息，只需把有用的信息左滑即可，超简单的。 碎片化的信息的充斥，当遇到一些好的文章、有价值的网页、优美的句子，总会把它收藏起来。其他还好，但是收藏零散的句子却挺麻烦的，一般先复制下来，打开便签，将复制的句子粘贴到便签上。这么多步，就算是有耐心的人都很烦这种操作。而且每次的不经意收藏，日后总会面对各种凌乱，那怎么办呢？ 方片收集：不一样的摘抄方式，方方片片都收集「方片收集」是一款碎片化信息的收集与内容整理神器，简单并没有繁多的步骤。 在电脑端加入了「方片收集」的插件后，将需要的资料（图片/文字/网址/视频）向左拖拽就可以了，它还会帮你表明来源呢。 而在手机端上就是各种复制，「方片收集」中的“历史剪切板”上就会出现复制过的信息，只需把有用的信息左滑即可，超简单的。不过，收集前记得把软件打开。 除了各种碎片化信息的收藏、摘抄，方片收集还有个“便签”功能，供用户自己进行输入，输入时不仅可以文输入字，还可以插入图片，超链接，视频等。 而且它还击中了一个痛点：可以实时保存。再也不怕手机突然发生故障，好不容易写的信息因没有保存而丧失。在「方片收集」中，每当输入一个字，就会自动保存一次，再也不会发生信息丧失了。想让你的生活更加简约流畅但又高信息化，那就试试这款软件吧。 收趣：收藏有趣的内容，你也会变得有趣 「收趣」一款偏向于稍后阅读的软件，收藏各种信息后会以卡片的形式呈现出来，而且用户可以自由整理标签，排序、搜索，超级方便阅读。在阅读的过程还可以对收藏信息进行修改、记笔记等。 除了自己收藏的内容外，「收趣」的“发现”还藏有不少好的文章供阅读。 当不想看的时候，可以“听”。收趣支持原文朗读，在文章中有个耳机的图标，点一下就可以让眼睛休息一下了，提前用WiFi下载好语音包，就不用担心流量了。零碎时间读读文章、做做笔记或者“听”一篇好文，不觉得挺好的吗？ 有道云笔记：这款笔记的收集功能，也是挺不错的 对于网址或链接，复制后打开「有道云笔记」，就可以收集所复制网址信息。而对于互联网的一些 APP 则有其特有的收藏方式，如打开微博，在下方评论&#64;有道云笔记就可以，挺新颖的。 生活中，很多时候在看到报纸、报刊、各种读物时，有一些比较好的文章想记下来，一般都是用手机一个字一个字的打，或者是用手抄，而「有道云笔记」可以帮助用户将想要的部分拍下来，通过图片识别文字功能来识别文字。 而且语音记录功能，也特别不错，如：开会时开启该功能，做会议记录的时候就不怕遗漏了。 「有道云笔记」在登录后会免费送 3G 的云空间，可以储存很多文章文档。而且，只要每天签到都能得到一定的云空间，只要每天签到，云空间就越来越庞大。 幕布：整理信息也许需要一款思维导图 很多时候，一味的收集各种信息，到头来会发现：凌乱。这时可以试试用「幕布」来做收集工具，它具有很强的整理能力。 收集起来的内容，可以根据各种需求进行编辑，简单的操作就能让凌乱的内容更具条理性、清晰了然。而且如果不想面对索然无味的文字，还可以转化为有趣的思维导图。 会整理信息也是一种技能，学会使用之几款神器，就相当于学会了一种新的技能。面对这些具有简约设计界面、粗暴强大功能的应用，相信很多朋友已经按捺不住了，那就一起去发掘它们吧！ 转载来源：碎片信息太多怎么办？这几款收集神器来帮你]]></content>
      <categories>
        <category>其它</category>
      </categories>
      <tags>
        <tag>软件</tag>
        <tag>有道云笔记</tag>
        <tag>网易有道</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【深度】微视重启与腾讯错失的一年]]></title>
    <url>%2F2018%2Fce3f8661%2F</url>
    <content type="text"><![CDATA[”对方听到也有些无奈，她告诉严然：“你目前看到的所谓微视补贴都不是微视给的钱。全是那些经纪公司和工会打着官方的幌子在外面招人，我们的补贴3月才会出来，他们现在只是为了抢占市场”。 今年二月份春节期间，看着朋友圈转发着各式各样的微视补贴政策，严然忍不住问了微视对接人一个问题，“官方补贴到底什么时候开始？为什么现在会流出这么多版本？” 对方听到也有些无奈，她告诉严然：“你目前看到的所谓微视补贴都不是微视给的钱，全是那些经纪公司和工会打着官方的幌子在外面招人，我们的补贴3月才会出来，他们现在只是为了抢占市场。” 情况和严然想象的差不多。但得到了对方会在三月开始发力的消息，她还是有些紧张，毕竟，按这个时间来算，她只剩下不到一个月的时间做准备。 作为一名资深达人经纪，严然手底下运营着几百号网红，做到这个阶段，她在意的已经不是补贴，而是腾讯到底有没有把短视频放在一个很重要的位置。“如果答案是肯定的，那微视至少会迎来几个月的红利期。而这样的平台和机会，对新人来说绝对是可遇不可求的。” 三月，官方补贴如期而至。令所有人意外的是，这一次，微视竟然给出了高达30亿的巨额补贴。不仅如此，4月10日下午，腾讯内容开放平台也发布回应称，未来将持续为微视提供优质的原创短视频内容。 一位接近腾讯的人士告诉界面新闻记者，微视拿出的这30亿其实正来源于企鹅号2018年的补贴计划。在去年底的腾讯全球合作伙伴大会上，腾讯集团首席运营官任宇昕曾介绍，腾讯将打造全新的企鹅号，计划2018年投入100亿元人民币，服务图文资讯和短视频的创作者，而微视就是这计划中的一环。 事实上，作为中国最早开发短视频产品的企业之一，腾讯的短视频之路一直充满坎坷。早在2013年，腾讯就已经以“微视”这一品牌推出了一款短视频产品，然而，到了2015年，这款产品就被腾讯战略边缘化，直至2017年4月10日被正式关闭。 某种意义上说，现在重新回归公众视野的微视已经是一款全新的产品，不仅账号数据未和老微视打通，产品设计也和以前大不相同。 回溯过去的几年，短视频似乎是一直是腾讯想做却做不好的内容，为什么会出现这种情况？如果说新微视的目标是追击抖音，那钱烧完了之后呢？当战争真正发展到产品和运营之争的时候，新微视又有几分胜算？ 为了解答这些问题，界面新闻记者采访到几位2013年就加入微视的老员工，包括部分接近微视的网红和代理商，试图还原一个真实的腾讯短视频发展史。 高光时刻事实上，腾讯内部最早提出要做一款短视频产品的是现任小米生态链副总裁、腾讯微博早期负责人高自光。然而，在更多人的认知里，与微视紧密相连的却是另一个名字——邢宏宇。 这位腾讯OMG的高管曾历任搜索产品部副总经理、网络媒体产品技术部总经理、微博事业部总经理等职务，直至2013年接手微视。 按微视早期员工Amy的话说，邢宏宇接手微视纯属临危受命。“Grandy（高自光）提出这个项目不久就去小米了，hy（邢宏宇）只能硬着头皮接。” 在Amy看来，微视就是一个烫手山芋，原因在于产品从立项就没有一个清晰的定位，”Grandy在内部总说我们要再造一个Vine，但当你问他更具体的思路时，他的回答就变成了，国外有，我们也必须有。” 换句话说，微视是高自光提出的项目，但高并没有想清楚这款产品的定位，直到后面邢宏宇接手，这个问题也依旧存在。而且，从后面的产品设计来看，刑对于这种针对年轻人的短视频社交产品也并不擅长。 据一位腾讯微博的老员工回忆，邢宏宇会接手微视纯粹是因为当时已经没有更好的选择。 2013下半年，由邢主导的腾讯微博已经开始疲软。微信出现以后，腾讯微博作为防御性产品的战略重要性下降，到了后期，核心成员更是大量流失。“对hy来说，接手微视也算是最后一搏。他甚至和Pony承诺，这次如果还做不好就立马走人。” 另一方面，短视频在当时确实是一个热门的创业方向。2013下半年，4G网络正在逐步推行，很多大公司都认可短视频是一个值得布局的新赛道。在阿里投资了趣拍，新浪微博投资了秒拍的情况下，无论是基于提前布局或是战略防御的考虑，腾讯都需要有自己的短视频产品，而微视就是在这样的背景下诞生。 2013年09月28日，微视上线App Store，主打8秒短视频，用户可以通过微信、QQ和QQ邮箱账号登陆，同时支持分享短视频到微信对话、微信朋友圈以及腾讯微博。 据Amy回忆，一开始，腾讯还是非常重视微视的，从邢宏宇往下，产品和运营的总监都可以和马化腾在一个群里直接汇报。“为了推广微视，Pony（马化腾）还在平台开了号，传了4条短视频。” 2014年春节，腾讯大手笔的邀请了李敏镐和范冰冰等明星为微视拍了一则电视广告，据当时的宣传通稿显示，春节期间，微视的日活跃用户一度高达4500万人，除夕至初一共有数百万人通过微视发布、观看拜年短视频，总播放量达上亿次。 在Amy心中，那几乎就是微视这四年来唯一的高光时刻。 问题爆发如果以现在的眼光审视上线之初的微视，你一定可以挑出一大堆毛病，例如页面不好看，画面不清晰，没有办法把女孩子拍美等等。 对于这些毛病即使是微视的前员工们也都是认可的。一位微视早期的运营人员林冉告诉界面记者，一开始，微视甚至都没有滤镜功能。“是黄一琳（昕薇模特，微视早期达人）提了很多次需求我们才上的。” 林冉把这种迟钝归结于团队的审美老旧，“我们团队大多数人都在70-80之间，要做一款90后甚至00后爱用的产品理解起来太难了。” Amy则有另一种看法，在她看来，很多问题其实会上都讨论过，但最后就是无法推进，归根结底还是体制问题。“腾讯太大了，很多需求就算提出来了落实到产品和技术上也会变得无比冗长。”而这种拖延对于团队的士气又是一种很大的消耗。 但无论如何，微视在那时还是少有对手，平台一开始采取的明星策略也吸引了一大批用户紧随而来，巅峰时期，范冰冰在微视的粉丝甚至多达600多万。但换一个角度来看，中心化太过严重也会导致普通人很难在这里找到存在感。 林冉彼时运营着一个上百人的微视达人群，通过她的观察，能进到这个群的，几乎都是本身就有硬实力和专业背景的人。“微视的门槛其实很高，它并不能把女孩子拍美，一条时长又只有8秒，要想在这么短的时间展示自己，需要非常强的硬实力。” 2014年4月，林冉明显感觉到，经历了春节的爆发增长，红人们已经开始有些疲惫。此时无论是上传视频还是用户活跃都呈现了下降趋势。 为了挽回颓势，在接下来的一段时间，微视一直在不停地调整运营策略，但在林冉看来，部分调整反倒是导致平台走错方向的导火索。“我承认运营方面责任很大，我们太主观了，对于想要一个什么样的社区没有明确的认知，导致很多真心创作的人得不到推荐，反而那些低俗的熊孩子系列，或者是那些所谓高逼格的国外系列得到了推荐，到后来平台上就充斥着诸如此类的样板内容，用户看着不舒服，我们也难受。” 更重要的是，在微视进入瓶颈期的同时，美拍上线了。虽然效果同样不够清晰，但美拍提供的滤镜和MV工具，很快就让其和同类产品拉开了距离。 Amy用了“噩梦”这样的词汇去形容美拍对微视的影响。“美拍上线之后我们才意识到，做一款年轻人的产品，把用户拍美真的是刚需。” 虽然微视后来也仿效美拍上线了小mv功能，但最后的呈现效果还是与美拍相差甚远。这件事儿对林冉打击很大，“抄美拍我觉得都没什么，但问题是最后出来的效果特别土，我就想不通，腾讯不应该比美拍技术更强吗?” 这之后不久，林冉就离开了微视，Amy虽然还在，但7月发生的一系列事情，开始让她对微视的发展前景产生怀疑。 2014年7月，微博事业部降级被整合到了腾讯新闻，在外界眼里，这几乎就是被腾讯战略放弃的意思。此前，微视一直都是挂靠在腾讯微博下运营，微博并入新闻后，微视也进行了组织架构调整，不仅成立了单独的产品部，还组建了版权合作、平台运营、产品技术以及客厅业务等四个部门。 在外界看来，这都是腾讯重视微视的表现，但在Amy看来，微博被放弃以后，微视就失去了赖以传播的土壤。再加上，微信当时也在筹备小视频功能，7月以后，微视在组织内的位置就变得非常尴尬。 “当时能明显感觉到腾讯内部对我们都是不看好的。从5月开始，团队里核心成员就在逐步流失，好像又重走了一回微博的老路。”在Amy的记忆里，2014年下半年对整个团队来说异常难熬。美拍和秒拍都在急速成长，腾讯又取消了微信对微视的资源支持，尽管后来微视将拍摄时间从8秒延长至5分钟，但体现在数据上还是没有任何火花。 “在腾讯这种赛马机制下，如果给过你资源效果还是不明显，最终就是产品线被裁撤的命。”9月，Amy也离开了微视。 邢宏宇在腾讯的最后一搏失败了，2015年3月，微视产品部被降级并入腾讯视频，随后邢宏宇离职加入58同城，微视的运营总监何钐则转岗加入腾讯内部一个新产品团队。 至此，微视基本已经被战略边缘化，腾讯的第一次短视频尝试也以失败告终。 微视复活此后长达两年，微视都只有几个技术在做简单的维护，体现在大众眼里，这款软件相当于就是消失了。 真正的死亡宣告书是在2017年3月下达的。彼时，腾讯发表声明，称将在4月10日正式关闭微视。不久，腾讯投资快手的消息传来。 罗休休是微视平台上发布视频最多的达人，即使在后来腾讯放弃运营微视，她也依然有坚持在微视上同步视频。听到微视即将下线的消息，罗休休把自己关在房里大哭了一场。 和其他人略微不同的是，罗休休就像是一个被微视一手带大的网红，大一的时候，她曾在腾讯微博实习2个月，后来，微视内测，运营人员一下子就想到她，而她也成为了微视最早的一批用户。 对罗休休这样在微视上上传过几十甚至上百条视频的达人来说，下线App，几乎就等同于抹掉了他们最珍贵的回忆。但腾讯已经做出决定。 然而，就在大家都觉得微视已经退出历史舞台之后，2017年5月，App Store里又出现了一个全新版本的微视。助理告诉罗休休，这个版本的微视和抖音很像，主打的是音乐短视频与对口型录制。 与其说这是老微视复活之后的产物，不如说，它已经是一款全新的产品。 据一位腾讯内部人士介绍，一开始，腾讯也没有想明白要怎么重新杀入短视频，直到2017年抖音的异军突起，才让腾讯内部真正的感到紧张，随后，微视便被复盘到了深圳SNG事业群去做，由QQ空间团队正式负责，此后，微视的迭代速度就明显加快。 2018年，微视相继推出了高能舞室、视频跟拍、歌词字幕、AI美颜美型滤镜等四个功能，并打通了QQ音乐的千万曲库。不仅如此，腾讯还宣布，不止音乐，腾讯生态里的所有游戏、动漫、影视、综艺将为微视提供内容支持。 在目前的环境下，短视频在腾讯内部的战略重要性无疑将再次提升，但单靠输血能不能达到很好的效果却仍需打一个问号。 界面新闻记者于近期采访了多位接近微视的网红经纪和工会组织，听到最多的一句话就是“微视现在太乱了”。 这种“乱”主要是因为微视选择了和供应商合作去招揽红人，但供应商群体庞大，包括工会、经纪公司、代理公司、MCN等等。这本身就是个鱼龙混杂的圈子，选择了走这条捷径，也就要承担“乱”的后果。 据界面新闻了解，现在和微视合作的代理公司至少也有100余家，每一家的抽佣标准都不同，这样就导致外部流传的微视补贴政策五花八门，甚至有人给出了上万的金额。严然说，这种看起来很高的补贴大多都是假的，“他们的目的就是招人，招到了以后给不给结算就不一定了。” 除此以外，也有一些实力较大的组织选择了暂时观望。越度传媒就是其中之一，该公司的网红中心总监杨涛告诉界面新闻，像他们这样规模比较大的网红公司，目前很难花费太多的精力去运营微视，“你只能说是以布局的心态去做，因为新平台前期基本是赚不到钱的，我们现在手头抖音的单子又多，很可能在微视辛苦拍一月视频赚的补贴在抖音一个单子就赚回来了。” 根据杨涛的介绍，目前抖音达人接一单活的市场均价大概是三万，而微视最高的S级补贴是一条1500元，换句话说，一个各方面素质不错的达人，在最勤奋的情况下，一个月也只能拿到2-3万的补贴。而微视对内容又有独家的要求，也就是说，如果选择拿微视的补贴，那基本就需要放弃全平台运营，这里面的机会成本，对于已经在其他平台成为头部的玩家来说几乎不可承受。 但无论如何，对那些在抖音和美拍暂时拿不到广告资源的中腰部网红来说，微视的补贴力度对他们来说还是有一定吸引力。 但就算能用钱买到内容，在抖音快手已经占据了大部分目标用户心智后，微视又能拿什么来抢夺这些用户的消费时间呢？ 一位曾和微视市场负责人沟通过的代理商表示，目前微视官方对于操盘方向也是一头雾水。“聊下来的感觉就是，他们真的很焦虑。毕竟，短视频创业也是有时间窗口的，时间窗口过了，不是有钱就能弥补。” 下图是微视官方给各大代理商提供的内容范畴，可以看到，目前微视还是较为侧重生活和搞笑类的内容，而这也是抖音现在的运营重点。 换句话说，现在微视初期的打法基本就是以抖音为对标。但问题在于，腾讯微博的例子已经能够证明，尽管腾讯可以为微视输送流量，提供资源，但是，本质上如果还是在用竞争对手的那一套逻辑去做产品，最后大概率也就是拖一拖对方后腿，一款防御型的产品不可能做到对竞品取而代之。 拼多多敢于正面对抗淘宝京东，本质在于他抓住了二者此前忽视的下沉群体，QQ有7.83亿用户，微信则有9.88亿，就算略过抖音快手现在2亿左右的重度用户不看，腾讯也还有很大的空间去寻找新的增量，没必要抓着竞品不放。 此外，现在对于微视来说其实是一个发展的黄金时期。毕竟，目前针对短视频应用的监管力度正在不断加大，抖音和快手都相对进入了增速放缓阶段，在这个时刻花钱买时间无可厚非，但钱烧光了之后呢？市场不需要更多的同类型产品，微视必须尽快想明白这一点。 （应采访者要求，Amy、林冉为化名） 转载来源：【深度】微视重启与腾讯错失的一年]]></content>
      <categories>
        <category>科技</category>
      </categories>
      <tags>
        <tag>移动互联网</tag>
        <tag>微视</tag>
        <tag>马化腾</tag>
        <tag>Vine</tag>
        <tag>任宇昕</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[结合产品从0到1，阐述各阶段的产品方法论]]></title>
    <url>%2F2018%2F5f9dc947%2F</url>
    <content type="text"><![CDATA[文章围绕产品从0-1的每个阶段，并结合作者自身实践总结了其中的方法论，希望能够给大家带来一些帮助。下文将围绕产品从0-1的每个阶段列举阐述其方法论，并加以具体的栗子说明，算是自己一年多的产品学习及实践总结，不足之处还望各位大佬们指正。 文章围绕产品从0-1的每个阶段，并结合作者自身实践总结了其中的方法论，希望能够给大家带来一些帮助。 产品分析的“五要素法”是什么？需求采集的“Z字采集法”又是什么？如何用“KANO模型”对需求进行分类及优先排序？如何确定MVP？“Hooked模型”是如何让用户对产品上瘾的？ 如果你有上述一个或多个疑问，我猜你产品经验不足2年或者没有系统的提炼产品体系，那么请继续阅读下去，下文将围绕产品从0-1的每个阶段列举阐述其方法论，并加以具体的栗子说明，算是自己一年多的产品学习及实践总结，不足之处还望各位大佬们指正。 产品从0到1的阶段一个产品从无到有再到用户手上基本都可分为3个大阶段。 第一阶段：一定要想清楚产品做什么。给B端用户还是C端用户，做工具型的还是内容型的，做交易还是做平台。主要用户是谁，要解决Ta们什么痛点，这些痛点又是在什么样的场景下产生，用什么样的解决方案可行，跟现有的解决方案相比有何竞争力。 Eg：之前有过一个诗词项目，一款拍照配诗工具，主要用户为有诗词情怀的人，帮他们在一些特定的想要分享表达的场景中筛选出应景的诗词，制作出有意蕴的图片。比起图片社交的“黄油相机”，保留图片文字处理、元素添加等功能，提供更丰富的诗词数据内容，比起诗词收录“西窗烛”，我们提供更细化的标签维度与筛选机制，将诗词与图片完美结合。 第二阶段：用最高效的方法、最舒服的协作将产品做出来。这期间包括原型评审、立项启动、敏捷开发等，解决方案务必得到整个项目组的一致认可，小版本迭代更新，所谓“天下武功，唯快不破”。 第三阶段：将产品推出去。这个无用多说，产品做到一定程度后都是为了盈利，只有上线后才能获取流量，才有机会将流量转化成有价值的用户，才能从用户身上获取收入实现盈利。 下面是我近期一个社交游戏项目的文件，基本是按照规范流程执行的，仅做参考。当然并不是每个产品都必须经历每个阶段，不同的公司不同，不同的产品也不同，适合自己的才是最好的。 每个阶段及涉及的方法论1、概念提取与筛选产品的概念包括核心用户，刚性需求，典型场景及竞争优势。 产品概念来源：**1、自身或他人的需求随着越来越多的85后开始考虑小孩上幼儿园问题，如何筛选出最合适的学校。教育择校平台产品——有钱兔APP2、现有产品的启发**如现在的社交游戏应用。（以H5小游戏切入）同桌游戏、快手小游戏、开心斗 体验竞品及分析的方法即可用到“五要素法”和“5W1H法” 1.1 五要素法_竞品体验及分析 产品体验五要素包括：表现层、框架层、结构层、范围层、战略层，正向可用来分析产品，逆向则可以用来创造产品。 以“拼图酱”体验分析为例，拿到应用时，主色调以浅灰和橙、粉为主，给人一种活泼、浪漫、女性、健康的“氧气质感”，整体调性偏小清新文艺，当体验完它的页面操作时即可知它的主业务流程：选择模式→选择图片→根据选择的图片数量分别列出不同的版式→滤镜添加文字等图片编辑→保存，主要功能为多种布局选择、加多种滤镜多种外框等进行拼图，其它功能为拼接长图、海报模型、主题推荐。回头再理解这个产品时，她就是一款多张图片美化、设计、拼接、制作及分享的APP工具，帮助那些热爱生活喜欢分享的年轻人文艺的记录那段回忆。 1.2 5W1H分析法_竞品体验及分析 以“网易严选为例” - what_零售行业的电商产品，ODM模式（原始设计制造商）包含供应链。- why_通过ODM模式与大牌制造商直连，剔除品牌溢价和中间环节，提升性价比。- who_注重品质、看重性价比、了解ODM模式的用户。- where_看中品牌但又不能接收品牌货价格的时候。- when_注重商品性价比，多方比较包括产品的品质/颜值/描述/价格等再做出购买决策。- how_线上性价比高版的“无印良品”，从信任我的“甄选”，到信任我的“品牌”。 当有一定的产品概念时，就需要结合内外实际情况对其进行筛选，看是否有价值做下去 1.3 产品概念筛选要素 如当时我们提出了一个做“幼儿园择校平台应用”的概念，可先满足家长对武汉市所有幼儿园信息的分类筛选与对比，但具体到幼儿园数据来源的时候，幼儿园简介、师生配套信息、费用、招生计划等信息需要幼儿园相关的很多人脉资源，最后这个概念也pass了。 外部筛选可考虑政策支持、市场空间等，用到PEST分析法、波特五力模型、SWOT分析法 1.4 PEST分析法_宏观环境分析 对于大环境要顺势而为，如蚂蚁森林就是逐步开放的“碳汇交易”的一个载体，而“河长制”也将为水利信息化带来更多的业务方向。 1.5 波特五力模型_行业环境分析 以“社交游戏”为例 - 同行业内现有竞争者的能力，即现有市场成熟度和竞争激烈程度——强。现有“同桌游戏”和“快手小游戏”等，均以H5小游戏切入陌生人社交，做国内的市场稍艰难，可考虑海外市场。- 潜在竞争者进入的能力——强。微信小游戏“跳一跳”一出就有现象级霸屏，故小游戏未来很有可能进一步优化其在对话列表的能力，即直接可以在对话界面玩互动游戏。- 替代品的替代能力——弱。增加更多玩法的视频社交暂时不会取代。- 供应商的议价能力——中等。上游游戏内容提供商不太集中，但也有可能跨界到社交应用领域。- 购买者的议价能力——弱。下游客户并非一个，暂不存在客大欺店问题。综上所述此款“社交游戏”国内市场竞争激烈，可考虑做海外市场，快速出版本验证迭代。 1.6 SWOT分析_了解竞品，制定产品策略 以“武汉吃虾地图”为例（一款专门提供吃虾信息服务的工具，利用抓虾互动游戏将线下活动和线上服务结合） - S：长江垄上作为湖北本土的服务“三农”媒体和产业集团，自身有着精细化的水产信息相关资源及运营基础，吃虾地图本身的数据易收集，上下游的配套服务也支持功能扩展，借助已有的新媒体资源易展开推广运营。- W：休闲互动小游戏玩法单一，用户留存堪忧。- O：虾作为美食届的网红，也是电商届的新宠，吃虾已经成为了越来越多人的一种生活选择方式，以精美的吃虾地图切入生活信息服务细分市场，与抓虾休闲互动小游戏结合，另有完善的上下游服务体系做支撑，定给产品带来无数的可能。- T：口碑网和美团已有稳定的用户群，在内容方面已部分涉及虾板块，如何深度挖掘存量市场是一大挑战。SO战略：收集丰富的数据，制作精美的吃虾地图是基础。ST战略：相较于品类服务丰富的口碑和美团，吃虾地图将虾这一细分市场做细做精，再将模式复制到蟹、水产乃至其它。WO战略：做好休闲互动小游戏的用户体验（包括操作简单流畅、视觉效果佳），更重要的是争取福利更多的赢券等运营活动，通过游戏引导用户参与。WT战略：挖掘更多美团和口碑忽略的内容，如上游供应商的来源等。 2、需求收集与分析转化当筛选完产品概念觉得它能产生用户价值和商业价值时，就该进一步收集更多的相关需求，收集的方法很多，包括产品规划前期确定产品方向是什么的“用户访谈”，项目早期产品功能优先级的“调查问卷”，项目实施过程中验证需求实现方案的“可用性测试”，以及产品上线后迭代优化依据的“数据分析”。此部分先说明用户访谈和调查问卷，可用性测试和数据分析后文再提及。 2.1 用户访谈 _需求采集 做任何一件事情都要清楚为什么要做这件事，如何做，做完之后能输出什么。故访谈前就要规划好访谈对象、访谈时间、访谈素材准备，若为产品规划阶段访谈对象可从周围朋友中选择“潜在用户”，若为了解某个现象背后的原因，可深度了解这些典型用户。条件允许可约几个或者十几个用户到公司聊一聊，一般持续几十分钟到几个小时，通过问答了解用户的目标和观点。 以我们诗词图片社交工具为例，当时第一个内部小版本出来后成片效果不够精美，基于我们的核心优势在于筛选出应景的诗词，所以主流图片应用的滤镜、元素自定义等多种功能我们没有做，这时就找了几个朋友（图片工具的一般/重度用户，也可定义为我们的“潜在用户”）了解下他们的观点。具体我是以下面几个问题切入的： - 平时拍照、有时加些有趣的贴纸文字等一般使用哪些APP比较多，有没有不好用的地方。——（了解竞品， 看到潜在竞争对手的不足，优化自己产品 ）- 用过黄油相机或者IN没，怎么知道它们的，感觉怎么样？（了解对标产品的吸引点）- 如果现在有一款拍照配诗的APP，它有哪些功能你才会去用它？（希望用户提些建设性意见）现在看看这个问题问的最傻，我们不能给用户提太难的问题，而应该引导他们去回忆、描述一些故事来发掘他们的需求，所以这个问题可以优化如下：用黄油相机是如何挑到自己喜欢的模板的？（了解他们的行为习惯，对标搜索、推荐功能）如果有主题查询功能，会偏向于哪种风格的主题？（了解内容偏好）做完访谈后就该对结果进行梳理分析，需求进入需求池。 2.2 调查问卷_需求采集 调查问卷可用于项目早期功能优先级的参考，也可用于项目过程中的验证，在不影响用户体验的情况下可在WEB端或APP端以弹窗或者小功能入口的形式存在，针对人群根据目的设计内容。若内容较多，则开篇要简单，需要思考敏感的问题放中间，跟被访者相关的个人信息放最后。 如天天P图的小调查 2.3 马斯洛需求原理_需求分析 此时我们应该收集了无数个需求，无论是是一手需求还是二手需求，除了一些表面需求（用户的观点和行为），我们还需要对需求深度分析，了解需求背后的目的以及反映的人性。马斯洛需求层次理论就是对人性需求最常见的解读； 以具体的用餐需求来说明这5个层次。首先食物本身满足消除饥饿的生理需求，其次相对健康的选材及独立隔音的小包间满足安全需求，用餐期间借酒助兴满足社交需求，用餐环境的优质与用餐服务的周到满足尊重需求，图片或视频分享来展示生活的仪式感和饮食文化满足自我需求。 2.4 七宗罪_人性解读 七宗罪揭露了人类原始本能欲望，在日常产品中都可找到印证。 2.5 需求转化及Y模型_需求转化 ① → ② → ③ 拿到用户需求时，首先要思考why，即需求背后的真实目标及动机，再思考how，即用什么方法什么功能来解决用户的问题，尝试进一步挖掘到需求的本质即④，列出所有的解决方案，最后再根据优先级和性价比来确定功能组合。需求转化的原则为“用心听，但别照着做”。 经典案例福特用户需求：“我需要一匹更快的马”真实目标：为了更快的到达某地最后转化的更靠谱的解决方案：汽车，而非研发新饲料或者改进训练方法让马跑得更快。 3、功能管理及MVP的确定3.1 KANO模型_功能分类 A、基础功能（不做很不满意，做了觉得理所应当）——必做- B、亮点功能（不做没感觉，做了赞不绝口） ——产品初创期实现个别低成本的- C、期望功能（也称线性需求，实现的越多用户越满意）——先做性价比高的- D、无差别功能（做不做用户对产品的感受无变化）——低成本验证后再做- E、反向功能（做的越多用户越讨厌，比如广告位）——需要权衡各方利益后再决定 探探作为社交产品的后来者，功能迭代并不多，但定位清晰、操作简单，适合简单的分析。若你作为探探的产品经理，如何将下列功能进行分类。 - 同城定位及推荐。用户信息卡片的展示，包括图像、年龄、距离、职业。- 筛选操作，右滑喜欢，左滑无感，相互喜欢即配对成功。- 聊天可发送文字、表情。- 聊天时的表情和文字全屏特效。- 滑动筛选时的反悔操作。- 用户最后活跃时间的显示。- 用户的个性签名、标签、兴趣爱好。- 匿名暗恋表白。- 筛选条件的设置。- 聊天时的辅助模块，真心话和私密话。- 超级喜欢功能。若对一个人操作了超级喜欢，则对方会收到此消息的推送。- 会员充值引导。- GIF表情斗图。- 朋友圈。- 被喜欢的总次数统计及炫耀分享。根据我个人理解：1、2、3、6、9为基础功能，这是探探最核心的功能“刷脸，相互喜欢才能聊天”，至于用户最后活跃时间我把它也列为基础功能，因为探探是同步聊天模式，粗暴的显示“刚刚活跃”或者“几分钟前活跃”可以让用户得知自己的“目标”是否在线，掌握对方信息，增强老用户粘性。4、11、15为亮点功能，这些功能都是为了提升用户体验，没有的话也不影响当前主操作。5、7、13、14为期望功能，可帮助用户多维度的了解与互动。 8、10为无差别功能，使用频次并不多，匿名暗恋表白主要是短信推广的一种方式，而真心话模块也是为了提升产品活跃所做的运营功能。12对于某些非RMB用户来说就是反向功能，而对于平台方来说则是盈利的关键。 对于一个功能需求如何判断属于哪种，则可利用KANO问卷，分别测量用户在面对存在或不存在某项功能时的反应。由正向和反向两个问题组成。数据统计完后占比最高的属性作为该功能的属性类别，参考分类对照表。例如“探探”的聊天系统是否增加“视频功能”。 3.2 波士顿矩阵_功能分类 若探探当前所面对的公司战略目标为“增加公司的盈利”。则“VIP功能”则为明星需求，既能赋予用户VIP特权提高用户体验，也与战略目标吻合。“启屏或其它广告”则为金牛需求，不利于用户体验但可盈利。“视频或互动小游戏”为问题需求，有助于加深用户聊天深度，但需要一定的开发成本。“拉黑或解除匹配”则某种程度上可理解为瘦狗需求，不利于用户体验也不能盈利。 3.3 MVP_功能优先级确定，版本规划 考虑探探核心流程的体验（个人资料设置→匹配→私信），MVP的功能可包含：登录、个人资料完善、用户信息卡片的展示、筛选匹配、基础聊天。从探探的版本记录可知，V1.X版本主要围绕匹配和私信功能展开（同城定位、匹配、个人信息展示、基础聊天系统）， V2.X版本围绕用户互动和联系，新增朋友圈、匿名暗恋表白、GIF表情斗图、VIP（超级喜欢、反悔操作、地理位置切换）等功能。 MVP确定后就是具体的敏捷开发及项目管理了，下篇将继续说明可用性/AB测试、smart原则、Hooked模型，以及数据分析的AARRR模型、二八定律。 本文由 &#64;放慢快乐64 原创发布于人人都是产品经理。未经许可，禁止转载。 题图来自PEXELS，基于CC0协议 转载来源：结合产品从0到1，阐述各阶段的产品方法论]]></content>
      <categories>
        <category>职场</category>
      </categories>
      <tags>
        <tag>软件</tag>
        <tag>社交网络游戏</tag>
        <tag>幼儿园</tag>
        <tag>小游戏</tag>
        <tag>摄影</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[继《魔戒》之后，托尔金又一力作即将出世：《郭冬临之陷落》！]]></title>
    <url>%2F2018%2F68b7830e%2F</url>
    <content type="text"><![CDATA[虽然《指环王》作者J.R.R.托尔金已经在1973年去世，但很快书迷们就能读到这位英国作家的新书了。 最近，“郭冬临之陷落”承包了不少网友的笑点。 ？？？ 小新你在说什么，都快到五一的节头了怎么把春晚名人儿搬出来啦？ 别急，起因是这样：据外媒报道，虽然《指环王》作者J.R.R.托尔金已经在1973年去世，但很快书迷们就能读到这位英国作家的新书了。 原来，HarperCollins将在今年8月出版托尔金的《贡多林的陨落(The Fall of Gondolin)》。 该书由托尔金的儿子克里斯托弗编辑，插画仍由英国艺术家艾伦·李负责，《指环王》的插画同样也是出自他手。 《贡多林的陨落》的故事将围绕贡多林的精灵城展开，它被认为是托尔金《失落的传说集(The Book of Lost Tales)》的组成部分。 本来这是一个令魔戒迷们振奋的好消息，微博博主&#64;和菜头就抑制不住内心的喜悦，为这本书取了一个粗暴且更接地气的译名：《郭冬临之陷落》…… 网友纷纷表示，这个翻译非常“信达雅”。 大家为这个译名笑得前仰后合，这本书也成功引起了郭老师的注意，他发微博皮了一下：“听说国外写我的新书要出版了！中文版啥时候出？” 网友们表示这本书能跟《精灵王冯巩之崛起》、《巨石郭达之稳》并列为“远古时期三大传奇”。 大家纷纷开动脑洞脑筋，深扒郭老师跟西方某种神秘魔幻力量的关系。 有网友惊呼，郭冬临还与另一部欧美史诗有着千丝万缕的联系，那就是《权力的游戏》！ 大热美剧《权力的游戏》的重要城池Winterfell，就正好和他的名字完美对应：郭(Castle，城郭)，冬（Winter），临（Fell）。 这一发现不得了，网上掀起了搜集各种脑洞翻译的大狂欢！ 网友率先捧出了一直统治脑洞翻译界的“山东天后”蕾哈娜。 蕾哈娜的好多歌名都能和山东的风土完美对接，比如： Where Have You Been，“威海油饼”。 We Found Love， “潍坊的爱”。 Fool In Love， “福临莱芜”。 Talk That Talk，“聊城”。 除了蕾哈娜的山东歌名，我们还有无数满分翻译： Wake Me Up When September Ends 《一觉睡到国庆节》 论翻译是怎么毁掉一首好歌的。 Lil Daggers 《刘大哥》 感觉刘大哥的亲切喊声已经穿脑而过。 Follow Your Heart 《怂》 We Are the Champions 《我们是昌平人》 继山东人热闹过后，是昌平人的狂欢。 Dimond Mine 《呆萌的我》 感觉不光呆萌，还比较蠢。。。 Somebody That I Used to Know 《有些人，用过了才知道》 这个歌名的翻译者可能有什么不得了的故事。。 The best of the Yardbirds 《绝味鸭脖》 绝味鸭脖真的绝了啊哈哈哈哈！ 还有什么能阻挡我天朝神翻译的！ Pearl Harbor珍珠港，我们叫：蚌埠！ New York纽约，我们叫：新乡！ Downton Abbey唐顿庄园，我们叫：唐家屯！ Red River Valley红河谷，我们叫：丹江口！ Greenland格陵兰，我们叫：青岛！ 全世界都是中国的，就问服！不！服！ 最后，再奉上小新心目中的今日最佳： An apple a day keeps the doctor away. 每天玩手机，让你博士毕不了业。 微笑。 本文来源：中国日报双语新闻 转载来源：继《魔戒》之后，托尔金又一力作即将出世：《郭冬临之陷落》！]]></content>
      <categories>
        <category>文化</category>
      </categories>
      <tags>
        <tag>郭冬临</tag>
        <tag>指环王</tag>
        <tag>美剧</tag>
        <tag>欧美电影</tag>
        <tag>权力的游戏</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[平安好医生赴港IPO的背后，是互联网医疗的彷徨]]></title>
    <url>%2F2018%2F5ab965c9%2F</url>
    <content type="text"><![CDATA[对行业头部企业都相当关键《财经》记者张利/文王小/编辑4月22日，成立仅三年多的平安健康医疗科技有限公司对外公布香港首次公开募股计划。 流量带来的收益不能抵消成本，大多数互联网医疗企业未能盈利，投资者渐趋谨慎。2018年，能否上市筹资或获得融资，对行业头部企业都相当关键 （2017年4月29日，北京全球移动互联网大会上， 微医展台。图/视觉中国） 《财经》记者 张利/文 王小/编辑**** 4月22日，成立仅三年多的平安健康医疗科技有限公司（下称平安好医生）对外公布香港首次公开募股（IPO）计划，预计5月4日上午9时，正式在香港联合交易所有限公司挂牌上市。公司招股价范围每股50.8港元至54.8港元，拟募资规模超过80亿港元（超过65亿人民币）。 “此时再不上市，就要找下一轮接盘侠喽。”对于2018年互联网医疗公司的“上市潮”，北京深行投资管理有限公司投资总监赵传礼分析。 两年前，他投资了一家做预约挂号的互联网医疗公司，如今“发现挣不着钱，挺彷徨的”。 互联网医疗的融资热潮已三年。2015年，公开互联网医疗融资事件187起，投资总额超18亿美元，比2014年增长近30%。到2016年，平安好医生A轮融资就达到5亿美元。 “（现在）钱都花得差不多了。”春雨医生药企事业部前总监王锴对《财经》记者说。对于普遍未能盈利的互联网医疗企业而言，2018年，“上市”成为一个具有操作性的选项。 尤其是，港交所在2月发布《新兴及创新产业公司上市制度咨询文件》，允许“未能通过财物资格测试的生物科技公司，包括未有收益或盈利记录的公司”在港上市。没有盈利的硬性要求，大陆的生物科技公司和互联网医疗平台跃跃欲试。 微医集团首席战略官陈弘哲对《财经》记者说，微医正在寻求上市前融资，微医四大业务平台之一 “微医疗”，预计今年下半年单独赴港上市。 丁香园CEO李天天告诉《财经》记者，丁香园也希望能够赶上资本利好，抓住一些窗口期。 一旦上市成功，对亏损中的企业而言，意味着有了较为稳定的资金流。资金可以加大头部企业的壁垒，也给了它们试错的底气。“你有花不完的钱，市场会把你推到正确的路上。”MediCool医库CEO徐宏钢说。 不能盈利，何以为继 按照赵传礼原来的设想，公司快速扩展用户规模，若能与几百家医院合作，做医疗大数据，想象空间很大，按照这种方式也可以上市。但很快，他发现营收是个大问题，线上业务甚至难以覆盖成本，需要不断找投资方去融资。 如今，赵传礼的公司已融资四五轮，投资方从一家增加为七家，现在又在准备新一轮融资。因为还没能找到投资者，他改变了战略，正与一家上市药企洽谈，希望并入其中，变相上市，能使前期投资顺利退出。 他的公司主营预约挂号。大多互联网医疗企业，做的是“搬运工”生意，试图把线下的流程挂号、问诊、诊断、随访、缴费、健康管理等搬到线上去。当热钱涌入之初，企业掷重金开拓市场、集聚流量并整合资源，打法是先做大用户规模，跑马圈地，希望通过流量变现。 赵传礼的公司靠用户增值服务和与医院合作的项目收费，也有互联网医疗企业靠交易佣金、数据营销等模式创收，但这些模式都难以形成规模化营收。“从医院收钱不成，从用户收钱市场没有培养出来，消费习惯没有形成。”赵传礼说。 平安好医生《招股说明书》显示，排除以股权为基础的付款及外汇盈亏额影响，2015年、2016年及2017年9月30日止九个月，平安好医生经调整净亏损分别为3.22亿元、9.81亿元、7.1亿元、3.3亿元。 此前爆出，微医2016年实现全面盈利。“这是误传。我们正在进行上市前的审计工作。”陈弘哲说。 互联网医疗难以盈利，未能深层次解决用户求医问诊需求是主因。 最早被看好并发掘的在线问诊，遇到的当头一棒是患者不能清晰描述自己的病情，直接影响医生的诊疗。医米调研2016年发布的《中国医生在线问诊行为报告》显示，仅有35.5%的患者可以大概描述自己的病情，仅有18%的医生认为在线问诊对国内病人帮助很大，六成医生选择“有一定作用但有限”。 平安好医生《招股说明书》中，截至2017年9月30日，当年在线医疗咨询服务为主的家庭医生服务收入仅为165.1万元。 盈利困境下，流量为王的互联网思维遭到质疑。“流量重要，但不能靠流量驱动整个商业模式。”李天天说。更何况，近年线上流量成本也趋高。 2016年，新增用户超过1.3亿，平安好医生耗费近6亿元推广费用和广告费用。 徐宏钢分析，在不计算人力和其他成本的情况下，获得普通用户成本约7元，获得留存活跃用户的成本约60元，总平均付费用户的获得成本350元－400元，而每位用户平均带来的毛收入达不到这个数字。 病人都倾向于往大医院跑，由于医疗行业特殊性，补贴拉动的用户留存率低，一旦补贴停掉，用户量、活跃度、卸载率等会高达50%以上，需要企业不断拉新、激活用户，然后将这部分流量变现，用户每一次来，都有成本，但不是每一次来，都能有营收。 “没有‘土豪爸爸’强力补贴和输送业务的情况下，其他的创业公司哪敢如此烧钱。”徐宏钢说。 探索中的企业，资金足是保障，这一两年互联网医疗头部企业找钱的动静会更大，扎堆掀起“上市潮”。“行业关注度增加，有利于资金融入，是很大的利好。”山东天业集团投资总监王博分析。 与医院的“恩怨” 为突破盈利困境，远程问诊、互联网医院，是互联网医疗最先关注的商业模式，现在又有企业尝试做全科诊所等。在纯线上模式折戟后，互联网医疗企业尝试逐步深入线下诊疗环节。 互联网医院本质是医院的延伸。2018年4月16日，国务院新闻办举行政策例行吹风会首度在官方层面承认了互联网医院的合法性，提出允许依托医疗机构发展互联网医院，医疗机构可以使用互联网医院作为第二名称，允许在线开展部分常见病、慢性病复诊等。但互联网医院必须落地在实体的医疗机构，线上线下要一致监管，并且必须得有实体医疗机构作为依托。国家卫生健康委员会副主任曾益新在此次吹风会上说，这对医院来说，等于拓展了业务范围，拓展了服务半径，是政策利好。 然而，患者在家中接受互联网诊断仍被禁止，互联网医疗企业只能做远程会诊、慢病管理等辅助性工作。国家卫生健康委员会医政医管局副局长焦雅辉在此次吹风会上强调，在互联网上初诊是绝对禁止的，这在世界上各国都是这样要求的。 互联网技术适合开展远程诊疗，被认为是实现分级诊疗的重要工具。曾益新表示，支持符合条件的第三方机构，搭建互联网信息平台，开展远程医疗。 迎合医改大势是个机会。2018年，好大夫在线重点转向基层医院，连接上下级医生，开展远程专家门诊业务；春雨医生则为医院提供系统改造，也是为引入远程会诊、远程慢病管理等。 不过，独角兽工作室创始人刘谦分析，分级诊疗尚未完全解决上下级医院之间的利益关系。 对于政府而言，远程问诊可帮助实现医改提倡的分级诊疗，即将三甲医院定位在解决疑难杂症，将50%的常见病分流下去。但一位做远程问诊的业内人士对《财经》记者分析，基层医院买不起检查设备很难诊断，即便能诊断在当地医院也治不了。 线上的远程问诊需要线下医院配合，可三甲医院没有兴趣做，远程诊疗是分流患者的，而患者意味着巨大的收入。因此，即便是互联网医院模式相对成熟的微医，目前更多以“小病”和“慢病”为主，医院间的大病远程会诊很少，基本难以触及。 “会诊本质上是一个医院之间的业务，第三方在其中很难获得发展，只能是一个提供基础设备和软件的角色。”医疗咨询公司Latitud Health发布的《远程医疗：价值、挑战和机会》报告称。 在医疗行业，形成壁垒的不是技术，而是资源。赵传礼分析，“医院活得好好的，为什么要跟你合作？”比如，在线挂号需要与医院的His系统（覆盖医院所有业务和业务全过程的信息管理系统）打通，然而，即使在卫计体系内部都很难打通所有的医院。 投资人王博在考量互联网医疗企业时，政府关系和资金状况是他重点考量的。“这决定着，企业能否拿到大医院的订单。”他对《财经》记者说。 政府提倡的分级诊疗，很重要的一个环节在于全科诊所。自建诊所，为互联网诊疗提供了医疗机构资质，又“占坑”线下就诊入口，因而，挺进诊所将是这两年互联网医疗企业的一个目标。迄今为止，有4家丁香诊所已运行；微医自建了6家线下全科中心和全科学院；3家企鹅诊所落地。 有医生资源做人才支撑，“为什么我要找公立医院合作”？李天天直问。他思考的是如何用好平台上的医生资源。丁香园有医生资源，运营诊所更有优势。 建立诊所，使互联网医疗企业必须直面与医院展开人才竞争。其实，整个医疗的终极在于医生。要激活现有医疗资源，政府乐于鼓励人才流动，互联网医疗企业则要放出更美好的“诱饵”。 对于互联网医疗的种种举动，中国医学科学院北京协和医学院整形外科医院院长祁佐良曾公开表示，“都是在有步骤地使大型公立医院面临这样必然到来的医疗体制改革。” 长远看，对互联网医疗企业而言，“（现阶段）与公立医院的合作是过渡”，中国社科院特约研究员贺滨认为，未来将出现更多可能性。 未来可能性的变量，还取决于政策因素。曾益新在上述吹风会上明确表示，鼓励逐步推进“互联网+保险结算”，拓展在线结付功能，包括异地结算、一站式的结算来方便病人。但目前，远程医疗，尚未被纳入医保报销范围。“真正的会诊业务很少，因为实在找不到人来买单。”山西迈普锡公司总经理程远宇对《财经》记者分析，远程医疗跟人工智能的困境是一样的，泡泡吹得再好，最终还要看医院、政府还是患者，谁来买单。 在李天天看来，与互联网不同，医疗健康产业还不是一个可以用资本快速催熟或清场对手的赛道，融资和IPO只是公司发展的一个基本保障，而不是把事情做成的关键。 那么，互联网医疗现在走到哪一步了？平安好医生的《招股说明书》将其描述为“一个新兴及不断演变的行业的初级阶段”。 深陷于其中者是痛并快乐着。陈弘哲认为“市场机会还很大”；好大夫在线CEO王航认为“互联网医疗的收获期还远着”；李天天给出的词是“曙光乍现”；健康160CEO罗宁政称“还处于极早期”。无论如何，互联网医疗企业总要回答“靠什么挣钱”。 那些迎着互联网医疗浪潮进入这个行业的人，当初憧憬的“大蛋糕”并没有预期而至。“慢得不像互联网。”一位等待其公司上市的员工说。 （本文首刊于2018年4月16日出版的《财经》杂志） 转载来源：平安好医生赴港IPO的背后，是互联网医疗的彷徨]]></content>
      <categories>
        <category>科技</category>
      </categories>
      <tags>
        <tag>投资</tag>
        <tag>IPO</tag>
        <tag>移动互联网</tag>
        <tag>平安保险</tag>
        <tag>丁香园</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[知识付费类产品竞品分析]]></title>
    <url>%2F2018%2F44411ce4%2F</url>
    <content type="text"><![CDATA[本文主要是对市场上知识付费排行榜中占有率较大的几款产品：知乎、得到、在行、百度知道的横向对比。二、竞品的选择从知识付费排行榜中选择市场占有率较大的三款产品：知乎、得到、在行；并从火爆一时的老牌知识付费产品中选择一款：百度知道。 本文主要是对市场上知识付费排行榜中占有率较大的几款产品：知乎、得到、在行、百度知道的横向对比，分析他们的优势、存在的问题、以及潜在机遇。并结合具体实践，提出产品设计方案。 一、本次竞品分析的目的 通过对市场上有竞争力产品的横向对比，分析他们的优势、存在的问题、以及潜在机遇；1. 将分析得到的收获，与具体学习生活场景相结合，提出我的产品设计方案。 二、竞品的选择 从知识付费排行榜中选择市场占有率较大的三款产品：知乎、得到、在行；1. 并从火爆一时的老牌知识付费产品中选择一款：百度知道。 三、选择原因维度：动机、成本衡量、内容质量 根据十字象限图，四个产品在这三个维度上各有特色，具有分析价值。 四、竞品分析1. 得到（1）公司层面 公司背景：罗辑思维，知识服务商和运营商。包括微信公众订阅号，知识类脱口秀视频节目：《罗辑思维》；知识服务App：得到APP。1. 产品定位：提供最省时间的高效知识服务。提倡碎片化学习方式，让用户短时间内获得有效知识。1. 发展历程：主推音频+文稿的内容形式；注重内容质量，大力度邀请各科领域优质内容生产者入驻。（2）产品层面 1）核心功能（知识+服务） 知识内容方面：建立以订阅专栏和每天听本书模块为核心，其他在线内容为补充的跨领域、多形态精品付费内容生态。其中订阅专栏是得到App的主要盈利来源，而每天听本书的低价模块用于养成用户使用习惯、提高用户粘性。 增值服务方面：基于用户行为及学习规律提供多种辅助功能，完善其学习闭环。 一方面，今日学习、学习记录及知识清单等工具型功能，通过对用户站内收听/浏览行为进行统计，帮助用户梳理学习轨迹，管理学习进程，提升学习效率；- 另一方面，内容推荐、学习小组、学习勋章等兴趣化功能，鼓励用户坚持在线学习，保持知识进阶，逐步建立跨领域知识图谱。2）竞争能力 以波特五力分析模型来看： 行业内竞争者现在的竞争能力：品牌上，得到是罗辑思维出品，信服度高，在占领用户心智上有着先天优势；内容形式上，其他竞品比如知乎有问答，在行有线上加线下的创新教育模式，得到仅仅是采用直接售卖的形式，较为单一，且没有互动。 替代品的替代能力：音频+文字的内容形式，行业内有竞争力的还有喜马拉雅FM，但是喜马拉雅付费模块没有全面展开，内容品质繁杂参差不齐，娱乐性大于专业性，与得到针对的目标用户有一定差异。 购买者的议价能力：订阅专栏199/年、大师课99/专题、听书4.99/本，得到在行业内的价格略高，用户考虑到知识内容大多是自我升值而不是生存必须，在衡量成本和动机时会有所顾虑。 供应商的供应能力：得到走高端路线致力于邀请知名度高权威性强的生产者。成本势必较高，普通消费者在学习课程之后，一般也无法达到App所要求的生产能力，所以无法构成产销合一的闭环。 潜在竞争者的竞争能力：若内容方不走平台直销知识，以其本身的内容资源的专业性与目标用户的重合性，对得到来说无疑是很大的打击。 2. 知乎（1）公司层面 公司背景： 2011年1月正式上线运营，作为知识分享平台，主要为用户提供问答、专栏、电子书等多种形式的信息服务。1. 产品定位：网络问答社区，用户围绕着某一感兴趣的话题分享彼此的知识、经验和见解。1. 发展历程：以问答社区为核心基础，2016年起，探索传统广告以外的商业路径，逐步拓展了多种形态的知识付费业务。通过多元知识变现模式吸引站外内容生产者的入驻，激励站内内容生产者生产优质付费内容，形成健康的内容生态。（2）产品层面 1）核心功能（核心基础**+**上层建筑） 以原本免费问答社区积累的内容与用户资源作为核心基础，探索更多的知识服务场景，形成了多条不同产品形态的知识付费业务线。 知乎live：付费进入课程，以问答形式参与答主短时、单场次的实时互动；- 付费咨询：通过支付标价费用向答主提问或直接观看已有问题；- 知乎书店：囊括了购买、阅读、讨论和传播的完整知识闭环；- 私家课：领域头部优秀回答者提供分模块、结构化、体系化的在线课程服务。2）竞争能力 以波特五力分析模型来看： 行业内竞争者现在的竞争能力：品牌上，知乎在行业内积累多年的声望与资源毋庸置疑，付费是众望所归也有助于内容质量的提升；传授形式上，包括了问答、线上授课、电子书等，媒介形式上包括文字、音频，现在也逐渐加入直播视频，多条业务线几乎包括了行业内的大部分产品形态。 替代品的替代能力：电子书与私家课模块行业内，有多个产品竞争且业务相对成熟。比如：得到、在行等，不具备竞争优势；知乎live模块，话题范围广、互动性强，业内的直接竞品有在行，但从价格和话题范围的广度看，在行的竞争性略低；付费咨询模块，在行在口碑与知名度上优势较弱。 购买者的议价能力：授课与问答价位不同，授课平均20-100，问答平均1-50，用户可以根据自身学习动机选择不同价位的学习形式，在成本衡量面较占优势。 供应商的供应能力：不仅有站外内容生产者的入驻，也有站内生产者生产优质付费内容，且兼顾到腰部与长尾力量，消费者学习达到饱和后也可贡献出一定的内容，形成产销合一的生态闭环。但需要对内容的品质、体系化进行有效整合。 潜在竞争者的竞争能力：以知识问答为核心基础的知识付费产品有潜力的是百度知道，但是从口碑、内容、用户质量上看，百度知道任重道远。 3. 在行一点（原分答）（1）公司层面 公司背景：果壳网，作为一个开放、多元的泛科技兴趣社区，提供负责任、有智趣的科技主题内容。1. 产品定位：是知识技能共享平台「在行」最新推出的付费语音问答服务，基于学者的问题与期望答案的个性化差异，以O2O和C2C模式让问答获得新的出路。1. 发展历程：果壳网在2015年3月13日推出产品 “在行”，可以约见不同领域的行家，与他们进行一对一见面约谈。2016年5月15日上线分答，由在行团队孵化，也是延续了知识传播与分享的分享方式。不仅是科学家，很多名人和各领域的专家也都加入付费问答的模式。2018年2月6日，分答更名在行一点，且对板块做了划分。（2）产品层面 1）核心功能（线上**+**线下） 基于在行起初一对一线下约见的授课形式，为了迎合碎片化学习时代的背景，果壳一步步从线下走入了线上，新一版的在行一点在首页清晰展现了四大功能模块：课、班、讲、问，对应不同的使用场景。 “课”是基于某个专题耕者专家深度学习；- “讲”跟改版前的“小讲”内容完全一致；- “班”则对应旧版本的“社区”；- 而之前的“找专家”和“快问”则汇总成了现在的“问”。行业内竞争者现在的竞争能力：品牌上，和知乎等一线产品相比优势不大；内容形式上，与大多竞品相仿，除了线下约见与在线社群互动略显特色。 替代品的替代能力：问、讲、课的模块，无论从内容资源的丰富度还是内容生产者的权威性，都不如知乎和得到，可替代性极强。但是“班”模块的社区功能中的互动问答，作业跟进反馈，极大提高学习效率，是一大亮点。 购买者的议价能力：标价在行业内偏高，但由于其内容范围专业性与实用性强，针对的目标用户动机纯粹，所以在价格衡量上会适当放宽要求。 供应商的供应能力：授课模块与得到类似，主要是头部内容生产者，问答模块主要是头部与腰部生产者。但是内容相较其他产品而言丰富性有待提高，且专业性太强消费者难以转化成生产者，产销合一的闭环难以实现。 潜在竞争者的竞争能力：与得到类似，且在行融合了直接售卖、知识社群、创新教育三种产品形态，形势虽然多样，但同时失去了产品特色。 4. 百度知道（1）公司层面 公司背景：百度全球最大的中文搜索引擎、最大的中文网站。1. 产品定位：一个基于搜索的互动式知识问答分享平台，用户自己有针对性地提出问题，通过积分奖励机制发动其他用户，来解决该问题。（2）产品层面 1）核心功能 提问（悬赏）+回答（互动）+分享（搜索）（悬赏人是提问者） 2）用户体验-表现层 优势：一对多的提问形式，鼓励用户回答问题，激发长尾内容生产者的力量。知乎与在行的问答是一对一，模块列表以答题人为主，且为答题人标价，产品付费体质明显，但也不利于长尾用户贡献内容生产力；百度知道答题页面以问题为主，可促进产销合一的生态构建 劣势：提问内容的质量，回答内容的专业性与系统性，用户不断流失的主要原因。 五、我的产品设计方案结合学习生活场景：有面试需求的求职者在网上寻找各种面试经验的过程中，发现所出来的内容都太笼统，质量不高，不落地。 目标用户：18-30岁即将毕业或刚毕业，有面试需求的求职者。 核心流程： 提供免费的增值服务：用户可以免费看面经；1. 付费看精品面经；1. 联系撰写者进行一对一辅导；1. 由（2）或（3）受益后的面试成功者获取相应荣耀值；1. 荣耀值高的用户成为生产者产出面经或去辅导别人。特点： 生态闭环，产销合一；1. 用户动机纯粹，就是想通过面试；1. 帮助用户理性筛选：点赞多的面经会被推至首页，面经以行业、公司、岗位、面试形式四个维度进行分类，帮助用户个性化筛选。用户留存方案： 免费的面试经验；1. 以付费用户学习后的面试反馈来调动潜在用户付费；1. 每日签到，积分代币；1. 荣誉感，通过面试多的人会有勋章，上光荣榜；1. 社群，激发用户去互动（以不同公司不同岗位分类）；1. 增值服务一：从拉勾或脉脉，获取一些及时的招聘信息个性推送给用户，一键投递；1. 增值服务二：邀请大厂员工，组织线上模拟面试（单面、群面）。商**业变现模式：** 平台抽成；1. 广告；1. 置换资源，合作共生（比如拉勾提供岗位资源，我们把用户信息以一定的手段曝光给拉钩）。风险与挑战**：** 面经的质量如何把控？平台与用户对于面经的评价标准不同，实用性因人而异；1. 产品初期如何和别人竞争？产品初期需要收集大量高质量的面经以及做一定推广，平衡成本方面需要仔细考量。本文由 &#64;Naom 原创发布于人人都是产品经理。未经许可，禁止转载 题图来自 Pexels，基于 CC0 协议 转载来源：知识付费类产品竞品分析]]></content>
      <categories>
        <category>传媒</category>
      </categories>
      <tags>
        <tag>软件</tag>
        <tag>雅虎知识堂</tag>
        <tag>在行</tag>
        <tag>罗辑思维</tag>
        <tag>百度知道</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[任志强房地产最新演讲：现在是抄底的机会，但这些地方不能碰]]></title>
    <url>%2F2018%2F005afb12%2F</url>
    <content type="text"><![CDATA[来源丨广州PLUS 昨天（4月22日）下午3点30分， 任志强房地产最新演讲：现在是抄底的机会，但这些地方不能碰 转载来源：任志强房地产最新演讲：现在是抄底的机会，但这些地方不能碰]]></content>
  </entry>
  <entry>
    <title><![CDATA[创记录！九价宫颈癌疫苗两日完成技术审评]]></title>
    <url>%2F2018%2Fda1b76c6%2F</url>
    <content type="text"><![CDATA[九价宫颈癌疫苗上市加速度，潜在市场空间望超千亿。 创记录！九价宫颈癌疫苗两日完成技术审评 转载来源：创记录！九价宫颈癌疫苗两日完成技术审评]]></content>
  </entry>
  <entry>
    <title><![CDATA[魏则西去世两年了，“医疗竞价”却死灰复燃，怎样才能让百度们长记性？]]></title>
    <url>%2F2018%2F61938f2a%2F</url>
    <content type="text"><![CDATA[是否以用户利益为先，我们不得而知，但可以确定，百度对“竞价排名”，恐怕是真爱。老毛病又犯了2016年，魏则西用生命换来了公众对“医疗竞价排名”的声讨，那一场声势浩大的讨论，想必大家记忆犹新。 转载来源：魏则西去世两年了，“医疗竞价”却死灰复燃，怎样才能让百度们长记性？]]></content>
  </entry>
  <entry>
    <title><![CDATA[个人电脑上训练深度学习模型：Uber开源「深度神经进化」加速版]]></title>
    <url>%2F2018%2Fc974d65d%2F</url>
    <content type="text"><![CDATA[通过使用遗传算法高效演化DNN，可以训练含有超过400万参数的深度卷积网络在像素级别上玩Atari游戏。 Uber 在去年底发表的研究中发现，通过使用遗传算法高效演化 DNN，可以训练含有超过 400 万参数的深度卷积网络在像素级别上玩 Atari 游戏；这种方式在许多游戏中比现代深度强化学习算法或进化策略表现得更好，同时由于更好的并行化能达到更快的速度。不过这种方法虽好但当时对于硬件的要求很高，近日 Uber 新的开源项目解决了这一问题，其代码可以让一台普通计算机在 4 个小时内训练好用于 Atari 游戏的深度学习模型。现在，技术爱好者们也可以接触这一前沿研究领域了。 项目 GitHub 地址：https&#58;//github.com/uber-common/deep-neuroevolution/tree/master/gpu_implementation Uber 去年底曾发表了五篇有关深度神经进化的论文，其中包括遗传算法可以解决深度强化学习问题的发现，以及一些有前景的其他方式，如深度 Q 学习和策略梯度。这些工作是 Salimans 等人 2017 年研究《Evolution Strategies as a Scalable Alternative to Reinforcement Learning》的后续，后者展示了另一种神经进化算法进化策略（ES）的能力。Uber 进一步介绍了如何通过添加探索新奇的智能体来改进 ES（论文《Improving Exploration in Evolution Strategies for Deep Reinforcement Learning via a Population of Novelty-Seeking Agents》），以及 ES 与梯度下降的相关性。所有这些研究在此前都是非常耗费计算资源的：需要使用 720-3000 块 CPU，在分布式大型高性能计算机集群上运行，这为大多数研究者、学生、公司和爱好者对深度神经进化的进一步探索带来了阻力。 不过这一问题最近已经得到了解决，Uber 于昨日开源的新代码使得快捷方便地展开此类研究成为可能。有了新的代码，原先需要花费 720 个 CPU、一个小时训练的 Atari 游戏深度神经网络，现在只需要在常见的台式机上单机花费 4 个小时就可以完成训练了。这是一个非常重要的进展，它极大地改变了人们对于此类研究所需资源的看法，使得更多研究者们可以进入这一领域。 神经进化技术是解决具有挑战性的深度强化学习问题颇具竞争力的方案，其可用范围包括 Atari 游戏、类人体仿真运动等等。上图展示了使用简单遗传算法进行深度神经网络训练的一些形式。 什么使其速度加快，并且可在一台计算机上运行？ 现代的高端计算机具备数十个虚拟核，这种计算机本身就像一个计算集群。如果采用适当的方式执行并行评估，那么在 720 个内核上耗时一小时的运行可在一个 48 核的个人计算机上运行，耗时 16 小时，速度较慢，但是也还可以。现代计算机还有 GPU，因此可以较快运行深度神经网络（DNN）。Uber 的代码最大化 CPU 和 GPU 的并行使用。在 GPU 上运行深度神经网络，在 CPU 上运行域（如视频游戏或物理模拟器），并且在同一批次中并行执行多个评估，这使得所有可用的硬件都得到高效利用。如下所述，它还包含自定义 TensorFlow 操作，极大地提高了训练速度。 在 GPU 上训练需要对神经网络操作的计算方式进行若干修改。在 Uber 的设置中，使用单个 CPU 运行单个神经网络的速度比使用单个 GPU 要快，但是当并行执行类似计算时（如神经网络的前向传播），GPU 的效果更好。为了利用 GPU，Uber 将多个神经网络前向传播集成到批次中。这种做法在神经网络研究中很常见，但是通常是同一个神经网络处理一批不同输入。但是 Uber 的做法使用的是多个不同的神经网络，不过即使网络不同，该操作仍然实现了加速（对内存的要求也提高了）。Uber 使用 TensorFlow 基础操作实现了多个神经网络批次处理，并实现了大约 2 倍的加速，将训练时间降低到大约 8 小时。但是，研究人员认为他们可以做得更好。尽管 TensorFlow 提供所有需要的操作，但是这些操作并不是为这种计算量身定做的。因此，Uber 添加了两种自定义 TensorFlow 操作，由此再次获得了 2 倍的加速，将在单个计算机上的训练时间减少到大约 4 小时。 第一个自定义 TensorFlow 操作大大加速了 GPU 的速度。它专为强化学习领域中异质神经网络计算而开发，这些计算中的 episode 长度不同，正如在 Atari 和很多其他仿真机器人学习任务中那样。该操作使 GPU 仅运行必须运行的神经网络，而不是每次迭代中都运行固定数量（大量）的神经网络。 这些改进使得 GPU 在成本方面优于 CPU。实际上，GPU 非常快，Atari 模拟（CPU）都跟不上了，即使使用了多进程库执行计算并行化。为了改进模拟性能，Uber 添加了第二套自定义 TensorFlow 操作，将 Atari 模拟的封装器从 Python 转到了自定义 TensorFlow 命令（重置、step、观测），便于利用 TensorFlow 提供的快速多线程功能，省略了 Python 和 TensorFlow 交互造成的减速。所有这些改变带来了 Atari 模拟器中大约 3 倍的加速。这些创新应该会加速任意具备多个并行运行实例的强化学习研究（如 Atari 或 MuJoCo 物理模拟器），该技术在强化学习领域中将越来越普遍，如分布式深度 Q 学习（DQN）和分布式策略梯度（如 A3C）。 一旦我们可以在 GPU 上快速运行多个不同神经网络，在 CPU 上运行更快速的模拟器，那么挑战就变成了尽可能地保存所有计算机运行的资源。如果我们在每个神经网络中都执行了前向传播，询问每个网络在当前状态下应该采取什么动作，那么尽管每个神经网络都在计算自己的答案，但是运行游戏模拟器的 CPU 什么也没做。类似地，如果我们执行了动作，并询问模拟器「这些动作会导致什么状态？」，那么运行神经网络的 GPU 在该模拟步中就处于空闲状态。这就是多线程 CPU+GPU option（如下图所示）。尽管单线程计算出现改进，但这仍然是无效的。 更好的解决方案是具备两个及以上与模拟器配对的神经网络子集，并使 GPU 和 CPU 在更新网络或根据即将采取的步骤（神经网络或模拟）而执行的来自不同集的模拟时同时运行。该方法如下最右图「pipelined CPU+GPU」所示。使用它以及上述改进，我们可以使具备 ~4M 参数的神经网络的训练时间降低到单个计算机大约 4 小时。 在强化学习中优化异质网络集群的调度。蓝色框是模拟器，如 Atari 游戏模拟器或 MuJoCo 物理引擎，它们的 episode 长度不同。使用 GPU 的普通方式（左）性能较差，原因有二：1）GPU 的批大小无法利用其并行计算能力；2）GPU 等待 CPU 时的空闲时间，反之亦然。多线程方法（中）通过使多个 CPU 并行运行模拟器实现 GPU 的更高效使用，但是这导致 GPU 在 CPU 运行时处于空闲状态，反之亦然。Uber 提出的流水线型实现（右）允许 GPU 和 CPU 都高效运行。该方法还可以同时运行多个 GPU 和 CPU，这已经应用于实践中。 快速、低成本实验的影响 Uber 的代码能够帮助研究社区中的每个参与者，包括学生和自学者快速上手试验性地迭代训练深度神经网络，研究各种具有挑战性的问题，如 Atari 游戏智能体。在此之前，只有财力雄厚的行业和学界实验室可以这样做。 快速的代码可以带来更快的研究进展。例如，Uber 的新代码可以用很少的成本对遗传算法启动广泛的超参数搜索，带来了大多数 Atari 游戏模拟器的性能提升。Uber 表示会在近期更新其在 arXiv 上发表的研究《Deep Neuroevolution&#58; Genetic Algorithms Are a Competitive Alternative for Training Deep Neural Networks for Reinforcement Learning》中的测试结果。更快的代码也加快了研究进度，通过缩短迭代时间来改进深度神经进化，使我们能够在更多的领域尝试每个新想法，并延长算法的运行时间。 Uber 的新软件中包含了深度遗传算法的实现、Salimans 等人的进化策略算法，以及 Uber 自己的随机搜索方法。Uber 欢迎研究社区的贡献者使用这些代码，并对这些代码进行进一步改进。例如，进行分布式 GPU 训练或加入为此类计算定制的其他 TensorFlow 操作——这可能会进一步提高速度。 深度神经进化的方向还有很多可以探索的东西，除了前面提到的 Uber 的研究、OpenAI 的工作以外，近期还有使用 DeepMind、Google Brain 和 Sentient 的进化算法带来的深度学习研究进展。Uber 希望通过通过此次开源进一步降低该领域的进入门槛。 最重要的是，这一工具可以降低研究成本，让各类背景的研究人员都能在其上尝试自己的想法，以改善深度神经进化并利用它实现自己的目标。 转载来源：个人电脑上训练深度学习模型：Uber开源「深度神经进化」加速版]]></content>
      <categories>
        <category>其它</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>机器学习</tag>
        <tag>Uber</tag>
        <tag>雅达利</tag>
        <tag>CPU</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[4 年，魅族沉沦记]]></title>
    <url>%2F2018%2F5c03412b%2F</url>
    <content type="text"><![CDATA[作者：郝影李小白在一场发生魅族营销体系的“内讧”席卷社交网络的喧哗之侧。魅族15周年纪念之作——“魅族15”如期发布了，而魅族的创始人黄章依旧隐居幕后。 转载来源：4 年，魅族沉沦记]]></content>
  </entry>
  <entry>
    <title><![CDATA[店员经典培训教材：发烧用布洛芬还是对乙酰氨基酚]]></title>
    <url>%2F2018%2Fe6add368%2F</url>
    <content type="text"><![CDATA[一般认为人体当口腔温度高于37.5 ℃，腋窝温度高于37℃，或一日之间体温相差在1℃以上，即为发烧。 作者：海浪 发烧，又称发热。是指致热原直接作用于体温调节中枢、体温中枢功能紊乱或各种原因引起的产热过多、散热减少，导致体温升高超过正常范围的情况。 一般认为人体当口腔温度高于37.5 ℃，腋窝温度高于37℃，或一日之间体温相差在1℃以上，即为发烧。发烧是疾病进展过程中的重要临床症状表现，可由多种感染性疾病和非感染性疾病引起。 发热本身不是疾病，而是一种机体自我保护机制，一种症状表现。发烧时体温升高，有些病源微生物活性和繁殖就会变得不那么活跃。而人体的免疫系统反应性则显著增强，包括白细胞计数增加，吞噬细胞和嗜中性粒细胞的杀菌活性增强等。发烧是人体进化获得的一种对抗病原微生物感染入侵的有益的保护性机制，对人体是有利的。 通常，测量体温未超过38.5℃时，不推荐马上服用退烧药。应尽量用物理降温的方式来减少不适感。但尽管采用物理降温仍需警惕超高温，当体温超过38.5℃时，或肛温大于41.5℃，或口温大于40℃，对人体的危害是很大的，特别是儿童。当出现高烧时，应及时看医生做出病因查找和对症治疗。 在常用退烧药中，布洛芬和对乙酰氨基酚是目前应用最广泛的两个OTC类药品，它们也是目前认为最适合儿童使用的退热药。可是，二者由于分子结构、作用特点、不良反应存在着明显的差别，该如何正确选择充分发挥药效呢？ 一、布洛芬和对乙酰氨基酚是同一类退热药吗？ 1. 布洛芬 布洛芬属于芳基丙酸类解热镇痛药，有明显的抗炎、解热、镇痛作用，强度与阿司匹林相当，对血小板功能有一定的抑制作用，可延长出血时间，但在常规治疗剂量使用时，不良反应发生率低，耐受性与对乙酰氨基酚相似。 布洛芬是世界卫生组织、美国FDA唯一共同推荐的儿童退烧药，是公认的儿童首选抗炎药。常见的以布洛芬为有效成分的药品有芬必得、美林等。 2. 对乙酰氨基酚 对乙酰氨基酚又称扑热息痛、泰诺林、必理通等为苯胺类解热镇痛药，其解热、镇痛作用强度与阿司匹林类似，但对凝血机制无影响，其抗炎作用也远不及布洛芬。 对乙酰氨基酚毒副作用少，较易耐受，是一种比较安全的退热药，很多复方制剂的感冒药里都含有它。 二、布洛芬和对乙酰氨基酚在抗炎、镇痛、解热功能方面的区别|适应症/功能主治|布洛芬|对乙酰氨基酚 布洛芬|头痛、偏头痛、牙痛、肌肉痛、神经痛、咽喉痛、痛经|对痛经更适合|对头痛更适合 对痛经更适合|腹痛|不适合|不适合 不适合|消炎作用|风湿性关节炎和骨关节炎，对与炎症相关的窦性头痛、肌肉痛、耳痛、牙痛效果好|没有消炎作用（弱的环氧酶抑制剂，不能抑制中性粒细胞的激活） 风湿性关节炎和骨关节炎，对与炎症相关的窦性头痛、肌肉痛、耳痛、牙痛效果好|退热时间|最大退热时间：183分钟|最大退热时间：134分钟（在30分钟时的体温下降速度比布洛芬更明显） 最大退热时间：183分钟 三、布洛芬和对乙酰氨基酚在毒副作用方面的区别 1、毒性方面：对乙酰氨基酚用量过大可能导致肝损害，且是不可逆损伤，在超剂量、脱水、营养不良情况下服用，肝损害风险会增加，长期大量用药也可导致肾功能异常。布洛芬长期使用可能造成肾损伤、心脏病发作和卒中，超剂量、脱水情况下，肾损害风险增加。 2、副作用方面：对乙酰氨基酚对胃肠副作用较少，对血小板、出血时间、尿酸排泄无影响。而布洛芬可导致胃溃疡出血、胃烧灼感、轻度消化不良，改变血小板功能、延长出血时间。 四、布洛芬和对乙酰氨基酚在适用年龄范围、剂型和用法用量方面的区别 1、适用年龄范围：对乙酰氨基酚适用于＞3个月的儿童和成人，布洛芬适合＞6个月的儿童和成人。 2、药物剂型：对乙酰氨基酚市售的有对乙酰氨基酚片、对乙酰氨基酚缓释片、对乙酰氨基酚泡腾片、对乙酰氨基酚颗粒、对乙酰氨基酚滴剂、对乙酰氨基酚口服溶液，对乙酰氨基酚栓等。布洛芬市售的有布洛芬片、布洛芬缓释片、布洛芬胶囊、布洛芬缓释胶囊、布洛芬颗粒、布洛芬悬浊液、布洛芬乳膏、布洛芬口服液、布洛芬栓等。医生或药师会根据患者身体状况、疾病特点选择合适的药物剂型。 3、用法用量：若持续疼痛或发热，对乙酰氨基酚可餐前服用，每4-6小时服一次，24小时内不得超过4次，儿童每次最大剂量：15mg/kg，日最大剂量为2g。而布洛芬需随餐或餐后服用，12岁以上儿童及成人一次2片，若持续疼痛或发热，可间隔4～6小时重复用药1次，24小时不超过4次，具体布洛芬用量见下表：|年龄（岁）|体重（公斤）|一次用量（片）|次数 体重（公斤） 次数|1～3|10～15|1/2若持续疼痛或发热，可间隔4～6小时重复用药1次，24小时不超过4次。 10～15 若持续疼痛或发热，可间隔4～6小时重复用药1次，24小时不超过4次。|4～6|16～21|1 16～21|7～9|22～27|1.5 22～27|10～12|28～32|2 28～32 五、使用布洛芬和对乙酰氨基酚的注意事项有哪些？ 1. 布洛芬 有消化性溃疡史、胃肠道出血、心功能不全、肝肾功能不全、高血压、凝血机制或血小板功能障碍（如血友病）的患者应慎用。布洛芬与阿司匹林有交叉过敏，故对阿司匹林过敏的患者禁用。脱水或水分补充不够等低血容量的患者使用布洛芬退热可增加肾功能损害的风险。 罹患水痘的患者使用布洛芬则可增加 A 组链球菌感染的风险。 2. 对乙酰氨基酚 对阿司匹林过敏者对对乙酰氨基酚通常不发生过敏，但＜5%的阿司匹林过敏的患者服用对乙酰氨基酚后可能会发生轻度支气管痉挛性反应。不要服用超过一种含有对乙酰氨基酚的药物，特别是含对乙酰氨基酚的复方制剂感冒药，服用药品前应仔细阅读说明书中成份组成部分，避免因重复用药而使对乙酰氨基酚过量，否则可能引起头痛、呕吐、倦怠低血压及皮疹等。严重肝肾功能不全者禁用；肝病或病毒性肝炎、轻至中度肝肾功能不全者、严重心肺疾病患者应慎用；如果出现黄疸症状需立即进行肝功能测试。如果是混悬液，用前要上下摇晃，使之搅拌均匀，使用产品包装中的量杯或容器来取相应的剂量。 综上，布洛芬和对乙酰氨基酚各有优缺，要严格按照临床医师或药师吩咐合理选择药物，使用二者任何一种时，不能同时服用其他含有解热镇痛药的药品（如某些复方抗感冒药）。 长按上图3秒钟，识别二维码关注 亲，中国药店公众号长期公开征集稿件，如您欲抒发心声、记录生活、分享经验……可以文字或图片形式将作品发送到yaodian2018&#64;163.com（邮箱），稿费=200元+阅读量*0.01元。温馨提示，投稿时请注明联系方式 喜欢的话就点赞吧！↙ 转载来源：店员经典培训教材：发烧用布洛芬还是对乙酰氨基酚]]></content>
      <categories>
        <category>健康</category>
      </categories>
      <tags>
        <tag>药品</tag>
        <tag>低血压</tag>
        <tag>黄疸</tag>
        <tag>痛经</tag>
        <tag>高血压</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[九价宫颈癌疫苗冲刺内地上市：默沙东提交的申请已获受理]]></title>
    <url>%2F2018%2Fe6852184%2F</url>
    <content type="text"><![CDATA[美国药企默沙东提交的九价HPV疫苗上市申请已经在4月20日获得了受理，该药品目前的办理状态为“审评审批中”。 九价宫颈癌（HPV）疫苗在中国内地上市的脚步越来越近了。 4月24日，澎湃新闻（www.thepaper.cn）记者在国家食药监局药品审评中心的网站上查询到，美国药企默沙东提交的九价HPV疫苗上市申请已经在4月20日获得了受理，该药品目前的办理状态为“审评审批中”。 宫颈癌是仅次于卵巢癌的女性生殖道恶性肿瘤，全球每年新发病例近60万，死亡约30万。中国每年新增病例约13.5万，其中8万人因此死亡。而99.7%的宫颈癌都是由HPV病毒感染所引起。 公开资料显示，默沙东的佳达修九价HPV疫苗于2014年在美国获批上市，其可接种的年龄为9-26岁，该疫苗可预防90%的宫颈癌病毒。默沙东的佳达修九价，于2017年11月在国内获批临床，至本次上市申请获得受理只用了不到半年的时间。 2006年香港成为中国最早开放接种宫颈癌疫苗的地区，而直到去年宫颈癌疫苗才开始在中国内地正式上市。目前内地上市的产品为英国葛兰素史克生产的希瑞适二价疫苗和默沙东的佳达修四价疫苗，而香港更为普及的则是默沙东的九价产品。 价代表的是病毒覆盖的范围，价越高，覆盖的病毒亚种越多，疫苗的预防性就越好。高危型HPV的持续感染是宫颈癌的主要病因。HPV系乳头瘤病毒科旗下的DNA病毒，以人为唯一宿主，最喜爱安家的部位是皮肤和黏膜。研究表明有15种高危型HPV——尤其是编号为16和18的两种——会导致宫颈癌癌前病变及宫颈癌的发生。HPV二价疫苗就可以预防16、18两种HPV病毒；HPV四价疫苗则可用于预防6、11、16、18四种HPV病毒亚型；HPV9价疫苗，则在原有四价的基础上新增了31、33、45、52和58五种HPV病毒亚型，对病毒的覆盖面也达到了90%。 这导致在很长一段时间内，前往香港接种疫苗的内地游客络绎不绝。 方正证券的研报分析指出，预防宫颈癌市场，至少有百亿市场。之前每年有近200万人到香港注射疫苗。有专家预计，每年新增1600多万人口中800万女性，假设50%接种宫颈癌疫苗，在市场稳定后每年能带来40亿元的市场规模。 不过，佳达修九价要在内地正式上市，还要等待监管部门的最终审评审批。 此前，北京大学人民医院知名妇科专家魏丽惠教授接受澎湃新闻采访时表示，从对宫颈癌的预防效果上来说，二价和四价的疫苗几乎没有区别。对于目标接种人群，应当在有接种条件的情况下尽早接种，不要因为等待错过了最佳接种时间。 转载来源：九价宫颈癌疫苗冲刺内地上市：默沙东提交的申请已获受理]]></content>
      <categories>
        <category>健康</category>
      </categories>
      <tags>
        <tag>癌症</tag>
        <tag>药品</tag>
        <tag>默克药厂</tag>
        <tag>卵巢癌</tag>
        <tag>魏丽惠</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何让用户自愿转发推荐你的产品？]]></title>
    <url>%2F2018%2F482ec1d7%2F</url>
    <content type="text"><![CDATA[如果你的产品是C端产品，那么你一定能发现，在所有用户购买的转化渠道中，亲朋好友推荐一定是占比最高的且最为重要的。 怎么样才能用户自愿转发推荐你的产品呢？在转化渠道中，哪一条占比是最高的呢？那一个品牌应该从哪些方面着手，使得推广更加有效呢？ 如果你的产品是C端产品，那么你一定能发现，在所有用户购买的转化渠道中，亲朋好友推荐一定是占比最高的且最为重要的。比起在电视上看到的商品广告，用户更容易相信好友推荐的产品，尽管后者可能是个默默无闻的品牌。 好友推荐以往都是经过街坊邻里的口口相传的方式进行着，但对于品牌方的难处在于——品牌难以介入这种亲友推荐的营销模式中。 但移动互联网的兴起，让好友之间的信息分享变得更加快捷且可监测，品牌信息的转发，让品牌方能够介入亲友推荐这一自古以来都是最为重要的转化渠道中。 因此逻辑就是——要促进产品更有效的销售转化，就需要促进熟人关系之间产品信息的分享转发。 那么，品牌方可以从以下两个方面入手： 一、品牌内容打造，给用户转发素材有时候用户并不是觉得转发品牌信息会影响其朋友圈的个人形象，而是品牌方没有提供足够的内容让用户拥有转发的素材。用户都是十分懒惰的，不会为了转发你的品牌信息而绞尽脑汁地去拍照修图想转发语，恨不得让你直接给他们现成的文案和图片，他们只要无脑复制就行。 也就是说：不是用户不愿意转发分享，而是你没有提供相应素材，你让用户去花脑力自发帮你做传播，出发产品确实很惊艳，否则是非常难的。 那么，品牌方要做的就是在内容打造上着力，让用户能够更容易对品牌信息进行转发分享。可以从以下两个方面着手： 1. 内容多样化，满足不同的转发需求总体来说品牌方的内容分为两个类型： 一类是销售类信息，比如说优惠券、优惠活动等信息，能够给产品带来直接转化的；- 另一类是品牌类信息，比如说感性的、情绪化的主张，能够给品牌带来长期溢价的。而用户在微信端有两个不同的分享渠道： 第一个是熟人间的好友分享，更多是基于强社交关系链的人群之间，比如家人、闺蜜群、寝室群、好友等；- 第二是朋友圈的分享转发，更多可以看做自媒体的信息发布，基于弱社交关系链的信息分享，单向发布生活状态，微博、抖音等平台也是如此。那么在品牌内容打造上，可以分别针对用户分享的不同渠道做不同内容的打造： 1）接地气的好友分享 针对强关系链中的好友分享，更多需要接地气的内容。因为在这种强关系链中，双方十分熟悉，可以去除人设伪装，直接对话分享，所以更需要直接了当尽快进入正题。 那么在这个场景之下，品牌方更多可以做一些娱乐化“魔性”的内容以及优惠类内容。 人们往往会在意自己在大众眼中的形象，所以即使看到有趣的内容，也会较为介意内容过于通俗化对自身人设形象的损害。看到优惠信息时即使有转发冲动，也会担心转朋友圈是否会对自身人格形象的档次拉低。因此，段子类内容和优惠分享更多存在与好友之间、熟人群之间的转发。 2）高大上的朋友圈分享 与强关系链的分享转发不同，弱关系链中人们会更在意自身人格形象的塑造。无论是在微信朋友圈，还是微博、抖音等自媒体属性渠道中，我们都很少能看到优惠类信息的转发。而更多的是美美的照片，高大上的生活方式，这并不是他们不会转发优惠信息和段子内容，而是渠道让他们做出了内容选择。 但正因为朋友圈等自媒体渠道的个人覆盖面会更广，所以品牌方也需要针对朋友圈的转发内容做出相应规划。大到具有品质感的宣传片，小到每日金句类的打卡日签，或者精美漂亮的文案及海报，都能让用户产生转发朋友圈的欲望，以维持朋友圈中的个人形象。 另一类是有趣但不低俗的内容也容易引发朋友圈的转发，比如：用漫画创作的图文、抖音上有趣的视频等。不低俗是基本，而有趣是营造品牌亲和力并制造槽点，给用户转发分享一个明确的分享点。 2. 内容模板化，让用户自发生产素材除了品牌方制作的原生内容外，用户自发生产的内容对于品牌方意义重大。它能够让品牌方在小投入下曝光指数级的增长，但许多品牌都希望能够让用户生产内容，成功案例并不多。 1）内容模块化 想让用户生产内容，一方面需要品牌方的传播内容模块化，并且足够简单。我们都能看到“友谊的小船”漫画的二次创作刷遍了全网，除了社会情绪等外在原因外，其核心在于该漫画非常容易规模化复制，只需要添加两句文案就可以成为一个新的内容。 模块化的内容让用户大大减少了学习成本，正因为创作门槛的降低，让用户有兴趣去尝试，而用户对于自己创作的东西是天然会有转发动力的，在转发的过程中也就增加了你的品牌曝光度。 2）专业化内容引导 每一个传播案例都会面临着初始投放的问题，而相对于内容生产而言，要想引导用户自发生产内容，品牌方需要准备大量的专业化生产内容，也就是通过PGC带动UGC。 通过品牌方专业化生产的内容，可以给用户一个示范效应，当你的初始用户达到一定量级或品牌方生产的内容达到一定数量时，就容易引发用户跟风式的内容创作。 另外，品牌方专业化生产的内容相对于大多数用户生产内容会质量更高，也更具有传播价值。 二、产品超预期，制造体验惊喜产品本身会是用户转发的最强动力，你的产品如果足够优秀，用户都会自发分享转发。归结到底，用户买的是你的产品或服务，产品的好坏本质上决定着用户的需求有没有被满足，给用户惊喜，便能让用户超出期待，事实上类似一种“捡到便宜”的心态，花了一定的成本却得到超过计划的产品，这样会让用户自发与好友分享。 其实产品超预期已经不是什么新鲜事，但也有几个要注意的点容易被忽略： 1. 100分才是及格线在今天产品过剩的时代，用户在每个领域都有大量的产品可以选择，产品达到100分仅仅是个及格线，并没有什么值得分享转发的。 这里所说的100分就是能够很好地完成产品应该有的功能，比如说：你卖的是洗衣服，把衣服洗得很干净并不是什么值得用户转发的事情。产品功能的本身的好坏并已经不是转发动力了，产品要达到该有的效果只是好产品的起跑线而已。 要让用户自发分享产品信息，必须有超出100分的体验，比如说：你的洗衣粉能够比别家更快的洗干净，别的品牌可能需要洗40分钟才能洗干净，你的只需要20分钟就可以达到同样的效果，这才是超出预期的产品。 这里面又涉及到一个锚定点的设定问题，即什么才是100分产品？ 这个解释往往是来自品牌对外宣传口径中，品牌对外向消费者保证的产品功效就是你产品的100分，达到你所保证的仅仅是及格。再比如：上面那个洗衣粉的例子，如果你的宣传物料中写到20分钟洗干净，那么用户在使用时就会以20分钟为锚定点，不会产生新的超预期。 因此，品牌可能需要善用“保留项目”，说出来的one more thing不一定能成为真正的one more thing。在对外宣传中并不主打的惊喜点，这可能会成为产品口碑传播的分享点。 2. 体验上的超预期，而非功能上对于超预期而言，更多会在于产品体验上的预期，而非产品功能。事实上，用户购买你的产品是为了解决他的特定需求，用户并不会因为你的产品功能更多而得到更多的好处，有时候产品功能过多的叠加会让用户在使用产品时更疑惑也更有压力，这就是“少就是多“。 堆叠产品功能会使产品体验的下降，用户也许只是想治个流感你非要给他来个全身体检，这会加重用户的负担。 超预期是产品体验上的超预期，从物流速度，到客服态度，再到产品包装，每一个用户体验环节都能制造超预期。但许多超预期的背后是大量固定资本投入与流程优化的结果，并没有看上去那么简单。 3. 性价比不是万能的消费升级的时代，用户对性价比其实并没有以往那么敏感，当然它依旧是一个关键。一般来说，价格的下调能够让用户对产品的期望值下降，从而更容易地达到超预期的体验。但同时，价格的下调也会让用户对产品抱有怀疑态度，这种心理因素的影响，很可能会导致产品体验的错觉。 名创优品就曾表示自己总是被质疑为什么价格那么便宜？是不是产品质量不过关。同样的情况也被众多心理实验及实操案例所证明，比如：一斤智利进口车厘子卖60元你可能觉得物有所值，如果价格降到40元，用户可能就开始抱怨质量问题了，比如：不够甜、易腐烂之类的问题，而事实上产品质量可能并没有明显差别。 总结一下，通过多元化内容的搭建，满足用户各渠道的推荐转发需求，并通过产品体验的超预期，达到用户的自发推荐，你的产品及品牌才能够在当下传播环境中获得更多曝光。 作者：郑卓然（公众号：传播体操），擅长广告文案、新媒体运营，现任某独角兽高级内容运营经理。 本文由 &#64;郑卓然 原创发布于人人都是产品经理。未经许可，禁止转载 题图来自Unsplash，基于CCO协议 转载来源：如何让用户自愿转发推荐你的产品？]]></content>
      <categories>
        <category>科技</category>
      </categories>
      <tags>
        <tag>移动互联网</tag>
        <tag>自媒体</tag>
        <tag>朋友圈</tag>
        <tag>市场营销</tag>
        <tag>漫画</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[中国硬独角兽TOP100：实业科技公司崛起！]]></title>
    <url>%2F2018%2F5f6bd87b%2F</url>
    <content type="text"><![CDATA[一份不一样的独角兽榜单 转载来源：中国硬独角兽TOP100：实业科技公司崛起！]]></content>
      <tags>
        <tag>i黑马</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于 Go 语言的 API 网关 Goku-API-Gateway | 软件推介]]></title>
    <url>%2F2018%2Fd030d96d%2F</url>
    <content type="text"><![CDATA[国内首个开源 Go 语言 API 网关，帮助企业进行 API 服务治理与 API 性能安全维护，为企业数字化赋能。 转载来源：基于 Go 语言的 API 网关 Goku-API-Gateway | 软件推介]]></content>
      <tags>
        <tag>开源中国</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[第72号“百名红通人员”潜逃泰国6年，意外触电客死他乡]]></title>
    <url>%2F2018%2Fd8eb082e%2F</url>
    <content type="text"><![CDATA[这是2016年“百名红通人员”第63号张某随上海市追逃办工作组回到上海后,在忏悔录中写下的一段话。然而,与之形成鲜明对比的是,第72号“百名红通人员”顾某却在潜逃泰国六年之后,因意外触电客死他乡。 转载来源：第72号“百名红通人员”潜逃泰国6年，意外触电客死他乡]]></content>
  </entry>
  <entry>
    <title><![CDATA[36氪专访丨马蜂窝首次回应黄轩广告争议：下一个时代属于00后？]]></title>
    <url>%2F2018%2F89af2990%2F</url>
    <content type="text"><![CDATA[36氪专访丨马蜂窝首次回应黄轩广告争议：下一个时代属于00后？_36氪 转载来源：36氪专访丨马蜂窝首次回应黄轩广告争议：下一个时代属于00后？]]></content>
  </entry>
  <entry>
    <title><![CDATA[巧用百度指数进行市场和竞品分析]]></title>
    <url>%2F2018%2F5685d526%2F</url>
    <content type="text"><![CDATA[百度指数，我们并不陌生，但是在大部分产品经理的日常工作中，使用频次并不多。原因可能是觉得它离我们实际工作太遥远，太宏观，没什么用，也不知道如何使用。在这篇文章中，将跟大家分享如何巧用百度指数这个免费的资源，帮助我们完成市场分析、竞品分析等产品经理必须的工作。 要知道如何利用百度指数，首先要知道百度指数到底是什么？有哪些特性？ 百度官方的定义：百度指数是以百度海量网民行为数据为基础的数据分享平台。在这里，你可以研究关键词搜索趋势、洞察网民需求变化、监测媒体舆情趋势、定位数字消费者特征；还可以从行业的角度，分析市场特点。 剖析官方定义，得知百度指数的如下几个属性： 数据来源：网民搜索行为数据；- 数据基础：搜索关键词；- 数据主要应用场景：探索市场趋势、了解用户需求、监测舆情、用户特征分析。针对几个典型的应用场景，结合具体案例，我们一起探索下究竟如何使用百度指数的趋势研究、需求图谱、舆情洞察、人群画像。 1. 趋势研究趋势研究可应用在研究市场趋势和竞品。以共享单车为例： 1.1 市场趋势研究通过在百度指数中搜索“共享单车”关键词，可以查看到该关键词的搜索趋势如下： 从以上趋势图可以看出，共享单车的搜索热度大概从2016年10月开始增长，在2017年2月左右出现爆发式增长，并从2017年7月开始逐步降低，趋于平缓。从关键词的热度趋势我们能大致估计出该行业的热度走势。 此外，我们可以再看看在相同时段内，该关键词的媒体指数，并挖掘出每个时段内值得关注的信息点。如下图： 通过上图可以看出，在2016年～2017年间，关键词“共享单车”的媒体指数与热度趋势基本趋同，说明媒体的报道与用户关注度是相互促进的。而且通过在同一时段最热的媒体报道可以总结出如下信息： 共享单车行业在2016年11月的时候已经进入恶性竞争阶段；- 行业内最强的竞争对手是摩拜和ofo；- 2016年12月共享单车已经出现野蛮生长态势；- 共享单车的管理、回收、监管等问题亟待解决；- 共享单车押金问题存在安全隐患；- ……如果我们要进入共享单车这个领域，以上问题和环境也将是自己面临的，可以帮助我们判断自己是否有足够的优势进入该领域。 1.2 竞品研究除了通过行业核心关键词研究行业趋势外，我们还可以通过竞品关键词研究竞品的趋势及优劣势。 我们以ofo为例做竞品分析。在百度指数中搜索关键词“ofo”，可得到如下趋势： 从以上趋势图，我们可以看出，ofo的搜索热度从2016年8月左右开始逐渐上升，在2017年1月开始断崖式下滑，然后从2月左右开始出现触底反弹，在2017年3月左右达到进入新一轮峰值。 在看到趋势及拐点之后，我们可以根据时间线索去搜索、寻找出现拐点的原因。通过查看ofo百科中的发展历程，以及每个时间段的媒体报道，我们可以发现： 在2016年8月时，ofo完成A+轮融资；- 2017年1、2月是春节，春节之后ofo马上推出了智能锁（智能锁推出前，车锁一直ofo硬伤）；- 大概在2017年3月时，ofo完成了D轮融资；- ….所以，我们可以通过分析每一个时间拐点上的：产品动作、运营动作、市场动作，去分析竞品的动向及结果。 最后，趋势分析还可以支持同时多个关键词的对比分析。例如，我们如果要同时比较摩拜和ofo的趋势，可以添加两个关键词进行趋势比较（百度指数最多支持5个关键词的比较检索），如下图： 从以上趋势可以简单看出，从2016年10月至2017年8月期间，ofo的搜索指数都是高于摩拜的，但是后续ofo下降明显，趋于平缓之后，只略微高于摩拜。 搜索指数的热度与市场份额应该是正相关。由此我们可以猜测，ofo在全国的市场份额是多于摩拜的。如果要看各地区，还可以通过修改关键词地区指数进行查看，从而了解在每个省份的市场份额情况，如下图： 2. 需求图谱需求图谱又叫需求分布，提供关键词的相关词及相关程度的信息。 其中，相关词距圆心的距离表示相关词相关性强度； 相关词自身大小表示相关词自身搜索指数大小，红色代表搜索指数上升，绿色代表搜索指数下降。相关词可以帮助我们了解，围绕关键词，用户的聚焦点，以及产品和服务的痛点。 如上图，搜索共享单车，通过调整下方的时间轴，可以看到在不同时间段内，共享单车的相关词分布情况。 以2018年4月2日至4月8日期间的相关词分布为例，“共享单车”的热门相关词包含：“摩拜单车”、“摩拜”、“哈罗”、“哈罗单车”、“ofo”、“共享单车盈利模式”等等。 从这些相关词中我们可以了解到： 共享单车领域几大主要市场玩家是摩拜、哈罗、ofo，而且这几大玩家的搜索热度都呈现上升趋势，与共享单车的相关性也很强，尤其是摩拜和哈罗。1. 针对共享单车这个行业，人们很关心的点是盈利模式是什么？单车怎么使用？有什么样的管理规则出台？再比如，我们以“ofo”为关键词进行分析，可以看到在2017年7月31日至2017年8月6日时间段内，相关词的分布如下图所示，发现“ofo”的其中一个相关词是“淘宝”，相关性较高且还呈现搜索热度上升趋势。 为了弄明白它们之间究竟是如何关联上的，于是在百度上搜索一组组合关键词：淘宝为了弄明白它们之间究竟是如何关联上的，于是在百度上搜索一组组合关键词：淘宝＋ofo，搜索结果如下： 从上图可以了解到，ofo曾发生了红包被盗刷的事件。 需求图谱除了可以看到相关词分布外，还可以查到top15的来源相关词和去向相关词（如下图所示）。 来源相关词是指用户在搜索中心词之前搜索过哪些关键词；去向相关词是指用户在搜索中心词之后搜索过哪些关键词。 例如，用户在搜索“共享单车”之前，经常搜索“哈罗”、“摩拜”、“共享单车盈利模式”等关键词，但是在搜索了“共享单车”之后，经常搜索“摩拜”、“共享单车新规”、“共享经济”等关键词。 3. 舆情洞察当我们需要了解某行业或某竞品在某个时间段内的主要动向时，可以使用舆情洞察。 例如，要了解ofo的动向，我们看到ofo的媒体报道热度是从2017年2月左右开始增长的，从热门报道看，这应该得益于4.5亿美元的D轮融资。此外，在2017年3月至2017年8月期间，ofo的主要动向： 完成D轮融资；- 在共享单车行业排名第一，并牵头制定行业标准，奠定行业地位；- 通过一些绿色公益活动、明星代言等形式提升品牌影响力；- 存在商标侵权的公关危机。除此以外，如上文中的案例所示，将舆情洞察与趋势研究中的拐点结合，也是分析趋势走势的一个得力工具。 4. 人群画像人群画像比较简单，就是体现搜索关键词的用户基本属性，这些属性包括：年龄、性别、地域等。 例如，我们可以通过，同时输入“摩拜”和“ofo”两个关键词，将对摩拜和ofo的用户进行对比，如下图所示，可得出如下结论： 摩拜和ofo的用户年龄分布基本一致，30～39岁年龄段最多，占尽50%；- 摩拜和ofo的用户性别分布基本一致，男性占比约70%，远远超过女性；- 从地域分布看，ofo主攻的是北上广，而摩拜主攻北京、广东、浙江、江苏、山东、湖北等地。 5. 检索技巧百度指数除了对单个关键词进行分析之外，还支持多个关键词的分析。 （1）比较检索 在多个关键词当中，用逗号将不同的关键词隔开，可以实现关键词数据的比较查询。 例如，如果要对几个竞品进行对比分析，可以检索“ofo,摩拜,哈罗”。百度指数最多支持5个关键词的比较检索。 （2）累加检索 在多个关键词当中，利用加号将不同的关键词相连接，可以实现不同关键词数据相加。相加后的汇总数据作为一个组合关键词展现出来。 例如，要更全面覆盖摩拜相关关键词的数据，可以检索“摩拜＋摩拜单车”等，百度指数最多支持3个关键词的累加检索。 （3）组合检索 即可以将“比较检索”和“累加检索”组合使用，例如“ofo+ofo共享单车，摩拜＋摩拜单车，哈罗＋哈罗单车” （4）地域检索 每一组关键词，都可以通过地区筛选不同省份的关键词搜索数据，百度指数最多支持5个地区对比检索。 以上即为通过百度指数的免费资源获取一些宏观数据信息的小技巧。希望能给大家的实际工作带来一定帮助。 作者：菜花，前阿里巴巴产品经理，微信公众号：菜花谈产品（ID：caihuatan2016） 本文由 &#64;菜花 原创发布于人人都是产品经理。未经许可，禁止转载。 题图来源于网络 转载来源：巧用百度指数进行市场和竞品分析]]></content>
      <categories>
        <category>职场</category>
      </categories>
      <tags>
        <tag>自行车</tag>
        <tag>产品经理</tag>
        <tag>职场</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Dlib 19.9的巨大进步，安装无须Boost.Python]]></title>
    <url>%2F2018%2Fe905ff27%2F</url>
    <content type="text"><![CDATA[Dlib Release 19.9 New Features and Improvements&#58;Major Changes in this Release- Switched the Python API from Boost.Python to pybind11. This means Python users don’t need to install Boost anymore, making building dlib’s Python API much easier.- ……（其他的更新不关心！） 我看到了什么？！ don’t need to install Boost anymore don’t need to install Boost anymore don’t need to install Boost anymore 重要的话说三遍！ 用过或者曾经试图用Python版Dlib未遂的人一定会对它安装过程中对于Boost.Python印象深刻，一定可以理解我为什么看到上面的更新日志那么激动。 对于那些使用dlib未遂，在安装过程中就放弃的人，我想说，我非常理解你们。也许是我用Python还是不够多，但是就我的浅薄的经验，在我用Python以来，安装个Python库还需要这么麻烦的，只有dlib一例。 我曾经花费大力气编译了Boost然后配置好了环境，后来每次安装的时候就方便多了。Dlib确实是一个非常好用的库，我用它做过一些非常有意思的事情，比如： 40行代码的人脸识别实践- 用Python给头像加上圣诞帽- 还有一些更有意思的还未来的及写出来用Python给头像加上圣诞帽 不少读者想要尝试自己做一些这些有意思的项目的时候通常都会卡死在安装上，不断有人问dlib安装的时候遇到找不到Boost的问题怎么办。为此我写了Dlib的配置教程： python下安装dlib（boost.python的编译）- Dlib在VS2015上的编译和配置（人脸检测人脸识别比OpenCV更好用）Dlib在VS2015上的编译和配置（人脸检测人脸识别比OpenCV更好用） 不少人尝试过，但是我敢肯定不少人放弃过，我在知识星球中不止一次的发布过dlib的更简单的安装方式，但是都避不开Boost。 这一次，Dlib终于摆脱了Boost这个沉重的包袱，轻装上路。Dlib的作者Davisking为此在19.8版本放发布一个多月的时候专门发布了一个新版本19.9。想必他也是被来问问题的人困扰了N久了。 这是一个巨大的进步！（this is a massive improvement!）Github上有人如是说，说出了大家的心声。 安装现在安装Dlib就不会想上面提到的教程那么繁琐了。 之前安装过dlib的需要先卸载以前的版本。 下载dlib19.9.zip，解压后再dlib19.9文件夹下会发现一个名为setup.py的文件。 然后在当前文件夹下打开命令行（Shift+右键）。输入一下命令： 如果cpu支持AVX指令集，还可以让dlib更快一点 如果有可用的GPU而且安装了CUDA，那么dlib还可以更快。 转载来源：Dlib 19.9的巨大进步，安装无须Boost.Python]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>编程语言</tag>
        <tag>GitHub</tag>
        <tag>Python</tag>
        <tag>GPU</tag>
        <tag>OpenCV</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[腾讯研究院：独家解读腾讯首款区块链游戏化应用白皮书]]></title>
    <url>%2F2018%2F33401f46%2F</url>
    <content type="text"><![CDATA[2018年4月23日，UP2018腾讯新文创生态大会上，腾讯游戏创新工作室与腾讯区块链联合发布了腾讯首款区块链游戏化应用《一起来捉妖》。 看完新闻，很多人的第一反应是迫不及待的去游戏官网进行预约。但是腾小研要在这里提醒你： 少年，想要了解一款区块链应用，得先从阅读白皮书开始啊！ 白皮书，其实就是区块链项目的说明书、产品文档、技术说明，起到向公众、使用者、技术研发人员解明项目的愿景、技术实现、主要功能的作用。 之所以在区块链领域会有“白皮书”这个概念，是因为“区块链”这个词其实本就出自于中本聪在2008年发布的《比特币白皮书》。所以，之后的几乎所有区块链项目都会发布一份白皮书。而阅读白皮书，也成为了深入了解一个区块链项目的必备环节。 那么，腾讯首款区块链游戏化应用《一起来捉妖》的白皮书在哪里呢？ 这你就来对地方了，《一起来捉妖》的白皮书由腾讯FiT（支付基础平台与金融应用线）、腾讯研究院、腾讯游戏创新工作室共同撰写。就在刚刚，和《一起来捉妖》游戏一起发布啦！ 不过，白皮书很长，你的心情很急切，我们摘出了白皮书中大家最关切的问题。如果你对白皮书全文感兴趣，可以跳过这一部分哦： 解读一、腾讯为什么要在游戏中融入区块链要素？在区块链发展的初期，产业生态以比特币为代表的数字货币为主。 2014年起，区块链的影响扩散到广义金融领域，在清结算、跨境支付、资产登记等领域应用层出不穷。 区块链与传统金融结合，在提升交易效率、降低交易成本方面大显身手。以太坊的出现提供了图灵完备的区块链底层，帮助开发者便捷地创建包含状态和完备逻辑的智能合约，大大降低了区块链项目的开发成本和技术门槛，使区块链的分布式、去信任的机制等广泛运用到金融之外的知识产权保护、供应链管理、公共管理、公益等各个领域。 区块链本身并不神秘，只有真正的产业场景落地才能彰显其内在价值。游戏为区块链与产业的融合打开了一扇大门。 游戏产业绝大多数环节都是纯数字化的、虚拟化的。游戏世界原本就存在用户社群、虚拟商品交易、代币结算，也与区块链应用的很多要素不谋而合。区块链的运行原理决定其自发性和不可篡改性。区块链的交易信息采用非对称加密，保证了交易信息的准确性和安全性。非对称加密除了保证信息的安全性之外，还可以进行身份验证，确保信息的准确性。 我们希望借助于区块链技术中的“公开、公正、公平”特点，解决传统游戏设计中由于黑盒、暗改、玩家地位不对等问题带来的玩家对运营方的不信任感问题，同时使用游戏化的方式普及区块链概念，玩家将在游戏道具的获取和交易中体会到区块链账本、智能合约相关的功能和机制。 游戏中将诞生1105亿只专属猫，它们可以彼此之间自由繁殖，保存在永不消失的中央游戏中。大家也可以把捉到的极品猫进行保存，这是非常令人激动的尝试和探索，意味着虚拟的价值可能超过一款产品成为更长期的存在。 解读二、先有游戏，后应用区块链《一起来捉妖》是一款腾讯游戏创新工作室制作的增强现实（Augmented Reality）游戏。 游戏的主要体验是玩家通过在游戏中使用AR功能抓捕身边的妖精，获取基础的生存和成长资源；使用这些资源对妖精进行培养，以完成游戏中PVE/PVP对战玩法和展示、收藏、交易等诸多功能。 区块链技术，作为游戏中一些功能落地应用，比如专属猫的搜集、收藏、繁育。同时，游戏中专属猫的交易也基于区块链技术。 因此，和其它的区块链游戏不同，《一起来捉妖》首先是一款好玩的游戏，然后从游戏的可玩性、功能性出发，融入和应用了区块链技术。 解读三、区块链如何提升游戏体验？既然核心玩法和其他区块链游戏并不一样，那么区块链技术在《一起来捉妖》中时如何体现的呢？ 根据白皮书介绍，腾讯利用区块链技术从数据可信任、游戏道具确权、区块链道具在游戏中的实际作用、通过区块链账本实现游戏进程中的传承与永久记载、安全保护、媒体节点引入六个方面实现了游戏体验的提升。 统而言之，与传统网络游戏、手机游戏中游戏道具和游戏资产只是运营公司服务器上的一个数字相比，《一起来捉妖》里的专属猫采用区块链技术确权、保存、交易，就算游戏运营方数据库被入侵，也不会造成用户游戏财产的丢失或盗用。同时，可以让玩家与游戏中的数字藏品建立情感连接，无论何时都可以调取，交互，成为玩家永恒的记忆。 解读四、所以这个游戏究竟怎么玩？在游戏中， 玩家通过游戏的相关AR系统，与游戏中的各种交互点进行不同的交互（抓捕、挑战、摆摊交易等），以获取游戏中的各种资源，完成成长线，收获游戏乐趣体验。 游戏中最核心的AR系统是AR抓捕：玩家通过向镜头中的AR对象，也就是游戏中的妖精，抛掷游戏中的抓捕法宝——梦灵，以完成抓捕。每次抛掷梦灵，都需要消耗一颗封妖魂珠的能量。而AR功能带来的人机交互和场景代入感是本核心玩法中最重要的情感体验。 玩家通过AR玩法进行妖精收集，并且可以通过重复抓捕妖精来获取妖精符印。玩家获得足够的符印后，可使用通过游戏中各种途径产出的游戏货币——云纹来对妖精进行升级、操作。妖精等级提升到一定程度后，就可以消耗一定数量的符印进行觉醒。觉醒后的妖精会拥有更炫酷的形象、更多的技能和更强的数值。 当玩家拥有妖精并达到一定的等级后，就可以参与游戏设计的封妖之路、妖精擂台、孔明灯挂机、须弥岛养成、妖精挂机等玩法。 同时，游戏还提供了聊天、罗盘、组队、圈子等社交玩法。玩家可以通过这些玩法寻找志同道合的玩家一起完成日常任务、攻克PVE关卡、挑战圈子任务等。 解读五、能解释一下其中区块链部分怎么玩吗？专属猫是《一起来捉妖》游戏中首个接入区块链技术的玩法。 专属猫是一种猫形数字宠物，玩家通过商城、猫的繁育、参与活动三种方式获取新的猫。专属猫数量有限，且可以在游戏中通过玩家之间的点券支付进行转让。 在《一起来捉妖》游戏中，我们将投放640000只“0代猫”，0代猫的投放速度由系统按游戏运营的实际需求控制。 游戏中的专属猫储存在区块链网络上由特性进行区分，由区块链技术保证每一只猫都独一无二，绝对不存在特性完全一致的两只猫。 玩家可以通过游戏道具召唤、从市场（其他玩家）购买、繁育、运营活动四个渠道中获得专属猫。 每只专属猫有不同的外观特性，游戏中总计有十数种特性类型，每种类型下都有若干个不同的样式。总计有超过千亿种组合。多数特性最早都会出现在0代猫身上，由系统控制投放。特性基因可通过繁殖有概率继承。 每只猫的出生信息、交易信息、繁育信息将作为“猫的一生”被记录在区块链区块链账本中，实现每只猫的透明、溯源。 解读六、未来打算如何发展？《一起来捉妖》演进路线图 发文时比特币价格 ￥58039.29 原文：腾讯研究院（https&#58;//mp.weixin.qq.com/s/1G-x1-Yd0F2REJ7JeFyTUw） 作者：腾讯研究院 编辑：野比大雄 稿源：http&#58;//www.8btc.com/tencent-blockchain-game-0423 版权声明： 作者保留权利。文章为作者独立观点，不代表巴比特立场。 转载来源：腾讯研究院：独家解读腾讯首款区块链游戏化应用白皮书]]></content>
      <categories>
        <category>科技</category>
      </categories>
      <tags>
        <tag>网游</tag>
        <tag>区块链</tag>
        <tag>数字货币</tag>
        <tag>比特币</tag>
        <tag>金融</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[阳光宽频网]]></title>
    <url>%2F2018%2F12980005%2F</url>
    <content type="text"><![CDATA[阳光宽频网 转载来源：阳光宽频网]]></content>
  </entry>
  <entry>
    <title><![CDATA[二甲双胍被称为“神药”，为什么很多糖友不愿意吃？]]></title>
    <url>%2F2018%2F4ffaaf22%2F</url>
    <content type="text"><![CDATA[二甲双胍，是2型糖尿病患者的一线**首选用药。不论是单药使用，还是与胰岛素等其他药物联合使用都能发挥很好的作用，且有辅助减重、降低心血管疾病、癌症等发病率的功效，甚至被媒体封为“神药**”。 但英国有研究发现，2型糖尿病患者对二甲双胍的接受度很低，30%的患者并未按要求服药，相比于DPP-4抑制、磺脲类、噻唑烷二酮类等药物，依从率最低。 为什么二甲双胍功效那么好，糖友的依从率却很低**，很多糖友宁可换药甚至不吃？** 糖友对二甲双胍的依从率低，主要是其常见副作用及糖友对它的误解引起的。 常见副作用 胃肠道反应 二甲双胍会刺激肠胃，很有糖友会产生常见的胃肠道反应，如：胃胀、腹泻、恶心、腹部不适、食欲差等，导致不想服用二甲双胍。 其实，大多数糖友经过一段时间的治疗，二甲双胍的副作用会逐渐消退。而且为了规避这些不良反应，医生也会调节用药，推荐初始剂量，适应后再逐渐增加。 糖友误解 损伤肝肾功能 网传二甲双胍会损伤肝肾功能，致使很多糖友在服用二甲双胍类药物时都提心吊胆，尤其是出现一些不良反应时，可能被吓的自行停药。其实并非如此。 事实上，二甲双胍吸收到体内后，有90%会以原型从肾脏中排泄出去，没有肝、肾毒性。 糖友服用二甲双胍后，在增加葡萄糖无氧酵解和减少糖异生的过程中，会产生一种酸性代谢物产物——乳酸。 正常情况下，乳酸会被进一步分解代谢，再从肾脏排出体外，并不会损伤肾脏。但如果糖友伴有肾功能受损，乳酸的代谢、排泄就会受阻，使其蓄积在体内，可引起乳酸性酸中毒，从而损伤肝、肾等脏器。 简单而言，对肾功能正常者，口服常规剂量的二甲双胍不会伤害肾脏。但如果肾功能不好，因排泄受阻，会引发药物不良反应。 那么，二甲双胍究竟是通过什么原理来降糖的？ 二甲双胍，不仅可以辅助降糖，还是目前口服降糖药中唯一被推荐可以有效降低糖尿病患者心血管事件的药物，其作用机理可以分为以下四个方面。 1**提高胰岛素敏感性** 二甲双胍可以提高机体对胰岛素的敏感性，增加肌肉、脂肪组织对葡萄糖的摄取、利用率，从而促进葡萄糖的无氧酵解，加大消耗。 2**减少肝糖输出** 肝脏贮存的葡萄糖，可以输送到血液供全身使用。而二甲双胍能抑制肝糖原异生，继而减少血葡萄糖，以降低血糖。 3**抑制食欲** 二甲双胍能抑制肠壁吸收葡萄糖，从而降低食欲，减少食用量。 4**降低心血管病发病率** 二甲双胍不仅可以辅助降低甘油三酯，还能抑制小肠胆固醇的合成与储存，降低血胆固醇，对于伴有肥胖的糖友，又能辅助减肥。 也就是说，二甲双胍能降低心血管疾病的风险因子，从而减少心血管疾病的发病率。 此外，二甲双胍不会增加胰岛素水平，所以一般不会引起低血糖，可以减少血糖的波动幅度。相对来说也更为安全。 但是，市面上的二甲双胍有各种剂型，怎么选择更适合？ 二甲双胍以剂型分类，主要有普通片剂、肠溶片、缓释片。 1、普通片剂 普通片剂，即盐酸二甲双胍片，是最普通的片剂，价格也相对低廉。 二甲双胍普通片剂大部分会在食道、胃中分解吸收，所以胃肠道反应相对较大。因此，建议餐时或餐后服用，一般每天至少服用2次。 2、肠溶片 肠溶片，即盐酸二甲双胍肠溶片（或胶囊**）**，是二甲双胍普通片的改良版。在肠溶材料的包裹下，它能直接到小肠，从而减少胃肠道尤其是上消化道的不良反应。 要注意的是：肠溶制剂不能掰开服用。一旦掰开，就无法达到肠溶的效果，影响药效、刺激胃部。 二甲双胍肠溶片应该放在餐前15-20分钟服用会比较好，服用次数一般也至少2次/天。 3、缓释片（或控释片） 二甲双胍缓释片，即盐酸二甲双胍缓释片（或控释片），是以凝胶包裹的药物，以达到缓慢释放的效果。 不仅能减少胃肠反应，还能减少服用次数，每日仅需服用1次，对长期用药者更为便利。但缓释片的吸收和起效普遍较慢，所以适宜餐前30分钟服用。 二甲双胍虽是首选药物，但也有注意事项，如下： 1、定期检查不能少服用二甲双胍期间，应定期检查空腹血糖、尿糖、尿酮及肝肾功能。2、避免过量饮酒酒精，会减慢二甲双胍的排泄，从而增加乳酸性酸中毒的风险，一定要避免过量饮酒。3、预防缺乏维生素B12长期服用二甲双胍，可能引起维生素B12的缺乏，尤其是摄入不足或吸收不良者，应监测血清维生素B12水平。4、特殊人群/期间忌服二甲双胍类药物禁用于肾功能不全，缺氧或接受大手术等糖尿病患者。如果要进行造影检查，也要暂时停用二甲双胍。 转载来源：二甲双胍被称为“神药”，为什么很多糖友不愿意吃？]]></content>
      <categories>
        <category>健康</category>
      </categories>
      <tags>
        <tag>癌症</tag>
        <tag>药品</tag>
        <tag>糖尿病</tag>
        <tag>心血管</tag>
        <tag>腹泻</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[为中国芯万字投书马化腾：从Intel和ARM争霸战，看看做芯片有多难]]></title>
    <url>%2F2018%2F59235777%2F</url>
    <content type="text"><![CDATA[作者：torvaldsing 【新智元导读】为什么中国做了30年芯片都没有出现英特尔、ARM这样的巨头？关键在于国产CPU缺少强大的生态系统。有着十余年芯片行业经验的torvaldsing投书新智元，把x86生态系统和ARM生态系统的艰难发展历程和残酷的市场竞争大起底。最后呼吁马化腾：请借助腾讯的强大生态，把CPU和OS这两个老大难问题给OTT掉！ 这几天中兴事件持续发酵以来，各种议论纷纷扰扰。 上周，新智元推送了《中国芯“逃兵”：缺芯是因为缺钱；中国芯“老炮”：芯片救国靠BAT不是开玩笑》一文，引起了无数从业者热议。 有十余年芯片从业经验的水木网友torvaldsing告诉新智元，这几天对他触动最大的，还是碧树西风写的这句话： 一碗牛肉面，真的要用牛肉，真的要用面，真的要炖很久，这么简单的道理，偌大一个国家，这么多精英，过去这么多年了，咋就不能懂呢？ 做芯片很难，做核心芯片更难，做需要生态系统的CPU芯片，比大家想象得都要难。 因此，torvaldsing投书新智元，尝试谈一谈x86生态系统和ARM生态系统的艰难发展历程和残酷的市场竞争，向大家介绍一下做CPU的各种困难，以及眼下能看到的一线希望。 以下是torvaldsing的雄文： 我尽量写得轻松一些，因为其实这个话题很有趣，仔细探究起来，很多看似爆炸性的新闻，其实草蛇灰线伏脉千里，在很早之前就发端了，这其中的故事，真的像演义小说一样好玩。 本文会罗列很多的往事和参考资料，保证有诚意。一些地方没忍住加上了一些三脚猫的分析，欢迎拍砖打脸。 （本文约1.3万字，建议先收藏后阅读） x86**生态系统**如今Intel在服务器市场占有率近乎100%，在桌面市场也大于80%，再加上Intel一贯重视宣传，在普通大众的心目中，Intel就是芯片的代称，甚至是高科技的代称。但Intel并非生而如此，它的牛X千真万确是熬出来的，是在列强环伺的竞争环境中杀出来的。 称王 七十年代，在搭上IBM PC这趟快车之前，Intel的8位处理器已经很成功，但也有很多竞争者，Zilog是其中翘楚，它研发的Z80系列产品和Intel的8080兼容，性价比高。一直到90年代，中国很多大学的微机实验课，还在用Zilog的板子。当时还有一款处理器风头不逊于8080系列，即MOS公司的6502。后来MOS把6502的ISA（指令集架构）授权给了众多厂商，流传甚广。70年代苹果创立之初的Apple-I和Apple-II，80年代任天堂的红白机，90年代初的小霸王学习机，90年代末的文曲星，都使用了6502系列的CPU。 IBM PC给了Intel和微软大发展的机会。但它俩必须面对竞争。IBM PC是IBM主导下的一个开放标准，各个零部件都是可以替换的。所以才有了“兼容机”的概念，和延续至今的装机市场。当时IBM要求Intel必须把x86指令集授权给其它厂商，避免CPU供应商一家独大。（详细的x86兼容处理器生产厂家列表见https&#58;//en.wikipedia.org/wiki/List_of_x86_manufacturers）IBM自己也有生成x86兼容CPU的权力。同时，为了限制微软的MS-DOS，IBM自己也做DOS操作系统，名为PC-DOS。 在IBM PC阵营内部，Intel面对其它CPU供应商的竞争，在阵营外部，还要和苹果的Macintosh电脑竞争。当时苹果已经换用Motorola 68000系列CPU，性能强劲，图形界面诱人。当时用Mac的人，逼格要高于用IBM PC的人。 Intel顶着阵营内外的竞争压力，苦心孤诣地发展壮大。这时候潜在的威胁在慢慢酝酿。从1981年的RISC-I开始，精简指令集（RISC）逐步流行起来，诞生了一系列RISC风格的CPU：1985年MIPS公司推出第一款商用的RISC芯片，HP公司在1986年推出PA-RISC，SUN公司在1987年推出SPARC，Motorola在1988年推出MC88000。当时大家普遍认为RISC优于以x86为代表的CISC风格CPU，就连Intel和AMD也害怕在RISC潮流中落伍，AMD在1987年推出了AM29000，Intel在1988年推出了i860/i960。 开始时RISC似乎并没有威胁到桌面市场，MIPS、PA-RISC、SPARC全是用来做服务器和工作站的。被苹果流放的乔布斯用MC88000系列CPU做NeXT桌面电脑，铩羽而归。1986年，英国的Acorn公司推出了一款名为ARM的RISC处理器，次年，它还配了个操作系统叫RISC OS，强攻桌面市场，可惜最终只在英国掀起来了一些波澜。 1991年，RISC阵营实实在在地杀入桌面市场。这一年，IBM看到在PC阵营里，Intel和微软这两个小弟坐大，慢慢不受自己的控制，索性拉拢Apple和在RISC市场不得志的Motorola，推出了PowerPC架构，由IBM和Motorola生产芯片，Apple做操作系统和整机，推出全新的Power Macintosh电脑。这三家组成了AIM（Apple-IBM-Motorola）联盟，气势汹汹地向Wintel联盟发起攻击。 结果是Wintel赢了，个中原因众说纷纭。有人说Wintel保持对已有软件的向下兼容，而Apple频繁更换底层的CPU，导致的不兼容气走了用户，然后由此强调软件生态的重要。我则以为，历史的发展有一定的偶然性，如果当时Wintel不是比尔盖茨和格鲁夫在掌舵，而Apple是乔布斯在掌舵，可能结局完全不同。2005年，乔布斯掌舵下的苹果，把Mac里面的CPU由PowerPC换成Intel的芯片，就完成得干脆利落，没怎么受到软件生态的牵绊。 总之，在80年代，大家就已经深深懂得CPU的ISA是软件生态系统的根基，不愿让这个“生态之根”被别人控制。整机和系统的制造商，通过强制CPU厂商给其它厂商授权自己的ISA，来保证有第二家甚至更多的供应商。如果不慎“生态之根”被别人控制了，例如IBM被Wintel篡了权，甚至不惜另起炉灶来竞争。 同样是把自己的指令集授权给其它厂商，Intel把几乎所有的其它供应商都挤死了，只省下AMD苟延残喘；MOS则销声匿迹了，完全靠其它生产商把6502系列延续到了二十一世纪。造成这一差异的原因纵有千万条，我想“打铁还需自身硬”是最根本的。 霸业 在桌面市场上，Windows 95和Windows 98这两款操作系统，让Wintel联盟登上了霸业的顶端。从1995年到2003年，Intel看起来简直是不可战胜的。 与此同时，Intel还把几乎所有的RISC架构的CPU都干趴下了，占领了服务器市场。原因大概有这么几点。 第一，从技术角度讲，RISC是一种设计CPU的理念，而不是具体的某一种ISA。像x86这样的复杂指令集，其实在实现过程中，也能借重RISC的理念。1989年的80486，已经隐隐地可以看到RISC风格的流水线，1995年的Pentium Pro，其核心已经是一个乱序执行的RISC了，只不过多了一个复杂的译码逻辑，把x86指令拆分成RISC风格的微操作。因此从技术角度讲，RISC指令集未必比x86有优势。 第二，RISC成也UNIX，败也UNIX。UNIX和C语言树立了很好的软件开发传统，确保同一套代码可以很方便地在不同CPU之间移植。80年代，一大堆RISC架构的CPU，都可以很快配上自己的UNIX，很快把已有的C语言编写的应用跑在CPU上，然后就可以卖了。SUN公司的SPARC配有Solaris，HP公司的PA-RISC配有HP-UX，IBM公司的PowerPC配有AIX。 这些林林总总的UNIX变体，反过来又进一步促使UNIX生态系统中软件开发人员重视代码的可移植性，大家都很小心地围绕POSIX标准来编程，避免过分依赖于某个操作系统独有的功能。这样，一旦Intel芯片携Linux（一种开源的UNIX变体）来和RISC架构的工作站竞争，软件应用就纷纷以很小的移植难度，离开了昂贵的专有UNIX工作站。 第三，当时PC市场比服务器市场大得多，Intel在PC市场的盈利帮助它研发更好的服务器芯片，巨大的出货量降低了芯片的制造成本。研发优势和成本优势，奠定了Intel最终胜利的基础。 这段时间，Intel还几次面临挑战，每次都成功保卫了自己对于生态系统的掌控权。 第一个挑战，来自Internet浏览器。Netscape Navigator诞生后，对微软和Intel都是挑战。虽然当时的动态网页还非常初级，但是已经有人喊出“Web is the computer”的概念。等到Java Applet出现之后，大家更是觉得可以在网页上实现桌面应用的效果，未来只需一个浏览器，就能取代桌面。Netscape的Marc Andreessen在1995年，就着手把Netscape浏览器打造成一个Internet OS。以那个时代的软硬件水平，毫无疑问地，这些尝试失败了。 用一个高层次的软件API，兜住所有的上层应用，然后让底层的硬件，都来支持这个API——这个主意不单单在技术上看起来很炫，从商业上，这是上层应用厂商消解底层平台厂商生态霸权的终极武器。因此，在那之后的二十年里，商业上的尝试一直在持续，包括： 腾讯开发的WebQQ和Q+，在网页里面提供一个类似Windows桌面的应用场景，后来失败了，回退到功能单一的SmartQQ。个中原因，我个人认为还是那个时代的PC性能不够。 腾讯开发的微信小程序，在微信里面通过HTML5和Javascript实现手机App的功能，可以横跨iOS和Android。 谷歌推出ChromeOS和ChromeBook笔记本，里面跑的应用，全都是基于HTML5和Javascript的。 我个人认为，微信小程序几乎一定会成功，它一旦成功，腾讯必然会重燃在PC平台上做Q+的野心。Intel在桌面的霸权，最大的威胁不是AMD，也不是ARM，而很可能是HTML5+Javascript，熟悉“降维打击”的人，对此不会感到意外吧。 第二个挑战，来自虚拟机（Virtual Machine）和JIT（Just-in-time）编译器。先锋是Java的虚拟机JVM，后来微软也推出了DotNet虚拟机，支持C#等语言。虚拟机有一套虚拟的指令集，源代码先被编译到这个虚拟的指令集上，在程序运行时，JIT编译器再把这套虚拟指令集编译为CPU的原生指令集。面向虚拟机开发的程序，例如Java Applet，可以在不同的CPU和操作系统平台上运行。 如果有某个虚拟机，它的指令集可以无缝支持所有的编程语言，还能保证高效率，那么所有CPU的都将被OTT（over-the-top）了，就像短信被微信OTT一样。可惜还没有一个虚拟机可以实现此目标。现在大家熟知的虚拟机，都是和语言绑定的，例如JVM只支持Java、scala、kotlin等；DotNet虚拟机只支持C#、VB.net等；V8只支持Javascript、typescript等；HHVM只支持PHP。 同一个VM上跑的语言相互调用很容易，跨VM很难互操作。由于虚拟机实在太多了，它们反而成了新的CPU架构的拦路虎：80年代只需要搞定C语言编译器就能卖Unix工作站，如今ARM服务器要想挑战Intel，必须把所有这些基于VM的编程语言都支持得很好，JIT编译器的效率都要做得比较高才行。 第三个挑战，来自Transmeta公司对x86指令集的Emulation（Emulation这个词很难翻译，索性不翻了）。简单地说，Emulation就是把x86指令集看成一个虚拟机的指令集，然后用类似JIT编译器的技术，在非x86的CPU上跑x86的程序。未经许可用别人的ISA做CPU是违法的，但用Emulation的方式实现ISA则不违法（Intel和Transmeta只打过专利的官司没打过ISA的官司，Intel还输了）。 如今最广为人知的Emulator是Qemu，上文提到的x86、MIPS、PowerPC、Sparc、MC68000它都可以支持。一般而言，Emulation会导致性能下降一个甚至若干个数量级，根本不足为虑。 1995年，Transmeta公司成立，经过艰苦的秘密研发，于2000年推出了Crusoe处理器，用Emulation的方式，在一款VLIW（超长指令字）风格的CPU上执行x86的程序，这样就规避了没有x86指令集授权的问题。Transmeta的牛X在于，虽然是Emulation，但实现了接近Intel处理器的性能，同时功耗低很多。2000年年底Transmeta的IPO大获成功，其风光程度，直到后来谷歌IPO的时候才被超过。 Transmeta最后还是失败了，Intel在渠道上打压它是次要原因，性能不足是主要原因。虽然VLIW在90年代中后期被广为推崇，但事实证明，它的性能比起乱序执行的超标量架构，还是差一截。另外Transmeta的芯片是在台积电制造的，那个时候不比现在，台积电的工艺水平比起Intel还差很多。2000年的时候，PC还远没有性能过剩，性能还是比功耗重要。等到2010年，Intel的Atom处理器慢得一塌糊涂，依然靠着低功耗，点燃了上网本的大火。 Transmeta虽然失败了，Emulation技术仍然在发展。NVidia在2008年购买了Transmeta的低功耗技术的授权。2014年，NVidia推出了Tegra K1芯片，其中的Denver处理器，利用Emulation技术，在底层的7路超标量架构上，实现了ARM64指令集。值得注意的是，NVidia拥有ARM64的指令集的授权，它不是用Emulation技术来规避什么，而是用Emulation来提升性能，实现比硬件直接执行还要高的性能。根据评测结果，Denver超过了当时苹果最好的手机CPU。近期推出的Denver2处理器的，性能更是秒杀苹果的A9X和华为的麒麟950。 Emulation技术如果真的发展到了比直接执行还要快，Intel的麻烦才刚刚开始。微软联合高通，推出基于SnapDragon835处理器的笔记本，运行Windows 10操作系统，上面可以安装x86的软件。Intel虽然很不爽，但Emulation并不需要指令集授权，所以他只能警告说，在实现Emulator时，不许侵犯Intel的专利，而这一点，微软和高通肯定早已考虑到了。 挫折 x86生态系统曾经面对过一次最严重的、近乎灭顶之灾的挑战。这次挑战来自于谁？就来自于它的缔造者Intel。 Intel心不甘情不愿地把自己的x86指令级授权给了AMD等一众供应商，眼睁睁看着他们分享自己的利润，很不爽，于是想在x86之外另起炉灶，建设自己独享的生态系统。正巧在90年代初期，升级64位计算成为一个风潮，1991年有MIPS R4000，1992年有DEC Alpha，1995年有SUN SPARC64。1994年开始，Intel联合HP，准备趁32位升级64位的时机，抛弃原有的x86架构，新推出一个EPIC（Explicitly Parallel Instruction Computing）架构，名为IA64（Intel Architecture 64-bit）。 x86架构兼容老旧应用程序的能力是出了名的。8086把8位的8080升级为16位的时候，80386升级到32位的时候，都完全兼容旧有的程序。直到今天，Intel的处理器依然支持虚拟8086模式，在此模式下，可以运行30多年前的8086程序。升级到64bit的时候，Intel居然要放弃所有之前的8位、16位、32位应用了！可想而知当时在业界会引起怎样的轩然大波。Linux的缔造者Linus Torvalds公开对此表示反对。 IA64进展得并不顺利，EPIC本质上就是一种VLIW，如前所述，VLIW的性能比乱序超标量要差。而且EPIC的编译器非常难以开发。原定1997年就会推出产品，但直到1999年才发布IA64指令集，2001年才推出产品。另外Intel也不敢完全放弃之前的32位x86应用，它给出的解决方案是Emulation，但EPIC不像Transmeta为Emulation做了很多专门优化，跑32位x86应用的性能很差。 这个时候，千年老二AMD站了出来，为x86续命。2000年，它推出了AMD64指令集，延续了x86架构兼容老旧应用程序的优良传统，可以原生执行8位、16位、32位的老程序。2003年，AMD推出Opteron服务器CPU和Athlon64桌面CPU。 AMD64从技术上和生态上都压了IA64一头，Opteron在服务器市场上为AMD赢得了前所未有的成功。2004年，Intel推出了代号为Nocona的至强服务器CPU，它支持一种称为EM64T的技术，EM64T就是AMD64的马甲。江湖有传言说，Intel曾想提出另外一套不同于AMD64的x86升级64位的方案，但微软为了避免x86生态的分裂，极力阻止了。2012年，Intel推出了最后一代IA64的CPU，关闭了这个不赚钱的产品线。 回顾这段历史，有几点特别令人感慨。 首先，即使是看似无比强大不可战胜的Intel，不顾生态系统中其它伙伴的利益，一意孤行也是会撞南墙的。 其次，幸好由于历史的原因，x86生态中，AMD和Intel是交叉授权的关系，AMD有权加入3DNow这种多媒体扩展指令，也有权加入64位指令，如果是像如今ARM的架构级授权方式，被授权的企业不能自行加以扩展，那可能还真没有办法阻止Intel了。 最后，Intel的执行力还真是超强，掉头极快，EM64T的CPU只比AMD64的CPU晚出了一年（当然不能排除Intel早就有备份方案）。 虽然在IA64上栽了跟头，但Intel靠着自己的技术实力，持续不断地推出性能和功耗表现更好的产品，AMD在64位战役中所取得的优势，慢慢也被消磨掉了。 岁月如梭。进入移动互联网和云计算时代之后，服务器的需求量上升。这时RISC架构的服务器CPU几乎快被消灭干净了，只剩下IBM Power奄奄一息。于是Intel几乎独享了服务器市场扩大所带来的红利。但它却高兴不起来，因为移动市场形成了ARM一家独大的局面，移动终端CPU这个市场，Intel怎么也挤不进去。 正巧Intel在刚刚火过一把的上网本市场里设计了一种低功耗的x86核心，即Atom。Intel以Atom为武器，杀入了手机芯片市场。2012年，Intel的老伙计联想，推出了第一款Intel芯片的手机K800。紧接着还有Motorola的XT890。2013年，中兴、华硕也有产品问世。但三星、小米、华为、OPPO、VIVO等出货量大的厂商，都没有采用Intel的芯片。这些手机大厂，看看x86生态中做整机的联想如何艰难度日，估计心里也是一万个不乐意让Intel到移动领域来继续称王。 到2014年，Intel芯的手机还是没有打开局面，市场唱衰之声一片。但Intel并不想放弃。手机攻不下，那就攻平板！大厂攻不下，那就攻白牌！嫌我的芯片贵，我就给补贴！又过了两年，平板也没有攻下来。在移动市场赔了上百亿美金的Intel，黯然离场。 Intel失利的原因众说纷纭，我觉得根本原因还是竞争力不足： 首先，这个时候的台积电已经不是Transmeta家Crusoe芯片诞生时的吴下阿蒙，它生产的手机芯片的功耗和性能并不输给Intel； 其次，这次Intel并无生态系统的优势，要靠名为houdini的Emulator来执行ARM指令集的程序，性能打了折扣。试想，Intel芯的手机如果性能和待机时间都是iPhone的两倍，谁能抵挡得住这种诱惑？ 几乎在进攻移动市场的同时，Intel也在推出产品试水物联网市场，只不过没有大举宣传。2013年10月，Intel推出一款叫做伽利略的Arduino开发板，上面的CPU叫做Quark（夸克）。Quark是比Atom（原子）还小的基本粒子，这个名字暗含着轻巧、低功耗的意思。接着，Intel在2014年的CES大会和2016年的IDF大会上，先后推出了升级的爱迪生和焦耳开发板。 Intel的大名和Arduino联系在一起多少有些奇怪。Arduino是一套可以跑在低端MCU上的C语言函数库，是电子创客们的最爱。淘宝上Arduino开发板才几十块钱。焦耳开发板上的处理器是4核心、1.5GHz，跑Arduino太浪费了。和它参数近似的Raspberry PI 3 Model B+开发板，四核64位ARM Cortex A53跑1.2GHz，淘宝价不到200块。焦耳开发板要369美元。谁会当这个冤大头？ 物联网市场极度分散，有无数应用但规模都不大，Intel赚大钱习惯了，在这个微利又需要贴近客户做服务的市场里，百般不适。2017年，Intel悄悄停产了针对物联网市场的开发板。 Intel接下来所可能面对的挫折，是ARM侵入服务器和桌面领域。这个话题下文还会有简单分析。 ARM**生态系统**近几年ARM风光无限，抢新闻头条的能力不逊于Intel。 在很多圈外人看来，这家高科技公司好像是在移动互联网时代新冒出来的，但其实它的历史和几乎和80286一样古老。而且它自诞生以来，就以移动（portable）设备为自己首要的目标市场。它等待一飞冲天的风口，等待了二十年。 发端 前文提到，ARM是Acorn电脑公司创造的。 Acorn电脑公司创立于1978年，在80年代初，它用6502系列CPU制造的BBC Micro电脑在英国大获成功。6502的性能慢慢跟不上时代了，Acorn想基于80286开发新的电脑，但是Intel连样片都不给——要是Intel大方些，ARM或许根本就不会诞生。 Acorn一气之下开发了ARM（Acorn RISC Machine），这是世界上第一款定位中低端（而非服务器）RISC处理器。1985年，ARM1诞生（但从未被商用），后来Acorn在1986年和1990年分别推出了ARM2和ARM3，1987年推出了RISC OS和桌面电脑Archimedes。它在英国的教育市场获得了一定的成功，但1990年之后，很快被Wintel的生态击败了。 1990年前后，研发掌上电脑成为一股风潮。当时有家叫做Active Book的公司，拿ARM2处理器开发一个叫做Personal Communicator的产品。可惜产品上市前，Active Book被AT&amp;T收购了，AT&amp;T把ARM2换成了自家的Hobbit处理器。 幸好东方不亮西方亮，当时的苹果公司看好ARM，把自己研发的Newton平台中的处理器，由AT&amp;T的Hobbit，换成了ARM。这个“彼此互换”的故事听起来让人头大，大家只需要记住，ARM的第一颗商用处理器ARM2，就曾被尝试拿来做手持的电脑。 ARM的东家是Acorn，和苹果在电脑市场上有竞争。苹果公司花了6周时间说服Acorn把ARM独立出来运营。1990年11月27日，合资公司ARM正式成立，苹果、Acorn和VLSI分别出资150万、150万、25万英镑，Acorn把ARM处理器相关的知识产权和12名员工放在了新成立的公司里。此后，ARM的缩写被转而解释为Advanced RISC Machine。 为了节省成本，新公司在剑桥附近租了一间谷仓作为办公室，全力为苹果的Newton研发ARM6处理器（4和5这两个编号被跳过去了）。 Newton（牛顿）是苹果花大力气研发的触屏移动技术平台，Newton OS是不同于Mac OS的操作系统（如同后来的iOS）。如果你听过苹果、牛顿和万有引力的故事，应该能体会苹果公司对Newton平台有多么高的期望。 Newton平台的第一款产品MessagePad于1993年8月上市了，采用32位ARM610处理器，频率为20MHz，屏幕大小为336×240，重量410克，采用4节7号电池供电，售价699美元（相当于今天的1129美元）。可惜的是，它销量很差，上市头四个月的销量不过5万台。 1998年，中国的恒基伟业公司推出了一款叫做 “商务通”的产品，像极了Newton Messagepad。它采用Dragonball处理器，主频仅16MHz，屏幕大小10汉字x10汉字，重量105克，采用2节5号电池供电，售价人民币1988元。靠着“呼机手机商务通，一个也不能少”的广告，商务通在1999年大卖100万台。虽然2001年后商务通及类似产品很快就被越来越强大的手机挤出了市场，但让人好奇的是，背靠营销能力更加强大的苹果，Newton为何没能一炮而红？ 其中一个重要的原因是，Newton重点宣传的手写识别功能表现很糟糕。而商务通对手写汉字的识别率——根据我个人的体验——还真是不错，考虑到它仅仅16MHz的CPU主频，能做到这么好简直是奇迹。当时商务通部分型号的卖点就是“连笔王”，对潦草的汉字识别得相当好。 软件对于一款产品的重要性，真的是生死攸关啊！ 深耕 扯远了，让我们回到ARM的故事上来。 1990年ARM创立之初，给自己定下的使命是“设计有竞争力的、低功耗、高性能、低成本的处理器，并且使它们成为目标市场中广为接受的标准”，目标市场包括：手持设备（Portable），嵌入式（Embedded Control）和汽车电子（Automotive）。跨越近三十年，这个使命和市场定位始终未变，直到今天。 而且，根据我了解到的知识，ARM是处理器的源代码授权这一商业模式的开创者。如今，芯片设计从Verilog等源代码出发，经过一系列自动化或半自动化的优化步骤，最终形成工厂制造芯片所需要的版图文件；整个过程类似软件从源代码被编译为CPU的机器码。但在80年代，芯片的设计自动化非常原始。七八十年代的处理器授权，都是指令集的授权。Synopsys公司于1986年成立，1987年推出把Verilog编译为门级网表的DesignCompiler，之后基于源代码的芯片自动化设计流程才慢慢地被建立起来。于是源代码授权才成为技术上可行的模式。 ARM从未自己生产过商用的芯片。它只是将自己研发的处理器的源代码的知识产权（IP）授权给芯片厂商，由它们推出最终芯片。受益于这一商业模式，尽管在1993年，Apple的Newton失败了，但ARM并未因为设备卖不出去而亏钱，还幸运地拿到了TI的订单，于是成功盈利了。员工数量也由12人增长到了42人。次年ARM又拿到了三星的订单，员工增长到70多人，搬出了谷仓。 除了源代码授权的模式之外，ARM也做指令集授权，1995年，ARM把指令集授权给DEC，DEC很快设计出了性能更好的StrongARM处理器。1997年，StrongARM产品线被卖给Intel，更名为XScale。 1995年，Motorola在香港的研发团队基于MC68000指令集开发出了针对手持设备的DragonBall处理器，在这之后的十年，DragonBall处理器一直都是ARM强大的竞争对手。不但Moto自己的手机用它，Palm、三星、Sony的手机也用它。当然还有前文提到的商务通。ARM相对于Dragonball处理器有什么优势？我认为最大的优势是从客户需求出发的、持续的创新；其次是ARM的开放的商业模式。 RISC指令集一般都采用32位定长指令，代码密度比起x86之类的CISC来要差一些，但手机的存储空间有限，对代码密度的要求高。1994年，ARM为此专门研发了16位的指令集Thumb，以及支持这一指令集的ARM7TDMI。 开放授权的商业模式，使得整机厂在选择芯片时，可以找到支持同一指令集的多种芯片产品，不容易被绑架。Nokia作为和Motolora旗鼓相当的手机制造商，肯定不会选择竞争对手的Dragonball，而ARM的技术实力和商业模式，正好符合Nokia的需求。 1997年，Nokia推出了一代经典6110，它采用TI的芯片，处理器核心是ARM7TDMI。6110是Nokia第一款带红外接口的手机，第一次内置了经典的贪吃蛇游戏，它的界面成为了之后Nokia手机的标准。从此，Nokia和ARM成为了好基友，Nokia的Symbian操作系统，一直都建立在ARM架构的基础上。 1998年，趁着6110大红大紫的东风，ARM在Nasdaq上市了。同一年，SGI公司看到处理器IP授权生意有利可图，把MIPS部门拆分出来，次年MIPS推出了它第一款可授权的处理器设计M4K。此后的十年里，MIPS一直都是ARM有力的竞争对手。 商务通在中国流行的那几年，国际市场上流行性能更高的掌上电脑和智能手机，操作系统包括Palm OS、微软的WinCE、Nokia的Symbian、RIM的Blackberry OS，Motorola的Wisdom OS。在这个领域里，ARM阵营中负责高性能的XScale大放异彩，暴击Dragonball。当Dragonball的频率还停留在33MHz/66MHz时，Xscale已经飙到了200~400MHz。MC68000指令集在手持设备领域败走。Palm OS的1.0~4.0都是基于MC68000指令集的，5.0就换成了ARM。后来Motorola的半导体部门Freescale干脆推出了基于ARM核的iMX系列产品，替代Dragonball产品线。 苹果作为掌上电脑的先行者，却在这次浪潮里无所作为，在Wintel的挤压下，它的桌面业务都已经濒临绝境，无暇顾及其它市场了。1997年，不温不火的Newton从苹果公司独立了出来。当乔布斯回归苹果之后，又火速把Newton收编了回来，并且干净利落地停掉了Newton产品线——乔帮主只想要Newton手里的ARM股份。1998年到2003年，苹果通过出售ARM的股票获利11亿美元。这笔钱，是乔布斯复兴战略的重要燃料，可以说是苹果的救命钱。 绽放 经过多年的深耕，ARM在新世纪开始时，已经是手机领域里的王者，依然在为客户的需求做着持续的创新，Java加速技术就是一个典型的例子。 从2000年开始，功能手机的性能提升到了足够高的水平，人们希望在手机上玩比较复杂的游戏，而不仅仅是贪吃蛇。但是手机的处理器和操作系统实在是太分散了，为了方便游戏跑在不同手机上，J2ME平台应运而生。从原理上讲，J2ME和Applet并无不同，都是基于JVM的。Java在并不分散的桌面领域没有获得成功，但在分散的手机领域获得了成功。 J2ME的游戏越做越复杂，但手机的处理能力毕竟有限，桌面和服务器上的JIT编译器在手机上跑得太吃力了。于是ARM在2001年推出了ARM926EJ-S处理器，它支持Jazelle DBX技术，可以直接解码和执行Java的字节码，省掉了JIT编译器的负担。这一功能大受欢迎，帮助ARM9系列成为了迄今最受欢迎的ARM处理器，总共有250多个授权厂家，其中100多个授权的是ARM926EJ-S。 在MTK助推山寨功能机火遍神州的那几年，主控芯片所使用的核全部都是ARM9。在iOS和安卓的应用商店诞生之前，功能手机全靠J2ME开发的应用来实现各种炫酷的功能。从某种意义上讲，在低端市场上，Jazelle是助力山寨机火爆的最大幕后功臣。 然而高性能ARM芯片的扛把子XScale，却被Intel于2006年6月卖给了Marvell。这是Intel实施x86-everywhere战略的一个步骤。Intel希望x86的生态也能进入到低功耗的移动领域，而不是用自己先进的工艺制程和设计能力帮ARM建设高端应用的生态。22个月之后，2008年4月，低功耗的Atom芯片诞生了。 高性能ARM芯片的扛把子换成了苹果。2004年，在卖光ARM股票的一年之后，乔布斯决定研发iPhone。2007年1月，在Intel放弃ARM之后仅半年，iPhone诞生了。苹果可不会采用低端市场上死守ARM9那种玩法，iPhone一代就采用了400MHz的ARM11；2009年的iPhone 3GS，升级为600MHz的Cortex A8；2010年的iPhone4，苹果自研的A4芯片升级为1GHz的Cortex A8。接下来苹果自研芯片性能一路狂飙的历程，大家都很熟悉了。 从ARM6到ARM11，这些IP核都是按照兼顾移动设备、汽车电子和嵌入式这三个市场的思路来设计的。从2003年起，ARM把产品线有针对性地划分为A、R、M三个系列，分别对应上述三个市场，而且IP核的名字都统一加上了Cortex的前缀。Cortex A8就是A系列的第一个作品。iPhone 3GS和iPhone4令Cortex A8大火，但让ARM一飞冲天的推手，却是iPhone的竞争对手——安卓（Android）。 有很多文章介绍安卓如何诞生，如何在移动设备领域干掉了除iOS之外的全部对手，毋须赘述。这里只想强调一个被普遍忽略的事实：安卓从诞生之初，就要求应用程序采用Java编写，并且跑在Dalvik虚拟机上；但iPhone上的应用，都是原生的ARM程序。要知道Android手机的处理器性能相对iPhone并无优势。 山寨之王MTK于2009年2月推出的首款智能手机芯片MT6516，采用406MHz的ARM9；2008年~2010年间由HTC推出的那几款卖得很好的Android手机，也无非是ARM11和Cortex A8的核，几百兆的频率，这种级别的处理器跑虚拟机还是蛮吃力的。另外虚拟机占用内存大的缺点，也不利于用户体验和降低成本。 谷歌宁可冒着让安卓出师不利的风险，也要推广Dalvik虚拟机。这是为什么？谷歌内部的决策过程我们无从得知。一个合理的猜测是，谷歌不愿看到手机领域里ARM一家独大，它希望给MIPS、x86等其它CPU一个机会。J2ME的成功，让谷歌看到完全建立在虚拟机上的手机应用生态，是完全可能的。 Dalvik虚拟机可以跑Java，但并不采用JVM那种基于堆栈的字节码，而是改用一种基于寄存器的方案。这么做当然是为了规避SUN公司（后被Oracle收购）的专利，同时也让无法直接运行JVM字节码的MIPS、x86能够实现轻量级的JIT编译器，无须Jazelle这样的技术。从另外一个角度讲，MIPS在电视、机顶盒、游戏机市场上占优，x86在桌面市场近乎垄断，支持它们，也意味着安卓有可能进军电视和桌面。 安卓对所有CPU而言，都是巨大的机会，谁抓住了这个机会，就可以一举改变竞争格局，实现霸业。 只可惜MIPS公司太不给力，一直也没有搞定靠谱的MIPS版Android。等到2011年1月，Synopsys公司给自家的ARC处理器移植好Dalvik虚拟机和浏览器用的V8虚拟机、Android环境已完备的时候，MIPS都还没动静。顺便说一句，Intel曾经的南桥芯片里都有ARC处理器，它是Active Management Technology（AMT）的重要基石。 这个时候，北京的君正公司坐不住了。君正靠做低成本的MP4播放器起家，2011年5月在创业板上市。君正拥有MIPS的架构级授权，对自己研发的XBurst处理器非常自信，准备靠它进攻手机和平板市场。2011年7月，基于君正JZ4760的MIPS智能手机通过Android兼容性测试。2011年12月，基于君正JZ4770平台的平板电脑，被谷歌选为Android4.0的首发产品，一时风光无限。 ARM的强大软件生态此时起到了护城河的作用。基于君正的平板，软件兼容性出了问题。原因在于谷歌没有强求所有的应用都跑在Dalvik虚拟机上，对于部分对性能有苛刻要求的app，例如游戏，谷歌允许用CPU的原生指令集来开发，为此还提供了NDK（Native Development Kit）。对于那些包含了ARM原生指令的游戏，君正的平板要么不支持，要么用emulator支持，总之用户体验都不好。 ARM生态圈里，在2011年，正好有两家芯片厂商异军突起：全志和瑞芯微，它们分别推出了采用Cortex A8处理器的A10芯片和RK2918芯片，成本极低，主打平板和安卓电视盒子。君正的平板梦被它们粉碎了，之后只好转战安卓手表，消沉了很多年。对于MIPS而言，还有一个坏消息是，在它们的强力助推下，电视盒子市场也成了ARM的天下。经营不善的MIPS于2012年卖给了Imagination，Imagination不但没能依靠MIPS在CPU市场中有所作为，反而在GPU市场里也败给了ARM，在2017年被迫整体卖身，MIPS业务卖回给了硅谷公司。 2012~2016年，Intel在安卓市场上挑战ARM，也失败了。于是安卓给CPU带来的红利，全部被ARM吃掉了。随着手机越来越重要，ARM也越来越重要，它所推出的最新的Cortex A系列处理器，被手机芯片争相采用。ARM生态也越来越强大，它的触角，慢慢伸出了手机领域。 渗透 2011年1月，微软在CES宣布要为ARM架构开发Windows 8 RT操作系统。在2012年年底，几乎和Intel芯手机上市的同时，包括微软自家的Surface RT在内的一大批二合一平板设备上市了。Windows 8 RT不支持所有之前为x86平台开发的应用程序，这成为它最大的软肋，相关的产品慢慢销声匿迹了。ARM渗透桌面市场的第一次尝试失败了。 最近微软和高通所推出的ARM芯的Windows 10，吸取了教训，用Eumlation的机制来支持旧有的x86桌面程序。这次尝试能否成功，我们拭目以待。 2009年，ARM推出了Cortex A9处理器，并且用40nm的工艺制造了双核的样片，跑到了2GHz。这是ARM第一次推出乱序超标量的处理器核，而乱序超标量是Intel实现高性能的关键技术，这是非常振奋人心的消息。2010年，Marvell推出了1.6GHz的4核A9的服务器芯片Armada XP。2013年，这款芯片被部署在百度的存储服务器上，这是ARM服务器第一次大规模商用。但Marvell并未继续推出新的服务器芯片。2011年，一家创业公司Calxeda采用Cortex A9，推出了共有480个CPU核的ARM服务器。但它的成就还不如Armada XP，2013年公司就倒闭了。 2012年，AMD收购了一家做高密度服务器的厂商SeaMicro，准备把它所采用的CPU核由Intel的Atom换成ARM架构的CPU。但直到2014年AMD才推出8核Cortex A57的服务器芯片Opteron A1100，之后从来也没有认真卖过它。2015年AMD就放弃了SeaMicro这个子品牌，不再做高密度服务器了。 ARM进攻服务器市场的第一次尝试失败了。Marvell和Calxeda都采用的是32位的ARM核，先天不足；AMD则三心二意，毕竟自己还有x86 Server的生意。另外服务器市场对于单核单线程的运算能力也有很高的要求，仅仅有低功耗和高通量（high throughput）是不够的。 在ARMv8这一64位指令集发布之后，Cavium和AppliedMicro这两家老牌网络芯片厂商不约而同地将自己原先芯片中的架构换成了ARMv8。因为产品的需要，Cavium和AppliedMicro都有自行设计处理器微架构的能力，前者做MIPS处理器，后者做PowerPC处理器。它们两家做ARMv8处理器时，也都采用了只授权指令集，微架构自研的模式。Cavium共推出过两代基于ARM的产品（2014、2016年），AppliedMicro推出过三代（2013、2015、2017年）。随着产品性能逐渐接近Intel的Xeon E5，它们渐渐不再满足于原先的网络领域，开始觊觎服务器市场。 最让人期待的还是高通的Centriq芯片，2015年年底量产24核版本，2016年年底量产升级48核版本，还得到了微软的强力支持。考虑到高通还和贵州成立了合资公司华芯通，Centriq很可能成为在国内大规模商用的第一款ARM服务器芯片。 另外具有国防背景的天津飞腾公司，也有ARM服务器芯片的产品，只是不知道这些产品何时能在通用市场上铺货。 其他确定在研发ARM Server芯片的大厂还包括Broadcom和华为，进度上要略慢一些。 ARM阵营对服务器发起的第二波冲击，阵容要强大得多豪华得多。因此ARM才敢于宣称，在2021年拿下25%的服务器市场份额。 要做好Server CPU，ARM架构还有些功课要一点一点补。多Socket服务器所需要的一致性协议，业界刚刚取得共识准备采用CCIX，但还没有具体的产品出来。做云端虚拟机所必备的虚拟化支持，ARM还有些性能问题。x86处理器提升Throughput的利器超线程技术，ARM阵营尚不能支持。Intel芯片近年来陆续增加的安全特性，也够ARM追赶一阵子的。但目前看来，ARM已经没有致命的短板，蚕食掉Intel的服务器市场份额是板上钉钉的事情，唯一的悬念是究竟多少份额？ 未来ISA将不那么重要从长远看，半导体厂商对建立于ISA之上的生态系统的掌控力会变弱，而ISA本身，会变得越来越不重要。这是软件技术发展的趋势决定的，如前所述，这些技术在90年代末就已经初有小成了。 第一是**Web**技术。网页开发领域，有一个大家视若无睹的奇迹：最后居然只有Javascript一种开发语言屹立至今。要知道在服务器端和移动App领域，开发语言多如过江之卿。其中原因我也分析不出。反正js的挑战者（微软的VBScript和谷歌的Dart）都失败了。网页开发领域面临的主要问题是浏览器差异大，API不太兼容。这个问题慢慢在缓解中，一来浏览器战争大局已定，Android和PC上的Chrome，以及iPhone和Mac上的safari是胜者；二来很多网页应用是跑在App里面的，例如微信和支付宝里，这种场景下Javascript的API已经被特定厂商规范过了。 由于开发语言和API的高度统一，H5（HTML5+Javascript）已经成了兼容所有硬件的最通用的软件开发平台。曾经有人鼓吹H5会赶走移动端和PC端的原生程序，后来被打脸了。但是移动端和PC端的原生App中，越来越多的界面是用H5生成的了，微信、支付宝、京东、淘宝、爱奇艺、有道词典……统统都是这样。 Javascript吞噬一切的进程还在持续。2007年，Stack Overflow的联合创始人Jeff Atwood曾经提出过一条Atwood定律：任何能够用JavaScript实现的应用系统，最终都必将用JavaScript实现。十年过去了，此定律基本奏效。把Javascript的一个子集当作汇编语言的asm.js及其后续的WebAssembly，更加使得网页应用有媲美原生应用的潜力，在浏览器里跑Unity3D的游戏都不是问题。 独立的应用程序仍然会是移动和桌面端的主流，因为没有独立程序，不方便做弹窗广告，不方便启动后台进程收集用户信息，不方便引诱用户安装其它独立程序。但Web的能力的确在快速提升，Web Component技术实现了类似GUI库的Widget复用，如今在浏览器里实现Office和IDE的功能都毫无问题（office365.com、docs.google.com、editor.construct.net、腾讯文档）；而WebGL已经能支持Unity3D这种大型游戏框架。 照此趋势发展下去，独立应用程序仅仅会作为一个包装而存在，开发者写一套H5，加上不同的包装，就成了PC、Mac、Android、iOS上的独立应用程序，不加包装，就是网站。微软去年开源的ReactXP，就是为了实现这一目标。 这意味着什么？不但底层的CPU被OTT了，操作系统也被OTT了。因为移植一个应用程序到各个平台上，几乎没有什么难度。谁将是生态系统的掌控者？若干个超级App，像微信、QQ、支付宝这样的。它们不但包装自家的应用，其它开发者也可以把自己的应用放在这个包装里面，借重超级App的广泛覆盖度，抵达最终用户。前文提到了，如果微信小程序获得成功，腾讯必然会重拾Q+的野心，把QQ变成桌面上各种H5应用的App Store。 如果真的会这样，微软岂不是会比Intel还着急？拜托，微软已经不是二十年前主要靠卖Windows和Office的光盘赚钱的那家公司了，未来它会专注于云计算。但Intel还和二十年前一样在卖芯片。 第二是编译技术尤其是虚拟机的发展。如今的编程语言太多了，80年代那种搞定C语言编译器就OK的好日子早已过去。任何一个新CPU架构要想在移动、桌面、服务器市场站稳脚跟，都得搞定无数的编译器（包括虚拟机用的JIT编译器），这是个坏消息。但好消息是，搞定这些编译器基本就差不多了，不用劝说开发者重写汇编代码。 老一代程序员对x86处理器架构和汇编都非常熟悉。求伯君当年开发WPS时，手写几十万行汇编；雷军读本科时，是系里20多年来拿过《汇编语言程序设计》满分成绩的两个学生之一；梁肇新开发超级解霸时，把MMX汇编玩得出神入化。感兴趣的读者可以看看梁的《编程高手箴言》，那里面，描绘了一个对现在的程序员而言，完全陌生的世界。在那个世界里，你开发的PC应用程序想要移植到Mac平台上，几乎要完全重写。 如今高层次的编程语言接管了一切，汇编语言从很多学校的本科课程里消失了，入门教材也从C改成了Java，甚至是Javascript或Python。程序员完全不熟悉底层的CPU。即使是真的需要拼性能的场合，编译器也在很大程度上代替了手写汇编。ARM的工程师告诉我说，ARM在开发开源的Compute Library过程中，主要依靠在C源码中加入标注来指导编译器生成SIMD指令，而不是像梁肇新那样手写。 在这种情况下，软件平台厂商就变得非常强势，因为他们知道，应用开发商只需付出重新编译一遍的代价。比如苹果，就要求所有的App都改为64位的。这样，未来苹果在手机CPU里放弃对32位应用的支持时，甚至都不会有人感觉得到。这对于x86生态系统而言，简直是天方夜谭，显然微软对此非常眼馋，并且尝试在Windows 10 S中复制这种掌控力。 至于谷歌，Android把所有应用都跑在虚拟机上的尝试虽然失败了，但如果未来它再针对AR/VR、AI或机器人发布一个什么软件平台的话，就很有可能完全禁止原生程序。 而Oracle，正在努力开发可以支持所有编程语言、能把所有CPU给OTT掉的全新VM：GraalVM。我们拭目以待。 第三是**Emulation**技术的发展。虽然眼下ARM阵营中靠Emulation进攻Intel的先锋是高通，但最可怕的选手其实是NVidia。NVidia拥有最厉害的Emulation技术，而且江湖传言Denver处理器的初衷就是针对x86的。当初NVidia的Tegra处理器曾被拿来做Windows 8 RT的二合一平板。如今Denver处理器跑Windows 10绝不会让人意外，那么它会怎么跑呢？肯定是直接在底层硬件上做x86的Emulation，而不是在Emulate出来的ARM指令集上再做一层Eumulation。 Denver处理器前些年没有跳出来抢Intel的饭碗，很大程度上是因为NVidia还在做Intel平台的主板芯片组，另外NVidia还没有那么强大。如今NVidia也不做芯片组生意了，还借AI的东风，股价扶摇直上。说不定哪天，NVidia就会放出Denver处理器的x86 Emulator，做到单线程性能不输Xeon，强攻服务器市场。想想看，在单芯片上集成GPU和x86版的Denver，云计算厂商能不动心？ 如果未来Emulation技术进一步发展并且被越来越多的厂商掌握，很可能会出现这种情况：CPU本身是某种外界不了解的指令集，官方发布时，只能Emulate某种开放的指令集，例如RISCV；但是用户可以给它安装不同的Emulator，让它变成x86-64处理器，或者ARM64处理器。在软件定义一切的时代，这并不是多么疯狂的想象。 总之，CPU依然不可或缺，但CPU用谁家的，是什么指令集，会越来越不重要。软件的发展，会在用户和底层的CPU之间加入足够大的缓冲带，CPU的差异，越来越难以被用户察觉到。 展望：让CPU不再难此文在最后修改之时，看到了梁宁的文章《一段关于国产芯片和操作系统的往事》，里面写到： 就像10多年前一样，只要搞定知识产权问题，选择技术路线，找会干的人，投入干，CPU/芯片就能够做出来。搞不定的依然是操作系统。差距大的依然是生态。当年，绕得过Intel，跨不过微软。如今，绕得过Arm，做不出安卓。 我也曾在北大参与过国产CPU的研发，生态之难体会颇深，真的，只是烧钱做芯片，无论烧多少都无法挑战Intel和ARM，何况过去二十年真的没烧多少。 但我并没有梁宁那么悲观，毕竟技术的潮流无法抗拒，借用马化腾的一句名言“可能你什么错都没有，最后就是错在自己太老了”。 Intel和ARM如此强大而且极少犯错，我们如此弱小就算它们犯错也无法利用——但我们可以欺负它们的“老”。 在此借新智元的宝地，向小马哥呼吁一声： 请借助腾讯的强大生态，把CPU和OS这两个老大难问题给OTT掉吧！ 做法非常简单，把Q+桌面再重新搞起来，做一款完全使用Javascript&amp;Webassembly编程的操作系统，里面用腾讯文档来替代Office，各种微信小程序都支持起来，适当支持游戏（但要加入家长监控系统）。补贴芯片厂，让它们使用ARM或RISC-V外加国产Imagination gpu做SoC，生产类似Surface这样的二合一平板。底层CPU使用的ISA完全不可见，上层编程完全用H5。这样，就帮祖国把CPU和OS这两个陈年大洞都补上了。 芯片要下苦功，别凡事都指望模式创新。这不假。但偏偏CPU真的面临一个十倍速变革的机会，真的有靠模式创新而胜出的机会，为什么不试试呢？如果腾讯不去尝试一下，谁还有资格呢？促进祖国的微电子发展功德无量，相信这次不会有人说腾讯垄断之类的闲话。 【加入社群】 新智元 AI 技术 + 产业社群招募中，欢迎对 AI 技术 + 产业落地感兴趣的同学，加小助手微信号&#58; aiera2015_1 入群；通过审核后我们将邀请进群，加入社群后务必修改群备注（姓名 - 公司 - 职位；专业群审核较严，敬请谅解）。 转载来源：为中国芯万字投书马化腾：从Intel和ARM争霸战，看看做芯片有多难]]></content>
      <categories>
        <category>科技</category>
      </categories>
      <tags>
        <tag>软件</tag>
        <tag>编程语言</tag>
        <tag>英特尔</tag>
        <tag>ARM</tag>
        <tag>IBM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[朝鲜买房指南]]></title>
    <url>%2F2018%2F8c704549%2F</url>
    <content type="text"><![CDATA[朝鲜买房指南 转载来源：朝鲜买房指南]]></content>
  </entry>
  <entry>
    <title><![CDATA[为什么中国神药总是一抓一个准？]]></title>
    <url>%2F2018%2F622e5183%2F</url>
    <content type="text"><![CDATA[为什么中国神药总是一抓一个准？ 转载来源：为什么中国神药总是一抓一个准？]]></content>
      <tags>
        <tag>冀连梅药师</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用开源项目的正确姿势，都是血和泪的总结！]]></title>
    <url>%2F2018%2F1e5b86f2%2F</url>
    <content type="text"><![CDATA[使用开源项目的正确姿势，都是血和泪的总结！ 转载来源：使用开源项目的正确姿势，都是血和泪的总结！]]></content>
      <tags>
        <tag>阿里技术</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一个老教师眼中的在线直播和录播课程-教育频道-手机搜狐]]></title>
    <url>%2F2018%2F05d2d21a%2F</url>
    <content type="text"><![CDATA[一个老教师眼中的在线直播和录播课程-教育频道-手机搜狐 转载来源：一个老教师眼中的在线直播和录播课程-教育频道-手机搜狐]]></content>
  </entry>
  <entry>
    <title><![CDATA[一个老教师眼中的在线直播和录播课程_搜狐教育_搜狐网]]></title>
    <url>%2F2018%2F3b410d1e%2F</url>
    <content type="text"><![CDATA[一个老教师眼中的在线直播和录播课程搜狐教育搜狐网 转载来源：一个老教师眼中的在线直播和录播课程搜狐教育搜狐网]]></content>
  </entry>
  <entry>
    <title><![CDATA[用数据做酷的事！手把手教你搭建问答系统]]></title>
    <url>%2F2018%2F86f4b533%2F</url>
    <content type="text"><![CDATA[本文介绍了如何基于 SQuAD 数据集搭建问答系统及其重要组件。 我最近很愉快地完成了斯坦福深度学习自然语言处理课程（CS224N），学到了很多新的东西。在结课项目中我基于斯坦福问答数据集（SQuAD）实现了一个问答系统。在这篇博客中，我将为大家介绍搭建问答系统所需要的主要模块。 完整代码 GitHub 地址：https&#58;//github.com/priya-dwivedi/cs224n-Squad-Project SQuAD 数据集 斯坦福问答数据集（SQuAD）是一个全新的阅读理解数据集，由众包人员基于一系列维基百科文章的提问和对应的答案构成，其中每个问题的答案是相关文章中的文本片段或区间。SQuAD 包含关于 500 多篇文章的超过 100000 个问答对，规模远远超过其他阅读理解数据集。 最近一段时间，各种类型的模型在 SQuAD 数据集上的效果获得了快速的发展，其中最新的一些模型在问答任务中甚至取得了和人类相当的准确率。 SQuAD 数据集中的语境、问题和答案的示例 语境：阿波罗计划于 1962 至 1972 年间进行，期间得到了同期的双子座计划（1962 年 - 1966 年）的支持。双子座计划为阿波罗计划成功必需的一些太空旅行技术做了铺垫。阿波罗计划使用土星系列火箭作为运载工具来发射飞船。这些火箭还被用于阿波罗应用计划，包括 1973 年到 1974 年间支持了三个载人飞行任务的空间站 Skylab，以及 1975 年和前苏联合作的联合地球轨道任务阿波罗联盟测试计划。 问题：哪一个空间站于 1973 到 1974 年间承载了三项载人飞行任务？ 答案：Skylab 空间站 SQuAD 的主要特点： i) SQuAD 是一个封闭的数据集，这意味着问题的答案通常位于文章的某一个区间中。 ii) 因此，寻找答案的过程可以简化为在文中找到与答案相对应部分的起始索引和结束索引。 iii) 75% 的答案长度小于四个单词。 机器理解模型关键组件 i) 嵌入层 该模型的训练集包括语境以及相关的问题。二者都可以分解成单独的单词，这些单词会被转换成使用预训练向量（如 GloVe）的词嵌入。想了解更多关于词嵌入的信息，参考《教程 | 用数据玩点花样！如何构建 skim-gram 模型来训练和可视化词向量》。同 one hot 向量相比，用词嵌入方式对单词进行表示可以更好地捕捉语境信息。考虑到没有足够的数据，我使用了 100 维的 GloVe 词嵌入并且在训练过程中没有对它们进行修改。 ii) 编码器层 RNN 编码器 我们将基于 RNN 的编码器加入到了模型的下一层当中。我们希望语境中的每一个单词能和它前后的单词产生联系。双向 GRU/LSTM 可以帮助我们达到这一目标。RNN 的输出是一系列向前、向后的隐藏向量，然后我们会将它们级联起来。类似地，我们可以使用相同的 RNN 编码器创建问题隐藏向量。 iii）注意力层 现在我们有了一个语境隐藏向量和问题隐藏向量。我们需要将这两个向量结合起来，以找到问题的答案。这时就需要用到注意力层。注意力层是问答系统的关键组成部分，因为它能帮助确定对于给定的问题我们应该「注意」文中的哪些单词。让我们从最简单的注意力模型开始： 点积注意力 CS224N 中基本注意力的可视化分析 点积注意力等于每个语境向量 c_i 乘每个问题向量 q_j 的结果向量 e^i（上图中的注意力分数）。之后，我们对 e^i 调用 softmax 函数来得到 α^i（上图中的注意力分布）。softmax 保证了所有 e^i 的和是 1。最终，我们计算出 a_i：注意力分布 α^i 与对应问题向量（上图中的注意力输出）的积。点积注意力也可以用下面的式子来描述： 上面提到的注意力已作为基线注意力机制在 GitHub 代码中实现。 更复杂的注意力——BiDAF 注意力 你可以用上述基本注意力层来运行 SQuAD 模型，但恐怕结果不尽人意。更复杂的注意力才能产出更好的性能。 我们来了解一下 BiDAF 论文（https&#58;//arxiv.org/abs/1611.01603）。该论文的主要观点是注意力应该是双向的——从语境到问题和从问题到语境。 我们首先计算相似度矩阵 S ∈ R^N×M，它包含每对语境和问题隐藏状态 (c_i , q_j) 的相似度分数。这里 c_i ◦ q_j 代表数组元素对应相乘，w_sim ∈ R 6h 是权重向量。S_ij 用下面的式子来表述： 之后，我们将展示 C2Q 注意力（与上面提到的点积注意力类似）。我们对 S 逐行调用 softmax 函数来获得注意力分布 α^i，用它得到问题隐藏状态 q_j 的加权和，最后得出 C2Q 注意力的输出 a_i。 现在，我们来执行 Q2C 注意力。对于每一个语境位置 i ∈ &amp;#1231, . . . , N&amp;#125，我们取相似度矩阵对应行的最大值： 之后我们对结果向量 m ∈ R^N 调用 softmax 函数，而这将给出关于语境位置的注意力分布 β ∈ R^N。之后，我们使用 β 得到语境隐藏状态的加权和 c_i，这也是 Q2C 注意力的输出结果 c’。以下是相关公式： 最终对于每一个语境位置 c_i，我们结合 C2Q 注意力和 Q2C 注意力的输出，下面是相关公式： 如果你觉得这一段令人费解，不用担心，注意力确实是一个复杂的话题。你可以试着一边喝茶，一边阅读这篇 BiDAF 论文。 iv) 输出层 我们就快成功了。模型的最后一层是一个 softmax 输出层，它帮助我们找出答案区间的开始和结束索引。我们通过结合语境隐藏状态和之前层的注意力向量来得到混合的结果。这些混合的结果最终会成为全连接层的输入，该层使用 softmax 来得到 p_start 向量（具备开始索引的概率）以及 p_end 结束（具备结束索引的概率）。我们知道大部分答案从开始索引到结束索引最多 15 个单词，由此我们可以寻找使 p_start 与 p_end 乘积最大的开始和结束索引。 损失函数是开始和结束位置的交叉熵损失之和。它使用 Adam Optimizer 来获得最小值。 我构建的最终模型比上面描述的要复杂一点，在利用测试集测试时获得了 75 分的 F1 分数。还行！ 下一步 关于未来探索的一些想法： 由于 CNN 运行起来比 RNN 快得多，并且更容易在 GPU 上并行计算，因此我最近一直都在用基于 CNN 的编码器而非上述 RNN 编码器进行实验。- 其他的注意力机制，如 Dynamic Co-attention（https&#58;//arxiv.org/abs/1611.01604）其他的注意力机制，如 Dynamic Co-attention（https&#58;//arxiv.org/abs/1611.01604） 转载来源：用数据做酷的事！手把手教你搭建问答系统]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>GitHub</tag>
        <tag>机器学习</tag>
        <tag>火箭</tag>
        <tag>可视化</tag>
        <tag>斯坦福大学</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[微头条]]></title>
    <url>%2F2018%2Fbb00f6dd%2F</url>
    <content type="text"><![CDATA[微头条 转载来源：微头条]]></content>
  </entry>
  <entry>
    <title><![CDATA[为什么东北人总让你快乐]]></title>
    <url>%2F2018%2F32a87ea0%2F</url>
    <content type="text"><![CDATA[为什么东北人总让你快乐 转载来源：为什么东北人总让你快乐]]></content>
      <tags>
        <tag>新世相</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[WebSocket协议深入探究]]></title>
    <url>%2F2018%2Fcbaa330e%2F</url>
    <content type="text"><![CDATA[一、内容概览WebSocket的出现，使得浏览器具备了实时双向通信的能力。本文由浅入深，介绍了WebSocket如何建立连接、交换数据的细节，以及数据帧的格式。此外，还简要介绍了针对WebSocket的安全攻击，以及协议是如何抵御类似攻击的。 二、什么是WebSocketHTML5开始提供的一种浏览器与服务器进行全双工通讯的网络技术，属于应用层协议。它基于TCP传输协议，并复用HTTP的握手通道。 对大部分web开发者来说，上面这段描述有点枯燥，其实只要记住几点： WebSocket可以在浏览器里使用1. 支持双向通信1. 使用很简单支持双向通信 1、有哪些优点 说到优点，这里的对比参照物是HTTP协议，概括地说就是：支持双向通信，更灵活，更高效，可扩展性更好。 支持双向通信，实时性更强。1. 更好的二进制支持。1. 较少的控制开销。连接创建后，ws客户端、服务端进行数据交换时，协议控制的数据包头部较小。在不包含头部的情况下，服务端到客户端的包头只有2~10字节（取决于数据包长度），客户端到服务端的的话，需要加上额外的4字节的掩码。而HTTP协议每次通信都需要携带完整的头部。1. 支持扩展。ws协议定义了扩展，用户可以扩展协议，或者实现自定义的子协议。（比如支持自定义压缩算法等）更好的二进制支持。 支持扩展。ws协议定义了扩展，用户可以扩展协议，或者实现自定义的子协议。（比如支持自定义压缩算法等） 对于后面两点，没有研究过WebSocket协议规范的同学可能理解起来不够直观，但不影响对WebSocket的学习和使用。 2、需要学习哪些东西 对网络应用层协议的学习来说，最重要的往往就是连接建立过程、数据交换教程。当然，数据的格式是逃不掉的，因为它直接决定了协议本身的能力。好的数据格式能让协议更高效、扩展性更好。 下文主要围绕下面几点展开： 如何建立连接1. 如何交换数据1. 数据帧格式1. 如何维持连接如何交换数据 如何维持连接 三、入门例子在正式介绍协议细节前，先来看一个简单的例子，有个直观感受。例子包括了WebSocket服务端、WebSocket客户端（网页端）。完整代码可以在 这里 找到。 这里服务端用了ws这个库。相比大家熟悉的socket.io，ws实现更轻量，更适合学习的目的。 1、服务端 代码如下，监听8080端口。当有新的连接请求到达时，打印日志，同时向客户端发送消息。当收到到来自客户端的消息时，同样打印日志。 2、客户端 代码如下，向8080端口发起WebSocket连接。连接建立后，打印日志，同时向服务端发送消息。接收到来自服务端的消息后，同样打印日志。 3、运行结果 可分别查看服务端、客户端的日志，这里不展开。 服务端输出： 客户端输出： 四、如何建立连接前面提到，WebSocket复用了HTTP的握手通道。具体指的是，客户端通过HTTP请求与WebSocket服务端协商升级协议。协议升级完成后，后续的数据交换则遵照WebSocket的协议。 1、客户端：申请协议升级 首先，客户端发起协议升级请求。可以看到，采用的是标准的HTTP报文格式，且只支持GET方法。 重点请求首部意义如下： Connection&amp;#58; Upgrade：表示要升级协议- Upgrade&amp;#58; websocket：表示要升级到websocket协议。- Sec-WebSocket-Version&amp;#58; 13：表示websocket的版本。如果服务端不支持该版本，需要返回一个Sec-WebSocket-Versionheader，里面包含服务端支持的版本号。- Sec-WebSocket-Key：与后面服务端响应首部的Sec-WebSocket-Accept是配套的，提供基本的防护，比如恶意的连接，或者无意的连接。Upgrade&amp;#58; websocket：表示要升级到websocket协议。 Sec-WebSocket-Key：与后面服务端响应首部的Sec-WebSocket-Accept是配套的，提供基本的防护，比如恶意的连接，或者无意的连接。 注意，上面请求省略了部分非重点请求首部。由于是标准的HTTP请求，类似Host、Origin、Cookie等请求首部会照常发送。在握手阶段，可以通过相关请求首部进行 安全限制、权限校验等。 2、服务端：响应协议升级 服务端返回内容如下，状态代码101表示协议切换。到此完成协议升级，后续的数据交互都按照新的协议来。 备注：每个header都以\r\n结尾，并且最后一行加上一个额外的空行\r\n。此外，服务端回应的HTTP状态码只能在握手阶段使用。过了握手阶段后，就只能采用特定的错误码。 3、Sec-WebSocket-Accept的计算 Sec-WebSocket-Accept根据客户端请求首部的Sec-WebSocket-Key计算出来。 计算公式为： 将Sec-WebSocket-Key跟258EAFA5-E914-47DA-95CA-C5AB0DC85B11拼接。1. 通过SHA1计算出摘要，并转成base64字符串。通过SHA1计算出摘要，并转成base64字符串。 伪代码如下： 验证下前面的返回结果： 五、数据帧格式客户端、服务端数据的交换，离不开数据帧格式的定义。因此，在实际讲解数据交换之前，我们先来看下WebSocket的数据帧格式。 WebSocket客户端、服务端通信的最小单位是帧（frame），由1个或多个帧组成一条完整的消息（message）。 发送端：将消息切割成多个帧，并发送给服务端；1. 接收端：接收消息帧，并将关联的帧重新组装成完整的消息；接收端：接收消息帧，并将关联的帧重新组装成完整的消息； 本节的重点，就是讲解数据帧的格式。详细定义可参考 RFC6455 5.2节 。 1、数据帧格式概览 下面给出了WebSocket数据帧的统一格式。熟悉TCP/IP协议的同学对这样的图应该不陌生。 从左到右，单位是比特。比如FIN、RSV1各占据1比特，opcode占据4比特。1. 内容包括了标识、操作代码、掩码、数据、数据长度等。（下一小节会展开）内容包括了标识、操作代码、掩码、数据、数据长度等。（下一小节会展开） 2、数据帧格式详解 针对前面的格式概览图，这里逐个字段进行讲解，如有不清楚之处，可参考协议规范，或留言交流。 FIN：1个比特。 如果是1，表示这是消息（message）的最后一个分片（fragment），如果是0，表示不是是消息（message）的最后一个分片（fragment）。 RSV1, RSV2, RSV3：各占1个比特。 一般情况下全为0。当客户端、服务端协商采用WebSocket扩展时，这三个标志位可以非0，且值的含义由扩展进行定义。如果出现非零的值，且并没有采用WebSocket扩展，连接出错。 Opcode&#58; 4个比特。 操作代码，Opcode的值决定了应该如何解析后续的数据载荷（data payload）。如果操作代码是不认识的，那么接收端应该断开连接（fail the connection）。可选的操作代码如下： %x0：表示一个延续帧。当Opcode为0时，表示本次数据传输采用了数据分片，当前收到的数据帧为其中一个数据分片。- %x1：表示这是一个文本帧（frame）- %x2：表示这是一个二进制帧（frame）- %x3-7：保留的操作代码，用于后续定义的非控制帧。- %x8：表示连接断开。- %x8：表示这是一个ping操作。- %xA：表示这是一个pong操作。- %xB-F：保留的操作代码，用于后续定义的控制帧。%x1：表示这是一个文本帧（frame） %x3-7：保留的操作代码，用于后续定义的非控制帧。 %x8：表示这是一个ping操作。 %xB-F：保留的操作代码，用于后续定义的控制帧。 Mask&#58; 1个比特。 表示是否要对数据载荷进行掩码操作。从客户端向服务端发送数据时，需要对数据进行掩码操作；从服务端向客户端发送数据时，不需要对数据进行掩码操作。 如果服务端接收到的数据没有进行过掩码操作，服务端需要断开连接。 如果Mask是1，那么在Masking-key中会定义一个掩码键（masking key），并用这个掩码键来对数据载荷进行反掩码。所有客户端发送到服务端的数据帧，Mask都是1。 掩码的算法、用途在下一小节讲解。 Payload length：数据载荷的长度，单位是字节。为7位，或7+16位，或1+64位。 假设数Payload length === x，如果 x为0~126：数据的长度为x字节。- x为126：后续2个字节代表一个16位的无符号整数，该无符号整数的值为数据的长度。- x为127：后续8个字节代表一个64位的无符号整数（最高位为0），该无符号整数的值为数据的长度。x为126：后续2个字节代表一个16位的无符号整数，该无符号整数的值为数据的长度。 此外，如果payload length占用了多个字节的话，payload length的二进制表达采用网络序（big endian，重要的位在前）。 Masking-key：0或4字节（32位） 所有从客户端传送到服务端的数据帧，数据载荷都进行了掩码操作，Mask为1，且携带了4字节的Masking-key。如果Mask为0，则没有Masking-key。 备注：载荷数据的长度，不包括mask key的长度。 Payload data：(x+y) 字节 载荷数据：包括了扩展数据、应用数据。其中，扩展数据x字节，应用数据y字节。 扩展数据：如果没有协商使用扩展的话，扩展数据数据为0字节。所有的扩展都必须声明扩展数据的长度，或者可以如何计算出扩展数据的长度。此外，扩展如何使用必须在握手阶段就协商好。如果扩展数据存在，那么载荷数据长度必须将扩展数据的长度包含在内。 应用数据：任意的应用数据，在扩展数据之后（如果存在扩展数据），占据了数据帧剩余的位置。载荷数据长度 减去 扩展数据长度，就得到应用数据的长度。 3、掩码算法 掩码键（Masking-key）是由客户端挑选出来的32位的随机数。掩码操作不会影响数据载荷的长度。掩码、反掩码操作都采用如下算法： 首先，假设： original-octet-i：为原始数据的第i字节。- transformed-octet-i：为转换后的数据的第i字节。- j：为i mod 4的结果。- masking-key-octet-j：为mask key第j字节。transformed-octet-i：为转换后的数据的第i字节。 masking-key-octet-j：为mask key第j字节。 算法描述为： original-octet-i 与 masking-key-octet-j 异或后，得到 transformed-octet-i。 j = i MOD 4transformed-octet-i = original-octet-i XOR masking-key-octet-j 六、数据传递一旦WebSocket客户端、服务端建立连接后，后续的操作都是基于数据帧的传递。 WebSocket根据opcode来区分操作的类型。比如0x8表示断开连接，0x0-0x2表示数据交互。 1、数据分片 WebSocket的每条消息可能被切分成多个数据帧。当WebSocket的接收方收到一个数据帧时，会根据FIN的值来判断，是否已经收到消息的最后一个数据帧。 FIN=1表示当前数据帧为消息的最后一个数据帧，此时接收方已经收到完整的消息，可以对消息进行处理。FIN=0，则接收方还需要继续监听接收其余的数据帧。 此外，opcode在数据交换的场景下，表示的是数据的类型。0x01表示文本，0x02表示二进制。而0x00比较特殊，表示延续帧（continuation frame），顾名思义，就是完整消息对应的数据帧还没接收完。 2、数据分片例子 直接看例子更形象些。下面例子来自MDN，可以很好地演示数据的分片。客户端向服务端两次发送消息，服务端收到消息后回应客户端，这里主要看客户端往服务端发送的消息。 第一条消息 FIN=1, 表示是当前消息的最后一个数据帧。服务端收到当前数据帧后，可以处理消息。opcode=0x1，表示客户端发送的是文本类型。 第二条消息 FIN=0，opcode=0x1，表示发送的是文本类型，且消息还没发送完成，还有后续的数据帧。1. FIN=0，opcode=0x0，表示消息还没发送完成，还有后续的数据帧，当前的数据帧需要接在上一条数据帧之后。1. FIN=1，opcode=0x0，表示消息已经发送完成，没有后续的数据帧，当前的数据帧需要接在上一条数据帧之后。服务端可以将关联的数据帧组装成完整的消息。FIN=0，opcode=0x0，表示消息还没发送完成，还有后续的数据帧，当前的数据帧需要接在上一条数据帧之后。 七、连接保持+心跳WebSocket为了保持客户端、服务端的实时双向通信，需要确保客户端、服务端之间的TCP通道保持连接没有断开。然而，对于长时间没有数据往来的连接，如果依旧长时间保持着，可能会浪费包括的连接资源。 但不排除有些场景，客户端、服务端虽然长时间没有数据往来，但仍需要保持连接。这个时候，可以采用心跳来实现。 发送方-&gt;接收方：ping- 接收方-&gt;发送方：pong接收方-&gt;发送方：pong ping、pong的操作，对应的是WebSocket的两个控制帧，opcode分别是0x9、0xA。 举例，WebSocket服务端向客户端发送ping，只需要如下代码（采用ws模块） 八、Sec-WebSocket-Key/Accept的作用 前面提到了，Sec-WebSocket-Key/Sec-WebSocket-Accept在主要作用在于提供基础的防护，减少恶意连接、意外连接。 作用大致归纳如下： 避免服务端收到非法的websocket连接（比如http客户端不小心请求连接websocket服务，此时服务端可以直接拒绝连接）1. 确保服务端理解websocket连接。因为ws握手阶段采用的是http协议，因此可能ws连接是被一个http服务器处理并返回的，此时客户端可以通过Sec-WebSocket-Key来确保服务端认识ws协议。（并非百分百保险，比如总是存在那么些无聊的http服务器，光处理Sec-WebSocket-Key，但并没有实现ws协议。。。）1. 用浏览器里发起ajax请求，设置header时，Sec-WebSocket-Key以及其他相关的header是被禁止的。这样可以避免客户端发送ajax请求时，意外请求协议升级（websocket upgrade）1. 可以防止反向代理（不理解ws协议）返回错误的数据。比如反向代理前后收到两次ws连接的升级请求，反向代理把第一次请求的返回给cache住，然后第二次请求到来时直接把cache住的请求给返回（无意义的返回）。1. Sec-WebSocket-Key主要目的并不是确保数据的安全性，因为Sec-WebSocket-Key、Sec-WebSocket-Accept的转换计算公式是公开的，而且非常简单，最主要的作用是预防一些常见的意外情况（非故意的）。确保服务端理解websocket连接。因为ws握手阶段采用的是http协议，因此可能ws连接是被一个http服务器处理并返回的，此时客户端可以通过Sec-WebSocket-Key来确保服务端认识ws协议。（并非百分百保险，比如总是存在那么些无聊的http服务器，光处理Sec-WebSocket-Key，但并没有实现ws协议。。。） 可以防止反向代理（不理解ws协议）返回错误的数据。比如反向代理前后收到两次ws连接的升级请求，反向代理把第一次请求的返回给cache住，然后第二次请求到来时直接把cache住的请求给返回（无意义的返回）。 强调：Sec-WebSocket-Key/Sec-WebSocket-Accept 的换算，只能带来基本的保障，但连接是否安全、数据是否安全、客户端/服务端是否合法的 ws客户端、ws服务端，其实并没有实际性的保证。 九、数据掩码的作用 WebSocket协议中，数据掩码的作用是增强协议的安全性。但数据掩码并不是为了保护数据本身，因为算法本身是公开的，运算也不复杂。除了加密通道本身，似乎没有太多有效的保护通信安全的办法。 那么为什么还要引入掩码计算呢，除了增加计算机器的运算量外似乎并没有太多的收益（这也是不少同学疑惑的点）。 答案还是两个字：安全。但并不是为了防止数据泄密，而是为了防止早期版本的协议中存在的代理缓存污染攻击（proxy cache poisoning attacks）等问题。 1、代理缓存污染攻击 下面摘自2010年关于安全的一段讲话。其中提到了代理服务器在协议实现上的缺陷可能导致的安全问题。猛击出处。 “We show, empirically, that the current version of the WebSocket consent mechanism is vulnerable to proxy cache poisoning attacks. Even though the WebSocket handshake is based on HTTP, which should be understood by most network intermediaries, the handshake uses the esoteric “Upgrade” mechanism of HTTP &#91;5&#93;. In our experiment, we find that many proxies do not implement the Upgrade mechanism properly, which causes the handshake to succeed even though subsequent traffic over the socket will be misinterpreted by the proxy.”&#91;TALKING&#93; Huang, L-S., Chen, E., Barth, A., Rescorla, E., and C. Jackson, “Talking to Yourself for Fun and Profit”, 2010, 在正式描述攻击步骤之前，我们假设有如下参与者： 攻击者、攻击者自己控制的服务器（简称“邪恶服务器”）、攻击者伪造的资源（简称“邪恶资源”）- 受害者、受害者想要访问的资源（简称“正义资源”）- 受害者实际想要访问的服务器（简称“正义服务器”）- 中间代理服务器受害者、受害者想要访问的资源（简称“正义资源”） 中间代理服务器 攻击步骤一： 攻击者浏览器 向 邪恶服务器 发起WebSocket连接。根据前文，首先是一个协议升级请求。1. 协议升级请求 实际到达 代理服务器。1. 代理服务器 将协议升级请求转发到 邪恶服务器。1. 邪恶服务器 同意连接，代理服务器 将响应转发给 攻击者。协议升级请求 实际到达 代理服务器。 邪恶服务器 同意连接，代理服务器 将响应转发给 攻击者。 由于 upgrade 的实现上有缺陷，代理服务器 以为之前转发的是普通的HTTP消息。因此，当协议服务器 同意连接，代理服务器 以为本次会话已经结束。 攻击步骤二： 攻击者 在之前建立的连接上，通过WebSocket的接口向 邪恶服务器 发送数据，且数据是精心构造的HTTP格式的文本。其中包含了 正义资源 的地址，以及一个伪造的host（指向正义服务器）。（见后面报文）1. 请求到达 代理服务器 。虽然复用了之前的TCP连接，但 代理服务器 以为是新的HTTP请求。1. 代理服务器 向 邪恶服务器 请求 邪恶资源。1. 邪恶服务器 返回 邪恶资源。代理服务器 缓存住 邪恶资源（url是对的，但host是 正义服务器 的地址）。请求到达 代理服务器 。虽然复用了之前的TCP连接，但 代理服务器 以为是新的HTTP请求。 邪恶服务器 返回 邪恶资源。代理服务器 缓存住 邪恶资源（url是对的，但host是 正义服务器 的地址）。 到这里，受害者可以登场了： 受害者 通过 代理服务器 访问 正义服务器 的 正义资源。1. 代理服务器 检查该资源的url、host，发现本地有一份缓存（伪造的）。1. 代理服务器 将 邪恶资源 返回给 受害者。1. 受害者 卒。代理服务器 检查该资源的url、host，发现本地有一份缓存（伪造的）。 受害者 卒。 附：前面提到的精心构造的“HTTP请求报文”。 2、当前解决方案 最初的提案是对数据进行加密处理。基于安全、效率的考虑，最终采用了折中的方案：对数据载荷进行掩码处理。 需要注意的是，这里只是限制了浏览器对数据载荷进行掩码处理，但是坏人完全可以实现自己的WebSocket客户端、服务端，不按规则来，攻击可以照常进行。 但是对浏览器加上这个限制后，可以大大增加攻击的难度，以及攻击的影响范围。如果没有这个限制，只需要在网上放个钓鱼网站骗人去访问，一下子就可以在短时间内展开大范围的攻击。 十、写在后面WebSocket可写的东西还挺多，比如WebSocket扩展。客户端、服务端之间是如何协商、使用扩展的。WebSocket扩展可以给协议本身增加很多能力和想象空间，比如数据的压缩、加密，以及多路复用等。 篇幅所限，这里先不展开，感兴趣的同学可以留言交流。文章如有错漏，敬请指出。 作者：程序猿小卡 转载来源：WebSocket协议深入探究]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>程序员</tag>
        <tag>比尔·盖茨</tag>
        <tag>镜音双子</tag>
        <tag>Origin</tag>
        <tag>环境污染</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[中国平安，精致的利己主义者]]></title>
    <url>%2F2018%2F76f53715%2F</url>
    <content type="text"><![CDATA[中国平安，精致的利己主义者 转载来源：中国平安，精致的利己主义者]]></content>
      <tags>
        <tag>慧保天下</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[GIF界的谷歌，凭什么靠 6 秒钟广告进账几十亿？]]></title>
    <url>%2F2018%2F8cdbcd6f%2F</url>
    <content type="text"><![CDATA[GIF界的谷歌，凭什么靠 6 秒钟广告进账几十亿？ 转载来源：GIF界的谷歌，凭什么靠 6 秒钟广告进账几十亿？]]></content>
      <tags>
        <tag>36氪</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[React应用架构设计]]></title>
    <url>%2F2018%2F45be0725%2F</url>
    <content type="text"><![CDATA[本节将开始详细分析如何搭建一个React应用架构。 一. 前言现在已经有很多脚手架工具，如create-react-app，支持一键创建一个React应用项目结构，很方便，但是享受方便的同时，也失去了对项目架构及技术栈完整学习的机会，而且通常脚手架创建的应用技术架构并不能完全满足我们的业务需求，需要我们自己修改，完善，所以如果希望对项目架构有更深掌控，最好还是从0到1理解一个项目。 二. 项目结构与技术栈我们这次的实践不准备使用任何脚手架，所以我们需要自己创建每一个文件，引入每一个技术和三方库，最终形成完整的应用，包括我们选择的完整技术栈。 第一步，当然是创建目录，如果你还没有代码，可以从Github获取： 生成项目结构如下图： 1、 src为应用源代码目录； 2、 webpack为webpack配置目录； 3、 webpack.config.js为webpack配置入口文件； 4、 package.json为项目依赖管理文件； 5、 yarn.lock为项目依赖版本锁文件； 6、 .babelrc文件，babel的配置文件，使用babel编译React和JavaScript代码； 7、 eslintrc 和 eslintignore 分别为eslint语法检测配置及需要忽略检查的内容或文件； 8、 postcss.config.js 为CSS后编译器postcss的配置文件； 9、 API.md为API文档入口； 10、 docs为文档目录； 11、 README.md为项目说明文档； 接下来的工作主要就是丰富src目录，包括搭建项目架构，开发应用功能，还有自动化，单元测试等，本篇主要关注项目架构的搭建，然后使用技术栈实践开发几个模块。 2.1 技术栈 项目架构搭建很大部分依赖于项目的技术栈，所以先对整个技术栈进行分析，总结： 1、 react和react-dom库是项目前提； 2、 react路由； 3、 应用状态管理容器； 4、 是否需要Immutable数据； 5、 应用状态的持久化； 6、 异步任务管理； 7、 测试及辅助工具或函数； 8、 开发调试工具； 根据以上划分决定选用以下第三方库和工具构成项目的完整技术栈： 1、react，react-dom； 2、react-router管理应用路由； 3、redux作为JavaScript状态容器，react-redux将React应用与redux连接； 4、Immutable.js支持Immutable化状态，redux-immutable使整个redux store状态树Immutable化； 5、使用redux-persist支持redux状态树的持久化，并添加redux-persist-immutable拓展以支持Immutable化状态树的持久化； 6、使用redux-saga管理应用内的异步任务，如网络请求，异步读取本地数据等； 7、使用jest集成应用测试，使用lodash，ramda等可选辅助类，工具类库； 8、可选使用reactotron调试工具 针对以上分析，完善后的项目结构如图： 三. 开发调试工具React应用开发目前已经有诸多调试工具，常用的如redux-devtools，Reactron等。 3.1 redux-devtool redux-devtools是支持热重载，回放action，自定义UI的一款Redux开发工具。 首先需要按照对应的浏览器插件，然后再Redux应用中添加相关配置，就能在浏览器控制台中查看到redux工具栏了。 然后安装项目依赖库： 然后在创建redux store时将其作为redux强化器传入createStore方法： 在开发环境下获取redux-devtools提供的拓展组合函数；1. 创建store时使用拓展组合函数组合redux中间件和增强器，redux-dev-tools便获得了应用redux的相关信息；创建store时使用拓展组合函数组合redux中间件和增强器，redux-dev-tools便获得了应用redux的相关信息； 3.2 Reactotron Reactotron是一款跨平台调试React及React Native应用的桌面应用，能动态实时监测并输出React应用等redux，action，saga异步请求等信息，如图： 首先安装： 然后初始化Reactotron相关配置： 然后启使用console.tron.overlay方法拓展入口组件： 至此就可以使用Reactotron客户端捕获应用中发起的所有的redux和action了。 四. 组件划分React组件化开发原则是组件负责渲染UI，组件不同状态对应不同UI，通常遵循以下组件设计思路： 1、 布局组件：仅仅涉及应用UI界面结构的组件，不涉及任何业务逻辑，数据请求及操作； 2、 容器组件：负责获取数据，处理业务逻辑，通常在render()函数内返回展示型组件； 3、 展示型组件：负责应用的界面UI展示； 4、 UI组件：指抽象出的可重用的UI独立组件，通常是无状态组件； 五. Redux现在的任何大型web应用如果少了状态管理容器，那这个应用就缺少了时代特征，可选的库诸如mobx，redux等，实际上大同小异，各取所需，以redux为例，redux是最常用的React应用状态容器库，对于React Native应用也适用。 Redux是一个JavaScript应用的可预测状态管理容器，它不依赖于具体框架或类库，所以它在多平台的应用开发中有着一致的开发方式和效率，另外它还能帮我们轻松的实现时间旅行，即action的回放。 1、 数据单一来源原则：使用Redux作为应用状态管理容器，统一管理应用的状态树，它推从数据单一可信来源原则，所有数据都来自redux store，所有的数据更新也都由redux处理； 2、 redux store状态树：redux集中管理应用状态，组织管理形式就好比DOM树和React组件树一样，以树的形式组织，简单高效； 3、 redux和store：redux是一种Flux的实现方案，所以创建了store一词，它类似于商店，集中管理应用状态，支持将每一个发布的action分发至所有reducer； 4、 action：以对象数据格式存在，通常至少有type和payload属性，它是对redux中定义的任务的描述； 5、 reducer：通常是以函数形式存在，接收state（应用局部状态）和action对象两个参数，根据action.type(action类型)执行不同的任务，遵循函数式编程思想； 6、dispatch：store提供的分发action的功能方法，传递一个action对象参数； 7、 createStore：创建store的方法，接收reducer，初始应用状态，redux中间件和增强器，初始化store，开始监听action； 5.1 中间件（Redux Middleware） Redux中间件，和Node中间件一样，它可以在action分发至任务处理reducer之前做一些额外工作，dispatch发布的action将依次传递给所有中间件，最终到达reducer，所以我们使用中间件可以拓展诸如记录日志，添加监控，切换路由等功能，所以中间件本质上只是拓展了store.dispatch方法。 5.2 增强器（Store Enhancer） 有些时候我们可能并不满足于拓展dispatch方法，还希望能增强store，redux提供以增强器形式增强store的各个方面，甚至可以完全定制一个store对象上的所有接口，而不仅仅是store.dispatch方法 最简单的例子代码如上，新函数接收redux的createStore方法和创建store需要的参数，然后在函数内部保存store对象上某方法的引用，重新实现该方法，在里面处理完增强逻辑后调用原始方法，保证原始功能正常执行，这样就增强了store的dispatch方法。 可以看到，增强器完全能实现中间件的功能，其实，中间件就是以增强器方式实现的，它提供的compose方法就可以组合将我们传入的增强器拓展到store，而如果我们传入中间件，则需要先调用applyMiddleware方法包装，内部以增强器形式将中间件功能拓展到store.dispatch方法 5.3 react-redux Redux是一个独立的JavaScript应用状态管理容器库，它可以与React、Angular、Ember、jQuery甚至原生JavaScript应用配合使用，所以开发React应用时，需要将Redux和React应用连接起来，才能统一使用Redux管理应用状态，使用官方提供的react-redux库。 react-redux库提供Provider组件通过context方式向应用注入store，然后可以使用connect高阶方法，获取并监听store，然后根据store state和组件自身props计算得到新props，注入该组件，并且可以通过监听store，比较计算出的新props判断是否需要更新组件。 5.4 createStore 使用redux提供的createStore方法创建redux store，但是在实际项目中我们常常需要拓展redux添加某些自定义功能或服务，如添加redux中间件，添加异步任务管理saga，增强redux等： 5.5 redux与Immutable redux默认提供了combineReducers方法整合reduers至redux，然而该默认方法期望接受原生JavaScript对象并且它把state作为原生对象处理，所以当我们使用createStore方法并且接受一个Immutable对象作应用初始状态时，reducer将会返回一个错误，源代码如下： 如上表明，原始类型reducer接受的state参数应该是一个原生JavaScript对象，我们需要对combineReducers其进行增强，以使其能处理Immutable对象，redux-immutable即提供创建一个可以和Immutable.js协作的Redux combineReducers。 如上代码，可以看见我们传入的initialState是一个Immutable.Map类型数据，我们将redux整个state树丛根源开始Immutable化，另外传入了可以处理Immutable state的reducers和sagas。另外每一个state树节点数据都是Immutable结构，如AppReducer： 这里默认使用Immutable.fromJS()方法状态树节点对象转化为Immutable结构，并且更新state时使用Immutable方法state.merge()，保证状态统一可预测。 六. React路由在React web单页面应用中，页面级UI组件的展示和切换完全由路由控制，每一个路由都有对应的URL及路由信息，我们可以通过路由统一高效的管理我们的组件切换，保持UI与URL同步，保证应用的稳定性及友好体验。 6.1 react-router React Router是完整的React 路由解决方案，也是开发React应用最常使用的路由管理库，只要用过它，绝对会喜欢上它的设计，它提供简单的API，以声明式方式实现强大的路由功能，诸如按需加载，动态路由等。 1、声明式：语法简洁，清晰； 2、按需加载：延迟加载，根据使用需要判断是否需要加载； 3、动态路由：动态组合应用路由结构，更灵活，更符合组件化开发模式； 6.2 动态路由与静态路由 使用react-router v4版本可以定义跨平台的应用动态路由结构，所谓的动态路由（Dynamic Routing）即在渲染过程中发生路由的切换，而不需要在创建应用前就配置好，这也正是其区别于静态路由（Static Routing）所在，动态路由提高更灵活的路由组织方式，而且更方便编码实现路由按需加载组件。 在react-router v2和v3版本中，开发React应用需要在开始渲染前就定义好完整的应用路由结构，所有的路由都需要同时初始化，才能在应用渲染后生效，会产生很多嵌套化路由，丧失了动态路由的灵活性和简洁的按需加载编码方式。 6.3 react-router v4.x 在react-router 2.x和3.x版本中，定义一个应用路由结构通常如下： 很简单，但是所有的路由结构都需要在渲染应用前，统一定义，层层嵌套；而且如果要实现异步按需加载还需要在这里对路由配置对象进行修改，使用getComponentAPI，并侵入改造该组件，配合webpack的异步打包加载API，实现按需加载： 1、 路由层层嵌套，必须在渲染应用前统一声明； 2、 API不同，需要使用getComponent，增加路由配置对象的复杂性； 3、 只是一个声明路由的辅助标签，本身无意义； 而使用react-router v4.x则如下： 相比之前版本，减少了配置化的痕迹，更凸显了组件化的组织方式，而且在渲染组件时才实现该部分路由，而如果期望按需加载该组件，则可以通过封装实现一个支持异步加载组件的高阶组件，将经过高阶组件处理后返回的组件传入即可，依然遵循组件化形式： 1、 灵活性：路由可以在渲染组件中声明，不需依赖于其他路由，不需要集中配置； 2、 简洁：统一传入component，保证路由声明的简洁性； 3、 组件化：作为一个真实组件创建路由，可以渲染； 6.3.1 路由钩子方 另外需要注意的是，相对于之前版本提供 onEnter, onUpdate, onLeave 等钩子方法API在一定程度上提高了对路由的可控性，但是实质只是覆盖了渲染组件的生命周期方法，现在我们可以通过路由渲染组件的生命周期方法直接控制路由，如使用 componentDidMount 或 componentWillMount 代替 onEnter。 6.4 路由与Redux 同时使用React-Router和Redux时，大多数情况是正常的，但是也可能出现路由变更组件未更新的情况，如： 1、 我们使用redux的 connect 方法将组件连接至redux：connect(Home); 2、 组件不是一个路由渲染组件，即不是使用 Route&gt; 组件形式： 声明渲染的； 这是为什么呢？，因为Redux会实现组件的 shouldComponentUpdate 方法，当路由变化时，该组件并没有接收到props表明发生了变更，需要更新组件。 那么如何解决问题呢？，要解决这个问题只需要简单的使用react-router-dom 提供的 withRouter方法包裹组件： 6.5 Redux整合 在使用Redux以后，需要遵循redux的原则：单一可信数据来源，即所有数据来源都只能是reudx store，react路由状态也不应例外，所以需要将路由state与store state连接。 6.5.1 react-router-redux 连接React Router与Redux，需要使用 react-router-redux 库，而且react-router v4版本需要指定安装 &#64;next 版本和 hsitory 库： 然后，在创建store时，需要实现如下配置： 1、 创建一个history对象，对于web应用，我们选择browserHisotry，对应需要从 history/createBrowserHistory 模块引入 createHistory 方法以创建history对象； 2、 添加 routerReducer 和 routerMiddleware 中间件“，其中 routerMiddleware 中间件接收history对象参数，连接store和history，等同于旧版本的 syncHistoryWithStore ； 在渲染根组件时，我们抽象出两个组件： 1、 初始化渲染根组件，挂载至DOM的根组件，由 组件包裹，注入store； 2、 路由配置组件，在根组件中，声明路由配置组件，初始化必要的应用路由定义及路由对象； 上面的 组件是项目的路由组件： 首先使用 react-router-redux 提供的 ConnectedRouter 组件包裹路由配置，该组件将自动使用 组件注入的 store，我们需要做的是手动传入 history 属性，在组件内会调用 history.listen 方法监听浏览器 LOCATION_CHANGE 事件，最后返回 react-router 的 组件，处理作为 this.props.children 传入的路由配置。 6.5.2 dispatch切换路由 配置上面代码后，就能够以dispatch action的方式触发路由切换和组件更新了： 这个reducer所做的只是将App导航路由状态合并入store。 七. redux持久化我们知道浏览器默认有资源的缓存功能并且提供本地持久化存储方式如localStorage，indexDb，webSQL等，通常可以将某些数据存储在本地，在一定周期内，当用户再次访问时，直接从本地恢复数据，可以极大提高应用启动速度，用户体验更有优势，我们可以使用localStorage存储一些数据，如果是较大量数据存储可以使用webSQL。 另外不同于以往的直接存储数据，启动应用时本地读取然后恢复数据，对于redux应用而言，如果只是存储数据，那么我们就得为每一个reducer拓展，当再次启动应用时去读取持久化的数据，这是比较繁琐而且低效的方式，是否可以尝试存储reducer key，然后根据key恢复对应的持久化数据，首先注册Rehydrate reducer，当触发action时根据其reducer key恢复数据，然后只需要在应用启动时分发action，这也很容易抽象成可配置的拓展服务，实际上三方库redux-persist已经为我们做好了这一切。 7.1 redux-persist 要实现redux的持久化，包括redux store的本地持久化存储及恢复启动两个过程，如果完全自己编写实现，代码量比较复杂，可以使用开源库 redux-persist，它提供 persistStore 和 autoRehydrate 方法分别持久化本地存储store及恢复启动store，另外还支持自定义传入持久化及恢复store时对store state的转换拓展。 7.1.1 持久化store 如下在创建store时会调用persistStore相关服务- RehydrationServices.updateReducers()： 该方法内实现了store的持久化存储： 会在localStorage存储一个reducer版本号，这个是在应用配置文件中可以配置，首次执行持久化时存储该版本号及store，若reducer版本号变更则清空原来存储的store，否则传入store给持久化方法 persistStore 即可。 该方法主要实现store的持久化以及分发rehydration action &#58; 1、 订阅 redux store，当其发生变化时触发store存储操作； 2、 从指定的StorageEngine（如localStorage）中获取数据，进行转换，然后通过分发 REHYDRATE action，触发 REHYDRATE 过程； 接收参数主要如下： 1、 store&#58; 持久化的store； 2、 config：配置对象 1）storage：一个 持久化引擎，例如 LocalStorage 和 AsyncStorage； 2）transforms： 在 rehydration 和 storage 阶段被调用的转换器； 3） blacklist： 黑名单数组，指定持久化忽略的 reducers 的 key； 3、 callback：ehydration 操作结束后的回调； 7.1.2 恢复启动 和persisStore一样，依然是在创建redux store时初始化注册rehydrate拓展： 该方法实现的功能很简单，即使用 持久化的数据恢复(rehydrate) store 中数据，它其实是注册了一个autoRehydarte reducer，会接收前文persistStore方法分发的rehydrate action，然后合并state。 当然，autoRehydrate不是必须的，我们可以自定义恢复store方式： 7.1.3 版本更新 需要注意的是redux-persist库已经发布到v5.x，而本文介绍的以v5.x为例，v4.x参考此处，新版本有一些更新，可以选择性决定使用哪个版本。 7.2 持久化与Immutable 前面已经提到Redux与Immutable的整合，上文使用的redux -persist默认也只能处理原生JavaScript对象的redux store state，所以需要拓展以兼容Immutable。 7.2.1 redux-persist-immutable 使用redux-persist-immutable库可以很容易实现兼容，所做的仅仅是使用其提供的 persistStore 方法替换redux-persist所提供的方法： 7.2.2 transform 我们知道持久化store时，针对的最好是原生JavaScript对象，因为通常Immutable结构数据有很多辅助信息，不易于存储，所以需要定义持久化及恢复数据时的转换操作： 如上，输出对象中的in和out分别对应持久化及恢复数据时的转换操作，实现的只是使用 fromJS() 和 toJS() 转换Js和Immutable数据结构，使用方式如下： 八. Immutable在项目中引入Immutable以后，需要尽量保证以下几点： 1、 redux store整个state树的统一Immutable化； 2、 redux持久化对Immutable数据的兼容； 3、 React路由兼容Immutable； 8.1 Immutable与React路由 前面两点已经在前面两节阐述过，第三点react-router兼容Immutable，其实就是使应用路由状态兼容Immutable，在React路由一节已经介绍如何将React路由状态连接至Redux store，但是如果应用使用了Immutable库，则还需要额外处理，将react-router state转换为Immutable格式，routeReducer不能处理Immutable，我们需要自定义一个新的RouterReducer： 将默认初始路由状态转换为Immutable，并且路由变更时使用Immutable API操作state。 8.2 seamless-Immutable 当引入Immutable.js后，对应用状态数据结构的使用API就得遵循Immutable API，而不能再使用原生JavaScript对象，数组等的操作API了，诸如，数组解构（&#91;a, b&#93; = &#91;b, c&#93;），对象拓展符（…）等，存在一些问题： 1、Immutable数据辅助节点较多，数据较大： 2、必须使用Immutable语法，和JavaScript语法有差异，不能很好的兼容； 3、和Redux，react-router等JavaScript库写协作时，需要引入额外的兼容处理库； 针对这些问题，社区有了 seamless-immutable 可供替换选择： 1、更轻：相对于Immutable.js seamless-immutable 库更轻小； 2、语法：对象和数组的操作语法更贴近原生JavaScript； 3、和其他JavaScript库协作更方便； 九. 异步任务流管理最后要介绍的模块是异步任务管理，在应用开发过程中，最主要的异步任务就是数据HTTP请求，所以我们讲异步任务管理，主要关注在数据HTTP请求的流程管理。 9.1 axios 本项目中使用axios作为HTTP请求库，axios是一个Promise格式的HTTP客户端，选择此库的原因主要有以下几点： 能在浏览器发起XMLHttpRequest，也能在node.js端发起HTTP请求；1. 支持Promise；1. 能拦截请求和响应；1. 能取消请求；1. 自动转换JSON数据；支持Promise； 能取消请求； 9.2 redux-saga redux-saga是一个致力于使应用中如数据获取，本地缓存访问等异步任务易于管理，高效运行，便于测试，能更好的处理异常的三方库。 Redux-saga是一个redux中间件，它就像应用中一个单独的进程，只负责管理异步任务，它可以接受应用主进程的redux action以决定启动，暂停或者是取消进程任务，它也可以访问redux应用store state，然后分发action。 9.2.1 初始化saga redux-saga是一个中间件，所以首先调用 createSagaMiddleware 方法创建中间件，然后使用redux的 applyMiddleware 方法启用中间件，之后使用compose辅助方法传给 createStore 创建store，最后调用 run 方法启动根saga： 9.2.2 saga分流 在项目中通常会有很多并列模块，每个模块的saga流也应该是并列的，需要以多分支形式并列，redux-saga提供的 fork 方法就是以新开分支的形式启动当前saga流： 如上，首先收集所有模块根saga，然后遍历数组，启动每一个saga流根saga。 9.2.3 saga实例 以AppSaga为例，我们期望在应用启动时就发起一些异步请求，如获取文章列表数据将其填充至redux store，而不等待使用数据的组件渲染完才开始请求数据，提高响应速度： takeLatest：在AppSaga 内使用 takeLatest 方法监听 REQUEST_POST_LIST action，若短时间内连续发起多次action 则会取消前面未响应的action，只发起最后一次action； getPostListSaga子Saga：当接收到该action时，调用getPostListSaga并将payload传递给它，getPostListSaga 是AppSaga的子级Saga，在里面处理具体异步任务； getPostList &#58; getPostListSaga 会调用getPostList 方法，发起异步请求, 拿到响应数据后，调用 receivePostListActionCreator，创建并分发action，然后由reducer处理相应逻辑； getPostList 方法内容如下： put 是redux-saga提供的可分发action方法，take，call等都是 redux-saga 提供的API。 之后便可以在项目路由根组件注入ActionCreator，创建action，然后saga就会接收进行处理了。 9.3 saga与Reactotron 前面已经配置好可以使用Reactotron捕获应用所有redux和action，而redux-saga是一类redux中间件，所以捕获sagas需要额外配置，创建store时，在saga中间件内添加sagaMonitor服务，监听saga&#58; 十. 总结本文较详细的总结了个人从0到1搭建一个项目架构的过程，对React， Redux应用和项目工程实践都有了更深的理解及思考，在大前端成长之路继续砥砺前行。 注：文中列出的所有技术栈，博主计划一步一步推进，目前源码中使用的技术有React，React Router，Redux，react-redux，react-router-redux，Redux-saga，axios。后期计划推进Immutable，Reactotron，Redux Persist。 完整项目代码见github 参考1、 React 2、 Redux 3、 React Router v4 4、 redux-saga 5、 Redux Persist 转载来源：React应用架构设计]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>软件</tag>
        <tag>编程语言</tag>
        <tag>JavaScript</tag>
        <tag>Git</tag>
        <tag>JSON</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python爬虫框架scrapy抓取旅行家网所有游记！从此出游不发愁！ - CSDN博客]]></title>
    <url>%2F2018%2F85a49d22%2F</url>
    <content type="text"><![CDATA[Scrapy是一个用 Python 写的 Crawler Framework ，简单轻巧，并且非常方便。Scrapy 使用 Twisted 这个异步网络库来处理网络通讯，架构清晰，并且包含了各种中间件接口，可以灵活的完成各种需求。 以上是网上摘录的一段介绍scrapy框架的文字，大过年的，懒癌高发期… 安装scrapy，pip可以解决你的问题： pip install scrapy。 这里插一句，如果你运行代码后看到这个错误： 1ImportError&amp;#58; No module named win32api 深坑出现，你需要安装pywin32，如果已经安装了pywin32，还出现错误，你仍需手动将你python安装目录下\Lib\site-packages\pywin32_system32下：pythoncom27.dll, pywintypes27.dll两个文件复制到c&#58;\windows\system32下！当然如果不是windows系统的话，请无视！ 话不多说，开始我们的爬虫吧！ 首先来分析网页结构： 1、url&#58;https&#58;//you.autohome.com.cn 打开旅行家的主页，这里我用的是火狐浏览器，看下图 点击精彩游记，然后跳出游记页面， 然后在点击全部游记，我们的目标就出现了，拉到最下面，一共3993页，1页20篇 很简单的一个网站 2、我们开始分析每页的数据，直接打开F12抓包，然后刷新网页或者点击其他页，看看服务器返回的请求都有哪些！ 找到一个get请求，里面是json格式的内容，里面有游记的作者、标题、缩略图等等内容，ok，我们可以开始写代码了！ Ps&#58;这里我们只做个简单的页面目录的爬虫，就不一 一抓取文章内容了（如果有需要的小伙伴可以自行添加相关内容）。 3、打开cmd新建一个scrapy框架，命令为：scrapy startproject autohome ,然后系统自动帮我们建立好相关的目录和py文件，我们仍需手动建立一个spider.py（文件名可自取）来放入我们的爬虫 先打开item.py，这里存放的是我们的目标，告诉爬虫我们要爬取的内容是什么！代码如下： 然后打开setting.py（如无必要，不要修改这里的内容），将ROBOTSTXT_OBEY的值改为False（不改的话，有些内容爬不到，这里是选择是否遵循robots协议）,然后将你的UA写入下面的头部信息中！ 其他都不用管了。最后打开spider文件夹，在这里我们要开始写我们的爬虫了！ 4、打开新建的py文件，先导入用到的模块 （导入模块后有错误提示可以不用理会），写入如下代码： 第6行的name是唯一的，可自行命名 第7行为定义爬虫的范围，也就是允许执行的url范围是：autohome.com.cn，注意这里是列表形式 第9.10.11行为抓取的内容所在url，通过yield Request返回，上图未截全部分为： 1yield Request(&apos;https&amp;#58;//you.autohome.com.cn/summary/getsearchresultlist?ps=20&amp;pg=&amp;#123&amp;#125&amp;type=3&amp;tagCode=&amp;tagName=&amp;sortType=3&apos;.format(pg),self.parse) 因为只有3993页，直接for循环取到所有页码，定义了start_requests函数后可省略start_urls列表也就是起始列表 第14行开始定义爬取方法 第15行，将json格式的内容赋值给一个变量 第16行，初始化导入的Items文件中所定义的类 第17-24行，循环json格式的内容，并将相应的值赋值给item，这里item是一个字典格式，然后返回给items文件 到这里就写完了这个爬虫，为方便使用，我们直接将结果写入json格式 打开cmd，命令：scrapy crawl autohome -o autohome.json -t json 因为我们爬取的内容很少，所以速度还是很快的 大概十来分钟吧，数据就抓取完成！来看看结果，因为是json格式，截取一小段找个在线解析的网页就可以看了 验证一下： So easy! 喜欢就关注下呗(；°○° )！ 爬虫07 爬取阿里旅行特价机票- &lt;img src=&quot;https&amp;#58;//avatar.csdn.net/B/3/C/3_yemoweiliang.jpg&quot; alt=&quot;yemoweiliang&quot; title=&quot;yemoweiliang&quot;&gt; - yemoweiliang - 2016-09-01 19&amp;#58;26&amp;#58;07 - &lt;i class=&quot;icon iconfont icon-read&quot;&gt;&lt;/i&gt;2723 Python爬虫源码—爬取猫途鹰官方旅游网站信息- 2018年04月10日 00&amp;#58;00 用phpspider框架做爬虫分析旅游数据- &lt;img src=&quot;https&amp;#58;//avatar.csdn.net/F/6/9/3_u010852160.jpg&quot; alt=&quot;u010852160&quot; title=&quot;u010852160&quot;&gt; - u010852160 - 2016-10-18 18&amp;#58;25&amp;#58;50 - &lt;i class=&quot;icon iconfont icon-read&quot;&gt;&lt;/i&gt;11551 Python网络爬虫专业级框架_scrapy- &lt;img src=&quot;https&amp;#58;//avatar.csdn.net/7/2/A/3_lu_yongchao.jpg&quot; alt=&quot;lu_yongchao&quot; title=&quot;lu_yongchao&quot;&gt; - lu_yongchao - 2017-03-25 22&amp;#58;01&amp;#58;56 - &lt;i class=&quot;icon iconfont icon-read&quot;&gt;&lt;/i&gt;1675 scrapy 安装包- 2017年07月30日 16&amp;#58;47 - 17.27MB - 下载 Python 爬虫-爬取阿里旅行特价机票信息（1）- &lt;img src=&quot;https&amp;#58;//avatar.csdn.net/7/A/9/3_vvbbbbb.jpg&quot; alt=&quot;VVBBBBB&quot; title=&quot;VVBBBBB&quot;&gt; - VVBBBBB - 2016-07-29 11&amp;#58;01&amp;#58;01 - &lt;i class=&quot;icon iconfont icon-read&quot;&gt;&lt;/i&gt;5927 python爬虫（上）–请求——关于旅游网站的酒店评论爬取（传参方法）- &lt;img src=&quot;https&amp;#58;//avatar.csdn.net/D/B/D/3_qq_29245097.jpg&quot; alt=&quot;qq_29245097&quot; title=&quot;qq_29245097&quot;&gt; - qq_29245097 - 2016-07-01 22&amp;#58;17&amp;#58;24 - &lt;i class=&quot;icon iconfont icon-read&quot;&gt;&lt;/i&gt;11619 Python爬虫框架Scrapy：爬取校花网- &lt;img src=&quot;https&amp;#58;//avatar.csdn.net/3/C/9/3_python233.jpg&quot; alt=&quot;python233&quot; title=&quot;python233&quot;&gt; - python233 - 2017-04-20 19&amp;#58;23&amp;#58;27 - &lt;i class=&quot;icon iconfont icon-read&quot;&gt;&lt;/i&gt;1145 scrapy爬虫框架教程（一）– Scrapy入门- &lt;img src=&quot;https&amp;#58;//avatar.csdn.net/2/B/C/3_woodenrobot.jpg&quot; alt=&quot;woodenrobot&quot; title=&quot;woodenrobot&quot;&gt; - woodenrobot - 2017-01-01 18&amp;#58;52&amp;#58;00 - &lt;i class=&quot;icon iconfont icon-read&quot;&gt;&lt;/i&gt;684 Python爬虫框架scrapy抓取旅行家网所有游记！从此出游不发愁！ - &lt;img src=&quot;https&amp;#58;//avatar.csdn.net/0/0/A/3_programmer_yf.jpg&quot; alt=&quot;programmer_yf&quot; title=&quot;programmer_yf&quot;&gt; - programmer_yf - 2018-02-22 15&amp;#58;43&amp;#58;54 - &lt;i class=&quot;icon iconfont icon-read&quot;&gt;&lt;/i&gt;33 个人资料 &lt;dl class=&quot;inf_bar clearfix&quot;&gt; &lt;dt class=&quot;csdn-tracking-statistics tracking-click&quot; data-mod=&quot;popu_381&quot;&gt; &lt;a href=&quot;https&amp;#58;//blog.csdn.net/programmer_yf&quot; target=&quot;_blank&quot;&gt; &lt;img src=&quot;https&amp;#58;//avatar.csdn.net/0/0/A/3_programmer_yf.jpg&quot; class=&quot;avatar_pic&quot;&gt; &lt;/a&gt; &lt;/dt&gt;&lt;dd&gt; &lt;h3 class=&quot;csdn-tracking-statistics tracking-click&quot; data-mod=&quot;popu_380&quot;&gt;&amp;#91;programmer_yf&amp;#93;(https&amp;#58;//blog.csdn.net/programmer_yf)&lt;/h3&gt; &lt;/dd&gt; &lt;/dl&gt; &lt;dl title=&quot;1&quot;&gt; &lt;dt&gt;原创&lt;/dt&gt; &lt;dd&gt;1&lt;/dd&gt; &lt;/dl&gt; &lt;dl title=&quot;0&quot;&gt; &lt;dt&gt;粉丝&lt;/dt&gt; &lt;dd id=&apos;fan&apos;&gt;0&lt;/dd&gt; &lt;/dl&gt; &lt;dl title=&quot;0&quot;&gt; &lt;dt&gt;喜欢&lt;/dt&gt; &lt;dd&gt;0&lt;/dd&gt; &lt;/dl&gt; &lt;dl title=&quot;0&quot;&gt; &lt;dt&gt;评论&lt;/dt&gt; &lt;dd&gt;0&lt;/dd&gt; &lt;/dl&gt; 等级： &lt;a href=&quot;https&amp;#58;//blog.csdn.net/home/help.html#level&quot; title=&quot;1级,点击查看等级说明&quot; target=&quot;_blank&quot;&gt; &lt;img class=&quot;grade-img&quot; src=&quot;https&amp;#58;//csdnimg.cn/jifen/images/xunzhang/jianzhang/blog1.png&quot; alt=&quot;1级,点击查看等级说明&quot;&gt; &lt;/a&gt; 访问量： 34 积分： 12 排名： 230万+ // 判断并设置用户名位置，没有博客专家与关注按钮时，用户名居中 $medals_children = $(‘.medals’).children().length; $span_add_follow = $(‘#span_add_follow’).length; if($medals_children === 0 &amp;&amp; $span_add_follow === 0)&amp;#123 $(‘.inf_bar dd’).css(‘vertical-align’,’10px’) &amp;#125 文章搜索 文章分类 python (1) 文章存档 &lt;!--归档统计--&gt; &lt;li&gt; &amp;#91;2018年2月&amp;#93;(https&amp;#58;//blog.csdn.net/programmer_yf/article/month/2018/02) (1) &lt;/li&gt; 阅读排行 &lt;li&gt; &lt;a href=&quot;https&amp;#58;//blog.csdn.net/programmer_yf/article/details/79347186&quot; title=&quot;Python爬虫框架scrapy抓取旅行家网所有游记！从此出游不发愁！&quot;&gt; Python爬虫框架scrapy抓取旅行家网所有游记！从此出游不发愁！ &lt;/a&gt; (32) &lt;/li&gt; &lt;li&gt; &lt;button class=&quot;left-fixed-btn btn-like csdn-tracking-statistics tracking-click&quot; data-mod=&quot;popu_373&quot; target=&quot;_self&quot; title=&quot;点赞&quot;&gt; &amp;#91;&lt;i class=&quot;icon iconfont icon-dianzan&quot;&gt;&lt;/i&gt;&amp;#93;(javascript&amp;#58;void(0);) &amp;#91;0&amp;#93;(javascript&amp;#58;void(0);) &lt;/button&gt; &lt;/li&gt; &lt;li&gt; &lt;button class=&quot;left-fixed-btn csdn-tracking-statistics tracking-click btn-collect&quot; data-mod=&quot;popu_374&quot; target=&quot;_self&quot; title=&quot;收藏&quot;&gt; &amp;#91;&lt;i class=&quot;icon iconfont icon-shoucang&quot;&gt;&lt;/i&gt;&amp;#93;(javascript&amp;#58;void(0);) &lt;/button&gt; &lt;/li&gt; &lt;li&gt; &lt;button class=&quot;left-fixed-btn btn-pinglun csdn-tracking-statistics tracking-click&quot; data-mod=&quot;popu_544&quot; title=&quot;评论&quot;&gt; &amp;#91;&lt;i class=&quot;icon iconfont icon-pinglun&quot;&gt;&lt;/i&gt;&amp;#93;(javascript&amp;#58;void(0);) &lt;/button&gt; &lt;/li&gt; &lt;li class=&quot;bdsharebuttonbox csdn-tracking-statistics tracking-click&quot; data-mod=&quot;popu_172&quot;&gt; &lt;a class=&quot;bds_tsina outside left-fixed-btn&quot; data-cmd=&quot;tsina&quot; title=&quot;分享到新浪微博&quot;&gt;&lt;/a&gt; &lt;i class=&quot;icon iconfont icon-xinlang&quot;&gt;&lt;/i&gt; &lt;/li&gt; &lt;li class=&quot;bdsharebuttonbox csdn-tracking-statistics tracking-click&quot; data-mod=&quot;popu_172&quot;&gt; &lt;a class=&quot;bds_weixin outside left-fixed-btn&quot; data-cmd=&quot;weixin&quot; title=&quot;分享到微信&quot;&gt;&lt;/a&gt; &lt;i class=&quot;icon iconfont icon-weixin&quot;&gt;&lt;/i&gt; &lt;/li&gt; &lt;li class=&quot;bdsharebuttonbox csdn-tracking-statistics tracking-click&quot; data-mod=&quot;popu_172&quot;&gt; &lt;a class=&quot;bds_qzone outside left-fixed-btn&quot; data-cmd=&quot;qzone&quot; title=&quot;分享到QQ空间&quot;&gt;&lt;/a&gt; &lt;i class=&quot;icon iconfont icon-QQ&quot;&gt;&lt;/i&gt; &lt;/li&gt; 转载来源：Python爬虫框架scrapy抓取旅行家网所有游记！从此出游不发愁！ - CSDN博客]]></content>
  </entry>
  <entry>
    <title><![CDATA[在steem上赚钱的模式有哪些？ - 简书]]></title>
    <url>%2F2018%2F32842c40%2F</url>
    <content type="text"><![CDATA[在steem上赚钱的模式有哪些？ - 简书 转载来源：在steem上赚钱的模式有哪些？ - 简书]]></content>
  </entry>
  <entry>
    <title><![CDATA[【应用】基于IPFS和GeoHash构建具有地理位置价值服务的DDApp（理论篇） – ipfs]]></title>
    <url>%2F2018%2F1d38106f%2F</url>
    <content type="text"><![CDATA[【应用】基于IPFS和GeoHash构建具有地理位置价值服务的DDApp（理论篇） – ipfs 转载来源：【应用】基于IPFS和GeoHash构建具有地理位置价值服务的DDApp（理论篇） – ipfs]]></content>
  </entry>
  <entry>
    <title><![CDATA[一步步教你开发、部署第一个去中心化应用 - 宠物商店 - Tiny熊 - 博客园]]></title>
    <url>%2F2018%2F5e4d58ac%2F</url>
    <content type="text"><![CDATA[一步步教你开发、部署第一个去中心化应用 - 宠物商店 - Tiny熊 - 博客园 转载来源：一步步教你开发、部署第一个去中心化应用 - 宠物商店 - Tiny熊 - 博客园]]></content>
  </entry>
  <entry>
    <title><![CDATA[IPFS搭建分布式文件系统 - 访问控制 - 秦鹏飞 - 博客园]]></title>
    <url>%2F2018%2Fd1edad43%2F</url>
    <content type="text"><![CDATA[IPFS搭建分布式文件系统 - 访问控制 - 秦鹏飞 - 博客园 转载来源：IPFS搭建分布式文件系统 - 访问控制 - 秦鹏飞 - 博客园]]></content>
  </entry>
  <entry>
    <title><![CDATA[IPFS挖矿实战演习之Storj – ipfs]]></title>
    <url>%2F2018%2F23e13995%2F</url>
    <content type="text"><![CDATA[IPFS挖矿实战演习之Storj – ipfs 转载来源：IPFS挖矿实战演习之Storj – ipfs]]></content>
  </entry>
  <entry>
    <title><![CDATA[爱奇艺正式递交IPO申请：小米持股8.4%股权 为第二大股东]]></title>
    <url>%2F2018%2F1e605a41%2F</url>
    <content type="text"><![CDATA[雷帝网 雷建平 2月28日报道 爱奇艺今日正式递交招股书，招股书显示，爱奇艺已申请在纳斯达克证券市场挂牌交易，证券代码为“IQ”。 爱奇艺拟募集资金15亿美元，此次上市募集的资金主要目的是加强品牌认知度，吸引并留存优秀员工，为他们提供股权激励，并获得更多资金。 具体如下：50%用于扩展和加强内容，10%用于加强技术，40%为运营资金及其他公司事务，此外，将运用一部分募集资金收购及购买产品、服务、科技。 爱奇艺会员数达到5080万 招股书显示，爱奇艺2017年总营收为173.784亿（26.710亿美元），较2016年的112.374亿增长54.6%。爱奇艺2015年的营收为53.186亿元。 其中，爱奇艺2017年会员收入为65.360亿（10.046亿美元），较2016年的37.622亿增长73.7%。爱奇艺2015年的会员服务收入为9.96亿。 爱奇艺的会员服务收入占总收入百分比从2015年的18.7%上升至2016年的33.5%，并在2017年进一步上升至37.6%。 从2011年起，爱奇艺开始探索视频会员服务的广大市场空间。2016年6月，爱奇艺宣布有效VIP会员数突破2000万。 2017年8月初百度公布2017年第二季度财报，披露爱奇艺会员数超过3000万。 而此次招股书披露，截至2017年12月31日，爱奇艺付费会员数为5080万，其会员业务规模增长幅度接近Netflix2017年全年2400万的新增会员数量。 爱奇艺的在线广告收入从2015年的33.999亿增长至2016年的56.504亿，同比增长66.2%，2017年为81.589亿（12.540亿美元），同比增长44.4%。 当前，中国在线视频平台主要靠会员服务和网络广告创造收入。爱奇艺收入结构的变化，也说明通过衍生产品变现预计将是另一个日益重要的收入来源。 以往严重依赖网络广告收入的在线视频产业，正在向一个更均衡的多元化创收模式转变。 据美国电影协会 统计，2016 年北美电影票房总收入为 114亿美元，而同期 Netflix 国内会员服务收入约为 51 亿美元。 这说明会员收入会在爱奇艺收入中的比例进一步的提升。 根据介绍，为了满足特别是用户对长尾内容的兴趣，爱奇艺向数千家专业内容提供商授权引进内容，并建立了庞大且多元化的专业制作内容库。 截至2017年12月31日，爱奇艺的内容库共拥有70,000 多部网络剧集、综艺节目、电影、儿童节目、纪录片、动画片、体育赛事和其他各种类型的节目，涵盖 30 多个内容类别。 李彦宏曾表示爱奇艺亏损比对手要少 爱奇艺2015年、2016年、2017年净亏分别为25.75亿、30.74亿和37.369亿（5.744亿美元），爱奇艺这三年的净亏损率分别为-48%、-27%、-22%。 可以看出，与收入的高速增长相比，爱奇艺的亏损增长控制得较好，2016年与2017年的净亏损增长幅度均在20%左右、大幅低于总收入的增速。 对于爱奇艺亏损的现状，百度CEO李彦宏在百度电话会议上有一个说法：爱奇艺依然处于市场领导者地位，在日活跃用户数，平均观看时长，付费用户数和盈利水平方面都是第一。 李彦宏说，“虽然爱奇艺还没有取得盈利，我们的亏损要比竞争对手要少。” 实际上，大型视频网站亏损是行业现状，优酷土豆、腾讯视频也不能避免。 根据阿里财报，阿里大文娱2017年第四季亏损38.28亿，阿里大文娱包括优酷土豆、UC、阿里影业等多个版块，但显而易见的亏损大头是优酷土豆。 爱奇艺归属普通股东的净收益从亏损变盈利 爱奇艺2015年归属于普通股东净收益-49.17亿、2016年为-79.49亿，2017年为9.72亿元。 之所以爱奇艺2017年归属于普通股东净收益为正，主要是计入了可赎回可转换优先股增值50.73亿元，又扣除了B轮优先股偿清及再发行3.63亿元。 而在2015年，爱奇艺可赎回可转换优先股增值为-23.42亿元，2016年，爱奇艺可赎回可转换优先股增值为为-48.74亿元。 可赎回优先股是指是指在发行后一定时期可按特定的赎买价格由发行公司收回的优先股票；可转换优先股是指发行后，在一定条件下允许持有者将它转换成普通股或其他种类优先股。 在新上市的公司中， 如果此前的认股权证没有转换为普通股，则认股权证公允价值变动将会使得公司净利润的产生较大波动。 爱奇艺Q4营收48.17亿元 同比增长53% 爱奇艺2017年第四季度营收为48.17亿元，较上一季度小幅下降3%，但较上年同期增长53%，爱奇艺2016年第四季度营收为31.55亿，也低于其上个季度。 爱奇艺2017年第四季度会员收入为19.29亿元，同比增长53%，较上一季度增长14%，占总收入的比例达到40.1%。 爱奇艺2017年第四季度净亏损为6.1亿元，较上一季度亏损10.5亿元下降35%，较上年同期亏损9.47亿元下降了35%。 爱奇艺2017年第四季度的净利润率为-12.7%。由于爱奇艺的营收增长幅度要大幅领先于爱奇艺的亏损幅度，这使得近年来爱奇艺的净亏损率逐渐在收窄。 百度持有爱奇艺近70%股权 百度当前持有爱奇艺69.6%股权，为爱奇艺最大股东，爱奇艺创始人、CEO龚宇持股为1.8%。 2014年11月，小米和顺为资本联合宣布，双方以18亿元（3亿美元）入股爱奇艺，爱奇艺大股东百度也追加对爱奇艺的投资。 此次小米持股也披露出来，持有爱奇艺8.4%股权。 2017年2月，爱奇艺完成15. 3 亿美元可转债认购，参与可转债认购的除百度外，还有高瓴资本、博裕资本、润良泰基金、IDG、光际资本、红杉等，这笔可转债已转化为爱奇艺股权。 当前，百度CEO李彦宏、集团总裁、COO陆奇、小米联合创始人、小米电视负责人王川在爱奇艺董事会。 爱奇艺还于2018 年1月19日与百度签署了主业务合作协议。 根据主业务合作协议，爱奇艺与百度一致同意在包括但不限于人工智能、智能设备/DuerOS（百度开发的对话式人工智能系统和开发平台）、云服务、在线广告、互联网流量、数据和内容领域开展相互合作，并在合作领域内相互将对方作为最优先的战略伙伴。 根据主业务合作协议，百度将为爱奇艺提供 6.5 亿（9990 万美元）的贷款，在授予之日届满五周年时到期。爱奇艺在2018年1月19日与百度就该贷款签署了贷款协议。该贷款免息。 2018年2月12日，爱奇艺还与百度的全资子公司百度控股有限公司签署一项股份购买协议，根据该协议，爱奇艺将向百度控股有限公司累计发行36,860,691股B类普通股。该项交易预计在2018年5月31日之前交割。 作为发行该股份的对价并受制于股份购买协议规定的条件，百度将针对其及其附属机构在线电影票和演出门票业务，对爱奇艺进行用户流量的导流、技术支持、授予爱奇艺特定域名和特定知识产权许可等。 随着爱奇艺递交上市招股书，意味着在经历长达8年的奋斗之后，爱奇艺创始人、CEO龚宇和百度将成为赢家，爱奇艺迈入新的里程碑，2018年，中国互联网还将迎来一个上市潮。 ————————————————— 雷帝触网由资深媒体人雷建平创办，其为头条签约作者，若转载请写明来源。 转载来源：爱奇艺正式递交IPO申请：小米持股8.4%股权 为第二大股东]]></content>
      <categories>
        <category>财经</category>
      </categories>
      <tags>
        <tag>IPO</tag>
        <tag>爱奇艺</tag>
        <tag>小米科技</tag>
        <tag>纳斯达克</tag>
        <tag>阿里影业</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何设计区块链项目的通证（token）模型]]></title>
    <url>%2F2018%2F06001821%2F</url>
    <content type="text"><![CDATA[2018年将是区块链的认知元年。过去一个多月，我自己对区块链与Token模式的认知发生了几次升级，不敢说认知有多深，但是想明白了之后我已经准备All in了。 这篇文章谈一谈我对Token模式的1.0思考。 Token是什么区块链最重要的应用就是价值在互联网上直接流通。通过实体或虚拟资产的Token化，将资产上链，实现资产的液化，能直接通过网络来跨国界、短时差、低成本进行资产交易与转移。 很多人把Token译为“代币”，我更认同元道先生翻译的版本——“通证”。我特别不愿意用代币这个词，因为不是所有Token都具有货币的属性，当然具有货币属性的Token也不见得仅仅只有货币一种属性。Token是一种可流通的加密数字权益证明。这意味着什么？意味着现实世界的各种权益证明（股权、债券、积分、票据等）都可以Token化，放到数字世界里去流通，这件事的想象空间太大了。 虽然已经有一大批先驱们都已经通过Token发行赚的盆满钵满了，但是Token经济的普及依然阻力重重，很多人并没有理解Token的真正价值。 ICO市场Token众生相ICO市场，就是一个江湖。门派林立，各揣心思，牛鬼蛇神，乱象丛生。翻阅了近百份白皮书，体验过若干Token交易之后，我有一个初步的总结。Token项目的三个段位、九种类型（请原谅我还是称为“币”了）。 段位一：垃圾Token 1、空气币 这类Token，完全没有映射到真实资产，存在故意欺诈的行为，其发行Token然后ICO的动机，完全就是来骗钱的，根本没有考虑生态的健康成长。在韭菜成堆、没有监管的市场，简直是这些骗子套利的天堂，各种CX币层出不穷。不过，这个注定不会持久。这是监管首先要打击的对象。 2、鸡肋币 这类Token，虽然映射了资产，但是却是毫无意义的资产，这些资产要么没有升值空间，要么没有流通能力，也注定了生态不可能生长起来。发行这种Token去ICO的组织，可能无意欺诈，但是跟欺诈也没什么区别。 3、侏儒币 这类Token，多是因为生态经济模型设计不科学或者团队能力不支撑，导致生态模式难以为继。即使成功ICO了，也不代表有未来，生态没有未来，Token怎会有长远的价值？比如，激励机制不合理、比如应用场景缺乏，都会使得生态成长乏力。 处在这个段位的Token，是没有投资价值的。但是当前市场上90%的项目都在这个阵营里。希望各路参与者能够擦亮眼睛。 段位二：普通Token 4、积分币 这种Token，本质上就是企业内部发行的积分，你说有价值么？对于使用者而言，有一点价值，至少可以积分换购。对于投资者而言呢？能升值么？升值的逻辑在哪？很多Token发行者根本来不及思考这些问题，却发现这样居然也能被疯抢？就会产生“ICO融资简直太爽了”这种错觉，这只是短暂的趋势红利而已。你的积分，换成Token，如果不去与其他资产打通来置换流动性，真的没什么投资价值，只是新瓶装旧酒而已。 5、会员币 这种Token，类似会员卡。我们大多数人去理发店都会办一张卡，新开的理发店店员会跟你说，现在办卡充值1000抵1200，你一盘算，还挺划算，反正你每个月要理一次，而且这个店开在你们楼下估计也不会轻易跑路，你就办了个理发卡。很多Token，指向的其实都是系统的使用权，只不过我提前预售这个使用权，所以才会有折扣（这个折扣再低也不会低于成本价）。那么，你觉得这种Token的投资价值如何？大多数的产品或者服务的使用权是没有稀缺性的，这种Token的上涨空间是不是很容易计算？你愿意接受吗？ 当然，这里面也有极端一点的例子。比如茅台，茅台酒如果发行产品Token（总量与动态产能挂钩），我相信很多人还是非常愿意持有的。这种Token就有一定的投资价值。 这种Token，对多数消费者而言，是有吸引力的。但是对于投资者而言，就不见得是最佳选择了。 6、分红币 这种Token，除了给到生态使用者类似会员卡优惠的福利之外，还会拿出一部分利润给生态参与者来分享，参与者购买Token，置换的是“使用权+分红权”。某中心化交易所就是这种设计逻辑。坦白来说，这种设计逻辑，我认为不是最理想的机制。 使用权Token的升值逻辑前面已经讲过。给到使用者以更优惠的使用权（比如预充值给折扣），这一点也无可厚非。 我们说一下利润回购这个逻辑。承诺利润回购或者分红的，估计是希望给到发行的Token以升值预期，但是，消费者真的相信股东会和董事会控制之下中心化机构的“利润回购”？很多不透明的因素在里面，很难让人信服。 而且，即使就是“分红权”，那也不应该是阳光普照，一定要奖励生态的关键建设者。华为的虚拟受限股也并没有奖励到每个人，以奋斗者为本嘛。 处在这个段位的Token，你觉得值得投吗？我觉得大部分的长期投资价值都不大（投资价值不大不意味着其Token没有使用价值，这是两码事），极个别除外（使用权具有稀缺性）。当前市场上9%的Token是在这个阵营。 段位三：价值Token 到了这个段位的Token，才是真正具有投资价值的。这也是区块链与Token经济学与生产关系真正融合的部分。什么叫生产关系？百科上是这么说的：生产关系，人们在物质资料的生产过程中形成的社会关系。生产资料所有制的形式是最基本的，起决定作用的生产关系。生产资料所有制是指人们在生产资料所有、占有、支配和使用等方面所结成的经济关系。 所以，真正有威力的Token，一定触及到深层次的生产关系。生产关系的关键就是“生产资料所有制”，必然会涉及到股权，只是这个东西又不是传统概念里股份公司制度之下的股权。 就像当年股份制公司诞生一样，人们也很难用当时的词语来准确描述它。Token也一样。人们现在很难用一个合适的词语来描述Token到底代表什么。很多人说这些Token非债非股，确实Token不是传统意义上的债权和股权，但是其通过Token形式把债权和股权投射进来了。不然，为什么会升值？那些没有将这些权益投射进来的Token，长期看也是没有升值空间的。 有投资价值的Token，至少是三权合一 第一，物权属性，代表了使用权，可交付产品或服务 第二，货币属性，可流通，至少在生态系统内是硬通货 第三，股权属性，可增值，长期收益可期，升值空间较大 如果非要排序的话，股权属性第一，物权属性第二，货币属性第三。 以太坊，就是典型的例子。以太坊可以被视作区块链世界类似于Windows和Android的底层操作系统。以太坊之上，会搭建各种各样的应用，这些应用在执行任务、提供服务时，需要调用以太坊底层的计算资源——这些都不是免费的。调用资源应用的用户需要支付的 “货币”就是代币“以太币”（Ether）。 你拥有了“以太币”，意味着你拥有这个系统的使用权；同时因为这个生态物种足够多样，你需要通过“以太币”这个交易媒介跟别人进行交换生产资料，这时“以太币”就有了货币属性，同时，“以太币”本身总量不是无穷无尽的，总量是相对有限的，那么在以太坊上搭建的应用越多，那应用所调用的资源也就越多，从而需要支付的以太币需求也就越大，这会使得在市场上流通的以太币价格越来越高。 Token的价值，长期一定取决其生态成长能力。 随着监管趋严，垃圾Token越没有市场了。良币终会驱逐劣币。没有价值依托的Token，很快就会被淘汰。而Token的价值，长期取决于其生态的成长潜力。 Token模式的组织，是天然的生态型组织（这个我多少有点发言权，也不枉几年对平台生态型商业模式的探索）。区块链时代的生态组织，大致可以分成这几种类型。一种是底层的技术生态，一种是中间层的商业生态，还有一种是应用层的社群生态。这三个领域都有诞生生态型组织的潜力，也意味着这三类组织发行的Token将具有较强的成长性。 （以下排名不分先后） 7、底层技术生态Token 诸多公有链，都在做这个领域的事。这也是最迫切需要被突破的，有了成熟的技术基础设施，区块链应用才得以广泛普及。 这类的Token也不少了，除了以太坊，还有EOS、NEO、Qtum、IOTA等等。现在处于军阀混战时期，远未到一统江山的时候。 虽然这种Token值得投，但是哪个值得投？或者值得长期持有？这还真是一个问题。 有人说交易所也是一个不错的阶段性机会，的确，但是交易所的Token有多大价值，取决于其将多少权益投射到这个Token里。不是每个交易所的Token都有投资价值的。 8、中层商业生态Token 这一类Token，探索的人有不少，但是真正搞明白的不多。大家都还是拘泥于传统互联网时代的商业模式，做一个媒体，或者一个工具，思维没有跳出来。思考一下，如果做一个区块链时代的阿里巴巴，应该怎么做？沿着旧地图，肯定找不到新大陆。本人要启动的创业项目就在这个领域，后面找时间专门分析一下这个领域的机会。 9、社群垂直生态Token 这一类Token，也有一定的投资价值。Token是非常好的社群商业连接器和润滑剂，传统的社群没能运转的很大因素是机制问题，Token恰好完美解决这个问题。 处在这个段位的Token，是具有长期投资价值的（对于价值投资者）。但到底哪个Token是你的菜，就看你的洞察能力了。当前市场上只有1%的Token处于这个阵营。 价值型Token该怎样判断？只有价值型Token才具备长期投资价值。那么该如何拥抱价值型Token呢？ 第一，如何判断其生态的远期成长能力？ 1、你这个新生态在解决什么问题？ 能否清晰定义原来存在的问题？关键的痛点？引入新生态之后能够带来什么改观？ 这个与创业要思考用户和痛点的逻辑是一样的。如果你没有在解决问题，区块链再神也救不了你。 只有一个提醒，思考生态的视角不要局限于商业，还要着眼于社会问题。因为Token生态的本质是一个社群经济体。 2、生态可能长成的规模有多大？ 再直白一点说，就是你这个生态未来会有多少人使用？使用的频次如何？ 区块链估值体系将从追求利润变成追求用户规模与互动频次。所以你的生态潜在规模不够的话，也会反过来影响到你Token的价值。 3、生态模型设计是否合理？ Token经济设计的核心理念，是把原来体系中耗散的交易成本集约起来，用技术手段把收益分散到体系内每一个参与者，使系统整体摩擦力不断下降，从而Token内在价值不断上升。如果Token机制设置不合理，是很难调动参与者的积极性的，参与者没有被调动起来，生态也不可能成长起来。 这里面重点提一句，关于Token的发行比例。很多Token发行方会一次性地把50%甚至更多的Token给到“黄牛型”的投资者，这势必会抬高生态真正使用者的参与成本，进而会影响到生态的建设。如果你是为了生态建设的话，着急融那么多钱干嘛呢？你的生态建设在这个阶段真的需要那么多钱吗？ 4、团队是否匹配？ 一是看团队的能力构成：有没有优秀的生态设计师？有没有优秀的技术开发团队？有没有优秀的运营团队？我自己的团队就是按照这个逻辑架构的。我算是生态设计师，但同时也需要区块链的技术人才，如果有靠谱的技术大牛欢迎看过来，我准备了好多橄榄树抛给你。 二是看团队的All in程度：我个人认为，Token模式下的创始团队的所有收益都应该与生态未来的预期成长有关，只能唯一体现在Token的升值上（因为已经给到你10%或者20%的一次性奖励了），如果不是这样，大家并没有构建成一个真正的利益共同体。 第二，如何衡量生态Token当下的价值空间？ 前两天一位和君老同学可月提了一个衡量Token价值的公式，我觉得很好，这里借用一下。 在Token模式当中，影响力决定组织/工具价值。交易场景下影响力直观的衡量方式，是流通价值，可以通过交易规模和流通频次计算当前价值=当前交易规模/1单位Token在当前的交易频次。 比如当前基于此Token的交易规模为￥100万；交易了1万次，每次1单位Token，那1单位Token在当前的交易频次就是1万次，当前价值就是￥100万/1万次=￥100。 极值价值=最大运用情况下的交易规模/1单位Token在此情况的交易频次。比如Token最大运用情况下交易规模为￥1亿；交易了100万次，每次0.01Token，那1单位Token在当前的交易频次就是1万次，当前价值就是￥1亿/1万次=￥1万。 当前预期价值是极值价值回溯到现在的价值，假设从当前到极限需要n年，每年经济增长率为r，那么，当前预期价值=极值价值/r的n次方。 价格应该是围绕当前预期价值上下波动的： Token的投资和股权投资都应当遵循价值导向的原则，只不过衡量价值的标准不一样了。一个是基于利润体现赚钱能力，一个是基于交易价值体现影响力。 不要小看这一变化，这种模型，让零和博弈变成了合作共赢！所有人的收益都与未来绑定，这才是真正的利益共同体。Token经济的核心，就是共赢！ ICO的监管重拳来袭，对于那些真正想借助区块链做事情的人，其实并没有什么影响，反而是好事！技术也好，模式也好，只是一种工具，用来做什么，还是背后的人。到最后，都是价值观的较量。 以上为一些不成熟思考，仅供交流。欢迎价值认同的各路伙伴与我联系，一起来探索区块链与Token经济，一起去推动更理想的社会图景！ 如需更多帮助，可私信小编（5年股龄，3年期龄，半年币龄），私信步骤：点击“区块链链长”头像，在其右边有一“私信”按钮，点击私信即可，如下图： 转载来源：如何设计区块链项目的通证（token）模型]]></content>
      <categories>
        <category>设计</category>
      </categories>
      <tags>
        <tag>投资</tag>
        <tag>经济</tag>
        <tag>设计师</tag>
        <tag>创业</tag>
        <tag>威力</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[来来来！教你自己发行空气币]]></title>
    <url>%2F2018%2Fe5349c69%2F</url>
    <content type="text"><![CDATA[正式开始前，需要准备： 1、科学上网的工具 2、chrome浏览器 第一步：在chrome浏览器上安装 metamask 的插件。 点击以下网址进行安装，点击添加至chrome。 https&#58;//chrome.google.com/webstore/detail/metamask/nkbihfbeogaeaoehlefnkodbefgpgknn 或者进入官网下载安装chrome插件： https&#58;//metamask.io 点击添加扩展程序 注：metamask除了是一个简单的钱包外，它可以使得Chrome浏览器和以太坊智能合约互动。 第二步：设置账号，点击chrome浏览器右上角的logo图标，同意隐私条款和使用条款。 输入并确认密码，创建账号 一定要把助记词保存好！ 账号创建好，可以向自己的地址转账，扫描二维码比较方便，也不容易出错，可以转0.02枚eth，用于发行代币 第三步：发行代币 进入下面网站： http&#58;//tokenfactory.surge.sh/#/factory 我以AIR 空气币 做范例，欲发行100W枚，第三栏数字 2 代表着后两位0的前边小数点，比如1E枚，数字填2，实际发行100W枚，代币名称：AIR。点击 Create Token： 如图所示：矿工费默认0.02ETH，我们可以调低到0.01ETH，降低费用，但执行速度会有些慢，耐心等待即可。矿工费尽量提前转进去，如果矿工费足够，点击绿色按钮，会显示：SUBMIT。 你可以在： https&#58;//etherscan.io 查询你的 metamask eth 地址找到你刚才的交易。找到 contract creation 这一笔。 搜索后出现： 点击0x一栏： 复制上图0x开头地址，将合约地址贴进去： http&#58;//tokenfactory.surge.sh/#/tokensearch 点击Go to Token,输入你想要转账的地址： 提前转一些矿工费，够用就好，坐等钱包入账。这样我的AIR空气币就算发行成功了。 转载来源：来来来！教你自己发行空气币]]></content>
      <categories>
        <category>其它</category>
      </categories>
      <tags>
        <tag>Chrome</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[陈春花：2018 计划怎么定，之后怎么做？一文讲透]]></title>
    <url>%2F2018%2F04c2107a%2F</url>
    <content type="text"><![CDATA[陈春花：2018 计划怎么定，之后怎么做？一文讲透 转载来源：陈春花：2018 计划怎么定，之后怎么做？一文讲透]]></content>
      <tags>
        <tag>春暖花开</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[区块链世界里的存储技术—IPFS - 简书]]></title>
    <url>%2F2018%2F27f84757%2F</url>
    <content type="text"><![CDATA[区块链世界里的存储技术—IPFS - 简书 转载来源：区块链世界里的存储技术—IPFS - 简书]]></content>
  </entry>
  <entry>
    <title><![CDATA[一夜身价暴涨千倍，程序员如何发布自己的 ICO？]]></title>
    <url>%2F2018%2F416073bf%2F</url>
    <content type="text"><![CDATA[ERC20 Token 合约开发现在我们的项目目录大概是这个样子： contracts/- Migrations.solMigrations.sol migrations/ 1_initial_migration.jstest/ package.json truffle-config.js 或 truffle.js 我们在编写智能合约时，需要在 contracts 目录下新建相应的智能合约文件。 在以太坊开发智能合约的编程语言叫做 Solidity (https&#58;//goo.gl/hCHh3w)。它是一种在语法上非常类似 JavaScript 的语言，其后缀名为 .sol 。 例如在这里我们可以创建一个名为 GitCoin.sol 的文件，命令如下。 // *nixtouch GitCoin.sol// wincopy NUL &gt; GitCoin.sol ERC20（Ethereum Request for Comments NO.20）(https&#58;//goo.gl/aX4x5F) 是官方发行的 token 标准。 如果你希望你发布的 token 能够在以太坊网络上流通、上市交易所、支持以太坊钱包，在开发 token 的合约时就必须遵从这一规范。 ERC20 规定了合约中的一系列变量、方法、事件，你可以参考官网教程 Create your own CRYPTO-CURRENCY with Ethereum (https&#58;//www.ethereum.org/token) 当中的示例代码： pragma solidity ^0.4.16; interface tokenRecipient &amp;#123 function receiveApproval(address _from, uint256 _value, address _token, bytes _extraData) public; &amp;#125contract TokenERC20 &amp;#123 // Public variables of the token string public name; string public symbol; uint8 public decimals = 18; // 18 decimals is the strongly suggested default, avoid changing it uint256 public totalSupply; // This creates an array with all balances mapping (address =&gt; uint256) public balanceOf; mapping (address =&gt; mapping (address =&gt; uint256)) public allowance; // This generates a public event on the blockchain that will notify clients event Transfer(address indexed from, address indexed to, uint256 value); // This notifies clients about the amount burnt event Burn(address indexed from, uint256 value); /** * Constrctor function * * Initializes contract with initial supply tokens to the creator of the contract */ function TokenERC20( uint256 initialSupply, string tokenName, string tokenSymbol ) public &amp;#123 totalSupply = initialSupply * 10 uint256(decimals); // Update total supply with the decimal amount balanceOf&#91;msg.sender&#93; = totalSupply; // Give the creator all initial tokens name = tokenName; // Set the name for display purposes symbol = tokenSymbol; // Set the symbol for display purposes &amp;#125 / * Internal transfer, only can be called by this contract */ function _transfer(address _from, address _to, uint _value) internal &amp;#123 // Prevent transfer to 0x0 address. Use burn() instead require(_to != 0x0); // Check if the sender has enough require(balanceOf&#91;_from&#93; &gt;= _value); // Check for overflows require(balanceOf&#91;_to&#93; + _value &gt; balanceOf&#91;_to&#93;); // Save this for an assertion in the future uint previousBalances = balanceOf&#91;_from&#93; + balanceOf&#91;_to&#93;; // Subtract from the sender balanceOf&#91;_from&#93; -= _value; // Add the same to the recipient balanceOf&#91;_to&#93; += _value; Transfer(_from, _to, _value); // Asserts are used to use static analysis to find bugs in your code. They should never fail assert(balanceOf&#91;_from&#93; + balanceOf&#91;_to&#93; == previousBalances); &amp;#125 /** * Transfer tokens * * Send `_value` tokens to `_to` from your account * * &amp;#64;param _to The address of the recipient * &amp;#64;param _value the amount to send */ function transfer(address _to, uint256 _value) public &amp;#123 _transfer(msg.sender, _to, _value); &amp;#125 /** * Transfer tokens from other address * * Send `_value` tokens to `_to` on behalf of `_from` * * &amp;#64;param _from The address of the sender * &amp;#64;param _to The address of the recipient * &amp;#64;param _value the amount to send */ function transferFrom(address _from, address _to, uint256 _value) public returns (bool success) &amp;#123 require(_value &lt;= allowance&#91;_from&#93;&#91;msg.sender&#93;); // Check allowance allowance&#91;_from&#93;&#91;msg.sender&#93; -= _value; _transfer(_from, _to, _value); return true; &amp;#125 /** * Set allowance for other address * * Allows `_spender` to spend no more than `_value` tokens on your behalf * * &amp;#64;param _spender The address authorized to spend * &amp;#64;param _value the max amount they can spend */ function approve(address _spender, uint256 _value) public returns (bool success) &amp;#123 allowance&#91;msg.sender&#93;&#91;_spender&#93; = _value; return true; &amp;#125 /** * Set allowance for other address and notify * * Allows `_spender` to spend no more than `_value` tokens on your behalf, and then ping the contract about it * * &amp;#64;param _spender The address authorized to spend * &amp;#64;param _value the max amount they can spend * &amp;#64;param _extraData some extra information to send to the approved contract */ function approveAndCall(address _spender, uint256 _value, bytes _extraData) public returns (bool success) &amp;#123 tokenRecipient spender = tokenRecipient(_spender); if (approve(_spender, _value)) &amp;#123 spender.receiveApproval(msg.sender, _value, this, _extraData); return true; &amp;#125 &amp;#125 /** * Destroy tokens * * Remove `_value` tokens from the system irreversibly * * &amp;#64;param _value the amount of money to burn */ function burn(uint256 _value) public returns (bool success) &amp;#123 require(balanceOf&#91;msg.sender&#93; &gt;= _value); // Check if the sender has enough balanceOf&#91;msg.sender&#93; -= _value; // Subtract from the sender totalSupply -= _value; // Updates totalSupply Burn(msg.sender, _value); return true; &amp;#125 /** * Destroy tokens from other account * * Remove `_value` tokens from the system irreversibly on behalf of `_from`. * * &amp;#64;param _from the address of the sender * &amp;#64;param _value the amount of money to burn */ function burnFrom(address _from, uint256 _value) public returns (bool success) &amp;#123 require(balanceOf&#91;_from&#93; &gt;= _value); // Check if the targeted balance is enough require(_value &lt;= allowance&#91;_from&#93;&#91;msg.sender&#93;); // Check allowance balanceOf&#91;_from&#93; -= _value; // Subtract from the targeted balance allowance&#91;_from&#93;&#91;msg.sender&#93; -= _value; // Subtract from the sender’s allowance totalSupply -= _value; // Update totalSupply Burn(_from, _value); return true; &amp;#125&amp;#125 我只是想割韭菜而已，用得着写几百行代码吗？ 当然不必，这时我们就需要使用到智能合约开发框架 OpenZeppelin (https&#58;//openzeppelin.org)，安装命令如下。 npm install zeppelin-solidity –save GitCoin.sol 引入 OpenZeppelin，代码如下。 // 声明 solidity 编译版本pragma solidity ^0.4.18;// 引入框架为我们提供的编写好的 ERC20 Token 的代码import “zeppelin-solidity/contracts/token/StandardToken.sol”;// 通过 is 关键字继承 StandardTokencontract GitToken is StandardToken &amp;#123 string public name = “GitToken”; // Token 名称 string public symbol = “EGT”; // Token 标识 例如：ETH/EOS uint public decimals = 18; // 计量单位，和 ETH 保持一样就设置为 18 uint public INITIAL_SUPPLY = 10000 * (10 ** decimals); // 初始供应量 // 与 contract 同名的函数为本 contract 的构造方法，类似于 JavaScript 当中的 constructor function GitToken() &amp;#123 totalSupply = INITIAL_SUPPLY; // 设置初始供应量 balances&#91;msg.sender&#93; = INITIAL_SUPPLY; // 将所有初始 token 都存入 contract 创建者的余额 &amp;#125&amp;#125 好了，至此一个可以用来交易的符合 ERC20 标准的 token 就编写完毕了。 就这么简单？就这么简单！当然智能合约的功能不止如此，token 中可以玩转设计的地方也不止这些。 不过我们要稍微放在后面一些来讨论，接下来还是赶快着手 ICO 合约开发，为我们的项目募集资金吧。 ICO Crowdsale 合约开发同样，以太坊官网文档在教程 CROWDSALE Raising funds from friends without a third party (https&#58;//www.ethereum.org/crowdsale) 中也为我们提供了用来 crowdsale 做 ICO 募资的示例代码： pragma solidity ^0.4.18;/** interface 的概念和其他编程语言当中类似，在这里相当于我们可以通过传参引用之前发布的 token 合约 我们只需要使用其中的转账 transfer 方法，所以就只声明 transfer/interface token &amp;#123 function transfer(address receiver, uint amount);&amp;#125contract Crowdsale &amp;#123 // 这里是发布合约时需要传入的参数 address public beneficiary; // ICO 募资成功后的收款方 uint public fundingGoal; // 骗多少钱 uint public amountRaised; // 割到多少韭菜 uint public deadline; // 割到啥时候 / 卖多贵，即你的 token 与以太坊的汇率，你可以自己设定 注意到，ICO 当中 token 的价格是由合约发布方自行设定而不是市场决定的 也就是说你项目值多少钱你可以自己编/uint public price;token public tokenReward; // 你要卖的 tokenmapping(address =&gt; uint256) public balanceOf;bool fundingGoalReached = false; // 是否达标bool crowdsaleClosed = false; // 售卖是否结束/ 事件可以用来记录信息，每次调用事件方法时都能将相关信息存入区块链中 可以用作凭证，也可以在你的 Dapp 中查询使用这些数据/event GoalReached(address recipient, uint totalAmountRaised);event FundTransfer(address backer, uint amount, bool isContribution); / Constrctor function* Setup the owner/function Crowdsale(address ifSuccessfulSendTo,uint fundingGoalInEthers,uint durationInMinutes,uint etherCostOfEachToken,address addressOfTokenUsedAsReward) &amp;#123beneficiary = ifSuccessfulSendTo;fundingGoal = fundingGoalInEthers 1 ether;deadline = now + durationInMinutes 1 minutes;price = etherCostOfEachToken 1 ether;tokenReward = token(addressOfTokenUsedAsReward); // 传入已发布的 token 合约的地址来创建实例&amp;#125 /** Fallback function* payable 用来指明向合约付款时调用的方法*/function () payable &amp;#123 require(!crowdsaleClosed);uint amount = msg.value;balanceOf&#91;msg.sender&#93; += amount;amountRaised += amount;tokenReward.transfer(msg.sender, amount / price);FundTransfer(msg.sender, amount, true);&amp;#125 /** modifier 可以理解为其他语言中的装饰器或中间件 当通过其中定义的一些逻辑判断通过之后才会继续执行该方法 _ 表示继续执行之后的代码/modifier afterDeadline() &amp;#123 if (now &gt;= deadline) _; &amp;#125 / Check if goal was reached* Checks if the goal or time limit has been reached and ends the campaign*/function checkGoalReached() afterDeadline &amp;#123 if (amountRaised &gt;= fundingGoal)&amp;#123fundingGoalReached = true; GoalReached(beneficiary, amountRaised); &amp;#125crowdsaleClosed = true;&amp;#125 /** Withdraw the funds* Checks to see if goal or time limit has been reached, and if so, and the funding goal was reached, sends the entire amount to the beneficiary. If goal was not reached, each contributor can withdraw the amount they contributed.*/function safeWithdrawal() afterDeadline &amp;#123 if (!fundingGoalReached) &amp;#123uint amount = balanceOf&amp;#91;msg.sender&amp;#93;; balanceOf&amp;#91;msg.sender&amp;#93; = 0; if (amount &gt; 0) &amp;#123 if (msg.sender.send(amount)) &amp;#123 FundTransfer(msg.sender, amount, false); &amp;#125 else &amp;#123 balanceOf&amp;#91;msg.sender&amp;#93; = amount; &amp;#125 &amp;#125 &amp;#125 if (fundingGoalReached &amp;&amp; beneficiary == msg.sender) &amp;#123 if (beneficiary.send(amountRaised)) &amp;#123 FundTransfer(beneficiary, amountRaised, false); &amp;#125 else &amp;#123 //If we fail to send the funds to beneficiary, unlock funders balance fundingGoalReached = false; &amp;#125 &amp;#125&amp;#125&amp;#125 至此我们的 ICO 合约也开发完毕了，基本上一行代码都没有写，只是改了几个参数，一个键盘上只有三个按键的程序员都能够完成这类智能合约的开发，没有比这更友好的编程体验了。 虽然 solidity 是一种非图灵完备的编程语言，但我们仍然能够用它编写许多逻辑。 上述的 ICO 示例代码写得算比较客气的一种，在最后的提款方法中，如果筹资达标，ICO 发布方则可以取走所有筹款，而如果未达标，参与者则能够取回自己的投资，由合约来持有所有款项。 但事实上，我们仍然可以随意修改其中的逻辑，看下面代码。 function () payable &amp;#123 require(!crowdsaleClosed); uint amount = msg.value; balanceOf&#91;msg.sender&#93; += amount; amountRaised += amount; tokenReward.transfer(msg.sender, amount / price); // 每次有人付款直接取走筹资 beneficiary.send(amountRaised); amountRaised = 0; FundTransfer(msg.sender, amount, true);&amp;#125// 删除剩余代码 补充说明与权限控制既然咱是铁了心来割韭菜的，如此简单的代码怎么能够满足咱的贪欲呢？一定要学比特币固定供给量吗？ 我是来卖 token 的呀，万一有一天卖完了怎么办，万一有人手里筹码比我自己都多了控盘怎么办，万一发的数量太多卖的不好怎么办？ 事实上解决这些问题的逻辑全部都可以写在智能合约里。 Ownable token 在我们的潜在观念里，区块链自有不可变属性。 这种不可变属性在一些狂热信徒的演绎当中变成了平权属性，甚至带有了共产主义色彩，仿佛拥抱区块链技术就能够为未来的人类文明带来希望，把人民从集权的手中解救出来。 然而事实上这种不可变性同样是两面的，它能够带来的也包括所有权的不可变性。 ERC20 标准只规定了我们的合约中应该包含哪些方法，而没有限制合约中不能出现哪些方法，因此在之前的基础上，我们还可以继续编写一些特殊的方法，赋予合约发布者一些管理员特权。 请看下面代码： contract Ownable &amp;#123 address public owner; function Ownable() public &amp;#123 owner = msg.sender; &amp;#125 // 通过 onlyOwner 我们可以限定一些方法只有所有者才能够调用 modifier onlyOwner &amp;#123 require(msg.sender == owner); _; &amp;#125 function transferOwnership(address newOwner) onlyOwner public &amp;#123 owner = newOwner; &amp;#125&amp;#125// 合约可以同时有多个继承contract GitToken is StandardToken, Ownable &amp;#123 … MintableToken 接下来我们来解决 token 不够卖的问题，万一我的 initial offer 卖断货了怎么办，万一我卖完一次还想卖怎么办？ 这时我们就需要把 token 编写成为 MintableToken，在我们想增发的时候就能增发，代码如下： // 用 onlyOwner 限定只有 token 的所有者才能够进行增发操作function mint(address _to, uint256 amount) onlyOwner public returns (bool) &amp;#123 totalSupply = totalSupply_.add(_amount); balances&#91;_to&#93; = balances&#91;_to&#93;.add(_amount); Mint(_to, _amount); Transfer(address(0), _to, _amount); return true;&amp;#125 BurnableToken 万一我们的 token 不小心发了太多，卖的时间久了贬值怎么办？ 当然是销毁了，可参照下面代码： /** Destroy tokens* Remove _value tokens from the system irreversibly* &#64;param _value the amount of money to burn*/function burn(uint256 _value) public returns (bool success) &amp;#123 require(balanceOf&#91;msg.sender&#93; &gt;= _value); // Check if the sender has enoughbalanceOf&#91;msg.sender&#93; -= _value; // Subtract from the sendertotalSupply -= _value; // Updates totalSupplyBurn(msg.sender, _value); return true;&amp;#125 万一有人手里的筹码太多，或者 token 被竞争对手买走了怎么办？没关系，我们还可以指定销毁某一账户中的 token，请看下面代码： /** Destroy tokens from other account* Remove _value tokens from the system irreversibly on behalf of _from.* &#64;param _from the address of the sender &#64;param _value the amount of money to burn*/function burnFrom(address _from, uint256 _value) public returns (bool success) &amp;#123 require(balanceOf&#91;_from&#93; &gt;= _value); // Check if the targeted balance is enoughrequire(_value &lt;= allowance&#91;_from&#93;&#91;msg.sender&#93;); // Check allowancebalanceOf&#91;_from&#93; -= _value; // Subtract from the targeted balanceallowance&#91;_from&#93;&#91;msg.sender&#93; -= _value; // Subtract from the sender’s allowancetotalSupply -= _value; // Update totalSupplyBurn(_from, _value); return true;&amp;#125 只要上述的方法全部都出现在合约里，我们发布的 token 就能够具备上述所有属性。 这样一来，不够的时候我们可以发钱，发多了可以销毁，我们成功创建了属于自己的一所中央银行，甚至看某人不爽还能够指定销毁其账户存款，这哪里是平权，简直是超级集权。 而事实上，在已发布的 ERC20 token 当中，例如排名第一的 EOS 的合约 (https&#58;//goo.gl/L2AmQP) 里也是存在类似方法的，如下所示。 function mint(uint128 wad) auth stoppable note &amp;#123 _balances&#91;msg.sender&#93; = add(_balances&#91;msg.sender&#93;, wad); _supply = add(_supply, wad);&amp;#125function burn(uint128 wad) auth stoppable note &amp;#123 _balances&#91;msg.sender&#93; = sub(_balances&#91;msg.sender&#93;, wad); _supply = sub(_supply, wad);&amp;#125 当然在其官方网站和白皮书中是标明了会发布多少 token，创始团队持有多少，投资人分配多少，公开发布多少，如何销毁等内容的。 但白皮书又不具备法律效力，token 的所有权也不在你手里，万一人家哪天想要跑路或者中途变卦岂是咱能拦得住的。 换个角度讲，假如你现在手里有一家可以印钱的公司，印多少就有多少，你印还是不印？ 通过这一部分内容的介绍，我只是想要证明，智能合约本身并不具备可无条件信任的特性，充其量就是一段没法改一直跑的程序而已。 你也可以在逻辑中加入管理员权限，token 的发布方并不比央行可信多少，只要所有者愿意可以随时进行修改。以太坊官方宣传的所谓 “trustless” 这一概念根本不成立。 没有第三方担保，没有法律法规的维护，仅凭智能合约本身你的投资得不到任何保证。智能合约的不可变性反而给割韭菜的一方提供了巨大的便利。 从前你看不惯某家公司还能够黑掉它的系统，获取管理员权限，如今所有程序都跑在区块链上，黑无可黑，集权永远都在合约发布者的手里。 讲到这里，希望你能理解这次分享的良苦用心，不要轻信任何 ICO 项目。 合约的发布及调试本地开发环境发布 合约开发完成之后，我们需要编译并发布合约至区块链网络中，只需要进行以下两步操作。 首先在 migrations 文件夹下新建 2-deploy-contract.js 文件，配置部署脚本如下。 // 引入我们编写的合约const GitCoin = artifacts.require(“./GitCoin.sol”)const GitCoinCrowdsale = artifacts.require(“./GitCoinCrowdsale.sol”)module.exports = function(deployer, network, accounts) &amp;#123 // 设定参数，此处的参数即使传入合约构造方法的参数，与你自己编写的合约保持一致 const ifSuccessfulSendTo = accounts&#91;0&#93; // 当前以太坊网络中的默认账户 const fundingGoalInEthers = 1000 const durationInMinutes = 36000000 const etherCostOfEachToken = 0.01 // 这里的 Promise 可以保证我们在发布完 token 合约之后再发布 ICO 合约，并将已发布 token 的地址作为参数传入 deployer.deploy(GitCoin).then(function() &amp;#123 return deployer.deploy(GitCoinCrowdsale, ifSuccessfulSendTo, fundingGoalInEthers, durationInMinutes, etherCostOfEachToken, GitCoin.address); &amp;#125);&#125; 接着在 truffle-config.js 或 truffle.js 中设置发布网络，脚本如下。 module.exports = &amp;#123 networks&#58; &amp;#123 development&#58; &amp;#123 host&#58; “127.0.0.1”, port&#58; 7545, // 与你本地的 ganache 设置保持一致 network_id&#58; “*” // Match any network id &amp;#125 &amp;#125&#125; 现在只需要开启 Ganache： 然后在命令行中输入： truffle compiletruffle migrate 你的合约就会顺利发布至测试网络中了。然后你可以输入： truffle console 这样就能够进入本地的命令行调试了： 所有的合约方法都是 Promise 对象truffle(development)&gt; GitCoinCrowdsale.deployed().then(inst=&gt;&amp;#123crowd=inst&amp;#125)truffle(development)&gt; GitCoin.deployed().then(inst=&gt;&amp;#123git=inst&amp;#125)truffle(development)&gt; crowd.sendTransaction(&amp;#123from&#58;web3.eth.accounts&#91;0&#93;,value&#58;web3.toWei(1, “ether”)&amp;#125)truffle(development)&gt; git.mint(web3.eth.accounts&#91;0&#93;,web3.toWei(100, “ether”)) 线上测试网络发布 以太坊网络分为测试网和主网，在正式发布主网之前，我们可以先发送到测试网络进行调试。 发布至以太坊网络也无需同步完整节点，我们可以使用 Infura 为我们提供的公共接口。 填写表单提交后，Infura 会为你提供专用的接口地址，然后我们只需要将网络地址填入到配置文件中，如下所示。 var HDWalletProvider = require(“truffle-hdwallet-provider”); // 在这里我们需要通过 js 调用以太坊钱包，通过 npm install truffle-hdwallet-provider 安装这个库var infura_apikey = “ubQWERwasd”; // infura 为你提供的 apikey 请与你申请到的 key 保持一致，此处仅为示例var mnemonic = “apple banana carray dog egg fault great”; // 你以太坊钱包的 mnemonic ，可以从 Metamask 当中导出，mnemonic 可以获取你钱包的所有访问权限，请妥善保存，在开发中切勿提交到 gitmodule.exports = &amp;#123 networks&#58; &amp;#123 development&#58; &amp;#123 host&#58; “127.0.0.1”, port&#58; 7545, network_id&#58; “*” &amp;#125, ropsten&#58; &amp;#123 provider&#58; function() &amp;#123 return new HDWalletProvider(mnemonic, “https&#58;//ropsten.infura.io/“+infura_apikey) &amp;#125, network_id&#58; 3, gas&#58; 3012388, gasPrice&#58; 30000000000 &amp;#125, main&#58; &amp;#123 provider&#58; function() &amp;#123 return new HDWalletProvider(mnemonic, “https&#58;//mainnet.infura.io/“+infura_apikey) &amp;#125, network_id&#58; 3, gas&#58; 3012388, gasPrice&#58; 1000000000 &amp;#125 &amp;#125&#125; 在以太坊网络中发布合约需要使用 ETH 支付矿工的 gas 费用，你可以在 Ethereum Ropsten Faucet (http&#58;//faucet.ropsten.be&#58;3001) 免费获取到用于 Ropsten 测试网络的 ETH。 由于网络环境的变化，不同的拥堵状况可能造成燃料费用和消耗的不同。 如果发布不成功，可以调整 gas/gasPrice 的数值，你可以通过 web3.getBlock(&#39;latest&#39;).gasLimit 这一数值判断当前网络的消耗。 在命令行输入如下命令： truffle migrate –network ropsten 通过 --network 设置发布的目标网络。 主网络发布 同理，在发布至主网络时，只需要执行如下命令。 truffle migrate –network main 但由于当前的以太坊网络的现实状况，如果设置燃料费太低，可能要等待数天后合约才会被网络确认，注意到我们编写的发布脚本是需要合约地址回调的。 介于这种状况，我们可以将 token 合约和 crowdsale 合约分开发布，只需要再新建 3-deploy-crowdsale.js 文件，脚本如下。 const LeekCoinCrowdsale = artifacts.require(“./GitCoinCrowdsale.sol”)module.exports = function(deployer, network, accounts) &amp;#123 const ifSuccessfulSendTo = accounts&#91;0&#93; const fundingGoalInEthers = 1000 const durationInMinutes = 36000 const etherCostOfEachToken = 0.01 const tokenAddress = ‘0x123456789ABCDFGHSDWDVC’ // 先单独发布 token 合约，上线成功后将其合约地址填在此处 deployer.deploy(GitCoinCrowdsale, ifSuccessfulSendTo, fundingGoalInEthers, durationInMinutes, etherCostOfEachToken, tokenAddress);&#125; 在发布至主网络时，可以分开两次进行，确保你设置的账户里有真实的 ETH 余额，注意设置好合理的 gas 数值，根据确认时间的长短，可能需要 0.08~1 ETH 不等。 上线合约验证无论是发布至以太坊的测试网络还是主网络，在发布完成之后都需要在 Etherscan (https&#58;//etherscan.io) 进行线上验证。 在 Etherscan 上打开你刚刚发布的合约地址，你可以看到如下内容： 点击 Verify And Publish 链接就可以进入验证页面： 在填写表单时有以下注意事项。 Compiler 选择最新版本；1. Optimization 选择 No。Optimization 选择 No。 虽然 solidity 支持 import 语法，但 Etherscan 对使用 import 进行开发的合约支持很鸡肋，目前它要求你需要把库文件也当作合约发布至网络才能够在表单中填写进行验证。 当然我们也可以选择手动把 import 库文件的内容手动复制粘贴到代码框里，注意要保留全部内容，包括 pragma 声明一行。 当然你也可以选择使用官方的 Remix (https&#58;//remix.ethereum.org/) 预先 concrete 你的合约文件，也可以安装 solidity compiler (https&#58;//goo.gl/aKsXxH) 在本地编译好再发布。 ICO 和 token 的合约如此简单，根本不需要这些玩意儿，所以此处不再赘述，感兴趣的同学可以自行研究。 Dapp 开发智能合约相当于我们的后端逻辑，以太坊的 EVM 就是我们的云服务器，Infura 为我们提供 API 接口，接下来我们就只需要给韭菜开发一个可以花钱消费的前端界面了。 ICO 项目的网站把握以下几个原则就好。 文字不要太多，页面要大片留白，简洁明了有现代感；1. 配色一定要深，加上动态几何图形，设计要有未来感；1. 开发团队全配齐，不是常春藤，没有硅谷背景的不要，一定要国际化；1. 各种站台大佬，海量媒体报道，一线互联网公司合作全放上去。配色一定要深，加上动态几何图形，设计要有未来感； 各种站台大佬，海量媒体报道，一线互联网公司合作全放上去。 言归正传，我们还是专注于技术。 web3.js 的使用web3.js (https&#58;//github.com/ethereum/web3.js) 为我们提供了一系列访问以太坊网络的 JavaScript 编程接口，完整的说明文档可以在 web3.js Doc (https&#58;//goo.gl/zp2yEQ) 中参阅。 我们一般通过如下脚本来初始化 web3 对象。 // 判断当前浏览器中有未注入 web3 对象if (typeof web3 !== ‘undefined’) &amp;#123 App.web3Provider = web3.currentProvider; web3 = new Web3(web3.currentProvider);&amp;#125 else &amp;#123 // 注意设置到你自己的 infura 地址 App.web3Provider = new Web3.providers.HttpProvider(‘https&#58;//ropsten.infura.io/ubQWERawsd’); web3 = new Web3(App.web3Provider);&amp;#125 Metamask 简介Metamask (https&#58;//metamask.io) 是一个浏览器插件，通过 Metamask 我们可以在浏览器中使用以太坊钱包，在访问 Dapp 应用时，也可以为其注入 web3 对象。 具体配合应用开发的文档可以在 MetaMask Compatibility Guide (https&#58;//goo.gl/7wKPtp) 查阅，一般我们通过如下脚本来监测 Metamask 状态获取以太坊账户。 var account = web3.eth.accounts&#91;0&#93;;var accountInterval = setInterval(function() &amp;#123 if (web3.eth.accounts&#91;0&#93; !== account) &amp;#123 account = web3.eth.accounts&#91;0&#93;; updateInterface(); &amp;#125&amp;#125, 100); truffle-contract 的使用web3.js 默认为我们提供的接口还是太底层，许多调用需要 hard code 设置参数，以太坊网络使用的 BigNumber 也需要我们手动转换。 我们可以选择使用 truffle-contract (https&#58;//github.com/trufflesuite/truffle-contract) 来调用更高一层的封装对象，并且在之前使用 truffle 开发构建的智能合约文件也能派上用场。 我们可以在 build/contracts/ 下找到编译好的 GitCoin.json 和 GitCoinCrowdsale.json 文件，之后可以在我们的应用中通过如下脚本获取合约对象。 var GitCoin; $.getJSON('contracts/GitCoin.json', function(data) &#123 // 获取编译好的合约文件 var GitCoinArtifact = data; // 通过 truffle-contract 获取合约对象 GitCoin = TruffleContract(GitCoinArtifact); // 将合约绑定至当前 web3 对象 GitCoin.setProvider(App.web3Provider); &#125); 之后我们就可以像在 truffle console 当中一样，对合约对象进行各种操作啦。 ICO 前端应用开发我们的 ICO 应用只需要解决一个核心需求，那就是买币；只需要两个核心功能，一个是选择买多少，另一个就是付款，所以我们的界面自然是相当简单，如下图所示。 然后再稍微美化一下，如下面两张图所示。 一场成功的 ICO，自然需要精雕细琢，完整的代码示例可以在 Leek Ecological Chain (http&#58;//lec.yubolun.com/) 找到，同时此网站也是上述教程的一个完整示例，你可以切换到 Ropsten 网络在本网站上购买 LEC (https&#58;//goo.gl/4uNskB) 韭菜币。 Dapp 部署既然我们开发的是 Dapp 去中心化应用，怎么能够部署在中心化的服务器上呢？这不是自掉身价吗？Dapp 自然有其部署的解决方案。 IPFS 简介IPFS 提供去中心化的点对点的 Web 服务。 说简单点，你可以把它理解成为一个 p2p 的网盘，你网站的静态文件可以发布到 IPFS 上面托管，而且只要 IPFS 的节点不挂，你的网站就永远都不会挂，而不像部署到单独服务器上。 同时 IPFS 上的一个文件也就对应着一个 hash 地址，普通用户可以通过公共的 http gateway 访问到你的页面，不像云服务器还要备案，正好也方便你割完韭菜跑路。 使用也非常简单，只需要在 Install Go IPFS (https&#58;//ipfs.io/docs/install) 下载安装。 发布应用只需要一行命令，把你 Dapp 的所有静态文件上传至 IPFS，命令如下。 ipfs add -r your-ico/# 返回 hash 地址，此处仅为示例added QWERabcd1234qwerABCD your-ico/ 然后你就能够通过 https&#58;//goo.gl/5SyBwN 访问你的网站。当然这样的域名十分不友好，为 IPFS 站点设置解析需要一些不常用的操作。 域名解析IPNS 你的站点必然包含多个文件，每个文件对应着独立的 hash 地址，而且你也不能保证你的网站只需要发布一次。 因此在网站发布后，我们需要使用 ipns 来获取到对应的唯一地址，之后的 DNS 解析也会对应到这一地址，同样只需要一行命令，如下所示。 站点发布后的 hash 地址，此处仅为示例ipfs name publish QWERabcd1234qwerABCD# 返回 ipns 地址Published to ABCDqwer1234abcdQWER之后你就能够通过 https&#58;//goo.gl/8YMLBi 访问你的站点了。在设置域名解析时，我们需要添加一条 TXT 类型的解析记录，解析值为： dnslink=/ipns/ABCDqwer1234abcdQWER 这样我们就能够通过 https&#58;//goo.gl/VjSm1K 访问你的 Dapp，这样是不是友好多了？ Nginx 反向代理 当然你也可能希望使用自己的独立域名，这时我们只需要使用 Nginx 设置反向代理即可。 server &amp;#123 listen 80; server_name yourico.com; location / &amp;#123 proxy_pass https&#58;//ipfs.io/ipns/yourico.com/; &amp;#125&amp;#125 写在后面以太坊官网，第一篇教程教你发 token，第二篇就教你卖 token，居心何在我也不好评判。 除了 ICO 还有 IMO/IFO ，IMO 你只用卖个路由器，IFO 只需要 fork 一份 Bitcoin 的代码，稍微调调参数，就不需要什么教程了。 程序员总是妄图通过技术手段解决社会问题，然而人性是不变的。以太坊希望建立一个 trustless 的网络，可惜被无数人滥用，巧立空气项目，搞空壳公司，逃避监管搞非法集资。 区块链和虚拟货币期望用点对点分布式的网络，脱离第三方，让世界上任何角落的两个人都能够低成本地进行交易，结果大量投机者涌入，导致网络堵塞，如今我们连一笔交易的矿工费都支付不起。 当然我信奉技术本身是无罪，就好像这篇教你割韭菜的文章一样，你是选择擦亮双眼，看清 ICO 的本质，从此势不两立；还是选择投机倒把，滥用以太坊技术，坠身同流合污？ 您可以在访问 https&#58;//github.com/discountry/gitcoin 查看完整的智能合约示例。 您可以访问 https&#58;//github.com/discountry/lec 查看完整的 Dapp 示例。 Read at your own risk. ————— 推荐阅读 ————— 点击图片即可阅读 &#91;&#93;(http&#58;//mp.weixin.qq.com/s?__biz=MjM5MjAwODM4MA==&amp;mid=2650694375&amp;idx=1&amp;sn=3b4bb615c18838cd9d858dfb1d4b49b1&amp;chksm=bea6133489d19a225d3ec5e02191bb53003bad618fa11fb1066ec3a98f9408bf585bbd573fec&amp;scene=21#wechat_redirect) &#91;&#93;(http&#58;//mp.weixin.qq.com/s?__biz=MjM5MjAwODM4MA==&amp;mid=2650694364&amp;idx=1&amp;sn=f6c6a74903d10c2a57782f7bad157440&amp;chksm=bea6130f89d19a192aaac7397b14ccf6a07941ebada94780ea6183b60639578e12d9566cea94&amp;scene=21#wechat_redirect) &#91;&#93;(http&#58;//mp.weixin.qq.com/s?__biz=MjM5MjAwODM4MA==&amp;mid=2650694362&amp;idx=1&amp;sn=c02759d5519c7ea7e4f47700a81cfea3&amp;chksm=bea6130989d19a1fcfa0628a16618497425688cbd897238e92bc8c7af543d645c9d04fdda0af&amp;scene=21#wechat_redirect) 转载来源：一夜身价暴涨千倍，程序员如何发布自己的 ICO？]]></content>
      <tags>
        <tag>CSDN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【Python】IPFS的Python接口 - CSDN博客]]></title>
    <url>%2F2018%2F61075f30%2F</url>
    <content type="text"><![CDATA[相关库 pip install ipfsapi 参考地址：&#91;https&#58;//github.com/ipfs/py-ipfs-api&#93;(https&#58;//github.com/ipfs/py-ipfs-api) 接口示例12345678910111213141516171819202122232425262728293031323334353637import ipfsapi# 连接IPFS，需要先启动节点服务器daemonapi = ipfsapi.connect(&apos;127.0.0.1&apos;, 5001)# 查看节点IDapi.id()&amp;#123&apos;Addresses&apos;&amp;#58; &amp;#91;&apos;/ip4/127.0.0.1/tcp/4001/ipfs/QmS2C4MjZsv2iP1UDMMLCYqJ4WeJw8n3vXx1VKxW1UbqHS&apos;, &apos;/ip6/&amp;#58;&amp;#58;1/tcp/4001/ipfs/QmS2C4MjZsv2iP1UDMMLCYqJ4WeJw8n3vXx1VKxW1UbqHS&apos;&amp;#93;, &apos;AgentVersion&apos;&amp;#58; &apos;go-ipfs/0.4.10&apos;, &apos;ID&apos;&amp;#58; &apos;QmS2C4MjZsv2iP1UDMMLCYqJ4WeJw8n3vXx1VKxW1UbqHS&apos;, &apos;ProtocolVersion&apos;&amp;#58; &apos;ipfs/0.1.0&apos;, &apos;PublicKey&apos;&amp;#58; &apos;CAASpgIwgg ... 3FcjAgMBAAE=&apos;&amp;#125# 上传文件res = api.add(&apos;test.txt&apos;)&amp;#123&apos;Hash&apos;&amp;#58; &apos;QmWxS5aNTFEc9XbMX1ASvLET1zrqEaTssqt33rVZQCQb22&apos;, &apos;Name&apos;&amp;#58; &apos;test.txt&apos;&amp;#125# 上传目录res = api.add(&apos;fake_dir&apos;, recursive=True)&amp;#91;&amp;#123&apos;Hash&apos;&amp;#58; &apos;QmQcCtMgLVwvMQGu6mvsRYLjwqrZJcYtH4mboM9urWW9vX&apos;, &apos;Name&apos;&amp;#58; &apos;fake_dir/fsdfgh&apos;&amp;#125, &amp;#123&apos;Hash&apos;&amp;#58; &apos;QmNuvmuFeeWWpxjCQwLkHshr8iqhGLWXFzSGzafBeawTTZ&apos;, &apos;Name&apos;&amp;#58; &apos;fake_dir/test2/llllg&apos;&amp;#125, &amp;#123&apos;Hash&apos;&amp;#58; &apos;QmX1dd5DtkgoiYRKaPQPTCtXArUu4jEZ62rJBUcd5WhxAZ&apos;, &apos;Name&apos;&amp;#58; &apos;fake_dir/test2&apos;&amp;#125, &amp;#123&apos;Hash&apos;&amp;#58; &apos;Qmenzb5J4fR9c69BbpbBhPTSp2Snjthu2hKPWGPPJUHb9M&apos;, &apos;Name&apos;&amp;#58; &apos;fake_dir&apos;&amp;#125&amp;#93;# 查看文件内容res = api.cat(&apos;QmWxS5aNTFEc9XbMX1ASvLET1zrqEaTssqt33rVZQCQb22&apos;)&gt;&gt; hello ipfs!# 下载文件res = api.get(&apos;QmWxS5aNTFEc9XbMX1ASvLET1zrqEaTssqt33rVZQCQb22&apos;) 转载来源：【Python】IPFS的Python接口 - CSDN博客]]></content>
  </entry>
  <entry>
    <title><![CDATA[以太坊白皮书（原版译文） - 简书]]></title>
    <url>%2F2018%2F91ffc5b0%2F</url>
    <content type="text"><![CDATA[以太坊白皮书（原版译文） - 简书 转载来源：以太坊白皮书（原版译文） - 简书]]></content>
  </entry>
  <entry>
    <title><![CDATA[npm 发布 2017 JavaScript 框架报告]]></title>
    <url>%2F2018%2F0626ec87%2F</url>
    <content type="text"><![CDATA[npm 发布 2017 JavaScript 框架报告 转载来源：npm 发布 2017 JavaScript 框架报告]]></content>
      <tags>
        <tag>前端大全</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[IPFS+IPNS+个人博客搭建 - 简书]]></title>
    <url>%2F2018%2F0f00756f%2F</url>
    <content type="text"><![CDATA[IPFS+IPNS+个人博客搭建 - 简书 转载来源：IPFS+IPNS+个人博客搭建 - 简书]]></content>
  </entry>
  <entry>
    <title><![CDATA[如何对非结构化文本数据进行特征工程操作？这里有妙招！]]></title>
    <url>%2F2018%2Fc4f43e43%2F</url>
    <content type="text"><![CDATA[雷锋网 AI 研习社按：本文是英特尔数据科学家 Dipanjan Sarkar 在 Medium 上发布的「特征工程」博客续篇。在本系列的前两部分中，作者介绍了连续数据的处理方法和离散数据的处理方法。本文则开始了一个新的主题，非结构化文本数据的传统处理方法。雷锋网 AI 研习社对原文进行了编译。 文本数据通常是由表示单词、句子，或者段落的文本流组成。由于文本数据非结构化（并不是整齐的格式化的数据表格）的特征和充满噪声的本质，很难直接将机器学习方法应用在原始文本数据中。在本文中，我们将通过实践的方法，探索从文本数据提取出有意义的特征的一些普遍且有效的策略，提取出的特征极易用来构建机器学习或深度学习模型。 研究动机想要构建性能优良的机器学习模型，特征工程必不可少。有时候，可能只需要一个优秀的特征，你就能赢得 Kaggle 挑战赛的胜利！对于非结构化的文本数据来说，特征工程更加重要，因为我们需要将文本流转化为机器学习算法能理解的数字表示。即使现在有高级的自动化特征工程，在把它们当作「黑盒子」应用之前，我们仍有必要去了解不同特征工程策略背后的核心思想。永远记住，「如果有人给了你一套修房子的工具，你应该知道什么时候该用电钻，什么时候该用锤子！」 理解文本数据我们虽然能够获得具有结构数据属性的文本数据，但它们为结构化数据，并不在今天的讨论范围之内。 在本文中，我们讨论以单词、短语、句子和整个文档的形式展现的文本流。从本质上讲，文本确实有一些句法结构，比如单词组成了短语，短语组成了句子，句子又组合成了段落。然而，与结构化数据集中固定的数据维度相比，文本文档没有固定的结构，因为单词有众多的选择，每个句子的长度也是可变的。本文就是一个很典型的案例。 特征工程的策略下面是一些流行且有效的处理文本数据的策略，这些方法也能应用在下游的机器学习系统中，用于提取有用的特征。大家可以在 GitHub中查看本文使用的所有代码。 首先加载一些基本的依赖关系和设置： import pandas as pdimport numpy as npimport reimport nltkimport matplotlib.pyplot as pltpd.options.display.max_colwidth = 200%matplotlib inline 下面是文档中的语料库，本文大部分内容都是基于该数据集的分析。语料库通常是属于一个或多个主题的文档的集合。 corpus = &#91;’The sky is blue and beautiful.’, ‘Love this blue and beautiful sky!’, ‘The quick brown fox jumps over the lazy dog.’, “A king’s breakfast has sausages, ham, bacon, eggs, toast and beans”, ‘I love green eggs, ham, sausages and bacon!’, ‘The brown fox is quick and the blue dog is lazy!’, ‘The sky is very blue and the sky is very beautiful today’, ‘The dog is lazy but the brown fox is quick!’ &#93; labels = &#91;’weather’, ‘weather’, ‘animals’, ‘food’, ‘food’, ‘animals’, ‘weather’, ‘animals’&#93; corpus = np.array(corpus) corpus_df = pd.DataFrame(&amp;#123’Document’&#58; corpus, ‘Category’&#58; labels&amp;#125) corpus_df = corpus_df&#91;&#91;’Document’, ‘Category’&#93;&#93; corpus_df 本文中应用的语料库案例 可以看到，我们已经从语料库中提取出几个不同类别的文档。在讨论特征工程之前，一如往常，首先得做数据预处理，删除一些不必要的字符、符号和标记。 文本预处理有很多种对文本数据进行清洗和预处理的方法。下面我将重点介绍在自然语言处理（NLP）流程中大量使用的方法。 删除标签：文本中通常会包含一些不必要的内容，比如 HTML 标签，这在分析文本时并没有太多价值。BeautifulSoup 库提供了清理标签的函数。- 清理重音字符：在许多文本语料库中，特别是在处理英文时，通常会遇到重音字符/字母。因此我们要确保将这些字符转换为标准的 ASCII 字符。一个简单的例子就是将 é 转换成 e。- 拓展缩写：在英文中，缩写基本上是单词或者音节的缩减版。缩减版通常是删除某些单词或者短语中特定的字母和声音而来。举例来说，do not 和 don’t , I would 和 I’d。将缩写单词转换为完整的原始形式有助于文本的标准化。- 删除特殊字符：特殊字符和非字母数字的符号通常会增加额外噪声。通常，可以通过简单的正则表达式来实现这一点。- 词干提取和词性还原：可以利用词干创造新的词汇，例如通过附加前缀和后缀等词缀来创造新的单词。这被称为词性变化。词干提取是将这个过程反过来。一个简单的例子是单词：WATCHES, WATCHING, 和 WATCHED，这些单词都把 WATCH 作为词根。词性还原与词干提取很相似，通过移除词缀以得到单词的基本形式。然而在词性还原里，单词的基本形式是词根（root word），而不是词干（root stem）。其不同之处在于词根（root word）总是字典上正确的词（即出现在词典中），但词干并不是这样。- 去除无用词：在从文本中构建有意义的特征时，没有意义的词被称为无用词。如果你在一个语料库中做一个简单的词频分析，这些无用词通常会以最大的频率出现。像 a , an 这样的词被认为是无用词。但是实际上并没有明确通用的无用词表，我们通常使用 nltk 的标准英语无用词表。大家也可以根据特定的需要添加无用词。清理重音字符：在许多文本语料库中，特别是在处理英文时，通常会遇到重音字符/字母。因此我们要确保将这些字符转换为标准的 ASCII 字符。一个简单的例子就是将 é 转换成 e。 删除特殊字符：特殊字符和非字母数字的符号通常会增加额外噪声。通常，可以通过简单的正则表达式来实现这一点。 去除无用词：在从文本中构建有意义的特征时，没有意义的词被称为无用词。如果你在一个语料库中做一个简单的词频分析，这些无用词通常会以最大的频率出现。像 a , an 这样的词被认为是无用词。但是实际上并没有明确通用的无用词表，我们通常使用 nltk 的标准英语无用词表。大家也可以根据特定的需要添加无用词。 除此之外，还可以使用其他的标准操作，比如标记化、删除多余的空格、文本大写转换为小写，以及其他更高级的操作，例如拼写更正、语法错误更正、删除重复字符等。 由于本文的重点是特征工程，我们将构建一个简单的文本预处理程序，其重点是删除特殊字符、多余的空格、数字、无用词以及语料库的大写转小写。 wpt = nltk.WordPunctTokenizer stop_words = nltk.corpus.stopwords.words(‘english’) def normalize_document(doc)&#58; # lower case and remove special characters\whitespaces doc = re.sub(r’&#91;^a-zA-Z\s&#93;’, ‘’, doc, re.I|re.A) doc = doc.lower doc = doc.strip # tokenize document tokens = wpt.tokenize(doc) # filter stopwords out of document filtered_tokens = &#91;token for token in tokens if token not in stop_words&#93; # re-create document from filtered tokens doc = ‘ ‘.join(filtered_tokens) return doc normalize_corpus = np.vectorize(normalize_document) 一旦搭建好基础的预处理流程，我们就可以将它应用在语料库中了。 norm_corpus = normalize_corpus(corpus)norm_corpusOutput——array(&#91;’sky blue beautiful’, ‘love blue beautiful sky’,’quick brown fox jumps lazy dog’,’kings breakfast sausages ham bacon eggs toast beans’,’love green eggs ham sausages bacon’,’brown fox quick blue dog lazy’, ‘sky blue sky beautiful today’,’dog lazy brown fox quick’&#93;,dtype=’ 上面的输出结果应该能让大家清楚的了解样本文档在预处理之后的样子。现在我们来开始特征工程吧! 词袋模型（Bag of Word）这也许是非结构化文本中最简单的向量空间表示模型。向量空间是表示非结构化文本（或其他任何数据）的一种简单数学模型，向量的每个维度都是特定的特征/属性。词袋模型将每个文本文档表示为数值向量，其中维度是来自语料库的一个特定的词，而该维度的值可以用来表示这个词在文档中的出现频率、是否出现（由 0 和 1 表示），或者加权值。将这个模型叫做词袋模型，是因为每个文档可以看作是装着单词的袋子，而无须考虑单词的顺序和语法。 from sklearn.feature_extraction.text import CountVectorizer cv = CountVectorizer(min_df=0., max_df=1.) cv_matrix = cv.fit_transform(norm_corpus) cv_matrix = cv_matrix.toarray cv_matrix 可以看到，文档已经被转换为数字向量，这样每个文档都由上述特征矩阵中的一个向量（行）表示。下面的代码有助于以一种更易理解的格式来表示这一点。 get all unique words in the corpus vocab = cv.get_feature_names # show document feature vectors pd.DataFrame(cv_matrix, columns=vocab) 词袋模型的文档特征向量 上面的表格应该更能助于理解！可以清楚地看到，特征向量中每个列（维度）都代表一个来自语料库的单词，每一行代表一个文档。单元格中的值表示单词（由列表示）出现在特定文档（由行表示）中的次数。因此，如果一个文档语料库是由 N 个单词组成，那么这个文档可以由一个 N 维向量表示。 N 元词袋模型（Bag of N-Gram Model）一个单词只是一个标记，通常被称为单元（unigram）或者一元（1-gram）。我们已经知道，词袋模型不考虑单词的顺序。但是如果我们也想要考虑序列中出现的短语或者词汇集合呢？N 元模型能够帮我们实现这一点。N-Gram 是来自文本文档的单词记号的集合，这些记号是连续的，并以序列的形式出现。二元表示阶数为二的 N-Gram，也就是两个单词。同理三元表示三个单词。N 元词袋模型是普通词袋模型的一种拓展，使得我们可以利用基于 N 元的特征。下面的示例展示了文档中二元的特征向量。 you can set the n-gram range to 1,2 to get unigrams as well as bigrams bv = CountVectorizer(ngram_range=(2,2)) bv_matrix = bv.fit_transform(norm_corpus) bv_matrix = bv_matrix.toarray vocab = bv.get_feature_names pd.DataFrame(bv_matrix, columns=vocab) 使用二元词袋模型的特征向量 在上面的例子中，每个二元特征由两个单词组成，其中的值表示这个二元词组在文档中出现的次数。 TF-IDF 模型在大型语料库中使用词袋模型可能会出现一些潜在的问题。由于特征向量是基于词的频率，某些单词可能会在文档中频繁出现，这可能会在特征集上掩盖掉其他单词。TF-IDF 模型试图通过缩放或者在计算中使用归一化因子来解决这个问题。TF-IDF 即 Term Frequency-Inverse Document Frequency，在计算中结合了两种度量：词频（Term Frequency）和逆文档频率（Inverse Document Frequency）。这种技术是为搜索引擎中查询排序而开发的，现在它是信息检索和 NLP 领域中不可或缺的模型。 在数学上，TF-IDF 可以定义为：tfidf = tf x idf，也可以进一步拓展为下面的表示： 在这里，tfidf（w, D）表示单词w 在文档D 中的 TF-IDF 分数。Tf（w,D）项表示单词w 在文档D 中的词频，这个值可以从词袋模型中获得。idf（w，D）项是单词w 的逆文档频率，可以由语料库中所有文档的总数量C 除以单词w 的文档频率df（w）的 log 值得到，其中文档频率是指语料库中文档出现单词w 的频率。这种模型有多种变种，但是给出的最终结果都很相似。下面在语料库中使用这个模型吧！ from sklearn.feature_extraction.text import TfidfVectorizer tv = TfidfVectorizer(min_df=0., max_df=1., use_idf=True) tv_matrix = tv.fit_transform(norm_corpus) tv_matrix = tv_matrix.toarray vocab = tv.get_feature_names pd.DataFrame(np.round(tv_matrix, 2), columns=vocab) 基于TF-IDF模型的文档特征向量 基于 TF-IDF 的特征向量与原始的词袋模型相比，展示出了缩放和归一化的特性。想要进一步深入了解该模型的读者可以参考 Text Analytics with Python的 181 页。 文档相似性文档相似性是使用从词袋模型或者 tf-idf 模型中提取出的特征，基于距离或者相似度度量判断两个文档相似程度的过程。 因此，可以使用在上一部分中提到的 tf-idf 模型提取出的特征，用其来生成新的特征。这些特征在搜索引擎、文档聚类以及信息检索等领域发挥着重要作用。 语料库中的配对文档相似性需要计算语料库中每两个文档对的文档相似性。因此，如果一个语料库中有 C 个文档，那么最终会得到一个 C*C 的矩阵，矩阵中每个值代表了该行和该列的文档对的相似度分数。可以用几种相似度和距离度量计算文档相似度。其中包括余弦距离/相似度、欧式距离、曼哈顿距离、BM25相似度、jaccard 距离等。在我们的分析中，我们将使用最流行和最广泛使用的相似度度量：余弦相似度，并根据 TF-IDF 特征向量比较文档对的相似度。 from sklearn.metrics.pairwise import cosine_similarity similarity_matrix = cosine_similarity(tv_matrix) similarity_df = pd.DataFrame(similarity_matrix) similarity_df 文档对的相似性矩阵(余弦相似度) 余弦相似度给出了表示两个文档特征向量之间角度的余弦值的度量。两个文档特征向量之间的角度越低，两个文档的相似度就越高，如下图所示： 仔细观察相似度矩阵可以清楚地看出，文档（0，1 和 6），（2，5 和 7）之间非常相似，文档 3 和 4 略微相似。这表明了这些相似的文档一定具有一些相似特征。这是分组或聚类的一个很好的案例，可以通过无监督的学习方法来解决，特别是当需要处理数百万文本文档的庞大语料库时。 具有相似特征的文档聚类聚类是利用无监督学习的方法，将数据点(本场景中即文档)分类到组或者 cluster 中。我们将在这里利用一个无监督的层次聚类算法，通过利用我们之前生成的文档相似性特征，将我们的玩具语料库中的类似文档聚合到一起。有两种类型的层次聚类方法，分别是凝聚方法（agglomerative）和分裂方法（divisive）。这里将会使用凝聚聚类算法，这是一种自下而上（bottom up）的层次聚类算法，最开始每个文档的单词都在自己的类中，根据测量数据点之间的距离度量和连接准则（linkage criterion），将相似的类连续地合并在一起。下图展示了一个简单的描述。 连接准则决定了合并策略。常用的连接准则有 Ward, Complete linkage, Average linkage 等等。这些标准在将一对 cluster 合并在一起（文档中低层次的类聚类成高层次的）时是非常有用的，这是通过最优化目标函数实现的。我们选择 Ward 最小方差作为连接准则，以最小化总的内部聚类方差。由于已经有了相似特征，我们可以直接在样本文档上构建连接矩阵。 from scipy.cluster.hierarchy import dendrogram, linkage Z = linkage(similarity_matrix, ‘ward’) pd.DataFrame(Z, columns=&#91;’Document\Cluster 1’, ‘Document\Cluster 2’, ‘Distance’, ‘Cluster Size’&#93;, dtype=’object’) 我们语料库的连接矩阵 如果仔细查看连接矩阵，可以看到连接矩阵的每个步骤（行）都告诉了我们哪些数据点（或者 cluster）被合并在一起。如果有 n 个数据点，那么连接矩阵 Z 将是（n-1）*4 的形状，其中 Z&#91;i&#93; 表示在步骤 i 合并了哪些 cluster。每行有四个元素，前两个元素是数据点或 cluster 的名称，第三个元素是前两个元素（数据点或 cluster）之间的距离，最后一个元素是合并完成后 cluster 中元素/数据点的总数。大家可以参考 scipy 文档，其中有详细解释。 下面，把这个矩阵看作一个树状图，以更好地理解元素！ plt.figure(figsize=(8, 3)) plt.title(‘Hierarchical Clustering Dendrogram’) plt.xlabel(‘Data point’) plt.ylabel(‘Distance’) dendrogram(Z) plt.axhline(y=1.0, c=’k’, ls=’–’, lw=0.5) 可以看到每个数据点是如何从一个单独的簇开始，慢慢与其他数据点合并形成集群的。从颜色和树状图的更高层次来看，如果考虑距离度量为 1.0（由虚线表示）或者更小，可以看出模型已经正确识别了三个主要的聚类。利用这个距离，我们可以得到集群的标签。 from scipy.cluster.hierarchy import fcluster max_dist = 1.0 cluster_labels = fcluster(Z, max_dist, criterion=’distance’) cluster_labels = pd.DataFrame(cluster_labels, columns=&#91;’ClusterLabel’&#93;) pd.concat(&#91;corpus_df, cluster_labels&#93;, axis=1) 可以清楚地看到，我们的算法已经根据分配给它们的标签，正确识别了文档中的三个不同类别。这应该能够给大家一个关于如何使用 TF-IDF 特征来建立相似度特征的思路。大家可以用这种处理流程来进行聚类。 主题模型也可以使用一些摘要技术从文本文档中提取主题或者基于概念的特征。主题模型围绕提取关键主题或者概念。每个主题可以表示为文档语料库中的一个词袋或者一组词。总之，这些术语表示特定的话题、主题或概念，凭借这些单词所表达的语义含义，可以轻松将每个主题与其他主题区分开来。这些概念可以从简单的事实、陈述到意见、前景。主题模型在总结大量文本来提取和描绘关键概念时非常有用。它们也可用于从文本数据中捕捉潜在的特征。 主题建模有很多种方法，其中大多涉及到某种形式的矩阵分解。比如隐含语义索引（Latent Semantic Indexing， LSI）就使用了奇异值分解。这里将使用另一种技术：隐含狄利克雷分布（Latent Dirichlet Allocation， LDA），它使用了生成概率模型，其中每个文档由几个主题组合而成，每个术语或单词可以分配给某个主题。这与基于 pLSI（probabilistic LSI）的模型很类似。在 LDA 的情况下，每个隐含主题都包含一个狄利克雷先验。 这项技术背后的数学原理相当复杂，所以我会试着总结一下，而不是罗列很多让人厌倦的细节。我建议读者可以看看 Christine Doig 的一个优秀的演讲，深入了解一下。 上图中的黑色框表示利用前面提到的参数，从 M 个文档中提取 K 个主题的核心算法。下面的步骤是对算法的解释。 初始化必要的参数。1. 随机初始化文档，将每个单词分配到 K 个主题中去。1. 按照如下方法迭代1. 对于每个文档 D：随机初始化文档，将每个单词分配到 K 个主题中去。 对于每个文档 D： a) 对于文档中的单词 W： i.对于主题 T： 计算 P(T|D)， 表示文档 D 中单词分配给 T 主题的比例。 计算 P(W|T)，表示在所有文档中，主题 T 包含单词 W 的比例。 ii. 通过计算概率 P(T|D)*P(W|T) 重新分配单词 W 的主题 T。 运行几个迭代之后，就能获得混合了每个文档的主题，然后就可以根据指向某个主题的单词生成文档的主题。像 gensim 或者 scikit-learn 这样的框架，使得我们能够利用 LDA 模型来生成主题。 大家应该记住，当 LDA 应用于文档-单词矩阵（TF-IDF 或者词袋特征矩阵）时，它会被分解为两个主要部分： 文档-主题矩阵，也就是我们要找的特征矩阵- 主题-单词矩阵，能够帮助我们查看语料库中潜在的主题主题-单词矩阵，能够帮助我们查看语料库中潜在的主题 使用 scikit-learn 可以得到如下的文档-主题矩阵。 from sklearn.decomposition import LatentDirichletAllocation lda = LatentDirichletAllocation(n_topics=3, max_iter=10000, random_state=0) dt_matrix = lda.fit_transform(cv_matrix) features = pd.DataFrame(dt_matrix, columns=&#91;’T1’, ‘T2’, ‘T3’&#93;) features 可以清楚地看到哪些文档对上述输出中的三个主题贡献最大，可以通过如下的方式查看主题及其组成部分。 tt_matrix = lda.components_ for topic_weights in tt_matrix&#58; topic = &#91;(token, weight) for token, weight in zip(vocab, topic_weights)&#93; topic = sorted(topic, key=lambda x&#58; -x&#91;1&#93;) topic = &#91;item for item in topic if item&#91;1&#93; &gt; 0.6&#93; print(topic) print 可以看到，由于组成术语不同，很容易区分这三个主题。第一个在讨论天气，第二个关于食物，最后一个关于动物。主题建模的主题数量选择是一门完整的课题，既是一门艺术，也是一门科学。获得最优主题数量的方法有很多，这些技术既复杂又繁琐，这里就不展开讨论了。 使用主题模型特征的文档聚类这里使用 LDA 法从词袋模型特征构建主题模型特征。现在，我们可以利用获得的文档单词矩阵，使用无监督的聚类算法，对文档进行聚类，这与我们之前使用的相似度特征进行聚类类似。 这次我们使用非常流行的基于分区的聚类方法——K-means 聚类，根据文档主题模型特征表示，进行聚类或分组。在 K-means 聚类法中，有一个输入参数 K，它制定了使用文档特征输出的聚类数量。这种聚类方法是一种基于中心的聚类方法，试图将这些文档聚类为等方差的类。这种方法通过最小化类内平方和来创建聚类。选择出最优的 K 的方法有很多，比如误差平方和度量，轮廓系数（Silhouette Coefficients）和 Elbow method。 from sklearn.cluster import KMeans km = KMeans(n_clusters=3, random_state=0) km.fit_transform(features) cluster_labels = km.labels_ cluster_labels = pd.DataFrame(cluster_labels, columns=&#91;’ClusterLabel’&#93;) pd.concat(&#91;corpus_df, cluster_labels&#93;, axis=1) 从上面的输出中可以看到，文档的聚类分配完全正确。 未来会涉及到的高级策略在这篇文章没有涉及近期出现的一些关于文本数据特征工程的高级方法，包括利用深度学习模型来提取单词特征的方法。我们将在本系列的下一部分中深入探讨这些模型，并详细介绍 Word2Vec和GloVe等流行的单词嵌入模型，敬请期待! 总结 这些例子应该能有助于大家理解文本数据特征工程的一些通用策略。本文中介绍的是基于数学概念、信息检索和自然语言处理的传统策略，这些久经考验的方法在各种数据集和问题上都表现优异。在下一篇文章中，我将详细介绍如何利用深度学习模型进行文本数据特征工程。 对连续数据特征工程感兴趣的读者，请查看本系列第一部分！ 对离散数据特征工程感兴趣的读者，请查看本系列第二部分！ 本文中所使用的所有代码和数据集都可以从 GitHub中访问。代码也可以作为Jupyter笔记本使用。 Via towardsdatascience.com雷锋网 AI 研习社编译整理。 转载来源：如何对非结构化文本数据进行特征工程操作？这里有妙招！]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>人工智能</tag>
        <tag>英语</tag>
        <tag>HTML</tag>
        <tag>动物</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[「IPFS + 区块链 系列」入门篇-IPFS+IPNS+个人博客搭建]]></title>
    <url>%2F2018%2Ff9fbed76%2F</url>
    <content type="text"><![CDATA[孔壹学院：国内区块链职业教育引领品牌。作者：黎跃春，孔壹学院创始人，区块链、高可用架构师区块链博客：http&#58;//liyuechun.org 在阅读这篇文章之前，你需要先学习上一篇【IPFS + 区块链 系列】 入门篇 - IPFS环境配置这篇文章。 目录 如何在IPFS新增一个文件- 1.1 新建file.txt文件- 1.2 查看ipfs相关命令- 1.3 将file.txt添加到ipfs节点1.2 查看ipfs相关命令 通过ipfs创建目录存储文件- 3. 如何在IPFS新增一个目录 如何在IPFS新增一个目录 3.1 使用ipfs add -r可以上传一整个目录- 3.2 通过路径访问contactme.txt文件数据- 3.3 通过Hash查看数据IPFS网络数据3.2 通过路径访问contactme.txt文件数据 创建简易的网页发布到IPFS- 4.1 创建一个index.html文件- 4.2 创建一个style.css文件- 4.3 添加到ipfs- 4.4 网络同步- 4.5 访问网站- 4.6 发布到IPNS4.2 创建一个style.css文件 4.4 网络同步 4.6 发布到IPNS 发布个人博客- 5.1 搭建静态博客- 5.2 节点ID替换- 5.3 浏览博客5.2 节点ID替换 下篇预报- 6.1 ipfs + ethereum`Dapp`开发入门1. 如何在IPFS新增一个文件 1.1 新建file.txt文件 打开终端，切换到桌面，新建一个文件夹1121，切换到1121中，通过vi新建一个文件file.txt，文件里面输入春哥微信号liyc1215保存并且退出。 1.2 查看ipfs相关命令 1.3 将file.txt添加到ipfs节点 当执行完ipfs add file.txt这个命令以后，会将file.txt添加到ipfs当前的节点中，并且会对file.txt文件生成一个唯一的hash QmbrevseVQKf1vsYMsxCscRf6D7S2dftYpHwxkYf94pc7T，如果想查看本地ipfs节点的数据，可以通过ipfs cat QmbrevseVQKf1vsYMsxCscRf6D7S2dftYpHwxkYf94pc7T进行查看。 ⚠️：当我试图通过http&amp;#58;//ipfs.io/ipfs/QmbrevseVQKf1vsYMsxCscRf6D7S2dftYpHwxkYf94pc7T进行数据访问时，无法访问，如下图所示： ⚠️：虽然数据已经添加到当前的你自己的IPFS节点中，但是并没有同步到IPFS网络，所以暂时在网络上无法访问。 ⚠️：重要：接下来执行下面的命令同步节点数据到IPFS网络，再试图在网络上查看数据。 同步节点新建一个终端，执行ipfs daemon。- 从IPFS网络查看数据浏览器访问https&#58;//ipfs.io/ipfs/QmbrevseVQKf1vsYMsxCscRf6D7S2dftYpHwxkYf94pc7T 2. 通过ipfs创建目录存储文件在这上面的步骤走，我们可以通过 ipfs cat QmbrevseVQKf1vsYMsxCscRf6D7S2dftYpHwxkYf94pc7T liyc1215查看添加到ipfs网络的file.txt文件的内容，如下： 当然，我们也可以通过ipfs的相关命令在ipfs的根目录下面创建文件夹，并且将file.txt文件移动或者拷贝到我们创建的文件夹中。 ⚠️：cp不会改变文件hash，mv会改变hash寻址。 3. 如何在IPFS新增一个目录3.1 使用ipfs add -r可以上传一整个目录 3.2 通过路径访问contactme.txt文件数据 如果我们上传的是目录，那么可以通过下面几种方式访问到contactme.txt文件的数据。 3.3 通过Hash查看数据IPFS网络数据 访问目录：https&#58;//ipfs.io/ipfs/QmSsjQDVw1fvmG5RsZMgp2GjihiXn2zDv64mfHZN3AREek 通过目录访问文件：https&#58;//ipfs.io/ipfs/QmSsjQDVw1fvmG5RsZMgp2GjihiXn2zDv64mfHZN3AREek/contactme.txt- 通过文件hash直接访问：https&#58;//ipfs.io/ipfs/QmYx4BnhnLXeMWF5mKu16fJgUBiVP7ECXh7qcsUZnXiRxc 4. 创建简易的网页发布到IPFS在这里我先自己写一个简单的网页给大家演示，先在桌面新建一个site文件夹，然后按照下面的步骤在site文件夹中建立index.html和style.css文件。 4.1 创建一个index.html文件 Hello IPFS!4.2 创建一个style.css文件 4.3 添加到ipfs 最后一行是项目根目录的hash，你先通过ipfs daemon同步网络，然后可以通过https&#58;//ipfs.io/ipfs/&lt;你的项目根目录hash&gt;，即https&amp;#58;//ipfs.io/ipfs/QmdVEGkT5u7LtzzatTrn8JGNEF3fpuMPVs2rPCfvqRykRp访问项目。 4.4 网络同步 4.5 访问网站 浏览器打开https&#58;//ipfs.io/ipfs/QmdVEGkT5u7LtzzatTrn8JGNEF3fpuMPVs2rPCfvqRykRp，效果图如下： 4.6 发布到IPNS 当我们修改网站内容重新添加到ipfs时，hash会发生变化，当我们网站更新时，我们可以将网站发布到IPNS，在IPNS中，允许我们节点的域名空间中引用一个IPFS hash，也就是说我们可以通过节点ID对项目根目录的IPFS HASH进行绑定，以后我们访问网站时直接通过节点·ID 访问即可，当我们更新博客时，重新发布到IPNS`即可。 当我们执行ipfs name publish命令时，会返回我们的节点ID，你可以通过ipfs id进行查看验证是否是你的节点ID。 ⚠️：验证 ⚠️：当然我们现在就可以通过IPNS进行访问了。 ⚠️⚠️⚠️：注意上面是ipns而不是ipfs。 ⚠️：如果你网站数据修改，需要重新发布到IPNS。 5. 发布个人博客你可以通过Hugo按照官方文档创建一个漂亮的静态博客Hugo官方网站，当然你也可以自己编写，或者使用其他开源项目搭建。 5.1 搭建静态博客 大家可以自己搭建，也可以直接下载我的博客源码直接搭建。 源码地址：http&#58;//github.com/liyuechun/ipfs_blogger 5.2 节点ID替换 查看你的节点ID在上面的源码中全局搜索将源码里面的QmdKXkeEWcuRw9oqBwopKUa8CgK1iBktPGYaMoJ4UNt1MP替换成你自己的ID。 接下来重复4. 创建简易的网页发布到IPFS的操作步骤即可。 5.3 浏览博客 浏览器打开https&#58;//ipfs.io/ipns/QmdKXkeEWcuRw9oqBwopKUa8CgK1iBktPGYaMoJ4UNt1MP/查看项目效果。 6. 下篇预报6.1 ipfs + ethereum Dapp开发入门 转载来源：「IPFS + 区块链 系列」入门篇-IPFS+IPNS+个人博客搭建]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>技术</tag>
        <tag>HTML</tag>
        <tag>Links</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[区块链开发（一）搭建基于以太坊go-ethereum的私有链环境 - CSDN博客]]></title>
    <url>%2F2018%2F4210122b%2F</url>
    <content type="text"><![CDATA[通过各方资料了解学习之后，决定自己开始搭建基于以太坊go-ethereum的私有链环境。由于本人的电脑系统为win8，为避免window环境出现过多莫名其妙的问题，特意通过vm搭建了一台ubuntu16.04版本的虚拟系统。以下内容均基于ubuntu16.04系统。 go-ethereum客户端下载地址&amp;参考手册首先，可以查看一下go-ethereum项目在git上的地址： &#91;https&#58;//github.com/ethereum/Go-ethereum&#93;(https&#58;//github.com/ethereum/Go-ethereum) 可以在点击项目上的wiki标签，也可以通过一下地址访问wiki： &#91;https&#58;//github.com/ethereum/Go-ethereum/wiki/Building-Ethereum&#93;(https&#58;//github.com/ethereum/Go-ethereum/wiki/Building-Ethereum) 在wiki页面选择ubuntu系统的安装说明，也可以直接访问下面链接： &#91;https&#58;//github.com/ethereum/go-ethereum/wiki/Installation-Instructions-for-Ubuntu&#93;(https&#58;//github.com/ethereum/go-ethereum/wiki/Installation-Instructions-for-Ubuntu) ubuntu下安装命令打开命令行窗口，或通过快捷键CTL+ALT+T，依次输入以下命令，即可安装成功： 12345sudo apt-get install software-properties-commonsudo add-apt-repository -y ppa&amp;#58;ethereum/ethereumsudo add-apt-repository -y ppa&amp;#58;ethereum/ethereum-devsudo apt-get updatesudo apt-get install ethereum PS：如果安装过程中需要依赖其他组件，则先安装其他组件。另外，在ubuntu16.04版本，sudo apt-get install命令可精简为sudo apt install。 安装测试安装完成之后在命令行输入： 1geth --help 如果现实出命令行各种参数提示信息，则说明安装成功。 创世块在以上安装成功之后，直接启动，即可连接公有链。现在通过配置创世块来创建私有链。同一个网络中，创世块必须是一样的，否则无法联通。 创建一个eth的根目录，在根目录下新建创世块json文件piccgenesis.json。内容如下： 1234567891011&amp;#123 &quot;nonce&quot;&amp;#58;&quot;0x0000000000000042&quot;, &quot;mixhash&quot;&amp;#58;&quot;0x0000000000000000000000000000000000000000000000000000000000000000&quot;, &quot;difficulty&quot;&amp;#58; &quot;0x4000&quot;, &quot;alloc&quot;&amp;#58; &amp;#123&amp;#125, &quot;coinbase&quot;&amp;#58;&quot;0x0000000000000000000000000000000000000000&quot;, &quot;timestamp&quot;&amp;#58; &quot;0x00&quot;, &quot;parentHash&quot;&amp;#58;&quot;0x0000000000000000000000000000000000000000000000000000000000000000&quot;, &quot;extraData&quot;&amp;#58; &quot;SecBroBlock&quot;, &quot;gasLimit&quot;&amp;#58;&quot;0x0000ffff&quot;&amp;#125 参数解释： 参数名称 参数描述 mixhash 与nonce配合用于挖矿，由上一个区块的一部分生成的hash。注意他和nonce的设置需要满足以太坊的Yellow paper, 4.3.4. Block Header Validity, (44)章节所描述的条件。 nonce nonce就是一个64位随机数，用于挖矿，注意他和mixhash的设置需要满足以太坊的Yellow paper, 4.3.4. Block Header Validity, (44)章节所描述的条件。 difficulty 设置当前区块的难度，如果难度过大，cpu挖矿就很难，这里设置较小难度 alloc 用来预置账号以及账号的以太币数量，因为私有链挖矿比较容易，所以我们不需要预置有币的账号，需要的时候自己创建即可以。 coinbase 矿工的账号，随便填 timestamp 设置创世块的时间戳 parentHash 上一个区块的hash值，因为是创世块，所以这个值是0 extraData 附加信息，随便填，可以填你的个性信息 gasLimit 该值设置对GAS的消耗总量限制，用来限制区块能包含的交易信息总和，因为我们是私有链，所以填最大。 启动私有链节点启动私有节点所需参数 参数名称 参数描述 identity 区块链的标示，随便填写，用于标示目前网络的名字 init 指定创世块文件的位置，并创建初始块 datadir 设置当前区块链网络数据存放的位置 port 网络监听端口 rpc 启动rpc通信，可以进行智能合约的部署和调试 rpcapi 设置允许连接的rpc的客户端，一般为db,eth,net,web3 networkid 设置当前区块链的网络ID，用于区分不同的网络，是一个数字 console 启动命令行模式，可以在Geth中执行命令 初始化&amp;启动本人启动eth所在目录为： 1/home/zhuzs/eth 此目录下放置刚才配置好的创世块json文件：piccgenesis.json 初始化初始化创世块有两种方法： 方法一：执行命令先进行初始化（注意需要在你准备防止eth的根目录下执行） 1$ geth init /path/to/genesis.json 方法二：在执行启动命令的参数中添加以下参数 1--genesis /path/to/genesis.json 以上两种方案注意path路径进行对应的替换； 启动因此直接执行如下命令： 1geth --identity &quot;secbro etherum&quot; --rpc --rpccorsdomain &quot;*&quot; --datadir &quot;/home/zhuzs/eth/chain&quot; --port &quot;30303&quot; --rpcapi &quot;db,eth,net,web3&quot; -- networkid 95518 console --dev PS：根据自己的环境进行对应的替换。注意，最后添加了–dev，以开发模式启动。 看到一下输出说明启动成功，并且是使用的私有链： 随后就是相关的命令操作，在下一篇博客中进一步说明。 &lt;li class=&quot;prev_article&quot;&gt; 上一篇 &amp;#91;Linux下 $(cd `dirname $0`;pwd)&amp;#93;(https&amp;#58;//blog.csdn.net/wo541075754/article/details/53033712) &lt;/li&gt; &lt;li class=&quot;next_article&quot;&gt; 下一篇 &amp;#91;区块链开发（二）以太坊客户端基本操作命令&amp;#93;(https&amp;#58;//blog.csdn.net/wo541075754/article/details/53073799) &lt;/li&gt; go-ethereum 部署私有链- &lt;img src=&quot;https&amp;#58;//avatar.csdn.net/7/2/A/3_a191030148.jpg&quot; alt=&quot;a191030148&quot; title=&quot;a191030148&quot;&gt; - a191030148 - 2017-10-25 16&amp;#58;27&amp;#58;21 - &lt;i class=&quot;icon iconfont icon-read&quot;&gt;&lt;/i&gt;2355 以太坊客户端Ethereum Wallet与Geth区别简介- &lt;img src=&quot;https&amp;#58;//avatar.csdn.net/2/7/F/3_wo541075754.jpg&quot; alt=&quot;wo541075754&quot; title=&quot;wo541075754&quot;&gt; - wo541075754 - 2017-08-27 12&amp;#58;01&amp;#58;31 - &lt;i class=&quot;icon iconfont icon-read&quot;&gt;&lt;/i&gt;14818 以太坊官网go-ethereum- 2017年02月09日 13&amp;#58;22 - 50MB - 下载 树莓派raspberry pi安装archlinux，并且在上面搭建以太坊（ethereum）环境- &lt;img src=&quot;https&amp;#58;//avatar.csdn.net/0/D/B/3_zhang_yu_joseph.jpg&quot; alt=&quot;Zhang_Yu_Joseph&quot; title=&quot;Zhang_Yu_Joseph&quot;&gt; - Zhang_Yu_Joseph - 2015-10-01 09&amp;#58;45&amp;#58;02 - &lt;i class=&quot;icon iconfont icon-read&quot;&gt;&lt;/i&gt;2534 以太坊go-ethereum客户端JSON-RPC API调用（一）- &lt;img src=&quot;https&amp;#58;//avatar.csdn.net/2/7/F/3_wo541075754.jpg&quot; alt=&quot;wo541075754&quot; title=&quot;wo541075754&quot;&gt; - wo541075754 - 2016-12-31 09&amp;#58;13&amp;#58;02 - &lt;i class=&quot;icon iconfont icon-read&quot;&gt;&lt;/i&gt;16911 Ethereum钱包区块同步问题- &lt;img src=&quot;https&amp;#58;//avatar.csdn.net/9/E/E/3_fengmm521.jpg&quot; alt=&quot;fengmm521&quot; title=&quot;fengmm521&quot;&gt; - fengmm521 - 2017-11-04 22&amp;#58;01&amp;#58;33 - &lt;i class=&quot;icon iconfont icon-read&quot;&gt;&lt;/i&gt;6988 Ethereum JSON-Api 的使用- &lt;img src=&quot;https&amp;#58;//avatar.csdn.net/D/3/D/3_guokaikevin.jpg&quot; alt=&quot;guokaikevin&quot; title=&quot;guokaikevin&quot;&gt; - guokaikevin - 2017-11-19 21&amp;#58;01&amp;#58;11 - &lt;i class=&quot;icon iconfont icon-read&quot;&gt;&lt;/i&gt;2949 以太坊客户端Geth命令用法-参数详解- &lt;img src=&quot;https&amp;#58;//avatar.csdn.net/2/7/F/3_wo541075754.jpg&quot; alt=&quot;wo541075754&quot; title=&quot;wo541075754&quot;&gt; - wo541075754 - 2018-03-18 11&amp;#58;32&amp;#58;30 - &lt;i class=&quot;icon iconfont icon-read&quot;&gt;&lt;/i&gt;516 安装geth客户端并转账- &lt;img src=&quot;https&amp;#58;//avatar.csdn.net/7/B/7/3_ddffr.jpg&quot; alt=&quot;DDFFR&quot; title=&quot;DDFFR&quot;&gt; - DDFFR - 2017-07-04 14&amp;#58;41&amp;#58;24 - &lt;i class=&quot;icon iconfont icon-read&quot;&gt;&lt;/i&gt;5962 区块链开发（二）以太坊客户端基本操作命令 - &lt;img src=&quot;https&amp;#58;//avatar.csdn.net/2/7/F/3_wo541075754.jpg&quot; alt=&quot;wo541075754&quot; title=&quot;wo541075754&quot;&gt; - wo541075754 - 2016-11-07 22&amp;#58;41&amp;#58;09 - &lt;i class=&quot;icon iconfont icon-read&quot;&gt;&lt;/i&gt;14094 个人资料 &lt;dl class=&quot;inf_bar clearfix&quot;&gt; &lt;dt class=&quot;csdn-tracking-statistics tracking-click&quot; data-mod=&quot;popu_381&quot;&gt; &lt;a href=&quot;https&amp;#58;//blog.csdn.net/wo541075754&quot; target=&quot;_blank&quot;&gt; &lt;img src=&quot;https&amp;#58;//avatar.csdn.net/2/7/F/3_wo541075754.jpg&quot; class=&quot;avatar_pic&quot;&gt; &lt;/a&gt; &lt;/dt&gt;&lt;dd&gt; &lt;h3 class=&quot;csdn-tracking-statistics tracking-click&quot; data-mod=&quot;popu_380&quot;&gt;&amp;#91;wo541075754&amp;#93;(https&amp;#58;//blog.csdn.net/wo541075754)&lt;/h3&gt; &lt;svg class=&quot;icon&quot; aria-hidden=&quot;true&quot;&gt; &lt;use xlink&amp;#58;href=&quot;#icon-bokezhuanjia&quot;&gt;&lt;/use&gt; &lt;/svg&gt; 博客专家 &lt;/dd&gt; &lt;/dl&gt; &lt;dl title=&quot;234&quot;&gt; &lt;dt&gt;原创&lt;/dt&gt; &lt;dd&gt;234&lt;/dd&gt; &lt;/dl&gt; &lt;dl title=&quot;694&quot;&gt; &lt;dt&gt;粉丝&lt;/dt&gt; &lt;dd id=&apos;fan&apos;&gt;694&lt;/dd&gt; &lt;/dl&gt; &lt;dl title=&quot;517&quot;&gt; &lt;dt&gt;喜欢&lt;/dt&gt; &lt;dd&gt;517&lt;/dd&gt; &lt;/dl&gt; &lt;dl title=&quot;373&quot;&gt; &lt;dt&gt;评论&lt;/dt&gt; &lt;dd&gt;373&lt;/dd&gt; &lt;/dl&gt; &lt;img src=&apos;https&amp;#58;//csdnimg.cn/jifen/images/xunzhang/xunzhang/zhuanlandaren.png&apos; alt=&apos;专栏达人&apos;&gt; &lt;img src=&apos;https&amp;#58;//csdnimg.cn/jifen/images/xunzhang/xunzhang/chizhiyiheng.png&apos; alt=&apos;持之以恒&apos;&gt; 等级： &lt;a href=&quot;https&amp;#58;//blog.csdn.net/home/help.html#level&quot; title=&quot;7级,点击查看等级说明&quot; target=&quot;_blank&quot;&gt; &lt;img class=&quot;grade-img&quot; src=&quot;https&amp;#58;//csdnimg.cn/jifen/images/xunzhang/jianzhang/blog7.png&quot; alt=&quot;7级,点击查看等级说明&quot;&gt; &lt;/a&gt; 访问量： 127万+ 积分： 1万+ 排名： 1283 // 判断并设置用户名位置，没有博客专家与关注按钮时，用户名居中 $medals_children = $(‘.medals’).children().length; $span_add_follow = $(‘#span_add_follow’).length; if($medals_children === 0 &amp;&amp; $span_add_follow === 0)&amp;#123 $(‘.inf_bar dd’).css(‘vertical-align’,’10px’) &amp;#125 以太坊研发技术交流群 文章搜索 博客专栏 &lt;table cellpadding=&quot;0&quot; cellspacing=&quot;0&quot;&gt; &lt;tr&gt; &lt;td style=&quot;padding&amp;#58;10px 10px 0 0;&quot;&gt; &lt;a href=&quot;https&amp;#58;//blog.csdn.net/column/details/13651.html&quot; target=&quot;_blank&quot;&gt; &lt;img src=&quot;https&amp;#58;//img-blog.csdn.net/20161122200728461&quot; style=&quot;width&amp;#58;75px;height&amp;#58;75px;&quot; /&gt; &lt;/a&gt; &lt;/td&gt; &lt;td style=&quot;padding&amp;#58;10px 0; vertical-align&amp;#58;top;&quot;&gt; &amp;#91;区块链实践&amp;#93;(https&amp;#58;//blog.csdn.net/column/details/13651.html) 文章：67篇 阅读：338731 &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td style=&quot;padding&amp;#58;10px 10px 0 0;&quot;&gt; &lt;a href=&quot;https&amp;#58;//blog.csdn.net/column/details/14599.html&quot; target=&quot;_blank&quot;&gt; &lt;img src=&quot;https&amp;#58;//img-blog.csdn.net/20170227104644223&quot; style=&quot;width&amp;#58;75px;height&amp;#58;75px;&quot; /&gt; &lt;/a&gt; &lt;/td&gt; &lt;td style=&quot;padding&amp;#58;10px 0; vertical-align&amp;#58;top;&quot;&gt; &amp;#91;Zookeeper从入门到专家&amp;#93;(https&amp;#58;//blog.csdn.net/column/details/14599.html) 文章：20篇 阅读：68736 &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td style=&quot;padding&amp;#58;10px 10px 0 0;&quot;&gt; &lt;a href=&quot;https&amp;#58;//blog.csdn.net/column/details/15277.html&quot; target=&quot;_blank&quot;&gt; &lt;img src=&quot;https&amp;#58;//img-blog.csdn.net/20170415200234279&quot; style=&quot;width&amp;#58;75px;height&amp;#58;75px;&quot; /&gt; &lt;/a&gt; &lt;/td&gt; &lt;td style=&quot;padding&amp;#58;10px 0; vertical-align&amp;#58;top;&quot;&gt; &amp;#91;Intellij IDEA日常使用&amp;#93;(https&amp;#58;//blog.csdn.net/column/details/15277.html) 文章：29篇 阅读：348843 &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td style=&quot;padding&amp;#58;10px 10px 0 0;&quot;&gt; &lt;a href=&quot;https&amp;#58;//blog.csdn.net/column/details/16183.html&quot; target=&quot;_blank&quot;&gt; &lt;img src=&quot;https&amp;#58;//img-blog.csdn.net/20170630105130174&quot; style=&quot;width&amp;#58;75px;height&amp;#58;75px;&quot; /&gt; &lt;/a&gt; &lt;/td&gt; &lt;td style=&quot;padding&amp;#58;10px 0; vertical-align&amp;#58;top;&quot;&gt; &amp;#91;Drools规则引擎&amp;#93;(https&amp;#58;//blog.csdn.net/column/details/16183.html) 文章：47篇 阅读：115919 &lt;/td&gt; &lt;/tr&gt; &lt;/table&gt; &lt;td style=&quot;padding&amp;#58;10px 10px 0 0;&quot;&gt; &lt;a href=&quot;https&amp;#58;//blog.csdn.net/column/details/14599.html&quot; target=&quot;_blank&quot;&gt; &lt;img src=&quot;https&amp;#58;//img-blog.csdn.net/20170227104644223&quot; style=&quot;width&amp;#58;75px;height&amp;#58;75px;&quot; /&gt; &lt;/a&gt; &lt;/td&gt; &lt;td style=&quot;padding&amp;#58;10px 0; vertical-align&amp;#58;top;&quot;&gt; &amp;#91;Zookeeper从入门到专家&amp;#93;(https&amp;#58;//blog.csdn.net/column/details/14599.html) 文章：20篇 阅读：68736 &lt;/td&gt; &lt;td style=&quot;padding&amp;#58;10px 10px 0 0;&quot;&gt; &lt;a href=&quot;https&amp;#58;//blog.csdn.net/column/details/16183.html&quot; target=&quot;_blank&quot;&gt; &lt;img src=&quot;https&amp;#58;//img-blog.csdn.net/20170630105130174&quot; style=&quot;width&amp;#58;75px;height&amp;#58;75px;&quot; /&gt; &lt;/a&gt; &lt;/td&gt; &lt;td style=&quot;padding&amp;#58;10px 0; vertical-align&amp;#58;top;&quot;&gt; &amp;#91;Drools规则引擎&amp;#93;(https&amp;#58;//blog.csdn.net/column/details/16183.html) 文章：47篇 阅读：115919 &lt;/td&gt; 文章分类 区块链 (68) Intellij IDEA (29) Zookeeper (20) Java基础 (18) Drools (47) 程序杂谈 (19) SpringBoot (13) Bootstrap (6) Linux (11) JDBC (1) 单元测试 (1) Dbvisualizer (3) SVN (4) Quartz (2) Spring (4) 日志 (2) ZK (1) Oracle (1) 前端 (5) MySQL (4) Nginx (1) Maven (6) dubbo (1) mybatis (4) SecureCRT (2) 自动化 (3) github (2) mq (1) xstream (1) Solidity (5) 加密算法 (2) 分布式 (4) ssh重启 (1) go语言 (3) 其他 (1) 文章存档 &lt;!--归档统计--&gt; &lt;li&gt; &amp;#91;2018年4月&amp;#93;(https&amp;#58;//blog.csdn.net/wo541075754/article/month/2018/04) (4) &lt;/li&gt; &lt;!--归档统计--&gt; &lt;li&gt; &amp;#91;2018年3月&amp;#93;(https&amp;#58;//blog.csdn.net/wo541075754/article/month/2018/03) (15) &lt;/li&gt; &lt;!--归档统计--&gt; &lt;li&gt; &amp;#91;2018年2月&amp;#93;(https&amp;#58;//blog.csdn.net/wo541075754/article/month/2018/02) (3) &lt;/li&gt; &lt;!--归档统计--&gt; &lt;li&gt; &amp;#91;2018年1月&amp;#93;(https&amp;#58;//blog.csdn.net/wo541075754/article/month/2018/01) (9) &lt;/li&gt; &lt;!--归档统计--&gt; &lt;li&gt; &amp;#91;2017年12月&amp;#93;(https&amp;#58;//blog.csdn.net/wo541075754/article/month/2017/12) (10) &lt;/li&gt; &lt;!--归档统计--&gt; &lt;li&gt; &amp;#91;2017年11月&amp;#93;(https&amp;#58;//blog.csdn.net/wo541075754/article/month/2017/11) (5) &lt;/li&gt; &lt;!--归档统计--&gt; &lt;li&gt; &amp;#91;2017年10月&amp;#93;(https&amp;#58;//blog.csdn.net/wo541075754/article/month/2017/10) (3) &lt;/li&gt; &lt;!--归档统计--&gt; &lt;li&gt; &amp;#91;2017年9月&amp;#93;(https&amp;#58;//blog.csdn.net/wo541075754/article/month/2017/09) (8) &lt;/li&gt; &lt;!--归档统计--&gt; &lt;li&gt; &amp;#91;2017年8月&amp;#93;(https&amp;#58;//blog.csdn.net/wo541075754/article/month/2017/08) (21) &lt;/li&gt; &lt;!--归档统计--&gt; &lt;li&gt; &amp;#91;2017年7月&amp;#93;(https&amp;#58;//blog.csdn.net/wo541075754/article/month/2017/07) (25) &lt;/li&gt; &lt;!--归档统计--&gt; &lt;li&gt; &amp;#91;2017年6月&amp;#93;(https&amp;#58;//blog.csdn.net/wo541075754/article/month/2017/06) (4) &lt;/li&gt; &lt;!--归档统计--&gt; &lt;li&gt; &amp;#91;2017年5月&amp;#93;(https&amp;#58;//blog.csdn.net/wo541075754/article/month/2017/05) (4) &lt;/li&gt; &lt;!--归档统计--&gt; &lt;li&gt; &amp;#91;2017年4月&amp;#93;(https&amp;#58;//blog.csdn.net/wo541075754/article/month/2017/04) (10) &lt;/li&gt; &lt;!--归档统计--&gt; &lt;li&gt; &amp;#91;2017年3月&amp;#93;(https&amp;#58;//blog.csdn.net/wo541075754/article/month/2017/03) (17) &lt;/li&gt; &lt;!--归档统计--&gt; &lt;li&gt; &amp;#91;2017年2月&amp;#93;(https&amp;#58;//blog.csdn.net/wo541075754/article/month/2017/02) (8) &lt;/li&gt; &lt;!--归档统计--&gt; &lt;li&gt; &amp;#91;2017年1月&amp;#93;(https&amp;#58;//blog.csdn.net/wo541075754/article/month/2017/01) (10) &lt;/li&gt; &lt;!--归档统计--&gt; &lt;li&gt; &amp;#91;2016年12月&amp;#93;(https&amp;#58;//blog.csdn.net/wo541075754/article/month/2016/12) (8) &lt;/li&gt; &lt;!--归档统计--&gt; &lt;li&gt; &amp;#91;2016年11月&amp;#93;(https&amp;#58;//blog.csdn.net/wo541075754/article/month/2016/11) (21) &lt;/li&gt; &lt;!--归档统计--&gt; &lt;li&gt; &amp;#91;2016年10月&amp;#93;(https&amp;#58;//blog.csdn.net/wo541075754/article/month/2016/10) (4) &lt;/li&gt; &lt;!--归档统计--&gt; &lt;li&gt; &amp;#91;2016年9月&amp;#93;(https&amp;#58;//blog.csdn.net/wo541075754/article/month/2016/09) (15) &lt;/li&gt; &lt;!--归档统计--&gt; &lt;li&gt; &amp;#91;2016年8月&amp;#93;(https&amp;#58;//blog.csdn.net/wo541075754/article/month/2016/08) (8) &lt;/li&gt; &lt;!--归档统计--&gt; &lt;li&gt; &amp;#91;2016年6月&amp;#93;(https&amp;#58;//blog.csdn.net/wo541075754/article/month/2016/06) (4) &lt;/li&gt; &lt;!--归档统计--&gt; &lt;li&gt; &amp;#91;2016年5月&amp;#93;(https&amp;#58;//blog.csdn.net/wo541075754/article/month/2016/05) (6) &lt;/li&gt; &lt;!--归档统计--&gt; &lt;li&gt; &amp;#91;2016年4月&amp;#93;(https&amp;#58;//blog.csdn.net/wo541075754/article/month/2016/04) (5) &lt;/li&gt; &lt;!--归档统计--&gt; &lt;li&gt; &amp;#91;2016年3月&amp;#93;(https&amp;#58;//blog.csdn.net/wo541075754/article/month/2016/03) (8) &lt;/li&gt; &lt;!--归档统计--&gt; &lt;li&gt; &amp;#91;2016年2月&amp;#93;(https&amp;#58;//blog.csdn.net/wo541075754/article/month/2016/02) (8) &lt;/li&gt; &lt;!--归档统计--&gt; &lt;li&gt; &amp;#91;2015年11月&amp;#93;(https&amp;#58;//blog.csdn.net/wo541075754/article/month/2015/11) (1) &lt;/li&gt; &lt;!--归档统计--&gt; &lt;li&gt; &amp;#91;2015年9月&amp;#93;(https&amp;#58;//blog.csdn.net/wo541075754/article/month/2015/09) (3) &lt;/li&gt; &lt;!--归档统计--&gt; &lt;li&gt; &amp;#91;2015年8月&amp;#93;(https&amp;#58;//blog.csdn.net/wo541075754/article/month/2015/08) (4) &lt;/li&gt; &lt;!--归档统计--&gt; &lt;li&gt; &amp;#91;2015年7月&amp;#93;(https&amp;#58;//blog.csdn.net/wo541075754/article/month/2015/07) (2) &lt;/li&gt; &lt;!--归档统计--&gt; &lt;li&gt; &amp;#91;2015年6月&amp;#93;(https&amp;#58;//blog.csdn.net/wo541075754/article/month/2015/06) (6) &lt;/li&gt; &lt;!--归档统计--&gt; &lt;li&gt; &amp;#91;2015年5月&amp;#93;(https&amp;#58;//blog.csdn.net/wo541075754/article/month/2015/05) (10) &lt;/li&gt; 阅读排行 &lt;li&gt; &lt;a href=&quot;https&amp;#58;//blog.csdn.net/wo541075754/article/details/51150035&quot; title=&quot;IntelliJ IDEA 控制台中文乱码解决方案&quot;&gt; IntelliJ IDEA 控制台中文乱码解决方案 &lt;/a&gt; (40713) &lt;/li&gt; &lt;li&gt; &lt;a href=&quot;https&amp;#58;//blog.csdn.net/wo541075754/article/details/54743138&quot; title=&quot;图解区块链：14张图看懂什么是“区块链技术”？&quot;&gt; 图解区块链：14张图看懂什么是“区块链技术”？ &lt;/a&gt; (37020) &lt;/li&gt; &lt;li&gt; &lt;a href=&quot;https&amp;#58;//blog.csdn.net/wo541075754/article/details/47829233&quot; title=&quot;IntelliJ IDEA 使用Subversion时忽略文件夹&quot;&gt; IntelliJ IDEA 使用Subversion时忽略文件夹 &lt;/a&gt; (32785) &lt;/li&gt; &lt;li&gt; &lt;a href=&quot;https&amp;#58;//blog.csdn.net/wo541075754/article/details/50717842&quot; title=&quot;Mysql事物锁等待超时 Lock wait timeout exceeded; try restarting transaction&quot;&gt; Mysql事物锁等待超时 Lock wait timeout exceeded; try restarting transaction &lt;/a&gt; (31522) &lt;/li&gt; &lt;li&gt; &lt;a href=&quot;https&amp;#58;//blog.csdn.net/wo541075754/article/details/54632929&quot; title=&quot;Merkle Tree（默克尔树）算法解析&quot;&gt; Merkle Tree（默克尔树）算法解析 &lt;/a&gt; (31321) &lt;/li&gt; &lt;li&gt; &lt;a href=&quot;https&amp;#58;//blog.csdn.net/wo541075754/article/details/46047051&quot; title=&quot;Intellij IDEA 配置Subversion插件&quot;&gt; Intellij IDEA 配置Subversion插件 &lt;/a&gt; (31254) &lt;/li&gt; &lt;li&gt; &lt;a href=&quot;https&amp;#58;//blog.csdn.net/wo541075754/article/details/50956860&quot; title=&quot;Intellij Idea 使用SVN更新到指定版本&quot;&gt; Intellij Idea 使用SVN更新到指定版本 &lt;/a&gt; (27302) &lt;/li&gt; &lt;li&gt; &lt;a href=&quot;https&amp;#58;//blog.csdn.net/wo541075754/article/details/51159518&quot; title=&quot;Intellij IDEA 默认打开上次项目设置&quot;&gt; Intellij IDEA 默认打开上次项目设置 &lt;/a&gt; (24458) &lt;/li&gt; &lt;li&gt; &lt;a href=&quot;https&amp;#58;//blog.csdn.net/wo541075754/article/details/49659597&quot; title=&quot;Intellij 解除（去除）SVN关联&quot;&gt; Intellij 解除（去除）SVN关联 &lt;/a&gt; (21501) &lt;/li&gt; &lt;li&gt; &lt;a href=&quot;https&amp;#58;//blog.csdn.net/wo541075754/article/details/53064877&quot; title=&quot;区块链开发（一）搭建基于以太坊go-ethereum的私有链环境&quot;&gt; 区块链开发（一）搭建基于以太坊go-ethereum的私有链环境 &lt;/a&gt; (21328) &lt;/li&gt; 最新评论 &lt;li&gt; &amp;#91;以太坊技术学习及交流相关事宜&amp;#93;(https&amp;#58;//blog.csdn.net/wo541075754/article/details/79761870#comments) &lt;p style=&quot;margin&amp;#58;0px;&quot;&gt;&amp;#91;defineconst&amp;#93;(https&amp;#58;//my.csdn.net/defineconst)&amp;#58; 能够微信转账支付不？ &lt;/p&gt; &lt;/li&gt; &lt;li&gt; &amp;#91;Spring boot xstre...&amp;#93;(https&amp;#58;//blog.csdn.net/wo541075754/article/details/52841287#comments) &lt;p style=&quot;margin&amp;#58;0px;&quot;&gt;&amp;#91;qq_33749431&amp;#93;(https&amp;#58;//my.csdn.net/qq_33749431)&amp;#58; 我曹，老哥牛逼，这样都被你发现了，必须给个赞，想得我脑壳疼，原来是springboot这b崽子 &lt;/p&gt; &lt;/li&gt; &lt;li&gt; &amp;#91;《Drools7.0.0.Fina...&amp;#93;(https&amp;#58;//blog.csdn.net/wo541075754/article/details/74456890#comments) &lt;p style=&quot;margin&amp;#58;0px;&quot;&gt;&amp;#91;weixin_38761382&amp;#93;(https&amp;#58;//my.csdn.net/weixin_38761382)&amp;#58; 博主大神，我刚接触drools，ecplise配置好drools6.5后，新建项目编译报错是什么原... &lt;/p&gt; &lt;/li&gt; &lt;li&gt; &amp;#91;以太坊交易池(txpool)的分析&amp;#93;(https&amp;#58;//blog.csdn.net/wo541075754/article/details/79812535#comments) &lt;p style=&quot;margin&amp;#58;0px;&quot;&gt;&amp;#91;Platinum0&amp;#93;(https&amp;#58;//my.csdn.net/Platinum0)&amp;#58; 666 &lt;/p&gt; &lt;/li&gt; &lt;li&gt; &amp;#91;以太坊go-ethereum客户端...&amp;#93;(https&amp;#58;//blog.csdn.net/wo541075754/article/details/53953933#comments) &lt;p style=&quot;margin&amp;#58;0px;&quot;&gt;&amp;#91;qq_26733783&amp;#93;(https&amp;#58;//my.csdn.net/qq_26733783)&amp;#58; &amp;#91;reply&amp;#93;d15269395665&amp;#91;/reply&amp;#93; 请问你现在调好了吗？我遇到跟你一样的情况了。 &#91;以太坊go-ethereum客户端…&#93;(https&#58;//blog.csdn.net/wo541075754/article/details/53953933#comments) &#91;qq_26733783&#93;(https&#58;//my.csdn.net/qq_26733783)&#58; &#91;reply&#93;wo541075754&#91;/reply&#93;参数类型错误？能说具体一点吗？我还是没调出来 &#91;以太坊如何搭建私有连联盟链&#93;(https&#58;//blog.csdn.net/wo541075754/article/details/78926177#comments) &#91;u012862638&#93;(https&#58;//my.csdn.net/u012862638)&#58; 博主您好，现在我这里已经在本地搭建起来一个3个节点的联盟链，我这有个疑问：我在 node2 中 … &#91;Intellij IDEA 201…&#93;(https&#58;//blog.csdn.net/wo541075754/article/details/77504461#comments) &#91;UncleTian&#93;(https&#58;//my.csdn.net/UncleTian)&#58; 好文章！~ &#91;以太坊源码分析-同步之Syncin…&#93;(https&#58;//blog.csdn.net/wo541075754/article/details/79649208#comments) &#91;wo541075754&#93;(https&#58;//my.csdn.net/wo541075754)&#58; &#91;reply&#93;zhugeaming2018&#91;/reply&#93;感谢支持 &#91;以太坊手续费不足异常（insuff…&#93;(https&#58;//blog.csdn.net/wo541075754/article/details/79537043#comments) &#91;zhizouxiao&#93;(https&#58;//my.csdn.net/zhizouxiao)&#58; 初始化私有链的时候，chainId不为0，否则出现insufficient funds for g… &#91;qq_33749431&#93;(https&#58;//my.csdn.net/qq_33749431)&#58; 我曹，老哥牛逼，这样都被你发现了，必须给个赞，想得我脑壳疼，原来是springboot这b崽子 &#91;Platinum0&#93;(https&#58;//my.csdn.net/Platinum0)&#58; 666 &#91;qq_26733783&#93;(https&#58;//my.csdn.net/qq_26733783)&#58; &#91;reply&#93;wo541075754&#91;/reply&#93;参数类型错误？能说具体一点吗？我还是没调出来 &#91;UncleTian&#93;(https&#58;//my.csdn.net/UncleTian)&#58; 好文章！~ &#91;zhizouxiao&#93;(https&#58;//my.csdn.net/zhizouxiao)&#58; 初始化私有链的时候，chainId不为0，否则出现insufficient funds for g… &lt;li&gt; &lt;button class=&quot;left-fixed-btn btn-like csdn-tracking-statistics tracking-click&quot; data-mod=&quot;popu_373&quot; target=&quot;_self&quot; title=&quot;点赞&quot;&gt; &amp;#91;&lt;i class=&quot;icon iconfont icon-dianzan&quot;&gt;&lt;/i&gt;&amp;#93;(javascript&amp;#58;void(0);) &amp;#91;1&amp;#93;(javascript&amp;#58;void(0);) &lt;/button&gt; &lt;/li&gt; &lt;li&gt; &lt;button class=&quot;left-fixed-btn csdn-tracking-statistics tracking-click btn-collect&quot; data-mod=&quot;popu_374&quot; target=&quot;_self&quot; title=&quot;收藏&quot;&gt; &amp;#91;&lt;i class=&quot;icon iconfont icon-shoucang&quot;&gt;&lt;/i&gt;&amp;#93;(javascript&amp;#58;void(0);) &lt;/button&gt; &lt;/li&gt; &lt;li&gt; &lt;button class=&quot;left-fixed-btn btn-pinglun csdn-tracking-statistics tracking-click&quot; data-mod=&quot;popu_544&quot; title=&quot;评论&quot;&gt; &amp;#91;&lt;i class=&quot;icon iconfont icon-pinglun&quot;&gt;&lt;/i&gt;&amp;#93;(javascript&amp;#58;void(0);) &lt;/button&gt; &lt;/li&gt; &lt;li class=&quot;bdsharebuttonbox csdn-tracking-statistics tracking-click&quot; data-mod=&quot;popu_172&quot;&gt; &lt;a class=&quot;bds_tsina outside left-fixed-btn&quot; data-cmd=&quot;tsina&quot; title=&quot;分享到新浪微博&quot;&gt;&lt;/a&gt; &lt;i class=&quot;icon iconfont icon-xinlang&quot;&gt;&lt;/i&gt; &lt;/li&gt; &lt;li class=&quot;bdsharebuttonbox csdn-tracking-statistics tracking-click&quot; data-mod=&quot;popu_172&quot;&gt; &lt;a class=&quot;bds_weixin outside left-fixed-btn&quot; data-cmd=&quot;weixin&quot; title=&quot;分享到微信&quot;&gt;&lt;/a&gt; &lt;i class=&quot;icon iconfont icon-weixin&quot;&gt;&lt;/i&gt; &lt;/li&gt; &lt;li class=&quot;bdsharebuttonbox csdn-tracking-statistics tracking-click&quot; data-mod=&quot;popu_172&quot;&gt; &lt;a class=&quot;bds_qzone outside left-fixed-btn&quot; data-cmd=&quot;qzone&quot; title=&quot;分享到QQ空间&quot;&gt;&lt;/a&gt; &lt;i class=&quot;icon iconfont icon-QQ&quot;&gt;&lt;/i&gt; &lt;/li&gt; 转载来源：区块链开发（一）搭建基于以太坊go-ethereum的私有链环境 - CSDN博客]]></content>
  </entry>
  <entry>
    <title><![CDATA[安装hyperledger fabric V1.0.1 - 猫不急 - 博客园]]></title>
    <url>%2F2018%2F37c7029b%2F</url>
    <content type="text"><![CDATA[安装hyperledger fabric V1.0.1 - 猫不急 - 博客园 转载来源：安装hyperledger fabric V1.0.1 - 猫不急 - 博客园]]></content>
  </entry>
  <entry>
    <title><![CDATA[Ubuntu部署基于Fabric的虚拟区块链服务 - CSDN博客]]></title>
    <url>%2F2018%2F61b4068c%2F</url>
    <content type="text"><![CDATA[关于Hyperledger Fabric的部署适合在Ubuntu或其它Linux上进行，本例在Ubuntu16.04LTS上操作。如果是Windows、MacOS系统，建议安装Virtual Box，在虚拟机上部署区块链环境。 准备： 1、源需要换成国内源，这样速度能快很多。 参考：http&#58;//www.linuxidc.com/Linux/2016-06/132518.htm 2、安装好git，ssh服务，配置go语言环境(我的version是1.9.2，官网下载最新版本，注意项目路径名与src包一致！！)，安装docker容器(我的version是1.12.6，建议1.12以上！！)并启动容器服务（配置过程比较简单，自行百度），分别输入需要的环境名字来检查是否正确安装，确认安装正确继续。 参考：http&#58;//www.linuxidc.com/Linux/2017-01/139985.htm 3、修改你的权限 sudo usermod -aG docker USER_NAME 4、注销并重新登录，然后添加阿里云的Docker Hub镜像（一定要注销后再重启docker服务！！） 参考：https&#58;//cr.console.aliyun.com/#/accelerator/ 5、然后重新加载守护进程，输入以下两行命令 sudo systemctl daemon-reloadsudo systemctl restart docker 6、安装python-pip sudo apt-get install python-pip 7、安装docker-compose 直接运行脚本：curl -L https&#58;//get.daocloud.io/docker/compose/releases/download/1.12.0/docker-compose-uname -s-uname -m &gt; ~/docker-composesudo mv ~/docker-compose /usr/local/bin/docker-compose chmod +x /usr/local/bin/docker-compose 部署： 1、建立fabric目录，用Git拉取源码 mkdir -p ~/go/src/github.com/hyperledger cd ~/go/src/github.com/hyperledger git clone https&#58;//github.com/hyperledger/fabric.git 2、切换v1.0.0版本的源码 cd ~/go/src/github.com/hyperledger/fabricgit checkout v1.0.0 3、下载Fabric docker镜像 cd ~/go/src/github.com/hyperledger/fabric/examples/e2e_cli/source download-dockerimages.sh -c x86_64-1.0.0 -f x86_64-1.0.0 若下载成功，输入docker images会看到如下内容 启动： 1、在e2e_cli文件夹内，启动Fabric网络的自动化脚本。 ./network_setup.sh up 2、启动成功，则出现如下： 测试： 1、测试官方example，打开另一个终端： docker exec -it cli bash peer chaincode query -C mychannel -n mycc -c ‘&amp;#123”Args”&#58;&#91;”query”,”a”&#93;&amp;#125’ 余额（Query Result）显示为90。 2、再把a账户的余额全部转给b账户，运行命令： peer chaincode invoke -o orderer.example.com&#58;7050 –tls true –cafile /opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/ordererOrganizations/example.com/orderers/orderer.example.com/msp/tlscacerts/tlsca.example.com-cert.pem -C mychannel -n mycc -c ‘&amp;#123”Args”&#58;&#91;”invoke”,”a”,”b”,”90”&#93;&amp;#125’ 运行结果为： 3、再次查询a的余额，结果为： 4、退出cli容器：exit 5、关闭Fabric网络：./network_setup.sh down 另附： docker常用命令： 1）删除一个容器 docker rm 2）强制删除一个容器 docker rm -f 3）强制删除全部容器 docker rm -f $(docker ps -aq) 4）删除一个镜像&#58; docker rmi 5）强制删除一个镜像&#58; docker rmi -f 6）强制删除全部镜像 docker rmi -f $(docker images -q) 结语：虚拟链搭建到此结束，接下来就需要部署合约，部署多机器节点，分析源码！ &lt;li class=&quot;prev_article&quot;&gt; 上一篇 &amp;#91;150个常用Linux命令精简合集&amp;#93;(https&amp;#58;//blog.csdn.net/sinat_35119798/article/details/78518819) &lt;/li&gt; &lt;li class=&quot;next_article&quot;&gt; 下一篇 &amp;#91;从 Kubernetes 谈容器网络&amp;#93;(https&amp;#58;//blog.csdn.net/sinat_35119798/article/details/78535807) &lt;/li&gt; Hyperledger Fabric 区块链多机部署- &lt;img src=&quot;https&amp;#58;//avatar.csdn.net/F/5/C/3_songbin830.jpg&quot; alt=&quot;songbin830&quot; title=&quot;songbin830&quot;&gt; - songbin830 - 2017-12-12 09&amp;#58;29&amp;#58;16 - &lt;i class=&quot;icon iconfont icon-read&quot;&gt;&lt;/i&gt;4045 自己动手部署区块链-hyperledger/fabric-02- &lt;img src=&quot;https&amp;#58;//avatar.csdn.net/C/9/B/3_yl_1314.jpg&quot; alt=&quot;YL_1314&quot; title=&quot;YL_1314&quot;&gt; - YL_1314 - 2016-12-28 17&amp;#58;51&amp;#58;34 - &lt;i class=&quot;icon iconfont icon-read&quot;&gt;&lt;/i&gt;10679 区块链 hyperledger fabric1.0 环境搭建- &lt;img src=&quot;https&amp;#58;//avatar.csdn.net/7/E/3/3_ming1215919.jpg&quot; alt=&quot;ming1215919&quot; title=&quot;ming1215919&quot;&gt; - ming1215919 - 2017-07-26 10&amp;#58;52&amp;#58;37 - &lt;i class=&quot;icon iconfont icon-read&quot;&gt;&lt;/i&gt;2658 区块链之Hyperledger（超级账本）Fabric v1.0 的环境搭建（超详细教程）- &lt;img src=&quot;https&amp;#58;//avatar.csdn.net/9/0/A/3_so5418418.jpg&quot; alt=&quot;so5418418&quot; title=&quot;so5418418&quot;&gt; - so5418418 - 2017-10-26 16&amp;#58;53&amp;#58;53 - &lt;i class=&quot;icon iconfont icon-read&quot;&gt;&lt;/i&gt;5198 超级账本Fabric区块链用弹珠游戏Marbles 部署- &lt;img src=&quot;https&amp;#58;//avatar.csdn.net/C/5/0/3_u013938484.jpg&quot; alt=&quot;u013938484&quot; title=&quot;u013938484&quot;&gt; - u013938484 - 2018-03-17 11&amp;#58;01&amp;#58;03 - &lt;i class=&quot;icon iconfont icon-read&quot;&gt;&lt;/i&gt;341 自己动手部署区块链-hyperledger/fabric-01- &lt;img src=&quot;https&amp;#58;//avatar.csdn.net/C/9/B/3_yl_1314.jpg&quot; alt=&quot;YL_1314&quot; title=&quot;YL_1314&quot;&gt; - YL_1314 - 2016-12-28 10&amp;#58;20&amp;#58;17 - &lt;i class=&quot;icon iconfont icon-read&quot;&gt;&lt;/i&gt;6246 用Kubernetes部署超级账本Fabric的区块链即服务(1)- &lt;img src=&quot;https&amp;#58;//avatar.csdn.net/C/9/2/3_q48s71bczbeylou9t0n.jpg&quot; alt=&quot;q48S71bCzBeYLOu9T0n&quot; title=&quot;q48S71bCzBeYLOu9T0n&quot;&gt; - q48S71bCzBeYLOu9T0n - 2017-08-13 00&amp;#58;00&amp;#58;00 - &lt;i class=&quot;icon iconfont icon-read&quot;&gt;&lt;/i&gt;865 干货 | 超级账本Fabric 1.0 多节点集群的部署(1)- &lt;img src=&quot;https&amp;#58;//avatar.csdn.net/C/9/2/3_q48s71bczbeylou9t0n.jpg&quot; alt=&quot;q48S71bCzBeYLOu9T0n&quot; title=&quot;q48S71bCzBeYLOu9T0n&quot;&gt; - q48S71bCzBeYLOu9T0n - 2017-07-02 00&amp;#58;00&amp;#58;00 - &lt;i class=&quot;icon iconfont icon-read&quot;&gt;&lt;/i&gt;1088 区块链投资现状和区块链应用部署的探讨- &lt;img src=&quot;https&amp;#58;//avatar.csdn.net/3/1/B/3_owndiandian.jpg&quot; alt=&quot;owndiandian&quot; title=&quot;owndiandian&quot;&gt; - owndiandian - 2016-12-20 18&amp;#58;28&amp;#58;23 - &lt;i class=&quot;icon iconfont icon-read&quot;&gt;&lt;/i&gt;1231 区块链开发（一）搭建基于以太坊go-ethereum的私有链环境 - &lt;img src=&quot;https&amp;#58;//avatar.csdn.net/2/7/F/3_wo541075754.jpg&quot; alt=&quot;wo541075754&quot; title=&quot;wo541075754&quot;&gt; - wo541075754 - 2016-11-07 11&amp;#58;38&amp;#58;18 - &lt;i class=&quot;icon iconfont icon-read&quot;&gt;&lt;/i&gt;21390 个人资料 &lt;dl class=&quot;inf_bar clearfix&quot;&gt; &lt;dt class=&quot;csdn-tracking-statistics tracking-click&quot; data-mod=&quot;popu_381&quot;&gt; &lt;a href=&quot;https&amp;#58;//blog.csdn.net/sinat_35119798&quot; target=&quot;_blank&quot;&gt; &lt;img src=&quot;https&amp;#58;//avatar.csdn.net/9/7/3/3_sinat_35119798.jpg&quot; class=&quot;avatar_pic&quot;&gt; &lt;/a&gt; &lt;/dt&gt;&lt;dd&gt; &lt;h3 class=&quot;csdn-tracking-statistics tracking-click&quot; data-mod=&quot;popu_380&quot;&gt;&amp;#91;sinat_35119798&amp;#93;(https&amp;#58;//blog.csdn.net/sinat_35119798)&lt;/h3&gt; &lt;/dd&gt; &lt;/dl&gt; &lt;dl title=&quot;9&quot;&gt; &lt;dt&gt;原创&lt;/dt&gt; &lt;dd&gt;9&lt;/dd&gt; &lt;/dl&gt; &lt;dl title=&quot;2&quot;&gt; &lt;dt&gt;粉丝&lt;/dt&gt; &lt;dd id=&apos;fan&apos;&gt;2&lt;/dd&gt; &lt;/dl&gt; &lt;dl title=&quot;0&quot;&gt; &lt;dt&gt;喜欢&lt;/dt&gt; &lt;dd&gt;0&lt;/dd&gt; &lt;/dl&gt; &lt;dl title=&quot;1&quot;&gt; &lt;dt&gt;评论&lt;/dt&gt; &lt;dd&gt;1&lt;/dd&gt; &lt;/dl&gt; &lt;img src=&apos;https&amp;#58;//csdnimg.cn/jifen/images/xunzhang/xunzhang/chizhiyiheng.png&apos; alt=&apos;持之以恒&apos;&gt; 等级： &lt;a href=&quot;https&amp;#58;//blog.csdn.net/home/help.html#level&quot; title=&quot;2级,点击查看等级说明&quot; target=&quot;_blank&quot;&gt; &lt;img class=&quot;grade-img&quot; src=&quot;https&amp;#58;//csdnimg.cn/jifen/images/xunzhang/jianzhang/blog2.png&quot; alt=&quot;2级,点击查看等级说明&quot;&gt; &lt;/a&gt; 访问量： 5578 积分： 155 排名： 110万+ // 判断并设置用户名位置，没有博客专家与关注按钮时，用户名居中 $medals_children = $(‘.medals’).children().length; $span_add_follow = $(‘#span_add_follow’).length; if($medals_children === 0 &amp;&amp; $span_add_follow === 0)&amp;#123 $(‘.inf_bar dd’).css(‘vertical-align’,’10px’) &amp;#125 文章搜索 文章分类 Go (1) 算法 (4) C++ (4) 区块链 (9) 数据挖掘 (1) 文章存档 &lt;!--归档统计--&gt; &lt;li&gt; &amp;#91;2018年3月&amp;#93;(https&amp;#58;//blog.csdn.net/sinat_35119798/article/month/2018/03) (2) &lt;/li&gt; &lt;!--归档统计--&gt; &lt;li&gt; &amp;#91;2018年1月&amp;#93;(https&amp;#58;//blog.csdn.net/sinat_35119798/article/month/2018/01) (1) &lt;/li&gt; &lt;!--归档统计--&gt; &lt;li&gt; &amp;#91;2017年12月&amp;#93;(https&amp;#58;//blog.csdn.net/sinat_35119798/article/month/2017/12) (5) &lt;/li&gt; &lt;!--归档统计--&gt; &lt;li&gt; &amp;#91;2017年11月&amp;#93;(https&amp;#58;//blog.csdn.net/sinat_35119798/article/month/2017/11) (10) &lt;/li&gt; &lt;!--归档统计--&gt; &lt;li&gt; &amp;#91;2017年8月&amp;#93;(https&amp;#58;//blog.csdn.net/sinat_35119798/article/month/2017/08) (1) &lt;/li&gt; 阅读排行 &lt;li&gt; &lt;a href=&quot;https&amp;#58;//blog.csdn.net/sinat_35119798/article/details/77662645&quot; title=&quot;CCF —— CSP认证&quot;&gt; CCF —— CSP认证 &lt;/a&gt; (1154) &lt;/li&gt; &lt;li&gt; &lt;a href=&quot;https&amp;#58;//blog.csdn.net/sinat_35119798/article/details/78774235&quot; title=&quot;数据集整理&quot;&gt; 数据集整理 &lt;/a&gt; (1147) &lt;/li&gt; &lt;li&gt; &lt;a href=&quot;https&amp;#58;//blog.csdn.net/sinat_35119798/article/details/78547194&quot; title=&quot;用qsort对二维数组进行排序&quot;&gt; 用qsort对二维数组进行排序 &lt;/a&gt; (483) &lt;/li&gt; &lt;li&gt; &lt;a href=&quot;https&amp;#58;//blog.csdn.net/sinat_35119798/article/details/78836138&quot; title=&quot;以太坊ICO实例（代码片不好用！！）&quot;&gt; 以太坊ICO实例（代码片不好用！！） &lt;/a&gt; (453) &lt;/li&gt; &lt;li&gt; &lt;a href=&quot;https&amp;#58;//blog.csdn.net/sinat_35119798/article/details/78518919&quot; title=&quot;Ubuntu部署基于Fabric的虚拟区块链服务&quot;&gt; Ubuntu部署基于Fabric的虚拟区块链服务 &lt;/a&gt; (452) &lt;/li&gt; &lt;li&gt; &lt;a href=&quot;https&amp;#58;//blog.csdn.net/sinat_35119798/article/details/79411454&quot; title=&quot;Dapp开发（一）&quot;&gt; Dapp开发（一） &lt;/a&gt; (335) &lt;/li&gt; &lt;li&gt; &lt;a href=&quot;https&amp;#58;//blog.csdn.net/sinat_35119798/article/details/78825076&quot; title=&quot;以太坊私有链简易部署过程&quot;&gt; 以太坊私有链简易部署过程 &lt;/a&gt; (317) &lt;/li&gt; &lt;li&gt; &lt;a href=&quot;https&amp;#58;//blog.csdn.net/sinat_35119798/article/details/78787761&quot; title=&quot;以太坊资料&quot;&gt; 以太坊资料 &lt;/a&gt; (196) &lt;/li&gt; &lt;li&gt; &lt;a href=&quot;https&amp;#58;//blog.csdn.net/sinat_35119798/article/details/78551443&quot; title=&quot;Go语言切片的本质&quot;&gt; Go语言切片的本质 &lt;/a&gt; (167) &lt;/li&gt; &lt;li&gt; &lt;a href=&quot;https&amp;#58;//blog.csdn.net/sinat_35119798/article/details/78634067&quot; title=&quot;以太坊和超级账本对比&quot;&gt; 以太坊和超级账本对比 &lt;/a&gt; (111) &lt;/li&gt; 最新评论 &lt;li&gt; &amp;#91;区块链与Git版本工具的比较&amp;#93;(https&amp;#58;//blog.csdn.net/sinat_35119798/article/details/79101986#comments) &lt;p style=&quot;margin&amp;#58;0px;&quot;&gt;&amp;#91;qq_27259753&amp;#93;(https&amp;#58;//my.csdn.net/qq_27259753)&amp;#58; 有没有联系方式？加我V15855526201 ，详聊 &lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;button class=&quot;left-fixed-btn btn-like csdn-tracking-statistics tracking-click&quot; data-mod=&quot;popu_373&quot; target=&quot;_self&quot; title=&quot;点赞&quot;&gt; &amp;#91;&lt;i class=&quot;icon iconfont icon-dianzan&quot;&gt;&lt;/i&gt;&amp;#93;(javascript&amp;#58;void(0);) &amp;#91;0&amp;#93;(javascript&amp;#58;void(0);) &lt;/button&gt; &lt;/li&gt; &lt;li&gt; &lt;button class=&quot;left-fixed-btn csdn-tracking-statistics tracking-click btn-collect&quot; data-mod=&quot;popu_374&quot; target=&quot;_self&quot; title=&quot;收藏&quot;&gt; &amp;#91;&lt;i class=&quot;icon iconfont icon-shoucang&quot;&gt;&lt;/i&gt;&amp;#93;(javascript&amp;#58;void(0);) &lt;/button&gt; &lt;/li&gt; &lt;li&gt; &lt;button class=&quot;left-fixed-btn btn-pinglun csdn-tracking-statistics tracking-click&quot; data-mod=&quot;popu_544&quot; title=&quot;评论&quot;&gt; &amp;#91;&lt;i class=&quot;icon iconfont icon-pinglun&quot;&gt;&lt;/i&gt;&amp;#93;(javascript&amp;#58;void(0);) &lt;/button&gt; &lt;/li&gt; &lt;li class=&quot;bdsharebuttonbox csdn-tracking-statistics tracking-click&quot; data-mod=&quot;popu_172&quot;&gt; &lt;a class=&quot;bds_tsina outside left-fixed-btn&quot; data-cmd=&quot;tsina&quot; title=&quot;分享到新浪微博&quot;&gt;&lt;/a&gt; &lt;i class=&quot;icon iconfont icon-xinlang&quot;&gt;&lt;/i&gt; &lt;/li&gt; &lt;li class=&quot;bdsharebuttonbox csdn-tracking-statistics tracking-click&quot; data-mod=&quot;popu_172&quot;&gt; &lt;a class=&quot;bds_weixin outside left-fixed-btn&quot; data-cmd=&quot;weixin&quot; title=&quot;分享到微信&quot;&gt;&lt;/a&gt; &lt;i class=&quot;icon iconfont icon-weixin&quot;&gt;&lt;/i&gt; &lt;/li&gt; &lt;li class=&quot;bdsharebuttonbox csdn-tracking-statistics tracking-click&quot; data-mod=&quot;popu_172&quot;&gt; &lt;a class=&quot;bds_qzone outside left-fixed-btn&quot; data-cmd=&quot;qzone&quot; title=&quot;分享到QQ空间&quot;&gt;&lt;/a&gt; &lt;i class=&quot;icon iconfont icon-QQ&quot;&gt;&lt;/i&gt; &lt;/li&gt; 转载来源：Ubuntu部署基于Fabric的虚拟区块链服务 - CSDN博客]]></content>
  </entry>
  <entry>
    <title><![CDATA[hyperledger fabric本地开发环境mac部署 - 黑神领主]]></title>
    <url>%2F2018%2F83e8ef98%2F</url>
    <content type="text"><![CDATA[hyperledger fabric本地开发环境mac部署 - 黑神领主 转载来源：hyperledger fabric本地开发环境mac部署 - 黑神领主]]></content>
  </entry>
  <entry>
    <title><![CDATA[快速搭建一个Fabric 1.0的环境-博客-云栖社区-阿里云]]></title>
    <url>%2F2018%2F2a3abcbb%2F</url>
    <content type="text"><![CDATA[快速搭建一个Fabric 1.0的环境-博客-云栖社区-阿里云 转载来源：快速搭建一个Fabric 1.0的环境-博客-云栖社区-阿里云]]></content>
  </entry>
  <entry>
    <title><![CDATA[解读R-Net：微软“超越人类”的阅读理解人工智能]]></title>
    <url>%2F2018%2Fd305e55f%2F</url>
    <content type="text"><![CDATA[人工智能的阅读能力在某些方面已经超越了人类，微软的 R-Net 就是达到了这一里程碑的人工智能之一。近日，谷歌工程师 Sachin Joglekar 在 Medium 上发文对 R-Net 进行了直观的介绍。 R-Net 论文：https&#58;//www.microsoft.com/en-us/research/wp-content/uploads/2017/05/r-net.pdf 今年 1 月 13 日，很多媒体的新闻报道称微软和阿里巴巴开发的人工智能在 SQuAD 数据集测试上，阅读能力上超越了人类。尽管这并不完全准确，但这些公司所开发的模型确实能在某些阅读任务的某些指标上超越人类水平。这篇文章为微软实现这一成果背后的人工智能 R-Net 提供了一个直观的介绍。 首先，给出阅读问题…… 给定一个段落 P： 「特斯拉于 1856 年 7 月 10 日（旧历法的 6 月 28 日）出生于奥地利帝国斯米连村（现属克罗地亚）的一个塞族家庭。他的父亲米卢廷·特斯拉是一位塞尔维亚东正教神父。特斯拉的母亲是久卡·特斯拉（娘家姓为 Mandić），她的父亲也是一位东正教神父；&#58;10 她非常擅长制作家庭手工工具、机械器具并且具有记忆塞尔维亚史诗的能力。久卡从没接受过正规教育。尼古拉将自己的记忆和创造能力归功于他母亲的遗传和影响。特斯拉的祖先来自塞尔维亚西部靠近黑山的地方。&#58;12」 然后询问一个问题 Q： 「特斯拉的母亲具有怎样的特殊能力？」 然后提供一部分连续文本作为答案 A： 「擅长制作家庭手工工具、机械器具并且具有记忆塞尔维亚史诗的能力」 斯坦福问答数据集（SQuAD）包含大约 500 篇文章，涉及的问答对数量接近 10 万（上面给出的例子就取自其中）。 在我们介绍微软的用于阅读理解的方法之前，我们先简要介绍一下他们论文中大量使用的两个概念： 1. 循环神经网络（RNN） RNN 是一种特殊的神经网络，可用于分析时间（或序列）数据。标准的前馈神经网络没有记忆的概念，而 RNN 则通过使用「反馈的」语境向量（context vector）而将这一概念整合了进来： 图 1：一种典型的 RNN 从本质上讲，其在任何时间步骤 t 的输出都是过去语境和当前输入的一个函数。 双向 RNN（BiRNN）是一种特殊的 RNN。标准 RNN 是通过「记忆」过去的数据来记忆历史语境，而 BiRNN 还会从反方向进行遍历以理解未来的语境： 图 2：BiRNN 需要指出，尽管 RNN 理论上可以记住任何长度的历史，但通常来说整合短期语境比整合长期信息（相距 20-30 步以上）更好。 注：R-Net 使用 RNN（更具体来说是门控循环单元）的主要目的是模拟「阅读」文本段落的动作。 2. 注意力（attention） 神经网络中的注意力是根据人类重点关注视觉输入中的特定部分并大略查看其余部分的观看方式而建模的。 注意力可以用在这样的应用中：你的数据点集合中并非所有部分都与当前的任务有关。在这样的情况下，注意力是作为该集合中所有点的 softmax 加权平均而计算的。其权重本身则是作为 1）向量集和 2）某个语境的某个非线性函数而计算的。 图 3：在「飞盘」的语境下，网络会重点关注实际的飞盘和与之相关的对象，同时略过其余部分。 注：R-Net 使用了注意力来在另一些文本的语境下突出文本的某些部分。 R-Net 从直观上讲，R-Net 执行阅读理解的方式与你我进行阅读理解的方式相似：通过多次（准确地说是 3 次）「阅读」（应用 RNN）文本以及在每次迭代中越来越好地「微调」（使用注意力）各词的向量表征。 让我们分开解读其中的每一次过程…… 第一次阅读：略览 我们从标准的 token（即词）向量开始，使用了来自 Glove 的词嵌入。但是，人类通常理解一个词在其周围词所构成的语境中的含义。 比如这两个例子：「May happen」和「the fourth of May」，其中「May」的含义取决于周围的词。另外也要注意背景信息可以来自前向，也可以来自反向。因此，我们在标准词嵌入之上使用了 BiRNN，以得到更好的向量。 问题和段落上都会应用这个过程。 第二次阅读：基于问题的分析 在第二次阅读中，该网络会使用文本本身的语境来调节来自段落的词表征。 让我们假设你已经在该段落的重点位置了： 「……她非常擅长制作家庭手工工具、机械器具并且具有记忆塞尔维亚史诗的能力。久卡从没接受过正规教育……」 在有了「制作」的前提下，如果你在问题 token 上应用注意力，你可能会重点关注： 「特斯拉的母亲具有怎样的特殊能力？」 类似地，网络会调整「制作」的向量，使之与「能力」在语义上靠得更近。 该段落中的所有 token 都会完成这样的操作——本质上讲，R-Net 会在问题的需求和文章的相关部分之间形成链接。原论文将这个部分称为门控式基于注意力的 RNN（Gated Attention-based RNN）。 第三次阅读：有自知的完整的段落理解 在第一次阅读过程中，我们在 token 临近周围词的语境中对这些 token 进行了理解。- 在第二次阅读过程中，我们根据当前问题改善了我们的理解。在第二次阅读过程中，我们根据当前问题改善了我们的理解。 现在我们要鸟瞰整个段落，以定位那些对回答问题真正有帮助的部分。要做到这一点，光是有周围词的短期语境视角是不够的。考虑以下部分中突出强调的词： 特斯拉的母亲是久卡·特斯拉（娘家姓为 Mandić），她的父亲也是一位东正教神父；&#58;10 她非常擅长制作家庭手工工具、机械器具并且具有记忆塞尔维亚史诗的能力。久卡从没接受过正规教育。尼古拉将自己的记忆和创造能力归功于他母亲的遗传和影响。 这都是指特斯拉的母亲所具有的能力。但是，尽管前者确实围绕描述该能力的文本（是我们想要的），但后面的能力则是将它们与特斯拉的才能关联了起来（不是我们想要的）。 为了定位答案的正确起始和结束位置（我们会在下一步解决这个问题），我们需要比较段落中具有相似含义的不同词，以便找到它们的差异之处。使用单纯的 RNN 是很难完成这个任务的，因为这些突出强调的词相距较远。 为了解决这个问题，R-Net 使用了所谓「自匹配注意力（Self-Matched Attention）」。 为什么需要自匹配？ 在应用注意力时，我们通常会使用一些数据（比如一个段落词）来衡量一个向量（比如问题词）的集合。但在这个迭代过程中，我们会使用当前的段落词来衡量来自该段落本身的 token。这有助于我们将当前词与段落其余部分中具有相似含义的其它词区分开。为了增强这个过程，这个阅读阶段是使用一个 BiRNN 完成的。 在我看来，使用自匹配注意力这个步骤是 R-Net 最「神奇」的地方：使用注意力来比较同一段落中相距较远的词。 最后一步：标记答案 在最后一步，R-Net 使用了一种指针网络（Pointer Networks）的变体来确定答案所处的起始和结束位置。简单来说： 我们首先根据问题文本计算另一个注意力向量。这被用作这一次迭代的「起始语境（starting context）」。使用这个知识，再为该起始索引计算一组权重（为该段落中的每个词）。得到最高权重的词作为其答案的「起始位置」。 除了权重之外，这个两步 RNN 还会返回一个新的语境——其中编码了有关该答案的起始的信息。 再将上述步骤重复一次——这一次使用新的语境而不是基于问题的语境，用以计算该答案的结束位置。 搞定！我们得到解答了！（实际上，我们上述例子中给出的答案就是 R-Net 实际得出的答案。） 如果你对 R-Net 的详细细节感兴趣，请阅读他们的论文。如果代码能帮助你更好地理解（至少对我而言是如此），请参阅 YerevaNN 试图用 Keras 重建 R-Net 的精彩博文：http&#58;//yerevann.github.io/2017/08/25/challenges-of-reproducing-r-net-neural-network-using-keras/。 转载来源：解读R-Net：微软“超越人类”的阅读理解人工智能]]></content>
      <categories>
        <category>科技</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>人工智能</tag>
        <tag>微软</tag>
        <tag>塞尔维亚</tag>
        <tag>正教会</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[阿里提出新神经网络算法，压缩掉最后一个比特]]></title>
    <url>%2F2018%2Fd0594453%2F</url>
    <content type="text"><![CDATA[**【新智元导读】**在利用深度网络解决问题的时候人们常常倾向于设计更为复杂的网络收集更多的数据以期获得更高的性能。但是，随之而来的是模型的复杂度急剧提升，参数越来越多，给深度学习在设备上的应用带来挑战。阿里iDST团队最新提出的ADMM神经网络压缩和加速算法，可以无损地压缩掉最后一个比特。论文已经被AAAI 2018录用为oral。 近年来，深度学习在人工智能领域取得了重大的突破。在计算机视觉、语音识别等诸多领域，深度神经网络(DNN, Deep Neural Network)均被证明是一种极具成效的问题解决方式。如卷积神经网络(CNN, Convolutional neural network)在计算机视觉诸多传统问题（分类、检测、分割）都超越了传统方法，循环神经网络(RNN, Recurrent Neural Networks)则在时序信号处理，如机器翻译，语音识别等超过传统方法。 在利用深度网络解决问题的时候人们常常倾向于设计更为复杂的网络收集更多的数据以期获得更高的性能。但是，随之而来的是模型的复杂度急剧提升，直观的表现是模型的层数越来越深，参数越来越多。这会给深度学习带来两个严重的问题： 随着模型参数的增多，模型的大小越来越大，给嵌入式端模型的存储带来了很大的挑战。1. 随着模型的增大，模型inference的时间越来越长，latency越来越大。随着模型的增大，模型inference的时间越来越长，latency越来越大。 以上两个问题给深度学习在终端智能设备上的推广带来了很大的挑战。比如，经典的深度卷积网络VGG-16的模型大小达到528M，用户很难接受下载一个如此大的模型到手机或者其他终端设备上。同时，在一般的智能手机上，VGG-16识别一张图像的时间高达3000+ms，这个latency对于大多数用户来说也是难以接受的。此外，由于深度网络的计算量很大，运行深度网络的能耗很高，这对于手机等终端设备也是一个巨大的挑战。 基于低比特表示技术的神经网络压缩和加速算法 在这个工作中，我们提出一种基于低比特表示技术的神经网络压缩和加速算法。我们将神经网络的权重表示成离散值，并且离散值的形式为2的幂次方的形式，比如&amp;#123-4，-2，-1，0，1，2，4&amp;#125。这样原始32比特的浮点型权重可以被压缩成1-3比特的整形权重，同时，原始的浮点数乘法操作可以被定点数的移位操作所替代。在现代处理器中，定点移位操作的速度和能耗是远远优于浮点数乘法操作的。 首先，我们将离散值权重的神经网络训练定义成一个离散约束优化问题。以三值网络为例，其目标函数可以表示为： 更进一步，我们在约束条件中引入一个scale参数。对于三值网络，我们将约束条件写成&amp;#123-a, 0, a&amp;#125, a&gt;0. 这样做并不会增加计算代价，因为在卷积或者全连接层的计算过程中可以先和三值权重&amp;#123-1, 0, 1&amp;#125进行矩阵操作，然后对结果进行一个标量scale。从优化的角度看，增加这个scale参数可以大大增加约束空间的大小，这有利于算法的收敛。如下图所示， 对于三值网络而言，scale参数可以将约束空间从离散的9个点扩增到4条直线。 为了求解上述约束优化问题，我们引入ADMM算法。在此之前，我们需要对目标函数的形式做一个等价变换。 其中Ic为指示函数，如果G符合约束条件，则Ic(G)=0，否则Ic(G)为无穷大。该目标函数的增广拉格朗日形式为： ADMM算法将上述问题分成三个子问题进行求解，即 与其它算法不同的是，我们在实数空间和离散空间分别求解，然后通过拉格朗日乘子的更新将两组解联系起来。 第一个子问题需要找到一个网络权重最小化： 在实验中我们发现使用常规的梯度下降算法求解这个问题收敛速度很慢。在这里我们使用Extra-gradient算法来对这个问题进行求解。Extra-gradient算法包含两个基本步骤，分别是： 第二个子问题在离散空间中进行优化。通过简单的数学变换第二个子问题可以写成： 该问题可以通过迭代优化的方法进行求解。当a或Q固定时，很容易就可以获得Q和a的解析解。 实验结果 ImageNet图像识别：我们分别在Alexnet、VGG16、Resnet18、Resnet50、GoogleNet等五个主流的CNN框架上验证了所提出的算法。实验中我们分别尝试了Binary网络、Ternary网络、&amp;#123-2, -1, 0, 1, 2&amp;#125、&amp;#123-4, -2, -1, 0, 1, 2, 4&amp;#125四种形式。在Imagenet上Top-1和Top-5准确度结果如下： Alexnet和VGG16： Resnet： GoogleNet： 其中BWN&#91;1&#93;和TWN&#91;2&#93;为我们对比的两种Binary网络和Ternary网络量化方法。从这些结果可以看出，在各个网络框架下，我们的算法都显著超过对比算法。同时，当比特数达到3时，量化之后的网络精度相比于原始网络几乎可以达到无损。在Alexnet和VGG16这两个冗余度比较高的网络上，量化之后的网络甚至可以取得超过原始网络的精度，这是因为量化操作可以起到一个正则的作用，从而提高这类网络的泛化性能。 Pascal VOC目标检测：我们在SSD检测框架下对算法进行验证，分别采用了VGG16+SSD和Darknet+SSD两种网络结构。对于检测任务，尝试了Ternary网络和&amp;#123-4, -2, -1, 0, 1, 2, 4&amp;#125两种量化形式。实验结果如下： 对于Darknet我们使用了两种设置，第一种设置中所有的权重进行相同的量化；第二种设置中，1x1的卷积核使用INT8量化，即括号中的结果。和识别中的结果类似，在VGG+SSD结构中，我们的算法几乎可以做到无损压缩。 参考文献： &#91;1&#93; Rastegari, M.; Ordonez, V.; Redmon, J.; and Farhadi, A. 2016. Xnor-net&#58; Imagenet classification using binary convolutional neural networks. European Conference on Computer Vision. &#91;2&#93; Li, F.; Zhang, B.; and Liu, B. 2016. Ternary weight networks. arXiv preprint arXiv&#58;1605.04711. ※论文题目（中英文）：极限低比特神经网络：通过ADMM算法压缩掉最后一个比特 / Extremely Low Bit Neural Network&#58; Squeeze the Last Bit Out with ADMM ※主要作者（中英文）：冷聪 窦则胜 李昊 朱胜火 金榕 / Cong Leng, Zesheng Dou, Hao Li, Shenghuo Zhu, Rong Jin 加入社群 新智元AI技术+产业社群招募中，欢迎对AI技术+产业落地感兴趣的同学，加小助手微信号&#58; aiera2015_1 入群；通过审核后我们将邀请进群，加入社群后务必修改群备注（姓名-公司-职位；专业群审核较严，敬请谅解）。 此外，新智元AI技术+产业领域社群(智能汽车、机器学习、深度学习、神经网络等)正在面向正在从事相关领域的工程师及研究人员进行招募。 转载来源：阿里提出新神经网络算法，压缩掉最后一个比特]]></content>
      <categories>
        <category>科学</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>机器学习</tag>
        <tag>人工智能</tag>
        <tag>比尔·盖茨</tag>
        <tag>固态硬盘</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[我用Python实现了12500张猫狗图像的精准分类]]></title>
    <url>%2F2018%2Fb95bd127%2F</url>
    <content type="text"><![CDATA[在这篇文章中，我们将展示如何建立一个深度神经网络，能做到以 90% 的精度来对图像进行分类，而在深度神经网络，特别是卷积神经网络兴起之前，这还是一个非常困难的问题。 深度学习是目前人工智能领域里最让人兴奋的话题之一了，它基于生物学领域的概念发展而来，现如今是一系列算法的集合。 事实已经证明深度学习在计算机视觉、自然语言处理、语音识别等很多的领域里都可以起到非常好的效果。 在过去的 6 年里，深度学习已经应用到非常广泛的领域，很多最近的技术突破，都和深度学习相关。 这里仅举几个例子：特斯拉的自动驾驶汽车、Facebook 的照片标注系统、像 Siri 或 Cortana 这样的虚拟助手、聊天机器人、能进行物体识别的相机，这些技术突破都要归功于深度学习。 在这么多的领域里，深度学习在语言理解、图像分析这种认知任务上的表现已经达到了我们人类的水平。 如何构建一个在图像分类任务上能达到 90% 精度的深度神经网络？ 这个问题看似非常简单，但在深度神经网络特别是卷积神经网络（CNN）兴起之前，这是一个被计算机科学家们研究了很多年的棘手问题。 本文分为以下三个部分进行讲解： 展示数据集和用例，并且解释这个图像分类任务的复杂度。- 搭建一个深度学习专用环境，这个环境搭建在 AWS 的基于 GPU 的 EC2 服务上。- 训练两个深度学习模型：第一个模型是使用 Keras 和 TensorFlow 从头开始端到端的流程，另一个模型使用是已经在大型数据集上预训练好的神经网络。搭建一个深度学习专用环境，这个环境搭建在 AWS 的基于 GPU 的 EC2 服务上。 一个有趣的实例：给猫和狗的图像分类 有很多的图像数据集是专门用来给深度学习模型进行基准测试的，我在这篇文章中用到的数据集来自 Cat vs Dogs Kaggle competition，这份数据集包含了大量狗和猫的带有标签的图片。 和每一个 Kaggle 比赛一样，这份数据集也包含两个文件夹： 训练文件夹：它包含了 25000 张猫和狗的图片，每张图片都含有标签，这个标签是作为文件名的一部分。我们将用这个文件夹来训练和评估我们的模型。- 测试文件夹：它包含了 12500 张图片，每张图片都以数字来命名。对于这份数据集中的每幅图片来说，我们的模型都要预测这张图片上是狗还是猫（1= 狗，0= 猫）。事实上，这些数据也被 Kaggle 用来对模型进行打分，然后在排行榜上排名。测试文件夹：它包含了 12500 张图片，每张图片都以数字来命名。对于这份数据集中的每幅图片来说，我们的模型都要预测这张图片上是狗还是猫（1= 狗，0= 猫）。事实上，这些数据也被 Kaggle 用来对模型进行打分，然后在排行榜上排名。 我们观察一下这些图片的特点，这些图片各种各样，分辨率也各不相同。图片中的猫和狗形状、所处位置、体表颜色各不一样。 它们的姿态不同，有的在坐着而有的则不是，它们的情绪可能是开心的也可能是伤心的，猫可能在睡觉，而狗可能在汪汪地叫着。照片可能以任一焦距从任意角度拍下。 这些图片有着无限种可能，对于我们人类来说在一系列不同种类的照片中识别出一个场景中的宠物自然是毫不费力的事情，然而这对于一台机器来说可不是一件小事。 实际上，如果要机器实现自动分类，那么我们需要知道如何强有力地描绘出猫和狗的特征，也就是说为什么我们认为这张图片中的是猫，而那张图片中的却是狗。这个需要描绘每个动物的内在特征。 深度神经网络在图像分类任务上效果很好的原因是，它们有着能够自动学习多重抽象层的能力，这些抽象层在给定一个分类任务后又可以对每个类别给出更简单的特征表示。 深度神经网络可以识别极端变化的模式，在扭曲的图像和经过简单的几何变换的图像上也有着很好的鲁棒性。让我们来看看深度神经网络如何来处理这个问题的。 配置深度学习环境 深度学习的计算量非常大，当你在自己的电脑上跑一个深度学习模型时，你就能深刻地体会到这一点。 但是如果你使用 GPUs，训练速度将会大幅加快，因为 GPUs 在处理像矩阵乘法这样的并行计算任务时非常高效，而神经网络又几乎充斥着矩阵乘法运算，所以计算性能会得到令人难以置信的提升。 我自己的电脑上并没有一个强劲的 GPU，因此我选择使用一个亚马逊云服务 (AWS) 上的虚拟机，这个虚拟机名为 p2.xlarge，它是亚马逊 EC2 的一部分。 这个虚拟机的配置包含一个 12GB 显存的英伟达GPU、一个 61GB 的 RAM、4 个 vCPU 和 2496 个 CUDA 核。 可以看到这是一台性能巨兽，让人高兴的是，我们每小时仅需花费 0.9 美元就可以使用它。当然，你还可以选择其他配置更好的虚拟机，但对于我们现在将要处理的任务来说，一台 p2.xlarge 虚拟机已经绰绰有余了。 我的虚拟机工作在 Deep Learning AMI CUDA 8 Ubuntu Version 系统上，现在让我们对这个系统有一个更清楚的了解吧。 这个系统基于一个 Ubuntu 16.04 服务器，已经包装好了所有的我们需要的深度学习框架（TensorFlow，Theano，Caffe，Keras），并且安装好了 GPU 驱动（听说自己安装驱动是噩梦般的体验）。 如果你对 AWS 不熟悉的话，你可以参考下面的两篇文章： https&#58;//blog.keras.io/running-jupyter-notebooks-on-gpu-on-aws-a-starter-guide.html- https&#58;//hackernoon.com/keras-with-gpu-on-amazon-ec2-a-step-by-step-instruction-4f90364e49achttps&#58;//hackernoon.com/keras-with-gpu-on-amazon-ec2-a-step-by-step-instruction-4f90364e49ac 这两篇文章可以让你知道两点： 建立并连接到一个 EC2 虚拟机。- 配置网络以便远程访问 jupyter notebook。配置网络以便远程访问 jupyter notebook。 用 TensorFlow 和 Keras 建立一个猫/狗图片分类器 环境配置好后，我们开始着手建立一个可以将猫狗图片分类的卷积神经网络，并使用到深度学习框架 TensorFlow 和 Keras。 先介绍下 Keras：Keras 是一个高层神经网络 API，它由纯 Python 编写而成并基于Tensorflow、Theano 以及 CNTK 后端，Keras 为支持快速实验而生，能够把你的 idea 迅速转换为结果。 从头开始搭建一个卷积神经网络 首先，我们设置一个端到端的 pipeline 训练 CNN，将经历如下几步：数据准备和增强、架构设计、训练和评估。 我们将绘制训练集和测试集上的损失和准确度指标图表，这将使我们能够更直观地评估模型在训练中的改进变化。 数据准备 在开始之前要做的第一件事是从 Kaggle 上下载并解压训练数据集。 我们必须重新组织数据以便让 Keras 更容易地处理它们。我们创建一个 data 文件夹，并在其中创建两个子文件夹： train- validationvalidation 在上面的两个文件夹之下，每个文件夹依然包含两个子文件夹： cats- dogsdogs 最后我们得到下面的文件结构： 这个文件结构让我们的模型知道从哪个文件夹中获取到图像和训练或测试用的标签。这里提供了一个函数允许你来重新构建这个文件树，它有 2 个参数：图像的总数目、测试集 r 的比重。 我使用了： n：25000（整个数据集的大小） - r：0.2- ratio = 0.2- n = 25000- organize_datasets(path_to_data=’./train/‘,n=n, ratio=ratio)r：0.2 n = 25000 现在让我们装载 Keras 和它的依赖包吧： 图像生成器和数据增强 在训练模型时，我们不会将整个数据集装载进内存，因为这种做法并不高效，特别是你使用的还是你自己本地的机器。 我们将用到 ImageDataGenerator 类，这个类可以无限制地从训练集和测试集中批量地引入图像流。在ImageDataGenerator 类中，我们将在每个批次引入随机修改。 这个过程我们称之为数据增强（dataaugmentation)。它可以生成更多的图片使得我们的模型不会看见两张完全相同的图片。这种方法可以防止过拟合，也有助于模型保持更好的泛化性。 我们要创建两个 ImageDataGenerator 对象。train_datagen 对应训练集，val_datagen 对应测试集，两者都会对图像进行缩放，train_datagen 还将做一些其他的修改。 基于前面的两个对象，我们接着创建两个文件生成器： train_generator- validation_generatorvalidation_generator 每个生成器在实时数据增强的作用下，在目录处可以生成批量的图像数据。这样，数据将会无限制地循环生成。 模型结构 我将使用拥有 3 个卷积/池化层和 2 个全连接层的 CNN。3 个卷积层将分别使用 32，32，64 的 3 * 3的滤波器（fiter）。在两个全连接层，我使用了 dropout 来避免过拟合。 我使用随机梯度下降法进行优化，参数 learning rate 为 0.01，momentum 为 0.9。 Keras 提供了一个非常方便的方法来展示模型的全貌。对每一层，我们可以看到输出的形状和可训练参数的个数。在开始拟合模型前，检查一下是个明智的选择。 model.summary() 下面让我们看一下网络的结构。 结构可视化 在训练模型前，我定义了两个将在训练时调用的回调函数 (callback function)： 一个用于在损失函数无法改进在测试数据的效果时，提前停止训练。- 一个用于存储每个时期的损失和精确度指标：这可以用来绘制训练错误图表。一个用于存储每个时期的损失和精确度指标：这可以用来绘制训练错误图表。 我还使用了 keras-tqdm，这是一个和 keras 完美整合的非常棒的进度条。它可以让我们非常容易地监视模型的训练过程。 要想使用它，你仅需要从 keras_tqdm 中加载 TQDMNotebookCallback 类，然后将它作为第三个回调函数传递进去。 下面的图在一个简单的样例上展示了 keras-tqdm 的效果。 关于训练过程，还有几点要说的： 我们使用 fit_generator 方法，它是一个将生成器作为输入的变体（标准拟合方法）。- 我们训练模型的时间超过 50 个 epoch。我们训练模型的时间超过 50 个 epoch。 这个模型运行时的计算量非常大： 如果你在自己的电脑上跑，每个 epoch 会花费 15 分钟的时间。- 如果你和我一样在 EC2 上的 p2.xlarge 虚拟机上跑，每个 epoch 需要花费 2 分钟的时间。如果你和我一样在 EC2 上的 p2.xlarge 虚拟机上跑，每个 epoch 需要花费 2 分钟的时间。 分类结果 我们在模型运行 34 个 epoch 后达到了 89.4% 的准确率（下文展示训练/测试错误和准确率），考虑到我没有花费很多时间来设计网络结构，这已经是一个很好的结果了。现在我们可以将模型保存，以备以后使用。 model.save(`./models/model4.h5) 下面我们在同一张图上绘制训练和测试中的损失指标值： 当在两个连续的 epoch 中，测试损失值没有改善时，我们就中止训练过程。 下面绘制训练集和测试集上的准确度。 这两个指标一直是增长的，直到模型即将开始过拟合的平稳期。 装载预训练的模型 我们在自己设计的 CNN 上取得了不错的结果，但还有一种方法能让我们取得更高的分数：直接载入一个在大型数据集上预训练过的卷积神经网络的权重，这个大型数据集包含 1000 个种类的猫和狗的图片。 这样的网络会学习到与我们分类任务相关的特征。 我将加载 VGG16 网络的权重，具体来说，我要将网络权重加载到所有的卷积层。这个网络部分将作为一个特征检测器来检测我们将要添加到全连接层的特征。 与 LeNet5 相比，VGG16 是一个非常大的网络，它有 16 个可以训练权重的层和 1.4 亿个参数。要了解有关 VGG16 的信息，请参阅此篇 pdf 链接&#58;https&#58;//arxiv.org/pdf/1409.1556.pdf 现在我们将图像传进网络来得到特征表示，这些特征表示将会作为神经网络分类器的输入。 图像在传递到网络中时是有序传递的，所以我们可以很容易地为每张图片关联上标签。 现在我们设计了一个小型的全连接神经网络，附加上从 VGG16 中抽取到的特征，我们将它作为 CNN 的分类部分。 在 15 个 epoch 后，模型就达到了 90.7% 的准确度。这个结果已经很好了，注意现在每个 epoch 在我自己的电脑上跑也仅需 1 分钟。 许多深度学习领域的大牛人物都鼓励大家在做分类任务时使用预训练网络，实际上，预训练网络通常使用的是在一个非常大的数据集上生成的非常大的网络。 而 Keras 可以让我们很轻易地下载像 VGG16、GoogleNet、ResNet 这样的预训练网络。想要了解更多关于这方面的信息，请参考这里：https&#58;//keras.io/applications/ 有一句很棒的格言是：不要成为英雄！不要重复发明轮子！使用预训练网络吧！ 接下来还可以做什么？ 如果你对改进一个传统 CNN 感兴趣的话，你可以： 在数据集层面上，引入更多增强数据。- 研究一下网络超参数（network hyperparameter)：卷积层的个数、滤波器的个数和大小，在每种组合后要测试一下效果。- 改变优化方法。- 尝试不同的损失函数。- 使用更多的全连接层。- 引入更多的 aggressive dropout。研究一下网络超参数（network hyperparameter)：卷积层的个数、滤波器的个数和大小，在每种组合后要测试一下效果。 尝试不同的损失函数。 引入更多的 aggressive dropout。 如果你对使用预训练网络获得更好的分类结果感兴趣的话，你可以尝试： 使用不同的网络结构。- 使用更多包含更多隐藏单元的全连接层。使用更多包含更多隐藏单元的全连接层。 如果你想知道 CNN 这个深度学习模型到底学习到了什么东西，你可以： 将 feature maps 可视化。- 可以参考：https&#58;//arxiv.org/pdf/1311.2901.pdf可以参考：https&#58;//arxiv.org/pdf/1311.2901.pdf 如果你想使用训练过的模型： 可以将模型放到 Web APP 上，使用新的猫和狗的图像来进行测试。这也是一个很好地测试模型泛化性的好方法。总结 这是一篇手把手教你在 AWS 上搭建深度学习环境的教程，并且教你怎样从头开始建立一个端到端的模型，另外本文也教了你怎样基于一个预训练的网络来搭建一个 CNN 模型。 用 Python 来做深度学习是让人愉悦的事情，而 Keras 让数据的预处理和网络层的搭建变得更加简单。 如果有一天你需要按自己的想法来搭建一个神经网络，你可能需要用到其他的深度学习框架。 现在在自然语言处理领域，也有很多人开始使用卷积神经网络了，下面是一些基于此的工作： 使用了 CNN 的文本分类：https&#58;//chara.cs.illinois.edu/sites/sp16-cs591txt/files/0226-presentation.pdf- 自动为图像生成标题：https&#58;//cs.stanford.edu/people/karpathy/sfmltalk.pdf- 字级别的文本分类：https&#58;//papers.nips.cc/pahttps&#58;//chara.cs.illinois.edu/sites/sp16-cs591txt/files/0226-presentation.pdf https&#58;//cs.stanford.edu/people/karpathy/sfmltalk.pdf https&#58;//papers.nips.cc/pa 转载来源：我用Python实现了12500张猫狗图像的精准分类]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>编程语言</tag>
        <tag>深度学习</tag>
        <tag>Python</tag>
        <tag>机器学习</tag>
        <tag>GPU</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[以太坊是什么 - 以太坊开发入门指南 - Tiny熊 - 博客园]]></title>
    <url>%2F2018%2F5d0352db%2F</url>
    <content type="text"><![CDATA[以太坊是什么 - 以太坊开发入门指南 - Tiny熊 - 博客园 转载来源：以太坊是什么 - 以太坊开发入门指南 - Tiny熊 - 博客园]]></content>
  </entry>
  <entry>
    <title><![CDATA[收藏｜15000个Python开源项目中精选Top30，Github平均star为3707]]></title>
    <url>%2F2018%2F3adde068%2F</url>
    <content type="text"><![CDATA[翻译 | AI科技大本营（ID：rgznai100） 参与 | SuiSui 继推出2017年机器学习开源项目Top 30榜单后，Mybridge AI又推出了一个Python开源项目Top 30榜单，包括开源Python库、工具等。该榜单基于项目质量、用户参与度以及其他几个方面进行了评估，从大约15000个开源项目中挑选了Top 30，差不多都是在2017年1-12月发布。这些项目在Github上的平均star为3707。 No 1：Home-assistant (v0.6+) 基于Python 3的开源家庭自动化平台&#91;Github 11357 stars，由Paulus Schoutsen提供&#93; https&#58;//github.com/home-assistant/home-assistant No 2：Pytorch PyTorch是使用GPU和CPU优化的深度学习张量库，基于Python语言编写。&#91;Github 11019 stars，由PyTorch团队的Adam Paszke和其他人提供&#93; https&#58;//github.com/pytorch/pytorch No 3：Grumpy Grumpy是一个Python to Go的源代码翻译编译器和运行时，旨在取代CPython 2.7。关键区别在于，Grumpy是将Python源码编译为Go源代码，然后将其编译为native code，而不是bytecode。这也就意味着Grumpy没有虚拟机（VM）。编译好的Go源码是对Grumpy运行时的一系列调用，一个Go库服务于具有相似目的的Python C API。 &#91;Github 8367 stars，由Google的Dylan Trotter及其他工作人员提供&#93;。 https&#58;//github.com/google/grumpy No 4：Sanic 该项目是一个类 Flask 的 Python 3.5+ 网页服务器，专为加速而设计。Sanic支持异步请求处理，意味着你可以使用Python 3.5中一些async/await语法。。&#91;Github 8028 stars，由Channel Cat和Eli Uriegas提供&#93; https&#58;//github.com/channelcat/sanic No 5：Python-fire 一个可以从任何Python对象自动生成命令行界面（CLI）的库。 &#91;Github 7775 stars，来自Google Brain 的 David Bieber&#93; https&#58;//github.com/channelcat/sanic No 6：spaCy（v2.0） 该项目是一个使用Python和Cython的进行高级自然语言处理（NLP）的开源库 &#91;Github 7633 stars，由Matthew Honnibal提供&#93; https&#58;//github.com/explosion/spaCy No 7：Pipenv Python.org官方推荐的Python打包工具。它会自动为项目创建和管理virtualenv，并在安装/卸载软件包时从Pipfile中添加/删除软件包。 &#91;Github 7273 stars，由Kenneth Reitz提供&#93; https&#58;//github.com/pypa/pipenv No 8：MicroPython 一个脱胎于Python且非常高效的Python实现，主要是为了能在嵌入式硬件上（这里特指微控制器级别）更简单地实现对底层的操作。&#91;Github 5728 stars&#93; https&#58;//github.com/micropython/micropython No 9：Prophet 该工具是Facebook开源的一款用于为多周期性的线性或非线性时间序列数据生成高质量预测的工具。&#91;Github 4369 stars，由Facebook提供&#93; https&#58;//github.com/facebook/prophet No 10：Serpent AI 该项目是一个Python写的游戏代理框架，简单而强大，可帮助开发者创建游戏代理。可将任何视频游戏变成一个Python写成的成熟沙箱环境。该框架的目的是为机器学习和AI研究提供一个有价值的工具，不过对于爱好者来说也是非常有趣的。&#91;Github 3411 stars，由Nicholas Brochu提供&#93; https&#58;//github.com/SerpentAI/SerpentAI No 11：Dash Dash是一个纯Python写成的框架，无需JavaScript即可构建交互式的分析类web应用程序。&#91;Github 3281 stars，由Chris P提供&#93; https&#58;//github.com/plotly/dash No 12：InstaPy Instagram机器人，喜欢/评论/Follow 自动化脚本。&#91;Github 3179 stars，由TimG提供&#93;。 https&#58;//github.com/timgrossmann/InstaPy No 13：Apistar 专为Python 3定制的Web API框架&#91;Github 3024 stars，Tom Christie提供&#93;。 https&#58;//github.com/encode/apistar No 14：Faiss 用于密集向量的高效相似性搜索库和聚类的库 &#91;GitHub 2717 stars，贡献者Facebook Research&#93; https&#58;//github.com/facebookresearch/faiss No 15：MechanicalSoup 一个与网站自动交互的Python库，自动存储和发送cookies，支持重定向，并可以跟踪链接和提交表格。&#91;Github 2244 stars&#93; https&#58;//github.com/MechanicalSoup/MechanicalSoup No 16：Better-exceptions 该项目以更友好的形式展示Python中的异常信息。&#91;Github 2121 stars，贡献者Qix&#93; https&#58;//github.com/Qix-/better-exceptions No 17：Flashtext 该项目基于FlashText算法，用以高效搜索句子中的关键词并进行替代。&#91;Github 2019 stars，由Vikash Singh提供&#93;。 https&#58;//github.com/vi3k6i5/flashtext No 18：Maya 在不同系统上的不同语言环境中，Python对日期时间的处理非常不畅，Maya主要就是为了解决解析网站时间数据问题。&#91;Github 1828 stars，Kenneth Reitz提供&#93; https&#58;//github.com/kennethreitz/maya No 19：Mimesis 是一个快速易用的Python库，可以用不同语言为基于不同的目的生成合成数据。这些数据在软件开发和测试阶段非常有用。&#91;Github 1732 stars，由LíkieGeimfari提供&#93; https&#58;//github.com/lk-geimfari/mimesis No 20：Open-paperless 该项目是一个一个文件管理系统，可扫描、索引和归档所有纸张文档。&#91;Github 1717 stars，由Tina Zhou提供&#93; https&#58;//github.com/zhoubear/open-paperless No 21：Fsociety 黑客工具包，渗透测试框架。&#91;Github 1585 stars，Manis Manisso提供&#93; No 22：LivePython Python代码实时可视化跟踪。&#91;Github 1577 stars，由Anastasis Germanidis提供&#93; https&#58;//github.com/agermanidis/livepython No 23：Hatch 一个Python项目、包以及虚拟环境的管理工具。&#91;Github 1537 stars，由Ofek Lev提供&#93; https&#58;//github.com/ofek/hatch No 24：Tangent 该项目是谷歌开源的一个用于自动微分的源到源纯Python库。&#91;Github 1433 stars，来自Google Brain的Alex Wiltschko以及其他人&#93;。 https&#58;//github.com/google/tangent No 25：Clairvoyant 一个Python程序，用于识别和监控短期库存移动的历史线索&#91;Github 1159 stars，由Anthony Federico提供&#93;。 https&#58;//github.com/anfederico/Clairvoyant No 26：MonkeyType 该项目是Instagram开源的一款适用于Python的工具，通过收集运行时类型来生成静态类型注释。&#91;Github 1137 stars，由Instagram工程师Carl Meyer提供&#93;。 https&#58;//github.com/Instagram/MonkeyType No 27：Eel 该项目是一个小型Python库，用于制作简单的类似 Electron的离线HTML/JS GUI应用程序，当前仅支持Python3。 &#91;Github 1137 stars&#93; https&#58;//github.com/ChrisKnott/Eel No 28：Surprise v1.0 用于构建和分析推荐系统的Python scikit &#91;Github 1103 stars&#93; https&#58;//github.com/NicolasHug/Surprise No 29：Gain Web爬虫框架。&#91;Github 1009 stars，由高久力提供&#93; https&#58;//github.com/gaojiuli/gain No 30：PDFTabExtract 一组用于从PDF文件中提取表格的工具，有助于在扫描的文档上进行数据挖掘。 &#91;Github 722 stars&#93; https&#58;//github.com/WZBSocialScienceCenter/pdftabextract 原文地址：https&#58;//medium.mybridge.co/30-amazing-python-projects-for-the-past-year-v-2018-9c310b04cdb3 转载来源：收藏｜15000个Python开源项目中精选Top30，Github平均star为3707]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>编程语言</tag>
        <tag>GitHub</tag>
        <tag>Python</tag>
        <tag>机器学习</tag>
        <tag>Google</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习实战：用胶囊网络识别交通标志]]></title>
    <url>%2F2018%2Ff4794f7d%2F</url>
    <content type="text"><![CDATA[每个人似乎都对胶囊网络（CapsNet）这种新的神经网络架构的出现很兴奋，我也不例外，忍不住用胶囊网络实现了一个交通标志识别系统，这篇文章就是对这一过程的介绍，当然，也包括胶囊网络的一些基本概念阐述。 项目使用TensorFlow开发，参考的论文是Sara Sabour，Nicholas Frosst和Geoffrey E. Hinton的《 胶囊间动态路由 》，代码保存在github（https&#58;//github.com/thibo73800/capsnet-traffic-sign-classifier）。如果你迫不及待想要试试Tensorflow等机器学习框架，可以访问汇智网的Python机器学习在线环境（http&#58;//www.hubwiz.com/python-ml.html?affid=vat）。 卷积神经网络有什么问题？卷积神经网络（CNN）的问题部分源于其对图像感知的泛化能力，例如一个训练好的图像识别网络可能会对同一图像的旋转版本识别错误，这就是为什么在训练时经常使用数据增强和平均/最大池（Average / Max Pooling）。 池化通过随机选择下一层的神经元子集建立一个新的层。 这可以有效降低上层的计算需求，而且也使得网络减小对特征出现的原始位置的依赖性。 这一简化的依据在于：我们假设特征出现的确切位置对目标识别而言影响不大。 和CNN一样，上层的胶囊可以覆盖更大的图像区域，但是与最大化池不同，我们不会丢弃该区域内目标物体的准确位置信息。 这使得模型对图像中的细微变化可以保持不变的输出。 另一方面，模型有可能忽视图像发生的位移变化。不变意味着无论检测到的字符的顺序和位置是否改变，网络的输出总是相同的。 因此该模型能够理解图像中的特征的旋转和位移，并产生适当的输出。 这对于使用池化来说是不可能的。 这就是启发我们发明这个新架构的原因。 胶囊网络胶囊网络赋予了模型理解图像中所发生变化的能力，从而可以更好地概括所感知的内容。 要了解这个架构如何运作，重要的是掌握胶囊的概念。 胶囊是一组神经元，其激活向量表示某种特定类型的实体（如对象或对象部分）的实例化参数。 我们习惯从深度角度来谈论深度学习，而胶囊网络则引入了嵌套的概念，嵌套为深度引入了一个新的维度。 不是采用添加层的方法来增加网络的深度，相反，胶囊网络是在另一个层中添加（多个）新的图层。 这有点抽象， 但是当你仔细观察时，就会发现情况并不是那么复杂。 在论文中，这一方法的核心分为两部分： 基础胶囊和数字胶囊 。 在我们的案例中，后一部分将被重命名为交通标志胶囊 。 胶囊和基础胶囊 这一层基于经典的卷积计算，创建一个新的由N C滤波器组成的卷积层。 N表示滤波器的数量，C表示每个胶囊的尺寸。 因此会创建出具有（T，T）大小的N C个新图像。 在上图中，每个胶囊的值在新创建的图像中以红色显示。 Tensorflow代码如下： 现在创建好了卷积操作，我们可以重排这些卷积以便创建胶囊操作： 然后我们得到T T N个大小为C的胶囊（在这个项目中是1152个胶囊）。 需要指出的是，卷积的第一个C值（见代码中的注释）等于第一个胶囊的值，正如上面代码所示。 最后，原论文中给出了一个新的非线性函数，可以单独应用于每个胶囊。 这个新函数被称为挤压 （Squashing），看起来像这样： 因此，我们使用非线性Squashing函数来确保将短矢量的长度压缩到接近零，而长矢量的长度压缩到略低于1 交通标志胶囊 在本项目中，这个层由43个胶囊组成，每个胶囊代表一种特定的交通标志。 为了确定模型的预测结果，我们可以选择具有最大长度的胶囊。 但在此之前，需要在前一层的1152个胶囊之间进行转换。 这将通过路由的方法完成。 该方法的作用是选中前一层的哪些胶囊与输出层胶囊进行关联。 换句话说，对于每个胶囊，会有一个新的神经网络进行判断：”嘿，这个胶囊对这个类的判别有价值吗？” 使用迭代的路由处理过程，每个活动胶囊将在上面的层中选择一个胶囊，作为它在树中的父节点。 在路由中，对特征的选择不再是像池化那样随意。 在这篇文章中，我不会详细介绍路由所使用的确切公式，论文中有这些公式的描述。 本项目的实现代码在我的github 。 我还在继续改进以使算法更具可扩展性。 对于交通标志胶囊和路由，我在实现中尽量遵循了论文中的数学公式。 图像重建这种方法有助于引导网络将胶囊向量视为实际的物体，允许在重建之前对每个图像进行编码。 这在正则化方面也得到了很好的结果。 我们使用额外的重建损失来鼓励数字胶囊对输入数字的实例化参数进行编码。 这部分实现代码也包含在项目的github中，代码中的图像重建实现，使用了卷积和最近邻算法来放大图像。 事实上，我不能只是创建一堆简单的层，因为要重建的图像包含3个输出通道。 尽管在MNIST数据中这个实现表现得相当好，但我对其在大规模解决方案中的有效性还存有一些怀疑，不过这只是我的个人观点。 因此，模型最终的损失是基于两种可选的损失： 边际损失：基于模型的实际预测。 这是最高标准的胶囊。- 重建损失：基于图像之间平方差的解码器损失的平均值。重建损失：基于图像之间平方差的解码器损失的平均值。 模型架构由于我处理的数据集与原论文不同，所以模型架构也做了一些调整。 第一个卷积使用256个滤波器、大小为9的核（VALID填充）、RELU激活，dropout取值0.7。 基础胶囊层包含16个滤波器、大小为5的核、16个胶囊。最终获得256个（10,10）大小的滤波器。 即1600个16值胶囊。 最后一层（交通标志胶囊）由大小为32的43个胶囊（43个类）组成。 上述结构的构建代码如下： 训练训练时我使用了Keras的ImageDataGenerator以便进行数据增强。 结果（准确度）： 训练：99％- 验证：98％- 测试：97％验证：98％ 这个结果没能达到经典的卷积神经网络的最佳效果。 但是，考虑到我大部分时间都是在实现胶囊网络，而不是花在超参数调整和图像处理方面，因此对我来说，97％算是初次尝试的好成绩。 我现在还在努力提高这个指标。 分类示例 如果你喜欢这篇文章，请关注我的头条号：新缸中之脑！原文：Understand and apply CapsNet on Traffic sign classification 转载来源：机器学习实战：用胶囊网络识别交通标志]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>GitHub</tag>
        <tag>机器学习</tag>
        <tag>图像处理</tag>
        <tag>新创建集团</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Django+Vue.js 初学入手的一些坑，已填坑 - CSDN博客]]></title>
    <url>%2F2018%2F2cbad18a%2F</url>
    <content type="text"><![CDATA[我用的django是1.11.0 vue是最新的，这是前提，之前因django版本不对导致一系列问题。另外要讲项目部署在Linux上运行，windows下运行也会出现很多坑。 接下来是我参考网上的一片文章开始搭建环境，其中有问题的地方会用红色加粗说明。 1. 创建Django项目命令： 1django-admin startproject ulb_manager 结构： 1234567.├── manage.py└── ulb_manager ├── __init__.py ├── settings.py ├── urls.py └── wsgi.py 2. 进入项目根目录，创建一个 app 作为项目后端命令： 12cd ulb_managerpython manage.py startapp backend 即：app 名叫做 backend 结构： 123456789101112131415.├── backend│ ├── __init__.py│ ├── admin.py│ ├── migrations│ │ └── __init__.py│ ├── models.py│ ├── tests.py│ └── views.py├── manage.py└── ulb_manager ├── __init__.py ├── settings.py ├── urls.py └── wsgi.py 3. 使用vue-cli创建一个vuejs项目作为项目前端命令： 1vue-init webpack frontend 即：项目名叫 frontend 结构： 123456789101112131415161718192021222324252627282930313233343536.├── backend│ ├── __init__.py│ ├── admin.py│ ├── migrations│ │ └── __init__.py│ ├── models.py│ ├── tests.py│ └── views.py├── frontend│ ├── README.md│ ├── build│ │ └── ....│ ├── config│ │ ├── dev.env.js│ │ ├── index.js│ │ ├── prod.env.js│ │ └── test.env.js│ ├── index.html│ ├── package.json│ ├── src│ │ ├── App.vue│ │ ├── assets│ │ │ └── logo.png│ │ ├── components│ │ │ └── Hello.vue│ │ └── main.js│ ├── static│ └── test│ └── ...├── manage.py└── ulb_manager ├── __init__.py ├── settings.py ├── urls.py └── wsgi.py 结构总结： backend Django的一个app- frontend Vuejs项目4. 接下来我们使用 webpack 打包Vusjs项目 命令： 123cd frontendnpm installnpm run build 结构： 123456789101112131415dist├── index.html└── static ├── css │ ├── app.42b821a6fd065652cb86e2af5bf3b5d2.css │ └── app.42b821a6fd065652cb86e2af5bf3b5d2.css.map ├── fonts │ ├── element-icons.a61be9c.eot │ └── element-icons.b02bdc1.ttf ├── img │ └── element-icons.09162bc.svg └── js ├── 0.8750b01fa7ffd70f7ba6.js ├── vendor.804853a3a7c622c4cb5b.js └── vendor.804853a3a7c622c4cb5b.js.map 构建完成会生成一个 文件夹名字叫dist，里面有一个 index.html 和一个 文件夹static ， 5. 使用Django的通用视图 TemplateView找到项目根 urls.py (即ulb_manager/urls.py)，使用通用视图创建最简单的模板控制器，访问 『/』时直接返回 index.html 12345urlpatterns = &amp;#91; url(r&apos;^admin/&apos;, admin.site.urls), **url(r&apos;^$&apos;, TemplateView.as_view(template_name=&quot;index.html&quot;)),** url(r&apos;^api/&apos;, include(&apos;backend.urls&apos;, namespace=&apos;api&apos;))&amp;#93; 6. 配置Django项目的模板搜索路径上一步使用了Django的模板系统，所以需要配置一下模板使Django知道从哪里找到index.html 打开 settings.py (ulb_manager/settings.py)，找到TEMPLATES配置项，修改如下&#58; 12345678910111213141516TEMPLATES = &amp;#91; &amp;#123 &apos;BACKEND&apos;&amp;#58; &apos;django.template.backends.django.DjangoTemplates&apos;, # &apos;DIRS&apos;&amp;#58; &amp;#91;&amp;#93;, **&apos;DIRS&apos;&amp;#58; &amp;#91;&apos;frontend/dist&apos;&amp;#93;**, &apos;APP_DIRS&apos;&amp;#58; True, &apos;OPTIONS&apos;&amp;#58; &amp;#123 &apos;context_processors&apos;&amp;#58; &amp;#91; &apos;django.template.context_processors.debug&apos;, &apos;django.template.context_processors.request&apos;, &apos;django.contrib.auth.context_processors.auth&apos;, &apos;django.contrib.messages.context_processors.messages&apos;, &amp;#93;, &amp;#125, &amp;#125,&amp;#93; 注意这里的 frontend 是VueJS项目目录，dist则是运行 npm run build 构建出的index.html与静态文件夹 static 的父级目录 7. 配置静态文件搜索路径打开 settings.py (ulb_manager/settings.py)，找到 STATICFILES_DIRS 配置项，配置如下&#58; 1234# Add for vuejsSTATICFILES_DIRS = &amp;#91; os.path.join(BASE_DIR, &quot;frontend/dist/static&quot;),&amp;#93; 这样Django不仅可以将/ulb 映射到index.html，而且还可以顺利找到静态文件 此时访问 /ulb 我们可以看到使用Django作为后端的VueJS helloworld 然后运行项目就可以看到所谓的vue界面了。 转载来源：Django+Vue.js 初学入手的一些坑，已填坑 - CSDN博客]]></content>
  </entry>
  <entry>
    <title><![CDATA[GitHub - ethereum/meteor-dapp-wallet]]></title>
    <url>%2F2018%2F07d3aead%2F</url>
    <content type="text"><![CDATA[GitHub - ethereum/meteor-dapp-wallet 转载来源：GitHub - ethereum/meteor-dapp-wallet]]></content>
  </entry>
  <entry>
    <title><![CDATA[以太坊代币入门指南 » EthFans | 以太坊爱好者]]></title>
    <url>%2F2018%2F3267cb46%2F</url>
    <content type="text"><![CDATA[以太坊代币入门指南 » EthFans | 以太坊爱好者 转载来源：以太坊代币入门指南 » EthFans | 以太坊爱好者]]></content>
  </entry>
  <entry>
    <title><![CDATA[Hyperledger Fabric概述 - 廉贞 - 博客园]]></title>
    <url>%2F2018%2Fbe58a435%2F</url>
    <content type="text"><![CDATA[Hyperledger Fabric概述 - 廉贞 - 博客园 转载来源：Hyperledger Fabric概述 - 廉贞 - 博客园]]></content>
  </entry>
  <entry>
    <title><![CDATA[阿里等联合论文：基于对抗学习的众包标注用于中文命名实体识别]]></title>
    <url>%2F2017%2F87f01e4a%2F</url>
    <content type="text"><![CDATA[机器之心发布 主要作者：杨耀晟，张梅山，陈文亮，王昊奋，张伟，张民 国际知名的人工智能学术会议 AAAI 2018 即将于 2 月份在美国新奥尔良举办，据机器之心了解，阿里巴巴共有 11 篇论文被接收。机器之心 AAAI 2018 论文专栏，将会对其中的数篇论文进行介绍，同时也欢迎读者推荐更多优质的 AAAI 2018 接收论文。 本文介绍了阿里巴巴业务平台事业部、深圳 Gowild（中文：狗尾草）智能科技有限公司、苏州大学联合发布的论文《Adversarial Learning for Chinese NER from Crowd Annotations》。该论文提出了一种在中文 NER 任务上，利用众包标注结果来训练模型的方法。 1. 文章目的与思想 为了能用较低的成本获取新的标注数据，我们采用众包标注的方法来完成这个任务。众包标注的数据是没有经过专家标注员审核的，所以它会包含一定的噪声。在这篇文章中，我们提出一种在中文 NER 任务上，利用众包标注结果来训练模型的方法。受到对抗学习的启发，我们在模型中使用了两个双向 LSTM 模块，来分别学习众包标注数据中的公有信息和属于不同标注员的私有信息。对抗学习的思想体现在公有块的学习过程中，以不同标注员作为分类目标进行对抗学习，从而优化公有模块的学习质量，使之收敛于真实数据（专家标注数据）。我们认为这两个模块学习到的信息对于任务学习都有积极作用，并在最终使用 CRF 层完成 NER 标注。 模型如下： 数据使用： 我们在对话数据和电商数据上对模型的性能进行验证。 1）对话数据是由 gowild 公司提供的，我们让 43 位标注员在两万句语料上标注「人名」和「歌名」实体。我们认为这份数据非常适合我们的任务。 （1）若让一位专家标注员标注对话数据，由于他的认知是有限的，所以当他出现标注失误时对模型的影响是比较大的。在这种情况下，多位标注员可以在一定程度上弥补单个标注员对于「歌名」和「人名」的认知不足。例如：歌手「周传雄」，但并不是所有人都知道他的另一个称呼「小刚」。多人的知识面肯定要比一个人来的广。 （2）人机对话语料中包含一定比例的语法错误： 你怎么子我都看的手机死机了，在弄自己开门进来干嘛都记得。- 你说谢谢的诗意哥哥吗？你说谢谢的诗意哥哥吗？ 不同的标注员对于上述句子的语义理解可能是不同的，我们也希望模型能学习到这些特征，使模型能更好收敛到最真实的数据分布，提高模型的泛化能力。 最终，我们的模型在对话数据上取得了近一个点的 F1 提升。 2）电商数据是由阿里巴巴提供。首先我们让五位标注员对标题数据和用户请求数据进行标注，目标是标注出已定义好的五类实体：品牌、产品、型号、规格、原料，每句标注任务随机分配给两位标注员。对于标注员的标注结果，我们通过样本抽样，分析得到造成标注噪声（标注不一致）的主要原因是不同标注员对于标注规范和每一句标注样例的认识是不同的。特别是在标题数据集中，产品、型号实体的边界定义非常容易造成标注不一致。 在上述众包标注得到的数据集上训练我们论文中提出的模型，可以得到一个点左右的提升： 文章分块解析： 相关工作： （1）序列标注：早期用来处理序列标注问题的模型都十分依赖人工设计的特征模板，例如：HMM, MEMM 和 CRF 模型，模型的性能很大程度上受限于特征模板的质量。神经网络热潮来临后，一个成熟的新模型被广泛应用：它使用双向 LSTM 来提取序列特征，并用 CRF 解码，在序列任务上取得了显著成果，这也是我们文章中的 baseline 模型。 （2）对抗训练：对抗网络最早被成功的应用在计算机视觉领域。近几年，「对抗」这一概念也被引入到 NLP 任务中，分别在跨语言、跨领域和多任务学习中取得突破。在这些任务中使用「对抗学习」，目的就在于学习到训练语料中的「共有特征」。我们的工作也是以这一目的为出发点，希望通过对抗学习的方式，让模型能分辨出「众包」数据中的「标注噪声」。 （3）众包标注模式：为了能在短时间内以较低成本获取标准语料，我们采用众包标注的模式，具体得到的数据情况见上面的「数据使用」。 Baseline 在文章的所有实验中，我们使用 BIOE 的标签集合。首先，我们训练 CRF 作为传统 baseline 模型。随后，尝试将序列特征映射到更高维度，也就是用 LSTM 模块提取特征。在中文任务中，输入单位为 char（字符），每个字符经过 lookup-table 映射成向量后，经过双向的 LSTM 层提取特征： 最终用 CRF 层进行解码，使模型能更好得学习标签之间的依赖关系： 这一部分的 loss 为： 优化目标为最小化这个 loss 值。 对抗学习部分：Worker Adversarial 我们使用的是众包数据作为训练语料，数据集中存在一定量的标注错误，即「噪声」。这些标注不当或标注错误都是由标注员带来的。不同标注员对于规范的理解和认识面是不同的，我们可以认为一位标注质量高的标注员的标注结果和专家标注员是非常相近的。对抗学习模块如下： 1.baseline 中的 BiLSTM 称为「private」，它的学习目标是拟合多为标注员的独立分布；再加入一个名为「common」的 BiLSTM 模块，common 与 private 的输入相同，它的作用是学习标注结果之间的共有特征： 2.再引入一个新的 BiLSTM 模块，名为「label」，以当前训练样例的标注结果序列为输入。 分别将 common 和 private 模块的输出合并，作为 NER 部分的输入： 最后用 CRF 解码，公式与 baseline 相同，不再贴出。 label 和 common 的输出合并，再输入 CNN 进行特征提取，最终对标注员进行分类： 要注意的是，我们希望标注员分类器最终失去判断能力，所以它在优化时要反向更新： 转载来源：阿里等联合论文：基于对抗学习的众包标注用于中文命名实体识别]]></content>
      <categories>
        <category>科技</category>
      </categories>
      <tags>
        <tag>美国</tag>
        <tag>电子商务</tag>
        <tag>歌手</tag>
        <tag>苏州大学</tag>
        <tag>张伟</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[中国人未来怎么租房子？]]></title>
    <url>%2F2017%2F8a03af59%2F</url>
    <content type="text"><![CDATA[中国人未来怎么租房子？ 转载来源：中国人未来怎么租房子？]]></content>
      <tags>
        <tag>凤凰财经</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Êý¾Ý½»»»¸ñÊ½FlatBuffers½éÉÜ - ÅÝÔÚÍøÉÏµÄÈÕ×Ó]]></title>
    <url>%2F2017%2F2751070a%2F</url>
    <content type="text"><![CDATA[Êý¾Ý½»»»¸ñÊ½FlatBuffers½éÉÜ - ÅÝÔÚÍøÉÏµÄÈÕ×Ó 转载来源：Êý¾Ý½»»»¸ñÊ½FlatBuffers½éÉÜ - ÅÝÔÚÍøÉÏµÄÈÕ×Ó]]></content>
  </entry>
  <entry>
    <title><![CDATA[ELK 为什么这么流行？｜GIAC 访谈]]></title>
    <url>%2F2017%2F635fc0ef%2F</url>
    <content type="text"><![CDATA[在大会前夕，高可用架构采访了本届大会讲师曾勇，就目前业界非常流行的 ELK 框架等问题进行了访谈。 曾勇（Medcl），Elastic 开发工程师与布道师，在分布式搜索、高性能、高可用架构、自动化运维等方面积累了超过七年的经验。Elastic 开源社区负责人。 1、曾勇，您好，很荣幸能采访到您。您作为 Elastic Stack 中国布道师，能否简单分享一下您在 ELK 布道过程中的收获和感受？ 这个问题很好，首先作为布道师，首要任务就是让更多的人了解我们的产品，链接社区和我们的开源产品，那自然而然就要接触到更多的人，这个是我之前工作经验中所不同的地方，收获就是能够听到更多用户的声音，然后再反馈到我们的产品改进中。Elastic 的产品一直在改进，我们一直在倾听社区和客户的声音，用户其实有很多诉求，功能，性能，可用性各方面，作为开源软件厂商，这些也都必须要考虑到，Elastic Stack 最近刚发布了全新的 6.0 版本，比如就有新增了用户经常反馈需要将搜索结果导出到 CSV 的功能，还有对视觉障碍更加友好的颜色和对比度，完善的键盘导航支持，屏幕读取文字转语音等这些人性化功能。 2、Elastic Stack 总部在美国，很多读者朋友很好奇您的工作方式是怎样的？是远程办公吗？两地的时间差会不会导致沟通效率下降？ 我们的工作方式确切的说是分布式办公，elastic 现在全球员工超过 600 人，分布于 30 多个国家和地区，大家可以选择自己最舒适的地方进行办公，时间差肯定有，所以我们的工作内容大部分都以异步方式进行处理，也用了很多的工具来进行协同，平时线上视频会议比较多，每年也有多次的线下面对面交流，从我们软件发版的速度大家应该可以看到，我们的效率是非常高的。 3、在大数据场景下，ELK 一经推出就变得非常流行。ELK 在大数据场景下有怎样的定位？解决了用户什么样的痛点？ ELK 一推出，最开始是在运维圈子里面火了起来，因为大家之前都没有很好的工具来进行运维的日志分析，ELK 由三个产品组成，Elasticsearch 负责海量数据的存储和搜索，Logstash 负责各种来自各种数据源的日志采集，而 Kibana 则很方便让你快速进行分析，三个组合在一起，一个完整的日志分析的解决方案就有了，不用编写一行代码。随着用户的众多，各种场景也被进一步挖掘出来，刚好最近几年也是大数据比较火热的时候，大家都在使用各种大数据的产品，然后发现 Elasticsearch 就有处理海量数据的能力，几十百 TB 也处理起来也很正常，并且比 Hadoop 要更方便，速度更快。 4、提到大数据，就不得说 Hadoop 生态圈。对比 Hadoop 生态圈和 ELK，您觉得它们各自有什么样的优势和缺点？ Elasticsearch 和 Hadoop 不一样，他不是一个编程框架，也不是一个需要你编写大量代码才能开始用的计算引擎，Elasticsearch 也是分布式架构，它提供了开箱即用的各种功能，比如搜索，你只需要构建 QueryDSL 查询就能拿到你想要的结果，再比如聚合，我们提供了常见的聚合操作，和搜索一样简单，所以，作为用户你可以花更多时间在业务上，而不用在具体的底层实现上花费大量的时间。另外 Elastic Stack 其他产品组合在一起，能够更好的在一起工作，从接入到最终分析展现都是现成的。这个是两相比较，Elastic 产品易用性上的优势。 还有一个就是，Elasticsearch 的速度 要快的多。Elasticsearch 天然的索引支持，能够在秒级甚至毫秒级别对海量的数据进行探索，这个是 Hadoop 及之上的系统所远远达不到的。Elasticsearch 同样也支持海量的数据，目前上 PB 规模的用户已有很多了，对于大部分的用户，数据规模其实达不到真正意义的海量，所以使用 Elasticsearch 完全就可以轻松解决大部分的场景需求，并且更加简单。 Elasticsearch 没有提供 Map Reduce 这种可以进行更加灵活的数据操作，不能作为一个任意的分布式计算平台，虽然也提供 script 支持，但是还是有一些局限，这个是 Elastic 设计的一个权衡，支持的特性，必须速度第一。 其实还有很多区别，比如同样都是分布式，Elasticsearch 部署和运维要更加简单等等。当然作为用户来说，我觉得要看其具体需求，了解各自的适用场景然后再进行选择。 5、作为 ELK 的核心组件，Elasticsearch 的最新版本已经是6.x，可谓发展迅猛。我很想知道，ES 在快速发展的背后，有没有踩过坑？或者有没有让您觉得有纪念意义的故事？ Elasticsearch 发展很快，甚至大家都觉得有点太快而不适应，软件公司发版都快赶上互联网公司产品的发版了。 作为产品，这个好事，有很多都是需求推动，恰恰说明我们产品的接受度非常高，很受大家的欢迎。 踩坑那是必然的，Elastic 一路走来踩了不少坑，一开始引入了 type 这概念，产生了很多误用引起的问题。对用户造成的困扰和产品的使用影响都是巨大的，所以在 6.0 我们已经不支持多个 Type 了，7.0 会完全移除。当然有很多值得纪念的里程碑的事件，第一个要数废弃 facts 和 aggregation 聚合的引入， 从此 Elasticsearch 不单是一个搜索引擎，更是分析引擎，第二个，要算 doc values 的引入，更快、更少占内存、同时更加稳定。在这次大会上也会介绍我们这一路走来的林林总总，欢迎关注。 6、从企业级分布式搜索，到大数据应用场景，ELK 有没有在其他的热门场景做一些探索？或者说它的下一步规划是什么？ Elastic Stack 除了分布式搜索和大数据应用场景，在很多领域都有广泛应用，比如 SIEM 领域，有很多公司用来进行安全方面的数据分析，做企业防入侵检测、异常流量分析、用户行为分析等。比如现在流行的 AIOPS，也有很多底层就是基于 Elasticsearch的。 除此之外，还有很多拿来替换 Zabbix、Nagios 来做基础设施监控，有一些公司是用来做业务指标分析。最近我们推出了 APM 的开源组件，未来应该会有很多公司用来进行应用性能分析。除了这几个主要的热门场景，其他的应用场景也真是五花八门。有用来做物联网数据分析的，什么电网使用量分析预测。还有做基因数据分析的。有用来实时监测伊波拉疫情，疾控预防。舆情监控等等。太多了就不一一列举了。 7、作为 GIAC 大数据平台专题的讲师，能否简单透露一下您将要给听众带来哪些值得期待的内容？ 这次分享主要会介绍 Elastic Stack 各产品一路走来的架构演进和主要的变化。曾经遇到的问题和如何解决的。也会介绍一些典型的使用场景，方便大家发散思维。另外会对未来的产品的演进方向做一个展望，总结过去、现在和未来，对我们产品感兴趣、想知道拿来都能干什么以及以后将会怎么样，都可以过来了解一下。 8、最后，您对 GIAC 有什么寄语或者期望？ 这是我第一次参加 GIAC，希望能够跟大家多多交流，也希望 GIAC 作为一个平台，能够起到传播技术、引领潮流，能够带来更多的思想碰撞。 本期 GIAC 大会上，大数据平台部分精彩的议题如下： 注：出品人及演讲议题以最新官网为准，全部最新演讲议题请点击“阅读原文”至官网查看。 参加 GIAC，盘点年底最新技术，目前单人购买优惠 600 元，多人购买有更多优惠。点击“阅读原文”了解大会更多详情。 转载来源：ELK 为什么这么流行？｜GIAC 访谈]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>软件</tag>
        <tag>ElasticSearch</tag>
        <tag>Hadoop</tag>
        <tag>大数据</tag>
        <tag>MapReduce</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【服务器】Docker 核心技术与实现原理]]></title>
    <url>%2F2017%2Fd378dd82%2F</url>
    <content type="text"><![CDATA[提到虚拟化技术，我们首先想到的一定是 Docker，经过四年的快速发展 Docker 已经成为了很多公司的生产环境中大规模使用，也不再是一个只能在开发阶段使用的玩具了。作为在生产环境中广泛应用的产品，Docker 有着非常成熟的社区以及大量的使用者，代码库中的内容也变得非常庞大。 同样，由于项目的发展、功能的拆分以及各种奇怪的改名 PR，让我们再次理解 Docker 的的整体架构变得更加困难。 虽然 Docker 目前的组件较多，并且实现也非常复杂，但是本文不想过多的介绍 Docker 具体的实现细节，我们更想谈一谈 Docker 这种虚拟化技术的出现有哪些核心技术的支撑。 首先，Docker 的出现一定是因为目前的后端在开发和运维阶段确实需要一种虚拟化技术解决开发环境和生产环境环境一致的问题，通过 Docker 我们可以将程序运行的环境也纳入到版本控制中，排除因为环境造成不同运行结果的可能。但是上述需求虽然推动了虚拟化技术的产生，但是如果没有合适的底层技术支撑，那么我们仍然得不到一个完美的产品。本文剩下的内容会介绍几种 Docker 使用的核心技术，如果我们了解它们的使用方法和原理，就能清楚 Docker 的实现原理。 Namespaces命名空间 (namespaces) 是 Linux 为我们提供的用于分离进程树、网络接口、挂载点以及进程间通信等资源的方法。在日常使用 Linux 或者 macOS 时，我们并没有运行多个完全分离的服务器的需要，但是如果我们在服务器上启动了多个服务，这些服务其实会相互影响的，每一个服务都能看到其他服务的进程，也可以访问宿主机器上的任意文件，这是很多时候我们都不愿意看到的，我们更希望运行在同一台机器上的不同服务能做到完全隔离，就像运行在多台不同的机器上一样。 在这种情况下，一旦服务器上的某一个服务被入侵，那么入侵者就能够访问当前机器上的所有服务和文件，这也是我们不想看到的，而 Docker 其实就通过 Linux 的 Namespaces 对不同的容器实现了隔离。 Linux 的命名空间机制提供了以下七种不同的命名空间，包括CLONE_NEWCGROUP、CLONE_NEWIPC、CLONE_NEWNET、CLONE_NEWNS、CLONE_NEWPID、CLONE_NEWUSER和CLONE_NEWUTS，通过这七个选项我们能在创建新的进程时设置新进程应该在哪些资源上与宿主机器进行隔离。 进程进程是 Linux 以及现在操作系统中非常重要的概念，它表示一个正在执行的程序，也是在现代分时系统中的一个任务单元。在每一个 *nix 的操作系统上，我们都能够通过ps命令打印出当前操作系统中正在执行的进程，比如在 Ubuntu 上，使用该命令就能得到以下的结果： 当前机器上有很多的进程正在执行，在上述进程中有两个非常特殊，一个是pid为 1 的 /sbin/init 进程，另一个是pid为 2 的 kthreadd 进程，这两个进程都是被 Linux 中的上帝进程 idle 创建出来的，其中前者负责执行内核的一部分初始化工作和系统配置，也会创建一些类似getty的注册进程，而后者负责管理和调度其他的内核进程。 如果我们在当前的 Linux 操作系统下运行一个新的 Docker 容器，并通过exec进入其内部的bash并打印其中的全部进程，我们会得到以下的结果： 在新的容器内部执行ps命令打印出了非常干净的进程列表，只有包含当前ps -ef在内的三个进程，在宿主机器上的几十个进程都已经消失不见了。 当前的 Docker 容器成功将容器内的进程与宿主机器中的进程隔离，如果我们在宿主机器上打印当前的全部进程时，会得到下面三条与 Docker 相关的结果： 在当前的宿主机器上，可能就存在由上述的不同进程构成的进程树： 这就是在使用clone(2)创建新进程时传入CLONE_NEWPID实现的，也就是使用 Linux 的命名空间实现进程的隔离，Docker 容器内部的任意进程都对宿主机器的进程一无所知。 Docker 的容器就是使用上述技术实现与宿主机器的进程隔离，当我们每次运行docker run或者docker start时，都会在下面的方法中创建一个用于设置进程间隔离的 Spec： 在setNamespaces方法中不仅会设置进程相关的命名空间，还会设置与用户、网络、IPC 以及 UTS 相关的命名空间： 所有命名空间相关的设置Spec最后都会作为Create函数的入参在创建新的容器时进行设置： 所有与命名空间的相关的设置都是在上述的两个函数中完成的，Docker 通过命名空间成功完成了与宿主机进程和网络的隔离。 网络如果 Docker 的容器通过 Linux 的命名空间完成了与宿主机进程的网络隔离，但是却有没有办法通过宿主机的网络与整个互联网相连，就会产生很多限制，所以 Docker 虽然可以通过命名空间创建一个隔离的网络环境，但是 Docker 中的服务仍然需要与外界相连才能发挥作用。 每一个使用docker run启动的容器其实都具有单独的网络命名空间，Docker 为我们提供了四种不同的网络模式，Host、Container、None 和 Bridge 模式。 在这一部分，我们将介绍 Docker 默认的网络设置模式：网桥模式。在这种模式下，除了分配隔离的网络命名空间之外，Docker 还会为所有的容器设置 IP 地址。当 Docker 服务器在主机上其中之后会创建新的虚拟网桥 docker0，随后在该主机上启动的全部服务在某人情况下都与该网桥相连。 在默认情况下，每一个容器在创建时都会创建一对虚拟网卡，两个虚拟网卡组成了数据的通道，其中一个会放在创建的容器中，会加入到名为 docker0 网桥中。我们可以使用如下的命令来查看当前网桥的接口： docker0 会为每一个容器分配一个新的 IP 地址并将 docker0 的 IP 地址设置为默认的网关。网桥 docker0 通过 iptables 中的配置与宿主机器上的网卡相连，所有符合条件的请求都会通过 iptables 转发到 docker0 并由网桥分发给对应的机器。 我们在当前的机器上使用docker run -d -p 6379&#58;6379 redis命令启动了一个新的 Redis 容器，在这之后我们再查看当前iptables的 NAT 配置就会看到在DOCKER的链中出现了一条新的规则： 上述规则会将从任意源发送到当前机器 6379 端口的 TCP 包转发到 192.168.0.4&#58;6379 所在的地址上。 这个地址其实也是 Docker 为 Redis 服务分配的 IP 地址，如果我们在当前机器上直接 ping 这个 IP 地址就会发现它是可以访问到的： 从上述的一系列现象，我们就可以推测出 Docker 是如何将容器的内部的端口暴露出来并对数据包进行转发的了；当有 Docker 的容器需要将服务暴露给宿主机器，就会为容器分配一个 IP 地址，同时向 iptables 中追加一条新的规则。 当我们使用redis-cli在宿主机器的命令行中访问 127.0.0.1&#58;6379 的地址时，经过 iptables 的 NAT PREROUTING 将 ip 地址定向到了 192.168.0.4，重定向过的数据包就可以通过 iptables 中的 FILTER 配置，最终在 NAT POSTROUTING 阶段将 ip 地址伪装成 127.0.0.1，到这里虽然从外面看起来我们请求的是 127.0.0.1&#58;6379，但是实际上请求的已经是 Docker 容器暴露出的端口了。 Docker 通过 Linux 的命名空间实现了网络的隔离，又通过 iptables 进行数据包转发，让 Docker 容器能够优雅地为宿主机器或者其他容器提供服务。 libnetwork整个网络部分的功能都是通过 Docker 拆分出来的 libnetwork 实现的，它提供了一个连接不同容器的实现，同时也能够为应用给出一个能够提供一致的编程接口和网络层抽象的容器网络模型。 The goal of libnetwork is to deliver a robust Container Network Model that provides a consistent programming interface and the required network abstractions for applications. libnetwork 中最重要的概念，容器网络模型由以下的几个主要组件组成，分别是 Sandbox、Endpoint 和 Network： 在容器网络模型中，每一个容器内部都包含一个 Sandbox，其中存储着当前容器的网络栈配置，包括容器的接口、路由表和 DNS 设置，Linux 使用网络命名空间实现这个 Sandbox，每一个 Sandbox 中都可能会有一个或多个 Endpoint，在 Linux 上就是一个虚拟的网卡 veth，Sandbox 通过 Endpoint 加入到对应的网络中，这里的网络可能就是我们在上面提到的 Linux 网桥或者 VLAN。 想要获得更多与 libnetwork 或者容器网络模型相关的信息，可以阅读 Design · libnetwork 了解更多信息，当然也可以阅读源代码了解不同 OS 对容器网络模型的不同实现。 挂载点虽然我们已经通过 Linux 的命名空间解决了进程和网络隔离的问题，在 Docker 进程中我们已经没有办法访问宿主机器上的其他进程并且限制了网络的访问，但是 Docker 容器中的进程仍然能够访问或者修改宿主机器上的其他目录，这是我们不希望看到的。 在新的进程中创建隔离的挂载点命名空间需要在clone函数中传入CLONE_NEWNS，这样子进程就能得到父进程挂载点的拷贝，如果不传入这个参数子进程对文件系统的读写都会同步回父进程以及整个主机的文件系统。 如果一个容器需要启动，那么它一定需要提供一个根文件系统（rootfs），容器需要使用这个文件系统来创建一个新的进程，所有二进制的执行都必须在这个根文件系统中。 想要正常启动一个容器就需要在 rootfs 中挂载以上的几个特定的目录，除了上述的几个目录需要挂载之外我们还需要建立一些符号链接保证系统 IO 不会出现问题。 为了保证当前的容器进程没有办法访问宿主机器上其他目录，我们在这里还需要通过 libcotainer 提供的pivor_root或者chroot函数改变进程能够访问个文件目录的根节点。 到这里我们就将容器需要的目录挂载到了容器中，同时也禁止当前的容器进程访问宿主机器上的其他目录，保证了不同文件系统的隔离。 这一部分的内容是作者在 libcontainer 中的 SPEC.md 文件中找到的，其中包含了 Docker 使用的文件系统的说明，对于 Docker 是否真的使用chroot来确保当前的进程无法访问宿主机器的目录，作者其实也没有确切的答案，一是 Docker 项目的代码太多庞大，不知道该从何入手，作者尝试通过 Google 查找相关的结果，但是既找到了无人回答的 问题，也得到了与 SPEC 中的描述有冲突的 答案 ，如果各位读者有明确的答案可以在博客下面留言，非常感谢。 chroot在这里不得不简单介绍一下：chroot（change root），在 Linux 系统中，系统默认的目录就都是以/也就是根目录开头的，chroot的使用能够改变当前的系统根目录结构，通过改变当前系统的根目录，我们能够限制用户的权利，在新的根目录下并不能够访问旧系统根目录的结构个文件，也就建立了一个与原系统完全隔离的目录结构。 与 chroot 的相关内容部分来自 理解 chroot 一文，各位读者可以阅读这篇文章获得更详细的信息。 CGroups我们通过 Linux 的命名空间为新创建的进程隔离了文件系统、网络并与宿主机器之间的进程相互隔离，但是命名空间并不能够为我们提供物理资源上的隔离，比如 CPU 或者内存，如果在同一台机器上运行了多个对彼此以及宿主机器一无所知的『容器』，这些容器却共同占用了宿主机器的物理资源。 如果其中的某一个容器正在执行 CPU 密集型的任务，那么就会影响其他容器中任务的性能与执行效率，导致多个容器相互影响并且抢占资源。如何对多个容器的资源使用进行限制就成了解决进程虚拟资源隔离之后的主要问题，而 Control Groups（简称 CGroups）就是能够隔离宿主机器上的物理资源，例如 CPU、内存、磁盘 I/O 和网络带宽。 每一个 CGroup 都是一组被相同的标准和参数限制的进程，不同的 CGroup 之间是有层级关系的，也就是说它们之间可以从父类继承一些用于限制资源使用的标准和参数。 Linux 的 CGroup 能够为一组进程分配资源，也就是我们在上面提到的 CPU、内存、网络带宽等资源，通过对资源的分配，CGroup 能够提供以下的几种功能： 在 CGroup 中，所有的任务就是一个系统的一个进程，而 CGroup 就是一组按照某种标准划分的进程，在 CGroup 这种机制中，所有的资源控制都是以 CGroup 作为单位实现的，每一个进程都可以随时加入一个 CGroup 也可以随时退出一个 CGroup。– CGroup 介绍、应用实例及原理描述 Linux 使用文件系统来实现 CGroup，我们可以直接使用下面的命令查看当前的 CGroup 中有哪些子系统： 大多数 Linux 的发行版都有着非常相似的子系统，而之所以将上面的 cpuset、cpu 等东西称作子系统，是因为它们能够为对应的控制组分配资源并限制资源的使用。 如果我们想要创建一个新的 cgroup 只需要在想要分配或者限制资源的子系统下面创建一个新的文件夹，然后这个文件夹下就会自动出现很多的内容，如果你在 Linux 上安装了 Docker，你就会发现所有子系统的目录下都有一个名为 docker 的文件夹： 9c3057xxx其实就是我们运行的一个 Docker 容器，启动这个容器时，Docker 会为这个容器创建一个与容器标识符相同的 CGroup，在当前的主机上 CGroup 就会有以下的层级关系：每一个 CGroup 下面都有一个tasks文件，其中存储着属于当前控制组的所有进程的 pid，作为负责 cpu 的子系统，cpu.cfs_quota_us文件中的内容能够对 CPU 的使用作出限制，如果当前文件的内容为 50000，那么当前控制组中的全部进程的 CPU 占用率不能超过 50%。如果系统管理员想要控制 Docker 某个容器的资源使用率就可以在docker这个父控制组下面找到对应的子控制组并且改变它们对应文件的内容，当然我们也可以直接在程序运行时就使用参数，让 Docker 进程去改变相应文件中的内容。 当我们使用 Docker 关闭掉正在运行的容器时，Docker 的子控制组对应的文件夹也会被 Docker 进程移除，Docker 在使用 CGroup 时其实也只是做了一些创建文件夹改变文件内容的文件操作，不过 CGroup 的使用也确实解决了我们限制子容器资源占用的问题，系统管理员能够为多个容器合理的分配资源并且不会出现多个容器互相抢占资源的问题。 UnionFSLinux 的命名空间和控制组分别解决了不同资源隔离的问题，前者解决了进程、网络以及文件系统的隔离，后者实现了 CPU、内存等资源的隔离，但是在 Docker 中还有另一个非常重要的问题需要解决 - 也就是镜像。镜像到底是什么，它又是如何组成和组织的是作者使用 Docker 以来的一段时间内一直比较让作者感到困惑的问题，我们可以使用docker run非常轻松地从远程下载 Docker 的镜像并在本地运行。Docker 镜像其实本质就是一个压缩包，我们可以使用下面的命令将一个 Docker 镜像中的文件导出： 你可以看到这个 busybox 镜像中的目录结构与 Linux 操作系统的根目录中的内容并没有太多的区别，可以说 Docker 镜像就是一个文件。 存储驱动Docker 使用了一系列不同的存储驱动管理镜像内的文件系统并运行容器，这些存储驱动与 Docker 卷（volume）有些不同，存储引擎管理着能够在多个容器之间共享的存储。 想要理解 Docker 使用的存储驱动，我们首先需要理解 Docker 是如何构建并且存储镜像的，也需要明白 Docker 的镜像是如何被每一个容器所使用的；Docker 中的每一个镜像都是由一系列只读的层组成的，Dockerfile 中的每一个命令都会在已有的只读层上创建一个新的层： 容器中的每一层都只对当前容器进行了非常小的修改，上述的 Dockerfile 文件会构建一个拥有四层 layer 的镜像：当镜像被docker run命令创建时就会在镜像的最上层添加一个可写的层，也就是容器层，所有对于运行时容器的修改其实都是对这个容器读写层的修改。 容器和镜像的区别就在于，所有的镜像都是只读的，而每一个容器其实等于镜像加上一个可读写的层，也就是同一个镜像可以对应多个容器。 AUFSUnionFS 其实是一种为 Linux 操作系统设计的用于把多个文件系统『联合』到同一个挂载点的文件系统服务。而 AUFS 即 Advanced UnionFS 其实就是 UnionFS 的升级版，它能够提供更优秀的性能和效率。 AUFS 作为联合文件系统，它能够将不同文件夹中的层联合（Union）到了同一个文件夹中，这些文件夹在 AUFS 中称作分支，整个『联合』的过程被称为联合挂载（Union Mount）： 每一个镜像层或者容器层都是/var/lib/docker/目录下的一个子文件夹；在 Docker 中，所有镜像层和容器层的内容都存储在/var/lib/docker/aufs/diff/目录中： 而/var/lib/docker/aufs/layers/中存储着镜像层的元数据，每一个文件都保存着镜像层的元数据，最后的/var/lib/docker/aufs/mnt/包含镜像或者容器层的挂载点，最终会被 Docker 通过联合的方式进行组装。 上面的这张图片非常好的展示了组装的过程，每一个镜像层都是建立在另一个镜像层之上的，同时所有的镜像层都是只读的，只有每个容器最顶层的容器层才可以被用户直接读写，所有的容器都建立在一些底层服务（Kernel）上，包括命名空间、控制组、rootfs 等等，这种容器的组装方式提供了非常大的灵活性，只读的镜像层通过共享也能够减少磁盘的占用。 其他存储驱动AUFS 只是 Docker 使用的存储驱动的一种，除了 AUFS 之外，Docker 还支持了不同的存储驱动，包括aufs、devicemapper、overlay2、zfs和vfs等等，在最新的 Docker 中，overlay2取代了aufs成为了推荐的存储驱动，但是没有overlay2驱动的机器上仍然会使用aufs作为 Docker 的默认驱动。 不同的存储驱动在存储镜像和容器文件时也有着完全不同的实现，有兴趣的读者可以在 Docker 的官方文档 Select a storage driver 中找到相应的内容。 想要查看当前系统的 Docker 上使用了哪种存储驱动只需要使用以下的命令就能得到相对应的信息： 作者的这台 Ubuntu 上由于没有overlay2存储驱动，所以使用aufs作为 Docker 的默认存储驱动。 总结Docker 目前已经成为了非常主流的技术，已经在很多成熟公司的生产环境中使用，但是 Docker 的核心技术其实已经有很多年的历史了，Linux 命名空间、控制组和 UnionFS 三大技术支撑了目前 Docker 的实现，也是 Docker 能够出现的最重要原因。 作者在学习 Docker 实现原理的过程中查阅了非常多的资料，从中也学习到了很多与 Linux 操作系统相关的知识，不过由于 Docker 目前的代码库实在是太过庞大，想要从源代码的角度完全理解 Docker 实现的细节已经是非常困难的了，但是如果各位读者真的对其实现细节感兴趣，可以从 Docker CE 的源代码开始了解 Docker 的原理。 Reference Chapter 4. Docker Fundamentals · Using Docker by Adrian Mount- TECHNIQUES BEHIND DOCKER- Docker overview- Unifying filesystems with union mounts- DOCKER 基础技术：AUFS- RESOURCE MANAGEMENT GUIDE- Kernel Korner - Unionfs&#58; Bringing Filesystems Together- Union file systems&#58; Implementations, part I- IMPROVING DOCKER WITH UNIKERNELS&#58; INTRODUCING HYPERKIT, VPNKIT AND DATAKIT- Separation Anxiety&#58; A Tutorial for Isolating Your System with Linux Namespaces- 理解 chroot- Linux Init Process / PC Boot Procedure- Docker 网络详解及 pipework 源码解读与实践- Understand container communication- Docker Bridge Network Driver Architecture- Linux Firewall Tutorial&#58; IPTables Tables, Chains, Rules Fundamentals- Traversing of tables and chains- Docker 网络部分执行流分析（Libnetwork 源码解读）- Libnetwork Design- 剖析 Docker 文件系统：Aufs与Devicemapper- Linux - understanding the mount namespace &amp; clone CLONE_NEWNS flag- Docker 背后的内核知识 —— Namespace 资源隔离- Infrastructure for container projects- Spec · libcontainer- DOCKER 基础技术：LINUX NAMESPACE（上）- DOCKER 基础技术：LINUX CGROUP- 《自己动手写Docker》书摘之三： Linux UnionFS- Introduction to Docker- Understand images, containers, and storage drivers- Use the AUFS storage driverTECHNIQUES BEHIND DOCKER Unifying filesystems with union mounts RESOURCE MANAGEMENT GUIDE Union file systems&#58; Implementations, part I Separation Anxiety&#58; A Tutorial for Isolating Your System with Linux Namespaces Linux Init Process / PC Boot Procedure Understand container communication Linux Firewall Tutorial&#58; IPTables Tables, Chains, Rules Fundamentals Docker 网络部分执行流分析（Libnetwork 源码解读） 剖析 Docker 文件系统：Aufs与Devicemapper Docker 背后的内核知识 —— Namespace 资源隔离 Spec · libcontainer DOCKER 基础技术：LINUX CGROUP Introduction to Docker Use the AUFS storage driver 关于图片和转载本作品采用知识共享署名 4.0 国际许可协议进行许可。 转载时请注明原文链接，图片在使用时请保留图片中的全部内容，可适当缩放并在引用处附上图片所在的文章链接，图片使用 Sketch 进行绘制。 原文链接：Docker 核心技术与实现原理 · 面向信仰编程网址：https&#58;//draveness.me/docker 转载来源：【服务器】Docker 核心技术与实现原理]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>NoSQL</tag>
        <tag>Redis</tag>
        <tag>CPU</tag>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[“每天AI资讯这么多！该看哪些？”推荐一份优质资料清单]]></title>
    <url>%2F2017%2Feb6a4100%2F</url>
    <content type="text"><![CDATA[原作 BAILOOL &amp; meetshah1995 Root 编译自 GitHub 量子位 出品 | 公众号 QbitAI 人工智能最近火到炸裂，不看吧担心和时代脱节，看吧每天资讯多到想哭，信息过载心好累肿么办？ 来&lt;(￣︶￣)↗跟着GitHub上的资深用户BAILOOL走，看看他们每日追踪的信息源，拿到第一手的学术进展和行业动态，不用再追着不同的网站看啦。 以下是原文 怎么避免“从入门到放弃”不少童鞋发现人工智能很火，产生墙裂的学习兴趣（主要是工资高dei不dei），所以现在想上车学习，于是开始到处看“一文看懂”系列，或者开始修AI领域大牛的课程。 结果却发现，看完之后什么也没看懂。或者课程听起来很吃力，慢慢觉得自己智商跟不上，不像是这块料，就放弃了。 上车姿势不对啊童鞋们！知道大家刚开始控制不住寄几，疲于奔命地到处搜罗入门资料和课程，GitHub上的好心人就整理一份机器学习上车指南，包含精挑细选的一手资讯、经无数人验证的教程和高质量的信息源。 Step 1：刚迈脚上车，然后要干嘛下了决心要转行AI，就等于一只脚上了车，不过一上来也别给自己整太大压力，上来就看大砖头的书或者报一门课程。 先来点简单的，好培养自己的兴趣和耐力，比如说，泡泡论坛。特别是如果你现在啥也不太懂，建议你每天打开电脑之后，别和妹纸聊微信了，别看农药解说的直播了。克制一下自己内心的及时行乐 好好在Reddit这几个论坛上泡一会，泡它一个早上： machine_learning（https&#58;//www.reddit.com/user/techrat_reddit/m/machine_learning/）- MachineLearning（https&#58;//www.reddit.com/r/MachineLearning/）- computervision（https&#58;//www.reddit.com/r/computervision/）- learnmachinelearning（https&#58;//www.reddit.com/r/learnmachinelearning/）（https&#58;//www.reddit.com/user/techrat_reddit/m/machine_learning/） （https&#58;//www.reddit.com/r/MachineLearning/） （https&#58;//www.reddit.com/r/computervision/） （https&#58;//www.reddit.com/r/learnmachinelearning/） 还有，Quora上的这几个板块也有很多料，： Machine Learning（https&#58;//www.quora.com/pinned/Machine-Learning）- Computer Vision（https&#58;//www.quora.com/pinned/Computer-Vision）- Deep Learning（https&#58;//www.quora.com/pinned/Deep-Learning）- Reinforcement Learning（https&#58;//www.quora.com/pinned/Reinforcement-Learning）（https&#58;//www.quora.com/pinned/Machine-Learning） （https&#58;//www.quora.com/pinned/Computer-Vision） （https&#58;//www.quora.com/pinned/Deep-Learning） （https&#58;//www.quora.com/pinned/Reinforcement-Learning） Step 2：站久了该找个座啦等到什么时候，你看论坛的内容吃不饱，觉得自己需要更高阶的知识充电，就可以转战去arXiv读论文了。 简单介绍一下（以下信息来自Wiki），arXiv呢，是个收集物理学、数学、计算机科学与生物学的论文预印本的网站。将预稿上传到arvix作为预收录，可以防止自己的idea在论文被收录前被别人剽窃。 因此arXiv是个可以证明论文原创性（上传时间戳）的文档收录网站。现今的很多科学家习惯先将其论文上传至arXiv.org，再提交予专业的学术期刊。 Computer Vision and Pattern Recognition（https&#58;//arxiv.org/list/cs.CV/recent）- Artificial Intelligence（https&#58;//arxiv.org/list/cs.AI/recent- Learning（https&#58;//arxiv.org/list/cs.LG/recent）- Neural and Evolutionary Computing（https&#58;//arxiv.org/list/cs.NE/recent）（https&#58;//arxiv.org/list/cs.CV/recent） （https&#58;//arxiv.org/list/cs.AI/recent （https&#58;//arxiv.org/list/cs.LG/recent） 不过arXiv有个缺点，就是自己搜索相应的论文很麻烦。 所以有个大神，Andrej，建立了一个论文自动推送网站arxiv-sanity.com，用户在上面建立一个自己的账号之后，往上丢几篇感兴趣的文章，这个网站就可以自动推送从arXiv上搜来且符合用户兴趣方向的相关论文。 Step 3：听听懂路的老司机报下站如果读完论文之后，感觉一脸大写的懵✘，辣可以上ShortScience.org那转转。 这个网站是专门的论文讨论网站，上面有很多大牛点评同行的工作，或者读paper时做的笔记。大家看paper时也喜欢扎堆在ShortScience上发表自己的观点和看法。 特别适合刚上车的新手，如果自己读paper功力不太够，get不到重点，需要有人导读啥的，那么可以上这个网站去看看其他大牛对这篇论文的评论（有点儿像看完电影，总是习惯性上豆瓣看影评的赶脚） Step 4：看看别人怎么开车另外，还有个把paper和code整理到了一起的网站GitXiv.com，看名字就知道相当于GitHub和arXiv的合体。 在这个圈子里，已经形成了一股趋势，学术上一有最新进展，科学家都会把paper往arXiv上扔，然后没几天开发者们就把完成需求的开源代码抢发在GitHub上，大家执行力都超强的说。 不过，两个出处的链接没有个统一的地方可以关联在一起，不方便大家检索，也不方便大家横向比较和讨论，所以产生了这么个根据地GitXiv，供大家扎堆，还可以同行打分评论。 至于怎么上手自己开车，第5步之后就要靠大家去摸索啦，最后分享几个高质的信息源给大家。 最后：一手+高质+深度的信息feed到现在这个阶段，可能一般的信息流已经满足不了你了。扔几个我们圈内人都会锁定的频道： HackerNews（https&#58;//news.ycombinator.com/news）硅谷技术圈和投资圈都会关注的新闻网站，资讯不仅和AI有关，还会涉及到创业和信息安全，需要自己筛选。- DataTau（datatau.com）专门给数据科学家看的HackerNews。- AITopics(https&#58;//aitopics.org/search)可以自己设置订阅规则或者订阅源的资讯阅读器。（https&#58;//news.ycombinator.com/news） DataTau（datatau.com） AITopics 可以自己设置订阅规则或者订阅源的资讯阅读器。 油管上有几个频道做得也不错，可以挑一个跟就好： 3Blue1Brown(https&#58;//www.youtube.com/channel/UCYO_jab_esuFRV4b17AJtAw)- Two Minute Papers（https&#58;//www.youtube.com/channel/UCbfYPyITQ-7l4upoX8nvctg）- Robert Miles（https&#58;//www.youtube.com/channel/UCLB7AzTwc6VFZrBsO2ucBMg）- S**iraj Raval**（https&#58;//www.youtube.com/channel/UCWN3xxRkmTPmbKwht9FuE5A）(https&#58;//www.youtube.com/channel/UCYO_jab_esuFRV4b17AJtAw) （https&#58;//www.youtube.com/channel/UCbfYPyITQ-7l4upoX8nvctg） （https&#58;//www.youtube.com/channel/UCLB7AzTwc6VFZrBsO2ucBMg） （https&#58;//www.youtube.com/channel/UCWN3xxRkmTPmbKwht9FuE5A） 几家AI巨头的官方博客也挺值得跟，可以扔进订阅源里： Google（https&#58;//research.googleblog.com/）- Facebook（https&#58;//research.fb.com/blog/）- Nvidia（https&#58;//blogs.nvidia.com/blog/category/deep-learning/）- Apple（https&#58;//machinelearning.apple.com/）（https&#58;//research.googleblog.com/） （https&#58;//research.fb.com/blog/） （https&#58;//blogs.nvidia.com/blog/category/deep-learning/） （https&#58;//machinelearning.apple.com/） 更新频率有点儿低，不过每次更新都是猛料的： Google Scholar（https&#58;//scholar.google.com/）- ResearchGate（https&#58;//www.researchgate.net/）- Distill（https&#58;//distill.pub/）（https&#58;//scholar.google.com/） （https&#58;//www.researchgate.net/） （https&#58;//distill.pub/） 推特上活跃的领域大牛必须关注一波（什么？你还没有推特？那你怎么活在AI圈子里！快，别墨迹，现在就去搞一个账号）：Deep Learning Hub；Marshall Kirkpatrick；Lynn Cherny；Top-N；Top 10 AI；Text Data, Vis &amp; Art。 这里就提这么几个人，等你到推特上之后关注完，推特会自己再给你推相关的大神的。 最后，有几个私人博客我觉得信息筛选的质量都符合我平时阅读的标准，大家挑一个喜欢的就好： The Wild Week in AI（https&#58;//www.getrevue.co/profile/wildml）周更，上面时不时还会有初创团队招人信息放出来，比较适合找工作的孩纸。- inFERENCe（inference.vc）不定期更，博主是隔了三年没碰机器学习，然后重新捡回来相关的研究进展。这个博是他个人的一个学习机器学习的成长记录地。- The Morning Paper（https&#58;//blog.acolyer.org/）日更，这个博客是一个原本什么都不懂的VC开的，初心是想开始积累自己在ML领域的认知，资讯的选取更多是从投资者的角度出发。- I**nside AI**（https&#58;//inside.com/ai）Inside网站旗下的AI话题板块，专注于资讯的深度。现在大多数新闻都是为了流量，吸引读者的眼球，而很少考虑读者的收获，导致优质新闻的产出和占道越来越少，很难到达渴望深度内容的读者。所以，Inside AI希望经过他们的筛选，订阅用户能重新获取有价值的新闻资讯。（https&#58;//www.getrevue.co/profile/wildml） inFERENCe（inference.vc） The Morning Paper 日更，这个博客是一个原本什么都不懂的VC开的，初心是想开始积累自己在ML领域的认知，资讯的选取更多是从投资者的角度出发。 （https&#58;//inside.com/ai） 最后，附上原文链接： https&#58;//github.com/BAILOOL/DoYouEvenLearn/blob/master/README.md — 完 — 诚挚招聘 量子位正在招募编辑/记者，工作地点在北京中关村。期待有才气、有热情的同学加入我们！相关细节，请在量子位公众号(QbitAI)对话界面，回复“招聘”两个字。 量子位 QbitAI · 头条号签约作者 վ’ᴗ’ ի 追踪AI技术和产品新动态 转载来源：“每天AI资讯这么多！该看哪些？”推荐一份优质资料清单]]></content>
      <categories>
        <category>科技</category>
      </categories>
      <tags>
        <tag>GitHub</tag>
        <tag>人工智能</tag>
        <tag>Quora</tag>
        <tag>Reddit</tag>
        <tag>信息安全</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[随机森林的直观理解]]></title>
    <url>%2F2017%2F669275ab%2F</url>
    <content type="text"><![CDATA[对于那些认为随机森林是黑匣子算法的人来说，这篇帖子会提供一个不同的观点。接下来，我将从4个方面去理解随机森林模型。 1.我们的特征有多重要？在sklearn随机森林中使用model.feature_importance来研究其重要特征是很常见的。重要特征是指与因变量密切相关的特征，并且对因变量的变化影响较大。我们通常将尽可能多的特征提供给随机森林模型，并让算法反馈对预测最有用的特征列表。但仔细选择正确的特征可以使我们的目标预测更加准确。 计算feature_importances的想法很简单，但却很有效。 把想法分解成简单的几步： 训练随机森林模型（假定有正确的超参数）1. 找到模型的预测分数（称之为基准分数）1. 多次（p次，p为特征个数）计算预测分数，每次打乱某个特征的顺序，可见下图1. 将每次预测分数与基准分数进行比较。如果随机调整特征顺序后预测分数小于基准分数，这意味着我们的模型如果没有这个特征会变得很糟糕。1. 删除那些不会降低基准分数的特征，并用减少后的特征子集重新训练模型。找到模型的预测分数（称之为基准分数） 将每次预测分数与基准分数进行比较。如果随机调整特征顺序后预测分数小于基准分数，这意味着我们的模型如果没有这个特征会变得很糟糕。 图1：计算特征重要性 注：将F4列打乱重新进行预测来判断特征F4的重要性 计算特征重要性的代码： 下面的代码将为所有特征提供一个结构为&amp;#123特征，重要性&amp;#125的字典。 图2：随机森林中的重要特征 输出： 2.我们对我们的预测有多大信心？一般来说，当企业想要有所预测时，他们的最终目的不是降低成本就是提高利润。在做出重大商业决策之前，企业十分热衷于去评估做出这个决定的风险的大小。但是，当预测结果并没有被展现在置信区间时，我们可能会无意中将企业至于更多的风险中，而不是降低风险。 当我们使用线性模型（基于分布假设的一般模型）时，比较容易找到我们预测的置信水平。但是当谈到随机森林的置信区间时，找起来并不是那么容易。 图3：偏差与方差的说明图 我想，任何上过线性回归课程的人都肯定看过这张图3。为了找到一个最佳线性模型，我们要去寻找偏差—方差最为折衷的模型。这张图片很好地说明了预测中偏差和方差的定义。（我们理解为这4张图分别是由四个不同的人掷飞镖所得）。 如果我们有高偏差和低方差值时（第三个人），我们投掷的飞镖会固定的远离红心。相反，如果我们有高的方差和低的偏差（第二个人），那么他投掷飞镖的结果就很不一样。如果有人去猜测他下一个飞镖击中的位置，那么它既有可能打到靶心也有可能远离靶心。现在我们来假设在现实生活中识别一起信用欺诈等同于上面例子击中靶心。如果信用公司拥有的的预测模型与上面第二人的掷飞镖行为很相似，那么该公司在大多数时候都不会抓住这个诈骗犯，尽管模型预测的是正确的。 因此，不仅仅是意味着预测的准确程度，我们还应该检查我们的预测的置信水平。 在随机森林中如何做到这一点？ 随机森林是由许多决策树组成。每棵树分别预测新的数据，随机森林从这些树中提取出平均预测值。预测置信水平的想法只是为了去看来自不同树木的预测有多少因为新的观测而产生变化，然后进一步分析。 基于方差树预测置信度的源代码： 注：偏差 = (up-down)/Yhat 以上代码的输出如下所示： 图4：基于方差树的置信树 从这个输出数据可以读出，我们可以说我们对于验证集上索引为14的观测的预测最没有信心。 3.什么是预测路径？如果我们想要分析哪些特征对于整体随机森林模型是重要的，则 特征重要性（如在第一部分中）是有用的。但是如果我们对某个特定的观察感兴趣，那么 Tree interpreter的角色就会发挥作用。 举个例子，现在有一个RF模型，这种模型会预测—一位来医院的患者X是否具有很高的概率再入院?，为了简单起见，我们考虑只有3个特征—患者的血压值，患者的年龄以及患者的性别。现在，如果我们的模型认为患者A有80％的可能会再次入院，我们怎么能知道这个被模型预测为他（她）将重新入院的患者A有什么特殊之处?在这种情况下，Tree interpreter会指示预测路径紧随那个特殊的患者。就像是，因为患者A是65岁的男性，这就是为什么我们的模型会预测他将再次入院。另一个被模型预测将再次入院的患者B ，可能时因为他有高血压（而不是因为年龄或性别）。 基本上，Tree interpreter给出了偏差的排序列表（在起始节点的平均值）以及单个节点对给定预测的贡献。 图5&#58;决策树路径（来源：http&#58;//blog.datadive.net/interpreting&amp; 图5的这棵决策树（深度：3层）基于波士顿房价数据集。根据中间节点的预测值以及导致数值发生变化的特征，它显示了决策路径的分解。单节点的贡献是该节点的值与前一个节点值的差值。 图6&#58;Tree interpreter（最终再次入院的概率=0.6) 图6 给出了对于患者A使用Tree interpreter的输出示例。图片显示年龄为65岁是模型预测再入院概率高于均值的最高贡献者。 图7&#58;将特征贡献通过瀑布图可视化展示图 图6同样也可以使用瀑布图7来表示。我从“ 瀑布图包 ”中选材做的这个快速简单的瀑布图。 上面的瀑布图可视化代码： 相关变量的阐释： • 值（图片B)是指通过节点预测目标值。（就是在该节点中落下的观测目标的平均值）。 • 贡献是当前节点的值减去上一节点的值（这是为一个路径提供的贡献特征）。 • 路径是为了到达叶节点而通过某些观察所获得的所有特征分割的组合。 tree interpreter包直接用来计算每个节点的贡献，链接：treeinterpreter 4.目标变量如何与重要特征相关？ Partial Dependence Plots找到最重要的特征后，下一步我们可能会感兴趣的是研究目标变量与兴趣特征之间的直接关系。从线性回归中得到的与其相类似的是模型系数。对于线性回归，系数以这种方式被计算，即我们可以通过说：“在XjXj中有1个单位变化，保持所有其他XiXi不变，YY会发生什么变化？”这样的方式来解释。 虽然我们有来自随机森林的特征重要性，但是它们只是给出YY的变化是由于XiXi的改变之间的相关性。我们不能直接地解释他们就像保持所有其他特征不变，YY该变量取决于XjXj中的单位的变化。 幸运的是，我们有看一被看作线性模型系数图表的局部依赖图，但同样也可被扩展为看起来像黑箱模型。这个想法是将预测中所做的改变孤立于一个特定的功能。它不同于XX对YY的散点图，因为散点图不能隔离XX对YY的直接关系，并且可能受XX和YY所依赖的其他变量的间接关系所影响。 PDP分析步骤如下： 训练一个随机森林模型（假设F1 … F4是我们的特征，Y是目标变量，假设F1是最重要的特征）。1. 我们有兴趣探索Y和F1的直接关系。1. 用F1（A）代替F1列，并为所有的观察找到新的预测值。采取预测的平均值。（称之为基准值）1. 对F1（B）… F1（E）重复步骤3，即针对特征F1的所有不同值。1. PDP的X轴具有不同的F1值，而Y轴是虽该基准值F1值的平均预测而变化。我们有兴趣探索Y和F1的直接关系。 对F1（B）… F1（E）重复步骤3，即针对特征F1的所有不同值。 图8&#58;PDP分析逻辑图8&#58;PDP分析逻辑 图 9 是partial dependence plot的一个例子。数据来自 kaggle bulldozer competition data，它显示了生产年份（YearMade）和（销售价格）SalesPrice的关系 图9&#58;partial dependence plot（YearMade与SalePrice的变化) 而图10是SalePrice与YearMade的线状图。我们可以看到，散点图/折线图可能无法像PDP那样捕获YearMade对SalesPrice的直接影响。 图10&#58;上述两个图片均来自(来源https&#58;//github.com/fastai/fastai/t 写在最后： 在大多数情况下，随机森林在预测中可以击败线性模型预测。针对随机森林经常提出的反对意见是：对它的理解没有线性模型那样直观，但是本文的讨论希望帮助你回答这样的反对意见。 作者个人简历：目前在旧金山大学学习数据科学（分析），在Manifold.ai做实习生。此前，曾在凯捷咨询公司担任数据科学家，在Altisource担任高级业务分析师。 更多精彩文章您可前往BigQuant社区查看并参与讨论：BigQuant社区 转载来源：随机森林的直观理解]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>随机森林</tag>
        <tag>F1赛车</tag>
        <tag>飞镖</tag>
        <tag>欢聚时代</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[阿里云Redis读写分离典型场景：如何轻松搭建电商秒杀系统]]></title>
    <url>%2F2017%2Fc2560231%2F</url>
    <content type="text"><![CDATA[秒杀活动是绝大部分电商选择的低价促销，推广品牌的方式。不仅可以给平台带来用户量，还可以提高平台知名度。一个好的秒杀系统，可以提高平台系统的稳定性和公平性，获得更好的用户体验，提升平台的口碑，从而提升秒杀活动的最大价值。 本次主要讨论阿里云云数据库Redis缓存设计高并发的秒杀系统。 秒杀的特征秒杀活动对稀缺或者特价的商品进行定时，定量售卖，吸引成大量的消费者进行抢购，但又只有少部分消费者可以下单成功。因此，秒杀活动将在较短时间内产生比平时大数十倍，上百倍的页面访问流量和下单请求流量。 秒杀活动可以分为3个阶段： 秒杀前：用户不断刷新商品详情页，页面请求达到瞬时峰值。- 秒杀开始：用户点击秒杀按钮，下单请求达到瞬时峰值。- 秒杀后：一部分成功下单的用户不断刷新订单或者产生退单操作，大部分用户继续刷新商品详情页等待退单机会。秒杀开始：用户点击秒杀按钮，下单请求达到瞬时峰值。 消费者提交订单，一般做法是利用数据库的行级锁。只有抢到锁的请求可以进行库存查询和下单操作。但是在高并发的情况下，数据库无法承担如此大的请求，往往会使整个服务blocked，在消费者看来就是服务器宕机。 秒杀系统系统架构图 秒杀系统的流量虽然很高，但是实际有效流量是十分有限的。利用系统的层次结构，在每个阶段提前校验，拦截无效流量，可以减少大量无效的流量涌入数据库。 利用浏览器缓存和CDN抗压静态页面流量 秒杀前，用户不断刷新商品详情页，造成大量的页面请求。所以，我们需要把秒杀商品详情页与普通的商品详情页分开。对于秒杀商品详情页尽量将能静态化的元素尽量静态化处理，除了秒杀按钮需要服务端进行动态判断，其他的静态数据可以缓存在浏览器和CDN上。这样，秒杀前刷新页面导致的流量进入服务段的流量只有很小的一部分 利用阿里云读写分离Redis缓存拦截流量 CDN是第一级流量拦截，第二级流量拦截我们使用支持读写分离的阿里云Redis。在这一阶段我们主要读取数据，读写分离Redis能支持高大60万以上qps的，完全可以支持需求。 首先通过数据控制模块，提前将秒杀商品的缓存到阿里云读写分离Redis，并设置秒杀开始标记： 秒杀开始前，服务集群读取goodsId_Start为0，直接返回未开始。 数据控制模块将goodsId_start改为1，标志秒杀开始。 服务集群缓存开始标记位并开始接受请求，并记录到redis中goodsId_access，商品剩余数量为(goodsId_count - goodsId_access)。 当接受下单数达到goodsId_count后，继续拦截所有请求，商品剩余数量为0 可以看出，最后成功参与下单的请求只有少部分可以被接受。在高并发的情况下，允许稍微多的流量进入。因此可以控制接受下单数的比例。 利用阿里云主从版Redis缓存加速库存扣量 成功参与下单，进入下层服务，开始进行订单信息校验，库存扣量。为了避免直接访问数据库，我们使用阿里云主从版Redis来进行库存扣量，阿里云主从版Redis提供10万级别的QPS。我们使用Redis来优化库存查询，提前拦截秒杀失败的请求，将大大提高系统的整体吞吐量。我们也是通过数据控制模块提前将库存存入Redis&#58; //我们将每个秒杀商品在redis中用一个hash结构表示 扣量时，服务器通过请求Redis获取下单资格，我们通过lua脚本实现，由于Redis时单线程模型，lua可以保证多个命令的原子性： lua脚本： 先使用SCRIPT LOAD将lua脚本提前缓存在Redis，然后调用EVALSHA调用脚本，比直接调用EVAL节省网络带宽： 秒杀服务通过判断Redis是否返回抢购个数n，即可知道此次请求是否扣量成功。 使用阿里云主从版Redis实现简单的消息队列异步下单入库 扣量完成后，需要进行订单入库。如果商品数量较少的时候，直接操作数据库即可。如果秒杀的商品是1万，甚至10万级别，那数据库锁冲突将带来很大的性能瓶颈。因此，利用消息队列组件，当秒杀服务将订单信息写入消息队列后，即可认为下单完成，避免直接操作数据库。 消息队列组件依然可以使用Redis实现，在R2中用list数据结构表示： 将订单内容写入Redis&#58; 异步下单模块从Redis中顺序获取订单信息，并将订单写入数据库： 我们通过使用Redis作为消息队列，异步处理订单入库，有效的提高了用户的下单完成速度。 数据控制模块，管理秒杀数据同步 最开始，我们利用阿里云读写分离Redis进行流量限制，只让部分流量进入下单。对于下单检验失败和退单等情况，我们需要让更多的流量进来。因此，数据控制模块需要定时将数据库中的数据进行一定的计算，同步到主从版Redis，同时再同步到读写分离的Redis，让更多的流量进来。 转载来源：阿里云Redis读写分离典型场景：如何轻松搭建电商秒杀系统]]></content>
      <categories>
        <category>设计</category>
      </categories>
      <tags>
        <tag>编程语言</tag>
        <tag>电子商务</tag>
        <tag>脚本语言</tag>
        <tag>NoSQL</tag>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[中科院计算所开源深度文本匹配开源工具 MatchZoo]]></title>
    <url>%2F2017%2F079a1dbc%2F</url>
    <content type="text"><![CDATA[via GitHub 雷锋网 AI 科技评论消息，中国科学院计算技术研究所网络数据科学与技术重点实验室近日发布了深度文本匹配开源项目 MatchZoo。MatchZoo 是一个 Python 环境下基于 TensorFlow 开发的开源文本匹配工具，可以应用于文本检索、自动问答、复述问题、对话系统等多种应用任务场景。 GitHub&#58; https&#58;//github.com/faneshion/MatchZoo 在 arxiv 上，MatchZoo&#58; A Toolkit for Deep Text Matching 介绍了开源项目的主要结构： 据雷锋网了解，这一开源工具能够让大家更加直观地了解深度文本匹配模型的设计、更加便利地比较不同模型的性能差异、更加快捷地开发新型的深度匹配模型。 MatchZoo主要特点MatchZoo 基于 Keras 开发，支持 TensorFlow、CNTK 及 Theano，并能在 CPU 与 GPU 上无缝运行。MatchZoo 包括数据预处理，模型构建，训练与评测三大模块： 数据预处理模块（data preparation） 该模块能将不同类型文本匹配任务的数据处理成统一的格式，具体包含如下的几个文件： word dictionary：每个单词的映射符，通过预设的规则进行过滤常用词，筛选少见词、噪声词。- corpus file：问题及回答内容文件，每行以（id, length, word_id）格式书写，分别表示问题或者回答的 id，文本长度，以及词 id；- relation file：包括训练、验证、测试文件，每行以（rel,query_id, doc_id）格式书写，分别表示问题与回答的相关度（数据中１为相关，０为不相关），问题的 id，以及答案的 id；corpus file：问题及回答内容文件，每行以（id, length, word_id）格式书写，分别表示问题或者回答的 id，文本长度，以及词 id； 同时，该模块针对不同的任务需求提供了不同的数据生成器，包括有基于单文档的数据生成器、基于文档对的数据生成器、以及基于文档列表的数据生成器。不同的数据生成器可适用于不同的文本匹配任务，如文本问答、文本对话、以及文本排序等。 模型构建模块（model construction） 该模块基于 Keras 以帮助我们快速开发。Keras 中包含了深度学习模型中广泛使用的普通层，如卷积层、池化层、全连接层等，除此之外，在 matchzoo/layers/中，研究人员还针对文本匹配定制了特定的层，如动态池化层、张量匹配层等。这些操作能够快速高效地实现复杂的深度文本匹配的模型，在 matchzoo/models/中，研究人员实现了目前主流的深度文本匹配模型（如 DRMM, MatchPyramid, DUET, MVLSTM, aNMM, ARC-I, ARC-II, DSSM, CDSSM 等）。 训练与评测模块（training and evaluation） 该模块提供了针对回归、分类、排序等问题的目标函数和评价指标函数。例如，在文本排序中常用的基于单文档的目标、基于文档对的目标、以及基于文档序列的目标。用户可以根据任务的需要选择合适的目标函数。在模型评估时，MatchZoo 也提供了多个广为使用的评价指标函数，如 MAP、NDCG、Precision，Recall 等。同时，在文本排序任务中，MatchZoo 还能生成兼容 TREC 的数据格式，可以方便地使用 trec_eval来进行模型评估。 运行 git clone https&#58;//github.com/faneshion/MatchZoo.gitcd MatchZoopython setup.py installpython main.py –phase train –model_file ./models/arci_ranking.configpython main.py –phase predict –model_file ./models/arci_ranking.config 基准测试在 Github 上，作者们以 WikiQA 数据为例来介绍 MatchZoo 的使用。 以 DRMM 为例，在 MatchZoo/matchzoo 中运行： python main.py –phase train –model_file models/wikiqa_config/drmm_wikiqa.config 在测试时可运行： python main.py –phase predict –model_file models/wikiqa_config/drmm_wikiqa.config 运行十个模型的结果如下： 训练 loss 曲线图如下： 测试 MAP 性能曲线图如下： 论文地址：https&#58;//arxiv.org/pdf/1707.07270.pdf，雷锋网整理 转载来源：中科院计算所开源深度文本匹配开源工具 MatchZoo]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>编程语言</tag>
        <tag>深度学习</tag>
        <tag>GitHub</tag>
        <tag>Python</tag>
        <tag>Word</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[vue.js应用从创建到运行，从入门到精通]]></title>
    <url>%2F2017%2F76e21f30%2F</url>
    <content type="text"><![CDATA[目录一个简单vue应用的创建 选择什么方式创建vue应用- 创建- 项目结构分析- 运行创建 运行 vue.js知识点的学习 vue.js是怎么构成一个spa应用- 组件- vue实例- 响应式- 模板语法组件 响应式 vue.js细化学习 过滤器——后台得到的数据格式不是你想要的？过滤器帮你！- 指令——还想用DOM操作节点？指令帮你实现！- 混合——个个组件重复写方法很麻烦？混合帮你解决！- 组件——如何开发出灵活的组件- 过渡——页面没动画？过渡可以做的很完美!指令——还想用DOM操作节点？指令帮你实现！ 组件——如何开发出灵活的组件 正篇 昨天写了一篇webpack+vue.js+element打造大型项目，看到网友们反应不错，于是小编一大早编起来编写接下来未完成的章节，今天完成的是Vue.js知识点的学习，它涵盖了开发中需要的知识，也涉及了一些大家开发中没主要的知识点和细节，希望的、能够帮助到广大网友们，如果大家有什么疑问和要求，大家可以在下方留言或者加我的微信&#58;Neho_Developer 一个简单vue应用的创建 选择什么方式创建vue应用写完了上一篇文章，我一直考虑有什么方式创建一个vue应用而纠结，用CDN引入的方式，从传统开发转过来的人很好理解，但事实上我们用vue.js开发都是比较大型的应用，这种方式学到最后是不需要的，最后还是选择了webpack开发vue应用，在创建之前，我们要确认电脑上安装了运行环境node.js和npm管理包。 创建一个vue应用在这里我们使用vue.js官方提供的vue脚手架vue-cli创建一个应用，所谓的脚手架，通俗来讲，就是通过人家定义好的一个应用文件模板，通过脚手架，我们可以下载下来供开发者自己使用。我们先安装vue-cli npm install -g vue-cli 使用vue-cli初始化项目,这个过程中，大家可以选择一直enter就好了，当要选择y/n时，选择n 进到目录 cd my-project-name 安装依赖 cnpm install 运行vue应用 以上步骤都必须正确执行，我们在浏览器上输入localhost&#58;8080便可以看到运行的结果。 项目文件分析安装好了之后我们打开项目的文件 创建好的项目文件 node_modules这个文件夹是在npm install后，将npm包放在此文件夹内，我们不需要修改 src我们开发修改的目录 babelrc使用bable转化文件，一般我们不需要修改 gitignore当我们协同开发项目时，我们往往需要通过版本控制工具协同开发，比如git，关于git后续会讲解git的使用技巧教程，这个文件就是配置git版本时，忽略不需要上传文件的目录清单 indexvue应用为spa（单页面应用），这个为打包之后在浏览器运行的html文件，一般不需要修改 package.jsonnpm包安装的目录清单，一般不需要修改 README.MDmarkdown格式的文件，可以在里面写一些这个项目的操作记录和介绍，一般不需要修改 webpack.config.jswebpack模块开发工具的配置文件，我们在这里主要修改加载器的配置信息 vue.js知识点的学习vue.js是怎么样构建一个spa应用的 理解学过html的人都知道，一个html文件是如何将超文本信息展示给用户的，是通过一个个元素，更加普遍的是通过DIV元素，像建屋子一样叠一个美丽的网页。类似的，vue也是通过一个个类似DIV一样的东西，我们称其为组件，他也想DIV一样，可以嵌套，叠加，还有DIV不能实现的其他功能，通过vnode渲染在index这个文件上，从而得到了我们常见的div组成的html文件。而我们开发的经理，则是去实现一个个组件的功能。 一个页面可以看作由组件树组成的 组件 webpack中vue组件的文件体现形式及实现页面的现实在webpack中组件以文件.vue格式体现。一个vue文件则为一个组件，我们来看看一个vue文件包含什么。 根组件app.vue文件 我们可以看到，一个vue文件包含三部分,这三个部分正好是网页的上个基本元素HTML CSS JAVASCRIPT,一个spa页面，通过一个或多个以vue文件格式体现的组件便可以构成一个页面，我们看看他是怎么实现通过组件展示一个网页 vue文件的实现方式 组件的作用域在组件中，变量是独立的，也就是说在中定义的变量，在其他的vue文件中是不可以访问的，当组件间需要交互数据，怎么办呢，下面就全面的介绍了 父子组件的通信子组件获取负组件的变量：在子组件中this.$parent,返回父组件的实例，便可以访问父组件的变量，也可以通过props向子组件注入变量。 子组件获取父组件的html：可以通过slot分发父组件的html。 当组件之间的关系并不是父子关系，那怎么办呢，这时我推荐大家使用vue的插件vuex，接下来的文章会专门介绍vuex vue实例 理解如果一个应用只有一个vue组件构成的话，从MVC框架来看的话，template标签定义的就是应用的视图层VIEW，script标签中的定义的就是model层了，这样说其实不是很准确，更加确切的说应该的实例中的data对象 data对象：model层 export default &amp;#123data()&amp;#123return&amp;#123//这里是一个应用的model层text&#58;’你好，Neho’&amp;#125&amp;#125&amp;#125 视图层 &amp;#123&amp;#123text&amp;#125&amp;#125 页面 你好，Neho vue实例一个vue组件对应一个vue实例，在script标签中，我们通过new Vue()方式创建一个vue实例，通常来说，不通过的组件vue实例是不一样的，我们可以通过向这个方法传入一个对象来实例化不同的vue实例。对象中常用的属性有data属性——定义数据，methods属性——定义操作data数据的函数，created属性——实力在一个vue实例中，不同的生命周期会调用不同的函数钩子，我们来看看一个简单的代码 根组件 效果 页面效果 响应式vue的一个很出色的特点是视图层view（template标签）和数据层model（data中定义的数据）是时时同步的，我们来看看简单的例子 写好代码我们看看效果代码 代码 页面 页面 这是一个很简单的页面，但我们试图去修改input标签的内容时，发现页面显示的内容也跟着改了 页面 修改后页面 敢于v-model的使用，下面会介绍到，这里我们只需要知道它可以同步input的内容到它绑定的data属性edit上。 模板语法 理解，写过后台的人对模板这个概念估计不会陌生，通常来说，在一个html文件中，我们不能直接将js代码写在html标签中，这样是会报错了，为了解决这个问题，模板诞生了，使用模板，通常来说，不同模板有不同后缀名，比如ejs模板，他的后缀名为ejs。在vue文件中，模板的文件后缀名就是vue，使用vue的模板，我们可以直接在temlate标签中使用js代码如上面的&amp;#123&amp;#123&amp;#125&amp;#125，他可以是我们在里面写js表达式。我们来看看vue模板有什么语法- &amp;#123&amp;#123&amp;#125&amp;#125——将里面的变量（script标签中data定义的变量）输出在页面上&amp;#123&amp;#123&amp;#125&amp;#125——将里面的变量（script标签中data定义的变量）输出在页面上 通过&amp;#123&amp;#123&amp;#125&amp;#125，我们可以在里面写入data定义好的变量，从而显示在页面上。 v-html——将传入的变量（script标签中data定义的变量）输出在页面上（不包括节点）使用&amp;#123&amp;#123&amp;#125&amp;#125很方便，但有时我们需要拼接节点的时候，他会把div这类节点字符串也输出出来，这是我们不愿看到的，我们可以使用v-html来解决这个问题，我们来看下代码 代码 代码 页面效果 页面 v-if——控制节点元素的显示，传入的变量为false时，该指令绑定的元素不显示代码 代码 页面——我们可以看到，页面并没有出现我们在div中写的文本 v-for——循环输出文本，这对于我们创建一些重复的节点非常有用代码，我们需要输出5行li列表，以前我们需要重复写5个li，现在只要写一个就好了 代码 页面效果 页面 v-bind——每种标签都有自己的属性，当我们需要将实例定义好的变量传入属性中，怎么办呢，v-bind可以帮我们解决这个问题，这个例子中，input有一个value属性，我们需要要将data中定义好的变量输出到页面上代码 代码 页面 页面 v-on——我们知道，web是事件驱动的应用，当我们需要在模板中添加事件监听怎么办呢，这里我们使用v-on，在这里我们在节点中添加点击事件，事件会调用实例中定义的函数clickMe,然后弹出警告框代码 代码 页面点击前 页面 页面点击按钮后 页面 v-model——在处理表单的时候，开发者最感到头疼的是如何获取表单的信息，在vue.js中，我们可以通过v-model时时将绑定的input元素内的文本同步到传入v-model的值，而且这个过程是双向的，用户修改input内的文本，data中的变量会跟随改变，通过方法改变data中的变量，页面input的文本也会随着改变。- 结语结语 vue.js知识点就将这么多了，这已经可以满足打开日常开发的需求，如果有什么不足或者不明白之处，大家可以下方留言或者加我的微信^_^,接下来讲讲大家不怎么注意，但在开发中合理运用会大大简化我们的开发 vue.js细化学习过滤器——后台得到的数据格式不是你想要的？过滤器帮你！ 为什么使用过滤器在开发当中，常常遇到一些后台返回的数据并不是我们想展示在页面上的，比如后台对于时间的处理，往往返回给前段的是时间戳，然而我们不可以能将时间戳展示在页面上，这样的话过滤器就排上用场了 怎么使用过滤器vue2.0后，过滤器只在模板语法&amp;#123&amp;#123&amp;#125&amp;#125中使用，对于时间戳处理我们可以这样 &amp;#123&amp;#123时间戳|时间戳过滤器&amp;#125&amp;#125 过滤器实战——金额过滤器 解决问题后台返回的金额往往只是一个number数据类型，但我们在页面显示时候，往往需要在金额前面加上金额符号￥，之时候我们可以使用过滤器解决这个问题 首先创建一个全局的过滤器priceFormat代码 代码 将过滤器引入入口文件main.js- 在组件中使用 页面 指令——还想用DOM操作节点？指令帮你实现！ 指令的定义 指令（Directives）是带有 v- 前缀的特殊属性。指令属性的值预期是单一 JavaScript 表达式（除了 v-for，之后再讨论）。指令的职责就是当其表达式的值改变时相应地将某些行为应用到 DOM 上 有哪些指令上面我们学到了模板语法，其中v-if,v-model等等都是官方封装好的指令，但有时候我们需要自己定义属于自己的指令 指令解决的问题vue.js开发应用的时候，大多时候，vue是不支持我们用DOM去看开发的，但有时候我们必须使用DOM的方式去解决一些问题，比如在用户点击购买按钮的时候会弹出一个信息输入组件，我们往往需要弹出的信息输入框会自动聚焦，方便用户的输入。这时候我们就需要写一个自动对焦的指令 首先我们先创建一个指令代码 将指令引入入口文件main.js- 在需要绑定的元素上绑定自定义指令v-focus代码 代码 页面——我们在页面添加一个购买按钮，当用户点击按钮，会弹出一个蒙版输入框，这时候输入框会自动获得焦点。 点击前 点击前的页面 点击购买后——可以看到，这时候输入框已经自动获得焦点了 点击后的页面 混合——个个组件重复写方法很麻烦？混合帮你解决！ 解决的问题在开发的过程中，有些方法似乎并不是单独一个组件使用，很多的组件都会使用到这种方法，比较典型的就是在大型的单页面开发中，往往会涉及到路由的转跳，在使用编程路由的时候，由于组件的作用域是独立的，methods中定义的方法并不能在所有组件中使用，为此我们必须在每个组件中重复的编写this.$router.push()这一简单方法，这时候我们可以通过定义一个全局的混合methods，便可以在所有的组件中使用路由转跳的方法 在这里，由于没有涉及路由的概念，我们通过一个简单的全局methods方法，来演示一下怎么使用混合- 定义一个全局混合定义一个全局混合 代码 在webpack入口文件main.js引入mixin- 在组件中使用全局混合方法 代码 点击前 页面 点击后 页面 结语Vue的知识点介绍就在这里了，在开发过程中常用的的知识都有所讲解，如果大家有什么不懂得，意见或者想重点介绍的，可以留言下方或者加我们QQ564648147,我们一起探讨进步吧^^,新手期，觉得小编写的不错的点个赞或者转发分享给周围的朋友吧(ง •̀•́)ง 下期vue-router从入门到精通——开发一个CMS登陆界面,有兴趣的关注我吧，定期更新 转载来源：vue.js应用从创建到运行，从入门到精通]]></content>
      <categories>
        <category>科技</category>
      </categories>
      <tags>
        <tag>HTML</tag>
        <tag>JavaScript</tag>
        <tag>Git</tag>
        <tag>科技</tag>
        <tag>文章</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于snowflake算法实现发号器 - CSDN博客]]></title>
    <url>%2F2017%2F1b323c24%2F</url>
    <content type="text"><![CDATA[#一、背景： 清分系统需要一套id生成器服务，保证分布式情况下全局唯一。 #二、算法描述： ##1、原始算法： （1）snowflake是twitter开源的分布式ID生成算法，其核心思想是：一个long型的ID，使用其中43bit作为毫秒数，3bit作为机房编号，5bit作为机器编码，12bit作为毫秒内序列号。这个算法单机每毫秒内理论上最多可以生成2^12，也就是4096个ID，完全能满足业务的需求。 （2）snowflake的结构如下(每部分用-分开)&#58; 0 - `0000000000000000000000000000000000000000000 - 000 -00000 - 000000000000` 一共加起来刚好64位，为一个Long型。(转换成字符串长度为18) snowflake生成的ID整体上按照时间自增排序，并且整个分布式系统内不会产生ID碰撞（由datacenter和workerId作区分），并且效率较高。 ##2、算法变形： （1）long类型最大值是9,223,372,036,854,775,807（2^63 -1），即19位十进制数。取前13位作为毫秒数，1位作为毫秒内序列号，2位作为机器编号，3位作为数据库表尾号。这个算法单机每毫秒内理论最多可以生产10个id(每秒内理论最多可以生产1w个id)，完全满足业务需求。 （2）结构如下(每部分用 - 分开)： 0000000000`000 - 0 - 00 -000` 加起来正好19位。生成的ID整体上按照时间自增排序，并且整个分布式系统内不会产生ID碰撞。 ##3、清分系统中的id样例： （1）服务器分配机器编码 （2）现清分系统中，tradeFlowId和summaryTradeFlowId前4位为YYMM，即1611开头的19位数。 新的snowflake id生成器生成的id是1613开头的19位数。如1613024787541981316，1613024787541为当前时间 (毫秒+偏移量1345400000000)，9为序列号，81为机器编码，316为数据库尾号。 （3）现清分系统中，batchid为26开头的10位数。 batchid不涉及数据库尾号，所以可以减少id的位数，保证batchid大于26开头的10位数就可以。生成id如1478484776931281，1478484776931为当前时间，2为序列号，81为机器编码。 #三、代码描述： ##1、加锁实现： ##2、无锁实现： #四、结果分析 ##1、对比与选择 通过对比加锁和无锁两种算法，发现并发数为100-10000的区间中，两者差别甚微。由于twitter选取的是加锁算法，我们也选择经过考证的加锁算法。 ##2、算法特点分析 算法支持单机qps为10000(自己mac上跑的结果)，目前业务情况是单台机器qps为50左右。- 单台机器生成的id为单调递增。- 算法支持的时间截止日期为2262年(取long型最高13位9223372036850作为时间戳)。- 算法支持99台机器。- 针对业务需求，机器号需要额外获取。##3、异常情况 （1）获取机器号出错。 服务启动时获取机器编号，获取失败则服务启动失败。 （2）在获取当前 Timestamp 时, 如果获取到的时间戳比前一个已生成 ID 的 Timestamp 还要小怎么办? 继续获取当前机器的时间, 直到获取到更大的 Timestamp 才能继续工作；- 把 NTP 配置成不会向后调整的模式，也就是说, NTP 纠正时间时, 不会向后回拨机器时钟。 转载来源：基于snowflake算法实现发号器 - CSDN博客]]></content>
  </entry>
  <entry>
    <title><![CDATA[你的选择是？盘点50多种有用的机器学习和预测API]]></title>
    <url>%2F2017%2Ffdf582dd%2F</url>
    <content type="text"><![CDATA[导读：本文为大家介绍了在机器学习、预测、文本分析和分类、人脸识别、语言翻译等方面，非常有用的50多个API的更新列表。 随着基于人工智能和机器学习的应用程序的发展，我们看到大量应用程序接口（API）的混搭应用。 API是一套用于构建软件应用程序的例程、协议和工具。在这篇文章中，我们删除了列表中从2015年就停止使用的API，还添加了新的API，比如最近来自IBM、谷歌和微软等巨型提供商。 所有的API被分类到新兴的应用程序组中： 人脸和图像识别- 文本分析、NLP、情感分析- 语言翻译- 预测和其他机器学习文本分析、NLP、情感分析 预测和其他机器学习 在每组申请中，我们按字母顺序列出。 API概述基于截至2017年2月3日在其网址上显示的信息。看看这些API投入使用，如果我们错过了一些流行的API，请在下面的评论中提出。 人脸和图像识别 1.Animetrics人脸识别：这个API可以用来检测图片中的人脸，并将其与一组已知的人脸进行匹配。 API还可以添加或删除可搜索的图库中的主题，并添加或删除主题中的一个面部。 2.Betaface：是一个面部识别和检测Web服务。其特点包括多个人脸检测、人脸裁剪、123个面部点检测（22个基本，101个高级），面临非常大的数据库中的验证、识别、相似性搜索。 3.视觉识别：专注于高端计算机视觉解决方案，主要针对物体检测和物体识别软件。一个识别服务，提供眼睛、脸部、车辆、版权和车牌检测的识别服务。 API的主要价值在于可以即时了解对象、用户和行为。 4.Face ++：面部识别和检测服务，提供应用程序中的检测、识别和分析功能。用户可以打电话来训练程序、检测面孔、识别面孔、分组面孔、操作人员、创建人脸集合、创建组和获取信息。 5.FaceMark：是一种能够检测正面照片上68个点的API，以及35个用于配置文件脸部照片的API。 6.FaceRect：是一个功能强大且完全免费的面部检测API。该API在单张照片上查找人脸（正面和侧面）或多张人脸，为找到的每张人脸生成JSON输出。另外，FaceRect可以为每个检测到的脸部（眼睛，鼻子和嘴巴）找到脸部特征。 7.谷歌云视觉API：由像TensorFlow这样的平台提供支持，已经启用了可以学习和预测图像内容的模型。它可以帮助你找到自己喜欢的图像，并快速大规模地获取丰富的注释。它将图像分成数千个类别（例如，“船”，“狮子”，“艾菲尔铁塔”），以相关的情绪检测脸部，并识别出多种语言印刷的文字。 8.新的IBM Watson视觉识别：了解图像的内容 - 视觉概念标记图像，查找人脸，估计年龄和性别，并在集合中查找类似的图像。您也可以通过创建自己的自定义概念来训练服务。 9.Kairos：是一个平台，可让你快速将情绪分析和人脸识别添加到你的应用程序和服务中。 10.微软认知服务 - 计算机视觉：基于云的API可以基于输入和用户选择以不同方式分析视觉内容。例如，基于内容的标签图像、图像分类、检测人脸并返回其坐标、识别特定域的内容、生成内容的描述、识别图像中的文字、标志成人内容。 11.Rekognition：为社交照片应用程序提供面部和场景图像识别优化。 API利用眼睛、嘴巴、脸部和鼻子以及情绪识别和性依赖特征，可以预测性别，年龄和情绪。 12.Skybiometry人脸检测和识别：提供人脸检测和识别服务。新版本的API包括将黑眼镜与清晰镜片区分开来。 文本分析、NLP、情感分析 1．Bitext提供市场上最准确的多语言话题。目前提供了四种语义服务：实体和概念提取、情感分析和文本分类。该API可以使用8种语言。 2.Diffbot分析：为开发人员提供可以识别、分析和提取任何网页上的主要内容和部分的工具。 3.免费自然语言处理服务：是一种免费服务，包括情绪分析、内容提取和语言检测。这是一个流行的API市场。 4.新的谷歌云自然语言API：分析文本的结构和含义，包括情感分析、实体识别和文本注释。 5.IBM Watson Alchemy语言：教导计算机学习如何阅读和进行文本分析（例如，用于将非结构化数据转化为结构化，特别是在社交媒体监控、商业智能、内容推荐、金融交易和有针对性的广告中）。 6.云文本分类：API执行预分类任务，如提取文本，标记化，停止删除和引理。 7.新的微软 Azure文本分析API是一套使用Azure机器学习构建的文本分析Web服务。该API可用于分析非结构化文本，如情感分析，关键短语提取、语言检测和主题检测。不需要训练数据。 8.微软认知服务 - 文本分析：从文本中检测情感、关键短语、主题和语言。与此API相同的其他API（语言的认知服务）包括必应拼写检查、语言理解、语言分析、网络语言模型。 9.nlpTools：是一个简单的JSON over HTTP RESTful Web服务，用于自然语言处理。它对在线新闻媒体的情绪分析和文本分类进行了解码。 10.新的Geneea：可以对所提供的原始文本，从给定的URL中提取的文本或直接提供的文档执行分析（自然语言处理）。 11.来自谷歌 Research的新成员使用机器学习模型来评估评论对于对话可能产生的影响，并标出令人反感的评论。 12.语义生物医学标记：具有内置的能力，使用文本分析识别133种生物医学实体类型，并将其语义链接到知识库系统中。 13．汤森路透Open Calais：使用自然语言处理、机器学习和其他方法，分类与链接文档与实体（人员，地点，组织等）、实体（人“x”为公司“y”工作）和事件（“z”人在x日被任命为公司董事长）。 14.Yactraq Speech2Topics是一项云端服务，通过语音识别和自然语言处理，将音频视频内容转换为主题元数据。 语言翻译 1.谷歌云翻译：可以动态翻译成千上万的语言对之间的文本。 API让网站和程序以编程方式与翻译服务集成。 2.新的IBM Watson语言翻译器：将文本从一种语言翻译成另一种语言。该服务提供了多个领域特定的模型，你可以根据你独特的术语和语言进行自定义。例如，客户可以用他们自己的语言进行沟通。 3.LangId：一种快速检索任何语言信息的方法，不需要指定语言（即可以识别需要分析的文本以哪种语言编写）。 4.新的微软认知服务 - 翻译：自动检测在翻译之前发送的文本的语言。它为所支持的9种语言中的任何一种语言进行语音翻译，并对60种支持的语言中的任何一种进行文本翻译。 MotaWord：是一个快速的人工翻译平台。它提供超过70种语言的翻译。 API还可以让开发人员获得每个翻译的引用，提交翻译项目以及文档和样式的指南，跟踪翻译项目的进度并实时获取活动提要。 WritePath翻译：API允许开发人员访问和集成WritePath与其他应用程序的功能。操作可以通过这个API来完成：检索单词数量，发布翻译文档以及检索已翻译的文档和文本。 预测和其他机器学习 1.亚马逊机器学习：查找数据模式。此API的示例用途是用于欺诈检测、预测需求、定向营销和点击预测。 2.BigML：为云托管的机器学习和数据分析提供服务。用户可以建立一个数据源，并创建一个模型，通过标准的HTTP来预测使用基本的监督和无监督的机器学习任务进行预测。 3.Ersatz：一个使用GPU支持的深度神经网络即服务的基于网络的预测程序。在Ersatz中，训练了一组不同的神经网络模型（集成方法），有时多达20种。 4.谷歌云预测：提供了一个RESTful API来构建机器学习模型。这些工具可以帮助分析数据，为你的应用程序添加各种功能，如客户情绪分析、垃圾邮件检测、推荐系统等。 5.全新的Google云语音API：使用快速而准确的语音识别功能，将音频从麦克风或文件转换为超过80种语言和变体的文本 6.Guesswork.co：为电子商务网站提供产品推荐引擎。猜测使用在谷歌预测 API之上运行的语义规则引擎准确地预测客户意图。 Hu&#58;toma：帮助世界各地的开发人员构建并通过提供免费访问专有平台提供工具和渠道来创建和分享会话AI的深度学习。 8.新的IBM Watson对话：构建了解自然语言的聊天机器人，并将其部署在任何设备上的消息传递平台和网站上。与此API相同的其他API（语言的认知服务）包括对话框、自然语言分类器、个性的见解和音调分析仪。 9.新的IBM Watson演讲：将语音转换为文本和文本转换为语音（例如，在联络中心转录通话或创建语音控制应用程序）。 10.新的IBM Watson 数据透视：这个集合包括三个API：AlchemyData新闻、发现和权衡分析。 AlchemyData提供充满自然语言处理的新闻和博客内容，以便进行高度针对性的搜索和趋势分析。权衡分析有助于人们在平衡多个目标时作出决定。 11.IBM Watson检索和排名：开发人员可以将其数据加载到服务中，使用已知的相关结果来训练机器学习模型（Rank）。服务输出包括相关文档和元数据的列表。例如，联络中心代理也可以快速找到答案，以改善平均呼叫处理时间。 12.Imagga：提供API自动分配标签到你的图像使你的图像可发现。它基于图像识别平台即服务。 13.indico：提供文本分析（例如，情绪分析、推特参与、情绪）和图像分析（例如，面部情绪、面部定位）。指示API可以自由使用，不需要训练数据。 14．Microsoft Azure认知服务API：正在取代Azure机器学习推荐服务，该服务基于预测分析提供解决方案。它为客户提供个性化的产品推荐，并改善销售。新版本具有新的功能，如批处理支持、更好的API浏览器、更清洁的API表面、更一致的注册/计费体验等。 15.新的微软Azure异常检测API：使用数值检测时间序列数据中的异常。 16.新的微软认知服务 - QnA Maker：将信息提炼成对话，易于浏览的答案。同一组中的其他API（知识认知服务）包括学术知识、实体链接、知识探索、建议。 17.新的微软认知服务 - 说话人识别：让你的应用程序能够知道谁在说话。与此API相同的其他API（语音认知服务）包括Bing Speech（将语音转换为文本并再次转换并理解其意图）和Custom Recognition（自定义识别）。 18.新的MLJAR提供原型、开发和部署模式识别算法的服务。 19.NuPIC：是一个用Python / C ++编写的开源项目，实现Numenta的皮质学习算法，由NuPIC社区维护。 API允许开发人员使用原始算法，将多个区域（包括层次结构）串联起来，并利用其他平台功能。 20.PredicSis：为大数据提供强大的洞察力，并通过预测分析提高营销绩效。 21.PredictionIO：是一个基于Apache 2.0许可证发布的Apache Spark，HBase和Spray上构建的开源的机器学习服务器。示例API方法包括创建和管理用户和用户记录、检索项目和内容以及基于用户创建和管理建议。 22.RxNLP - 集群句子和简短文本：文本挖掘和自然语言处理服务。其中一个API，群集句子API可以将句子（例如来自多个新闻文章的句子）或短文本（例如来自Twitter或Facebook状态更新的文章）分组为逻辑组。 23.新的Recombee：通过RESTful API提供使用数据挖掘、查询语言和机器学习算法（例如协同过滤和基于内容的推荐）的服务。 24.Sightcorp F.A.C.E .&#58;一个web服务，允许第三方应用程序更好地了解用户的行为，并检索相关的面部分析，如年龄、性别、面部表情、头部姿势或种族。 其他API集合：Mashape Blog＆Programmable Web 错过了你最喜欢的API吗？可以在评论中留下你喜欢的一种分享给更多的人！ 转载来源：你的选择是？盘点50多种有用的机器学习和预测API]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>Google</tag>
        <tag>人工智能</tag>
        <tag>云计算</tag>
        <tag>Azure</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[最全深度学习资源集合（Github：Awesome Deep Learning）]]></title>
    <url>%2F2017%2F932c2179%2F</url>
    <content type="text"><![CDATA[偶然在github上看到Awesome Deep Learning项目，故分享一下。其中涉及深度学习的免费在线书籍、课程、视频及讲义、论文、教程、网站、数据集、框架和其他资源，包罗万象，非常值得学习。 其中研究人员部分篇幅所限本文未整理进来。另外上面的GIF录制于MIT自动驾驶课程（MIT 6.S094&#58; Deep Learning for Self-Driving Cars） PS：github上取名“awesome”的一般都非常牛逼，此项目亦然！ 以下整理至： Awesome Deep Learninghttps&#58;//github.com/ChristosChristofidis/awesome-deep-learning 由于无法限制站外链接，且排版等原因，可以在上述原网址里查看。 免费在线数据、课程 视频和讲义 论文 教程 网站 数据集 框架 其他资源 转载来源：最全深度学习资源集合（Github：Awesome Deep Learning）]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>GitHub</tag>
        <tag>机器学习</tag>
        <tag>技术</tag>
        <tag>麻省理工学院</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[站酷 (ZCOOL) - 设计师互动平台 - 打开站酷，发现更好的设计！]]></title>
    <url>%2F2017%2F6826055b%2F</url>
    <content type="text"><![CDATA[站酷 (ZCOOL) - 设计师互动平台 - 打开站酷，发现更好的设计！ 转载来源：站酷 (ZCOOL) - 设计师互动平台 - 打开站酷，发现更好的设计！]]></content>
  </entry>
  <entry>
    <title><![CDATA[Google发布自然语言处理解析器SLING，免除模块化分析级联效应产生的缺陷]]></title>
    <url>%2F2017%2F9b60799c%2F</url>
    <content type="text"><![CDATA[雷锋网 AI科技评论消息，日前，Google发布自然语言框架语义解析器SLING，它能以语义框架图（semantic frame graph）的形式，将自然语言文本直接解析为文本语义表示。这一系统避免了级联效应，另外还减少了不必要的计算开销。 详细消息雷锋网 AI科技评论编译整理如下： 直到最近，大多数实际的自然语言理解(NLU)系统都采用的是从词性标签和依存句法分析（dependency parsing）到计算输入文本的语义表示的分析。虽然这使得不同分析阶段易于模块化，但前期的错误会在后期和最终表示上产生层叠效应，中间阶段的输出也可能会与这一阶段本身并不相关。 例如，一个典型的NLP系统可能在早期执行依存句法分析的任务，在结束阶段时执行共指解析（coreference resolution）任务，早期依存句法分析阶段出现的任何错误都会产生级联效应，影响共指解析的输出。 今天我们发布SLING实验系统，它能以语义框架图（semantic frame graph）的形式，将自然语言文本直接解析为文本语义表示。 输出框架图能直接捕获用户感兴趣的语义标注（semantic annotation），因为没有运行任何中间阶段，所以避免了上述那种管道系统的缺陷，另外还减少了不必要的计算开销。 SLING使用具有特殊用途的循环神经网络模型，通过框架图上的增量编辑操作（incremental editing operation）来计算输入文本的输出表示。框架图足够灵活，可以捕获大家感兴趣的许多语义任务(下面有更多介绍)。SLING中的分析器（parser）只使用输入词来进行训练，不需要额外再生成标注(例如依存句法分析)。 SLING通过提供高效的、可扩展的框架存储实现（frame store implementation）和JIT编译器来生成高效的代码来执行循环神经网络，从而在推理（inference）时能快速进行句法分析。 尽管SLING还处于实验阶段，但得益于高效的框架存储和神经网络编译器，它在台式机CPU上能实现超过2500符号/秒的解析速度。 SLING使用C++，目前可以在GitHub上下载。这个系统在技术报告中有详细描述。 框架语义句法分析（Frame Semantic Parsing） 框架语义表示文本的含义（例如一句话），是一套正规表述。每个正规表述都被称为一个框架，可以被看作是知识或语义的一个单元，还包含与与它相关的概念或其他框架的相互作用。 SLING将框架组织成属性槽（slot）列表，其中每个属性槽都有对应的名称(角色)和值（可能是literal或是到另一个框架的链接）。 下面是一个例句&#58; “很多人都宣称自己预测到了黑色星期一。”（Many people now claim to have predicted Black Monday） 下图是SLING识别提到的实体(例如人物、地点或事件)、度量(例如日期或距离)和其他概念(例如动词)，并将它们放置在正确的语义角色中的说明。 上面的例子相当简单，框架图的功能强大到可以模拟各种复杂的语义标注任务。对于初学者来说，这种框架可以非常方便地将语言的内外部信息类型(例如知识库)结合起来。这可以用于处理复杂的语言理解问题，例如引用、隐喻、转喻等。这些任务的框架图只在框架类型、角色和链接约束条件上有所不同。 SLING SLING通过优化语义框架来训练循环神经网络。网络隐藏层中学到的内部表示取代了在前面那种管道系统中的手工特性组合和中间表示。 解码器使用伴随反复出现的特征一起的表示，来计算用于框架图更新的一系列过渡，以获得输入语句的预期框架语义表示。在SLING中用TensorFlow和DRAGNN来训练模型。 下面的动图展示了使用过滤操作将框架和角色逐渐添加到框架图中的构建过程。 正如一开始讨论的那个简单例句，SLING使用ARG1角色将动词和事件框架连接起来，表示事件框架是被预测的概念。 这个过渡系统的一个关键层面是，有一个很小的固定大小的框架缓冲区，它代表了最近被唤起或修改的框架，用橙色方框标记。这个缓冲区会捕捉到我们倾向于记住的最近被唤起、提及或强化的知识的直觉。如果一个框架不再使用，那么当新的框架出现时，它最终会从这个缓冲区中被清除掉。我们发现这种简单的机制在捕捉大量框架间链接的片段时非常有效。 下一步 上面所描述的实验仅仅是对诸如知识提取、解析复杂引用和对话理解等语义句法分析研究任务的启动研究。 在Github上发布的SLING中有上述任务的预训练模型，还有一些示例和方法，大家可以在提供的综合数据或自己的数据上来训练解析器。希望SLING能对大家有所帮助有用，我们期待着在其他语义句法分析任务上应用和扩展SLING。 Github地址：https&#58;//github.com/google/sling Via：Google Research Blog 雷锋网 AI科技评论编译整理。 转载来源：Google发布自然语言处理解析器SLING，免除模块化分析级联效应产生的缺陷]]></content>
      <categories>
        <category>科技</category>
      </categories>
      <tags>
        <tag>GitHub</tag>
        <tag>机器学习</tag>
        <tag>Google</tag>
        <tag>人工智能</tag>
        <tag>编译器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何设计一款多场景分布式发号器（Vesta）-极客]]></title>
    <url>%2F2017%2F1fc4384e%2F</url>
    <content type="text"><![CDATA[如何设计一款多场景分布式发号器（Vesta）-极客 转载来源：如何设计一款多场景分布式发号器（Vesta）-极客]]></content>
  </entry>
  <entry>
    <title><![CDATA[CNN在NLP领域的实践（1） 文本分类 - CSDN博客]]></title>
    <url>%2F2017%2F128c950f%2F</url>
    <content type="text"><![CDATA[众所周知，卷积神经网络（CNN）在计算机视觉领域取得了极大的进展，但是除此之外CNN也逐渐在自然语言处理（NLP）领域攻城略地。本文主要以文本分类为例，介绍卷积神经网络在NLP领域的一个基本使用方法，由于本人是初学者，而且为了避免东施效颦，所以下面的理论介绍更多采用非数学化且较为通俗的方式解释。 0.文本分类 所谓文本分类，就是使用计算机将一篇文本分为a类或者b类，属于分类问题的一种，同时也是NLP中较为常见的任务。 一.词向量 提到深度学习在NLP中的应用就不得不提到词向量，词向量（Distributed Representation）在国内也经常被翻译为词嵌入等等，关于词向量的介绍的文章已经有很多，比如这位大神的博客：http&amp;#58;//blog.csdn.net/zhoubl668/article/details/23271225 本文则用较为通俗的语言帮助大家了解词向量。 所谓词向量就是通过神经网络来训练语言模型，并在训练过程钟生成一组向量，这组向量将每个词表示为一个n维向量。举个例子，假如我们要将&quot;北京&quot;表示为一个2维向量，可能的一种结果如 北京=（1.1,2.2）,在这里北京这个词就被表示为一个2维的向量。但是除了将词表示为向量以外，词向量还要保证语义相近的词在词向量表示方法中的空间距离应当是相近的。比如 &apos;中国&apos; - &apos;北京&apos; ≈ &apos;英国&apos; - &apos;伦敦&apos; 。上述条件可在下列词向量分布时满足，&apos;北京&apos;=（1.1,2.2），&apos;中国&apos;=（1.2,2.3） ，’伦敦’=（1.5,2.4），’英国’=(1.6,2.5)。 一般训练词向量可以使用google开源word2vec程序。 二.**卷积神经网络与词向量的结合** 有关CNN的博客非常之多，如果不了解CNN的基本概念可以参见这位大神的博客如下：http&#58;//blog.csdn.net/zhoubl668/article/details/23271225 这里就不在赘述。 通常卷积神经网络都是用来处理类似于图像这样的二维（不考虑rgb）矩阵，比如一张图片通常都可以表示为一个2维数组比如255*255，这就表示该图片是一张255像素宽，255像素高的图片。那么如何将CNN应用到文本中呢，答案就是词向量。 我们刚刚介绍了词向量的概念，下面介绍下如何将文本通过词向量的方式转换成一个类图像类型的格式。一般来说一篇文本可以被视为一个词汇序列的组合，比如有篇文本内容是 ‘书写代码，改变世界’。可以将其转换为（’书写’，’代码’，’改变’，’世界’）这样一个文本序列，显然这个序列是一个一维的向量，不能直接使用cnn进行处理。 但是如果使用词向量的方式将其展开，假设在某词向量钟 ‘书写’ =（1.1,2.1），’代码’ = （1.5,2.9），’改变’ = （2.7,3.1） ，’世界’ = （2.9,3.5）,那么（’书写’，’代码’，’改变’，’世界’）这个序列就可以改写成（（1.1,2.1），（1.5,2.9），（2.7,3.1），（2.9,3.5）），显然原先的文本序列是41的向量，改写之后的文本可以表示为一个42的矩阵。 推而广之任何以文本序列都可以表示为m*d的数组，m维文本序列的词数，d维词向量的维数。 三.**用于文本分类的神经网络结构设计** **本文前面介绍了词向量、卷积神经网络等概念，并提出可以将文本转换成一个由词序列和词向量嵌套而成的二维矩阵，并通过CNN对其进行处理，下面以文本分类任务为例，举例说明如何设计该神经网络的样式。 3.1 文本预处理部分的流程 这部分主要是分3步，共4种状态。1.将原始文本分词并转换成以词的序列 2.将词序列转换成以词编号（每个词表中的词都有唯一编号）为元素的序列 3.将词的编号序列中的每个元素（某个词）展开为词向量的形式。下面通过一张图（本人手画简图。。。。囧）来表示这个过程，如下图所示： 3.2 神经网络模块的设计 本文关于神经网络设计的思想来自于以下博文： http&#58;//www.wildml.com/2015/12/implementing-a-cnn-for-text-classification-in-tensorflow/ 由于该文章是纯英文的，某些读者可能还不习惯阅读这类文献，我下面结合一张神经网络设计图，来说明本文中所使用的神经网络，具体设计图（又是手画图，囧）如下： 简要介绍下上面的图，第一层数据输入层，将文本序列展开成词向量的序列，之后连接 卷积层、激活层、池化层 ，这里的卷积层因为卷积窗口大小不同，平行放置了三个卷积层，垂直方向则放置了三重（卷积层、激活层、池化层的组合）。之后连接全脸阶层和激活层，激活层采用softmax并输出 该文本属于某类的概率。 3.3 编程实现所需要的框架和数据集等 3.3.1 框架：本文采用keras框架来编写神经网络，关于keras的介绍请参见莫言大神翻译的keras中文文档：http&amp;#58;//keras-cn.readthedocs.io/en/latest/ 。 3.3.2 数据集：文本训练集来自20_newsgroup,该数据集包括20种新闻文本，下载地址如下：http&amp;#58;//www.qwone.com/~jason/20Newsgroups/ 3.3.3 词向量：虽然keras框架已经有embedding层，但是本文采用glove词向量作为预训练的词向量，glove的介绍和下载地址如下（打开会比较慢）： http&#58;//nlp.stanford.edu/projects/glove/ 3.4 代码和相应的注释 在3.2部分已经通过一张图介绍了神经网络的设计部分，但是考虑到不够直观，这里还是把所使用的代码，罗列如下，采用keras编程，关键部分都已经罗列注释，代码有部分是来源自keras文档 中的example目录下的：pretrained_word_embeddings.py,但是该程序我实际运行时出现了无法训练的bug，所以做了诸多改变，最主要的是我把原文中的激活层从relu改成了tanh，整体的设计结构也有了根本性的改变。对keras原始demo有兴趣的可以参见： http&#58;//keras-cn.readthedocs.io/en/latest/blog/word_embedding/ 下面就是本文中所使用的文本分类代码： 四.**总结** 本文描述了如何使用深度学习和keras框架构建一个文本分类器的全过程，并给出了相应的代码实现，为了方便大家使用，下面给出本文代码的下载地址一（简单版）： https&#58;//github.com/894939677/deeplearning_by_diye/blob/master/pretrain_text_class_by_diye.py 下面给出本文代码的下载地址二（完整版）： https&#58;//github.com/894939677/deeplearning_by_diye/blob/master/pre_merge_3.py 五.后记 本文描述的是使用类似于googlenet的网络结构，实际上也可以使用类似与resnet的网络结构来做这个事情 转载来源：CNN在NLP领域的实践（1） 文本分类 - CSDN博客]]></content>
  </entry>
  <entry>
    <title><![CDATA[谷歌开源「Tangent」：一个用于自动微分的源到源Python库]]></title>
    <url>%2F2017%2F8fb17f0e%2F</url>
    <content type="text"><![CDATA[选自Google Research Blog 机器之心编译 参与：黄小天、刘晓坤 近日，谷歌在其官方博客上开源了「Tangent」，一个用于自动微分的源到源 Python 库；它通过 Python 函数 f 生成新函数，来计算 f 的梯度，从而实现更好的梯度计算可视化，帮助用户更容易地编辑和调试梯度；本文还扼要概述了 Tangent API，包括如何使用 Tangent 在 Python 中生成易于理解、调试和修改的梯度代码。 Tangent 是一个免费、开源的新 Python 库，用于自动微分。和目前已有的机器学习库不同，Tangent 是一个源到源（source-to-source）的系统，利用 Python 函数 f 生成一个新的 Python 函数，来计算 f 的梯度。这为用户提供了更好的梯度计算可视化，使用户可以容易地对梯度进行编辑和调试。Tangent 在调试和设计机器学习模型上有如下特征： 易于调试反向传播过程- 快速编辑和调试梯度- 正向模式（Forward mode）自动微分- 高效的 Hessian 向量内积（Hessian-vector products）- 代码优化快速编辑和调试梯度 高效的 Hessian 向量内积（Hessian-vector products） 本文对 Tangent API 进行了概述，包括如何使用 Tangent 在 Python 中生成易于理解、调试和修改的梯度代码。 神经网络（NN）使机器学习模型处理图像、视频、音频和文本的能力出现巨大进步。训练神经网络在这些任务上获得高性能的基本抽象概念是一个有着 30 年历史的思想——「反向模式自动微分」（也叫做反向传播），它由神经网络中的两个传播过程组成：首先运行「前向传播」计算每一个节点的输出，然后运行「反向传播」计算一系列导数以决定权重的更新率，从而提高模型的准确性。 训练神经网络和在新型架构上做研究需要准确、高效和简易地计算这些导数。当模型训练结果不好时，或者尝试建立一些尚未理解的东西时，调试这些导数的能力非常必要。自动微分，或简称为「autodiff」，是一种计算表征一些数学函数的计算机程序的导数的技术，并可以在几乎所有的机器学习库中实现。 目前已有的库通过追踪程序的执行（在运行时，比如 TF Eager、PyTorch 和 Autograd）或建立动态数据流图然后对图微分（预编，比如 TensorFlow），实现自动微分。与之相反，Tangent 能自主在 Python 源代码上进行预编的自动微分，并生成 Python 源代码作为其输出。 因此，你可以把自动微分代码当做程序的余下部分进行阅读。对于那些不仅想在 Python 编写模型，还希望在不牺牲速度和灵活性的前提下阅读和调试自动生成导数的代码的研究者和学生，Tangent 是很有用的。 用 Tangent 编写的模型易于检查和调试，而不需要特殊的工具或间接的方式。Tangent 能提供其它 Python 机器学习库没有的额外自动微分的特征，具有强大的性能，并和 TensorFlow 以及 Numpy 兼容。 Python 代码的自动微分 我们如何自动生成纯 Python 代码的导数？数学函数比如 tf.exp 或 tf.log 含有可以用来构建反向传播的导数。相似地，句法片段（比如子程序、条件和循环）也有反向传播版本。Tangent 有办法为每个 Python 句法片段生成生成导数代码，同时调用很多的 NumPy 和 TensorFlow 函数。 Tangent 有一个单一函数 API： 下面的动图展示了如何一个 Python 函数上调用 tangent.grad： 如果你想要打印出导数，你可以运行： 在 hood 之下，tangent.grad 首先抓取你传递给它的 Python 函数源代码。Tangent 有一个 Python 句法导数和 TensorFlow Eager 函数的大型方法库。tangent.grad 函数逆序运行你的代码，查找匹配的反向传播方法，并将其添加到导数函数的尾部。这一逆序处理技术被称之为反向模式自动微分（reverse-mode automatic differentiation）。 df 函数只适用于标量（非数组）输入。Tangent 同样支持 使用 TensorFlow Eager 函数处理数字数组- 子程序- 控制流子程序 尽管我们从 TensorFlow Eager 支持开始，Tangent 并没有受限于任何数字库，我们非常欢迎添加 PyTorch 或 MXNet 导数方法的请求。 下一步 Tangent 现在是开源的（github.com/google/tangent），但仍处于试验阶段，难免存在一些 bug，如果你能在 GitHub 上指出，我们将很快修复。 我们正致力于在 Tangent 支持 Python 语言的更多属性（比如闭包、内嵌函数定义、类、更多的 Numpy 和 TensorFlow 函数），同样计划在未来添加更多高级的自动微分和编译功能，比如内存与计算之间的自动博弈，更主动的优化以及λ升降。最后，我们非常期望能与社区一起开发 Tangent。 转载来源：谷歌开源「Tangent」：一个用于自动微分的源到源Python库]]></content>
      <categories>
        <category>科技</category>
      </categories>
      <tags>
        <tag>编程语言</tag>
        <tag>GitHub</tag>
        <tag>Python</tag>
        <tag>机器学习</tag>
        <tag>Google</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[为什么XGBoost在机器学习竞赛中表现如此卓越？]]></title>
    <url>%2F2017%2Fe18ce50b%2F</url>
    <content type="text"><![CDATA[挪威科技大学 Didrik Nielsen 的硕士论文《使用 XGBoost 的树提升：为什么 XGBoost 能赢得「每一场」机器学习竞赛？（Tree Boosting With XGBoost - Why Does XGBoost Win “Every” Machine Learning Competition?）》研究分析了 XGBoost 与传统 MART 的不同之处以及在机器学习竞赛上的优势。机器之心技术分析师对这篇长达 110 页的论文进行了解读，提炼出了其中的要点和核心思想，汇成此篇。本文原文发表在机器之心英文官网上。 论文原文：https&#58;//brage.bibsys.no/xmlui/bitstream/handle/11250/2433761/16128_FULLTEXT.pdf- 解读文章英文原文：https&#58;//syncedreview.com/2017/10/22/tree-boosting-with-xgboost-why-does-xgboost-win-every-machine-learning-competition/解读文章英文原文：https&#58;//syncedreview.com/2017/10/22/tree-boosting-with-xgboost-why-does-xgboost-win-every-machine-learning-competition/ 引言 tree boosting（树提升）已经在实践中证明可以有效地用于分类和回归任务的预测挖掘。 之前很多年来，人们所选择的树提升算法一直都是 MART（multiple additive regression tree/多重累加回归树）。但从 2015 年开始，一种新的且总是获胜的算法浮出了水面：XGBoost。这种算法重新实现了树提升，并在 Kaggle 和其它数据科学竞赛中屡获佳绩，因此受到了人们的欢迎。 在《Tree Boosting With XGBoost - Why Does XGBoost Win “Every” Machine Learning Competition?》这篇论文中，来自挪威科技大学的 Didrik Nielsen 研究调查了： XGBoost 与传统 MART 的不同之处1. XGBoost 能赢得「每一场」机器学习竞赛的原因XGBoost 能赢得「每一场」机器学习竞赛的原因 这篇论文分成三大部分： 回顾统计学习的一些核心概念1. 介绍 boosting 并以函数空间中数值优化的方式对其进行解释；进一步讨论更多树方法以及树提升方法的核心元素1. 比较 MART 和 XGBoost 所实现的树提升算法的性质；解释 XGBoost 受欢迎的原因介绍 boosting 并以函数空间中数值优化的方式对其进行解释；进一步讨论更多树方法以及树提升方法的核心元素 统计学习的基本概念 这篇论文首先介绍了监督学习任务并讨论了模型选择技术。 机器学习算法的目标是减少预期的泛化误差，这也被称为风险（risk）。如果我们知道真实的分布 P(x,y)，那么风险的最小化就是一个可以通过优化算法解决的最优化任务。但是，我们并不知道真实分布，只是有一个用于训练的样本集而已。我们需要将其转换成一个优化问题，即最小化在训练集上的预期误差。因此，由训练集所定义的经验分布会替代真实分布。上述观点可以表示成下面的统计学公式： 其中 是模型的真实风险 R(f) 的经验估计。L(.) 是一个损失函数，比如平方误差损失函数（这是回归任务常用的损失函数），其它损失函数可以在这里找到：http&#58;//www.cs.cornell.edu/courses/cs4780/2017sp/lectures/lecturenote10.html。n 是样本的数量。 当 n 足够大时，我们有： ERM（经验风险最小化）是一种依赖于经验风险的最小化的归纳原理（Vapnik, 1999）。经验风险最小化运算 f hat 是目标函数的经验近似，定义为： 其中 F 属于某个函数类，并被称为某个模型类（model class），比如常数、线性方法、局部回归方法（k-最近邻、核回归）、样条函数等。ERM 是从函数集 F 中选择最优函数 f hat 的标准。 这个模型类和 ERM 原理可以将学习问题转变成优化问题。模型类可以被看作是候选的解决方案函数，而 ERM 则为我们提供了选择最小化函数的标准。 针对优化问题的方法有很多，其中两种主要方法是梯度下降法和牛顿法；MART 和 XGBoost 分别使用了这两种方法。 这篇论文也总结了常见的学习方法： 常数 线性方法 局部最优方法 基函数扩展：显式非线性项、样条、核方法等 自适应基函数模型：GAM（广义相加模型）、神经网络、树模型、boosting 另一个机器学习概念是模型选择（model selection），这要考虑不同的学习方法和它们的超参数。首要的问题一直都是：增加模型的复杂度是否更好？而答案也总是与模型自身的泛化性能有关。如下图 1 所示，我们也许可以在模型更加复杂的同时得到更好的表现（避免欠拟合），但我们也会失去泛化性能（过拟合）： 图 1：泛化性能 vs 训练误差 为平方损失使用预期条件风险的经典的偏置-方差分解（bias-variance decomposition），我们可以观察风险相对于复杂度的变化： 图 2：预期风险 vs 方差 vs 偏置 为此通常使用的一种技术是正则化（regularization）。通过隐式和显式地考虑数据的拟合性和不完善性，正则化这种技术可以控制拟合的方差。它也有助于模型具备更好的泛化性能。 不同的模型类测量复杂度的方法也不一样。LASSO 和 Ridge（Tikhonov regularization）是两种常用于线性回归的测量方法。我们可以将约束（子集化、步进）或惩罚（LASSO、Ridge）直接应用于复杂度测量。 理解 Boosting、树方法和树提升 Boosting boosting 是一种使用多个更简单的模型来拟合数据的学习算法，它所用的这些更简单的模型也被称为基本学习器（base learner）或弱学习器（weak learner）。其学习的方法是使用参数设置一样或稍有不同的基本学习器来自适应地拟合数据。 Freund 和 Schapire (1996) 带来了第一个发展：AdaBoost。实际上 AdaBoost 是最小化指数损失函数，并迭代式地在加权的数据上训练弱学习器。研究者也提出过最小化对数损失的二阶近似的新型 boosting 算法：LogitBoost。 Breiman (1997a,b 1998) 最早提出可以将 boosting 算法用作函数空间中的数值优化技术。这个想法使得 boosting 技术也可被用于回归问题。这篇论文讨论了两种主要的数值优化方法：梯度提升和牛顿提升（也被称为二阶梯度提升或 Hessian boosting，因为其中应用了 Hessian 矩阵）。下面，让我们一步一步了解 boosting 算法。 boosting 拟合同一类的集成模型（ensemble model）： 其可以被写成自适应基函数模型： 其中 f_0(x)=θ_0 且 f_m(x)=θ_m*Φ_m(x)，m=1,…,M，Φm 是按顺序累加的基本函数，可用于提升当前模型的拟合度。 因此，大多数 boosting 算法都可以看作是在每次迭代时或准确或近似地求解 所以，AdaBoost 就是为指数损失函数求解上述等式，其约束条件为：Φm 是 A=&amp;#123-1,1&amp;#125 的分类器。而梯度提升或牛顿提升则是为任意合适的损失函数近似求解上述等式。 梯度提升和牛顿提升的算法如下： 是梯度 是使用数据学习到的弱学习器 是经验 Hessian- 梯度提升：梯度提升： 是由线搜索（line search）确定的步长- 牛顿提升：牛顿提升： 最常用的基本学习器是回归树（比如 CART），以及分量形式的线性模型（component-wise linear model）或分量形式的平滑样条（component-wise smoothing spline）。基本学习器的原则是要简单，即有很高的偏置，但方差很低。 boosting 方法中的超参数有： 迭代次数 M：M 越大，过拟合的可能性就越大，因此需要验证集或交叉验证集。1. 学习率 η ：降低学习率往往会改善泛化性能，但同时也会让 M 增大，如下图所示。学习率 η ：降低学习率往往会改善泛化性能，但同时也会让 M 增大，如下图所示。 在 Boston Housing 数据集上的不同学习率的样本外（out-of-sample）RMSE 树方法 树模型是简单和可解释的模型。它们的预测能力确实有限，但将多个树模型组合到一起（比如 bagged trees、随机森林或在 boosting 中），它们就可以变成一种强大的预测模型。 我们可以将树模型看作是将特征空间分割成几个不同的矩形和非重叠区域集合，然后它可以拟合一些简单的模型。下图给出了使用 Boston Housing 数据得到的可视化结果： 终端节点的数量和树的深度可以看作是树模型的复杂度度量。为了泛化这种模型，我们可以在复杂度度量上轻松地应用一个复杂度限制，或在终端节点的数量或叶权重的惩罚项上应用一个惩罚（XGBoost 使用的这种方法）。 因为学习这种树的结构是 NP 不完全的，所以学习算法往往会计算出一个近似的解。这方面有很多不同的学习算法，比如 CART（分类和回归树）、C4.5 和 CHAID。这篇论文描述了 CART，因为 MART 使用的 CART，XGBoost 也实现了一种与 CART 相关的树模型。 CART 以一种自上而下的方式生长树。通过考虑平行于坐标轴的每次分割，CART 可以选择最小化目标的分割。在第二步中，CART 会考虑每个区域内每次平行的分割。在这次迭代结束时，最好的分割会选出。CART 会重复所有这些步骤，直到达到停止标准。 给定一个区域 Rj，学习其权重 wj 通常很简单。令 Ij 表示属于区域 Rj 的索引的集合，即 xi∈Rj，其中 i∈Ij。 其权重是这样估计的： 对于一个树模型 f_hat，经验风险为： 其中我们令 L_j hat 表示节点 j 处的累积损失。在学习过程中，当前树模型用 f_before hat 和 f_after hat 表示。 我们可以计算所考虑的分割所带来的增益： 对于每一次分割，每个可能节点的每个可能分割都会计算这种增益，再取其中最大的增益。 现在让我们看看缺失值。CART 会使用替代变量（surrogate variable）来处理缺失值，即对于每个预测器，我们仅使用非缺失数据来寻找分割，然后再基于主分割寻找替代预测因子，从而模拟该分割。比如，假设在给定的模型中，CART 根据家庭收入分割数据。如果一个收入值不可用，那么 CART 可能会选择教育水平作为很好的替代。 但 XGBoost 是通过学习默认方向来处理缺失值。XGBoost 会在内部自动学习当某个值缺失时，最好的方向是什么。这可以被等价地看作是根据训练损失的减少量而自动「学习」缺失值的最佳插补值。 根据类别预测器，我们可以以两种方式处理它们：分组类别或独立类别。CART 处理的是分组类别，而 XGBoost 需要独立类别（one-hot 编码）。 这篇论文以列表的形式总结了树模型的优缺点： 优点（Hastie et al., 2009; Murphy, 2012）： •容易解释 •可以相对快地构建 •可以自然地处理连续和分类数据 •可以自然地处理缺失数据 •对输入中的异常值是稳健的 •在输入单调变换时是不变的 •会执行隐式的变量选择 •可以得到数据中的非线性关系 •可以得到输入之间的高阶交互 •能很好地扩展到大型数据集 缺点（Hastie et al., 2009; Kuhn and Johnson, 2013; Wei-Yin Loh, 1997; Strobl et al., 2006）： •往往会选择具有更高数量的不同值的预测器 •当预测器具有很多类别时，可能会过拟合 •不稳定，有很好的方差 •缺乏平滑 •难以获取叠加结构 •预测性能往往有限 树提升 在上述发展的基础上，现在我们将 boosting 算法与基本学习器树方法结合起来。 提升后的树模型可以看作是自适应基函数模型，其中的基函数是回归树： 提升树模型（boosting tree model）是多个树 fm 的和，所以也被称为树集成（tree ensemble）或叠加树模型（additive tree model）。因此它比单个树模型更加平滑，如下图所示： 拟合 Boston Housing 数据的叠加树模型的可视化 在提升树模型上实现正则化的方法有很多： 在基函数扩展上进行正则化 在各个树模型上进行正则化 随机化 一般来说，提升树往往使用很浅的回归树，即仅有少数终端节点的回归树。相对于更深度的树，这样的方差较低，但偏置更高。这可以通过应用复杂度约束来完成。 XGBoost 相对于 MART 的优势之一是复杂度的惩罚，这对叠加树模型而言并不常见。目标函数的惩罚项可以写成： 其中第一项是每个单个树的终端节点的数量，第二项是在该项权重上的 L2 正则化，最后一项是在该项权重上的 L1 正则化。 Friedman(2002) 最早引入了随机化，这是通过随机梯度下降实现的，其中包括在每次迭代时进行行子采样（row subsampling）。随机化有助于提升泛化性能。子采样的方法有两种：行子采样与列子采样（column subsampling）。MART 仅包含行子采样（没有替代），而 XGBoost 包含了行子采样和列子采样两种。 正如前面讨论的那样，MART 和 XGBoost 使用了两种不同的 boosting 算法来拟合叠加树模型，分别被称为 GTB（梯度树提升）和 NTB（牛顿树提升）。这两种算法都是要在每一次迭代 m 最小化： 其基函数是树： 其一般步骤包含 3 个阶段： 确定一个固定的候选树结构的叶权重 ； 使用前一阶段确定的权重，提出不同的树结构，由此确定树结构和区域； 一旦树结构确定，每个终端节点中的最终叶权重（其中 j=1,..,T）也就确定了。 算法 3 和 4 使用树作为基函数，对算法 1 和 2 进行了扩展： XGBoost 和 MART 的差异 最后，论文对两种树提升算法的细节进行了比较，并试图给出 XGBoost 更好的原因。 算法层面的比较 正如之前的章节所讨论的那样，XGBoost 和 MART 都是要通过简化的 FSAM（Forward Stage Additive Modelling/前向阶段叠加建模）求解同样的经验风险最小化问题： 即不使用贪婪搜索，而是每次添加一个树。在第 m 次迭代时，使用下式学习新的树： XGBoost 使用了上面的算法 3，即用牛顿树提升来近似这个优化问题。而 MART 使用了上面的算法 4，即用梯度树提升来做这件事。这两种方法的不同之处首先在于它们学习树结构的方式，然后还有它们学习分配给所学习的树结构的终端节点的叶权重的方式。 再看看这些算法，我们可以发现牛顿树提升有 Hessian 矩阵，其在确定树结构方面发挥了关键性的作用，XGBoost： 而使用了梯度树提升的 MART 则是： 然后，XGBoost 可以直接这样定义牛顿树提升的叶权重： 使用梯度树提升的 MART 则这样定义： 总结一下，XGBoost 使用的 Hessian 是一种更高阶的近似，可以学习到更好的树结构。但是，MART 在确定叶权重上表现更好，但却是对准确度更低的树结构而言。 在损失函数的应用性方面，牛顿树提升因为要使用 Hessian 矩阵，所以要求损失函数是二次可微的。所以它在选择损失函数上要求更加严格，必须要是凸的。 当 Hessian 每一处都等于 1 时，这两种方法就是等价的，这就是平方误差损失函数的情况。因此，如果我们使用平方误差损失函数之外的任何损失函数，在牛顿树提升的帮助下，XGBoost 应该能更好地学习树结构。只是梯度树提升在后续的叶权重上更加准确。因此无法在数学上对它们进行比较。 尽管如此，该论文的作者在两个标准数据集上对它们进行了测试：Sonar 和 Ionosphere（Lichman, 2013）。这个实证比较使用了带有 2 个终端节点的树，没有使用其它正则化，而且这些数据也没有分类特征和缺失值。梯度树提升还加入了一个线搜索（line search），如图中红色线所示。 这个比较图说明这两种方法都能无缝地执行。而且线搜索确实能提升梯度提升树的收敛速度。 正则化比较 正则化参数实际上有 3 类： 1.boosting 参数：树的数量 M 和学习率η 树参数：在单个树的复杂度上的约束和惩罚 随机化参数：行子采样和列子采样 两种 boosting 方法的主要差别集中在树参数以及随机化参数上。 对于树参数，MART 中的每个树都有同样数量的终端节点，但 XGBoost 可能还会包含终端节点惩罚 γ，因此其终端节点的数量可能会不一样并且在最大终端节点数量的范围内。XGBoost 也在叶权重上实现了 L2 正则化，并且还将在叶权重上实现 L1 正则化。 在随机化参数方面，XGBoost 提供了列子采样和行子采样；而 MART 只提供了行子采样。 为什么 XGBoost 能赢得「每一场」竞赛？ 通过使用模拟数据，论文作者首次表明树提升可以被看作是自适应地确定局部邻域。 使用 生成 然后使用局部线性回归（使用了两种不同灵活度的拟合）来拟合它： 然后使用平滑样条函数（使用了两种不同灵活度的拟合）来拟合它： 现在我们尝试提升的树桩（boosted tree stump）（两个终端节点）拟合： 本论文详细说明了权重函数影响拟合的确定的方式，并且表明树提升可以被看作是直接在拟合阶段考虑偏置-方差权衡。这有助于邻域保持尽可能大，以避免方差不必要地增大，而且只有在复杂结构很显然的时候才会变小。 尽管当涉及到高维问题时，树提升「打败了」维度的诅咒（curse of dimensionality），而没有依赖任何距离指标。另外，数据点之间的相似性也可以通过邻域的自适应调整而从数据中学习到。这能使模型免疫维度的诅咒。 另外更深度的树也有助于获取特征的交互。因此无需搜索合适的变换。 因此，是提升树模型（即自适应的确定邻域）的帮助下，MART 和 XGBoost 一般可以比其它方法实现更好的拟合。它们可以执行自动特征选择并且获取高阶交互，而不会出现崩溃。 通过比较 MART 和 XGBoost，尽管 MART 确实为所有树都设置了相同数量的终端节点，但 XGBoost 设置了 Tmax 和一个正则化参数使树更深了，同时仍然让方差保持很低。相比于 MART 的梯度提升，XGBoost 所使用的牛顿提升很有可能能够学习到更好的结构。XGBoost 还包含一个额外的随机化参数，即列子采样，这有助于进一步降低每个树的相关性。 机器之心分析师的看法 这篇论文从基础开始，后面又进行了详细的解读，可以帮助读者理解提升树方法背后的算法。通过实证和模拟的比较，我们可以更好地理解提升树相比于其它模型的关键优势以及 XGBoost 优于一般 MART 的原因。因此，我们可以说 XGBoost 带来了改善提升树的新方法。 本分析师已经参与过几次 Kaggle 竞赛了，深知大家对 XGBoost 的兴趣以及对于如何调整 XGBoost 的超参数的广泛深度的讨论。相信这篇文章能够启迪和帮助初学者以及中等水平的参赛者更好地详细理解 XGBoost。 参考文献 Chen, T. and Guestrin, C. (2016). Xgboost&#58; A scalable tree boosting system. In Proceedings of the 22Nd ACM SIGKDD International Conference on Knowl- edge Discovery and Data Mining, KDD 』16, pages 785–794, New York, NY, USA. ACM.- Freund, Y. and Schapire, R. E. (1996). Experiments with a new boosting algorithm. In Saitta, L., editor, Proceedings of the Thirteenth International Conference on Machine Learning (ICML 1996), pages 148–156. Morgan Kaufmann.- Friedman, J. H. (2002). Stochastic gradient boosting. Comput. Stat. Data Anal., 38(4)&#58;367–378.- Hastie, T., Tibshirani, R., and Friedman, J. (2009). The Elements of Statistical Learning&#58; Data Mining, Inference, and Prediction, Second Edition. Springer Series in Statistics. Springer.- Kuhn, M. and Johnson, K. (2013). Applied Predictive Modeling. SpringerLink &#58; Bu ̈cher. Springer New York.- Lichman, M. (2013). UCI machine learning repository.- Murphy, K. P. (2012). Machine Learning&#58; A Probabilistic Perspective. The MIT Press.- Strobl, C., laure Boulesteix, A., and Augustin, T. (2006). Unbiased split selection for classification trees based on the gini index. Technical report.- Vapnik, V. N. (1999). An overview of statistical learning theory. Trans. Neur. Netw., 10(5)&#58;988–999.- Wei-Yin Loh, Y.-S. S. (1997). Split selection methods for classification trees. Statistica Sinica, 7(4)&#58;815–840.- Wikipedia&#58;&#58;Lasso. https&#58;//en.wikipedia.org/wiki/Lasso_(statistics)- Wikipedia&#58;&#58;Tikhonov regularization. https&#58;//en.wikipedia.org/wiki/Tikhonov_regularizationFreund, Y. and Schapire, R. E. (1996). Experiments with a new boosting algorithm. In Saitta, L., editor, Proceedings of the Thirteenth International Conference on Machine Learning (ICML 1996), pages 148–156. Morgan Kaufmann. Hastie, T., Tibshirani, R., and Friedman, J. (2009). The Elements of Statistical Learning&#58; Data Mining, Inference, and Prediction, Second Edition. Springer Series in Statistics. Springer. Lichman, M. (2013). UCI machine learning repository. Strobl, C., laure Boulesteix, A., and Augustin, T. (2006). Unbiased split selection for classification trees based on the gini index. Technical report. Wei-Yin Loh, Y.-S. S. (1997). Split selection methods for classification trees. Statistica Sinica, 7(4)&#58;815–840. Wikipedia&#58;&#58;Tikhonov regularization. https&#58;//en.wikipedia.org/wiki/Tikhonov_regularization 转载来源：为什么XGBoost在机器学习竞赛中表现如此卓越？]]></content>
      <categories>
        <category>科学</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>Kaggle</tag>
        <tag>麻省理工学院</tag>
        <tag>维基百科</tag>
        <tag>数学</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hinton的Capsule论文全公开！首发《胶囊间的动态路由》原文精译]]></title>
    <url>%2F2017%2F35ef43a6%2F</url>
    <content type="text"><![CDATA[雷锋网AI研习社按：日前，深度学习教父Geoffrey Hinton关于Capsule（胶囊）的新论文一发出，马上引起了热烈讨论。雷锋字幕组趁热对论文做了全文翻译，想了解具体细节的读者欢迎仔细阅读。有翻译不当的地方欢迎指出，更期待您可以加入我们（申请加入，联系微信 julylihuaijiang）。 胶囊间的动态路由 摘要 本论文所研究的胶囊意为一组神经元，其激活向量反映了某类特定实体（可能是整体也可能是部分）的表征。本论文使用激活向量的模长来描述实体存在的概率，用激活向量的方向表征对应实例的参数。某一层级的活跃胶囊通过矩阵变换做出预测，预测结果会用来给更高层级的胶囊提供实例参数。当多个预测值达成一致时，一个高层级的胶囊就会被激活。论文中展示了差异化训练的多层胶囊系统可以在MNIST上达到当前最高水平的表现，在识别高度重叠的数字上也要比卷积网络要好得多。网络的实现中运用迭代的一致性路由机制：当低层级的胶囊的预测向量和高层级胶囊的激活向量有较大的标量积时，这个低层级胶囊就会倾向于向高层级胶囊输出。 一、简介 人类视觉通过使用仔细确定的固定点序列来忽略不相关的细节，以确保只有极小部分的光学阵列以最高的分辨率被处理。要理解我们对场景的多少知识来自固定序列，以及我们从单个固定点中能收集到多少知识，内省不是一个好的指导，但是在本文中，我们假设单个固定点给我们提供的不仅仅是一个单一的识别对象及其属性。我们假设多层视觉系统在每个固定点上都会创建一个类似解析树这样的东西，并且单一固定解析树在多个固定点中如何协调的问题会被我们忽略掉。 解析树通常通过动态分配内存来快速构建，但根据Hinton等人的论文「Learning to parse images，2000」，我们假设，对于单个固定点，从固定的多层神经网络中构建出一个解析树，就像从一块岩石雕刻出一个雕塑一样（雷锋网 AI 科技评论注： 意为只保留了部分树枝）。每个层被分成许多神经元组，这些组被称为“胶囊”（Hinton等人「Transforming auto-encoders，2011」），解析树中的每个节点就对应着一个活动的胶囊。通过一个迭代路由过程，每个活动胶囊将在更高的层中选择一个胶囊作为其在树中的父结点。对于更高层次的视觉系统，这样的迭代过程就很有潜力解决一个物体的部分如何层层组合成整体的问题。 一个活动的胶囊内的神经元活动表示了图像中出现的特定实体的各种属性。这些属性可以包括许多不同类型的实例化参数，例如姿态（位置，大小，方向），变形，速度，反照率，色相，纹理等。一个非常特殊的属性是图像中某个类别的实例的存在。表示存在的一个简明的方法是使用一个单独的逻辑回归单元，它的输出数值大小就是实体存在的概率（雷锋网 AI 科技评论注： 输出范围在0到1之间，0就是没出现，1就是出现了）。在本文中，作者们探索了一个有趣的替代方法，用实例的参数向量的模长来表示实体存在的概率，同时要求网络用向量的方向表示实体的属性。为了确保胶囊的向量输出的模长不超过1，通过应用一个非线性的方式使矢量的方向保持不变，同时缩小其模长。 胶囊的输出是一个向量，这一设定使得用强大的动态路由机制来确保胶囊的输出被发送到上述层中的适当的父节点成为可能。最初，输出经过耦合总和为1的系数缩小后，路由到所有可能的父节点。对于每个可能的父结点，胶囊通过将其自身的输出乘以权重矩阵来计算“预测向量”。如果这一预测向量和一个可能的父节点的输出的标量积很大，则存在自上而下的反馈，其具有加大该父节点的耦合系数并减小其他父结点耦合系数的效果。这就加大了胶囊对那一个父节点的贡献，并进一步增加了胶囊预测向量和该父节点输出的标量积。这种类型的“按协议路由”应该比通过最大池化实现的非常原始的路由形式更有效，其中除了保留本地池中最活跃的特征检测器外，忽略了下一层中所有的特征检测器。作者们论证了，对于实现分割高度重叠对象所需的“解释”，动态路由机制是一个有效的方式。 卷积神经网络（CNN）使用学习得到的特征检测器的转移副本，这使得他们能够将图片中一个位置获得的有关好的权重值的知识，迁移到其他位置。这对图像解释的极大帮助已经得到证明。尽管作者们此次用矢量输出胶囊和按协议路由的最大池化替代CNN的标量输出特征检测器，他们仍然希望能够在整个空间中复制已习得的知识，所以文中构建的模型除了最后一层胶囊之外，其余的胶囊层都是卷积。与CNN一样，更高级别的胶囊得以覆盖较大的图像区域，但与最大池化不同，胶囊中不会丢弃该区域内实体精确位置的信息。对于低层级的胶囊，位置信息通过活跃的胶囊来进行“地点编码”。当来到越高的层级，越多的位置信息在胶囊输出向量的实值分量中被“速率编码”。这种从位置编码到速率编码的转变，加上高级别胶囊能够用更多自由度、表征更复杂实体的特性，表明更高层级的胶囊也相应地需要更高的维度。 二、如何计算一个胶囊的向量输入和输出 已经有很多方法可以实现胶囊的大致思路。这篇文章的目的，不是去探究所有可能的方法，而只是表明非常简单直接的方式就可以取得很好的效果，而且动态路由也可以起到帮助。 作者们用胶囊输出向量的模长来表示一个胶囊所表征的实体在输入中出现的概率。因此作者们采用一个非线性函数对向量进行“压缩”，短向量被压缩到几乎为零，长向量也被压缩到1以下长度。判别学习中充分利用这个非线性函数。 （式1） 其中vj是胶囊j的输出向量，sj是它的全部输入。 除了第一层胶囊，胶囊sj的全部输入是对预测向量uj|i的加权求和。这些预测向量都是由低一层的胶囊产生，通过胶囊的输出ui 和一个权重矩阵Wij相乘得来。 （式2） 其中cij是由迭代的动态路径过程决定的耦合系数。 胶囊i和其上一层中所有胶囊的耦合系数的和为1，并由“routing softmax”决定。这个“routing softmax”的初始逻辑值bij 是胶囊i耦合于胶囊j的对数先验概率。 （式3） 这个对数先验可以和其他权重一起被判别学习。他们由两个胶囊的位置和类型决定，而不是当前的输入图像决定。耦合系数会从初始值开始迭代，通过测量每个高一层胶囊j的当前输出vi和低一层胶囊i的预测值ui|j之间的一致性。 所述一致性是简单的点积aij=vj . ui|j。这个一致性可被看做最大似然值，并在计算出所有将胶囊i连接到更高层胶囊得到的新耦合值前，加到初始逻辑值bi,j上。 在卷积胶囊层中，胶囊内每一个单元都是一个卷积单元。因此每一个胶囊都会输出一个向量网格而不是一个简单的向量。 路由计算的伪码如下图 三、某类数字是否存在的边缘损失 作者们用实例化向量的模长来表示胶囊要表征的实体是否存在。所以当且仅当图片里出现属于类别k的数字时，作者们希望类别k的最高层胶囊的实例化向量模长很大。为了允许一张图里有多个数字，作者们对每一个表征数字k的胶囊分别给出单独的边缘损失函数(margin loss)&#58; （式4） 其中Tc=1当且仅当图片中有属于类别C的数字，m+=0.9，m-=0.1。是为了减小某类的数字没有出现时的损失，防止刚开始学习就把所有数字胶囊的激活向量模长都压缩了。作者们推荐选用 λ = 0.5。总损失就是简单地把每个数字胶囊的损失加起来的总和。 四、CapsNet 结构 图1：一个简单的3层CapsNet。这个模型的结果能和深层卷积网络（比如. Batch-normalized maxout network in network，2015）的结果媲美。DigitCaps层每个胶囊的激活向量模长给出了每个类的实例是否存在，并且用来计算分类损失。 是PrimaryCapsules中连接每个 ui, i ∈ (1, 32 × 6 × 6) 和每个vj , j ∈ (1, 10)的权重矩阵。 图2：从DigitCaps层来重构数字的解码结构。训练过程中极小化图像和Sigmoid层的输出之间的欧氏距离。训练中作者们用真实的标签作为重构的目标。 图1展示的是一个简单的CapsNet结构。 这是一个很浅的网络，只有2个卷积层和1个全连接层。Conv1有256个9*9的卷积核，步长取1，激活函数为ReLU。这层把像素亮度转化成局部特征检测器的激活，接下去这个值会被用来作为原始胶囊(primary capsules)的输入。 原始胶囊是多维实体的最底层。这个过程和图形生成的视角相反，激活了一个原始胶囊就和刚好是图形渲染的逆过程。与先分别计算实例的不同部分再拼在一起形成熟悉的总体理解（图像中的每个区域都会首先激活整个网络而后再进行组合）不同，这是一种非常不同的计算方式。而胶囊的设计就很适合这样的计算。 第二层PrimaryCapsules是一个卷积胶囊层，有32个通道，每个通道有一个8维卷积胶囊（也就是说原始胶囊有8个卷积单元，99的卷积核，步长为2）。这一层中的胶囊能看到感受野和这个胶囊的中心重合的所有25681 Conv1单元的输出。PrimaryCapsules一共有&#91;32,6,6&#93;个输出（每个输出是一个8维向量），&#91;6,6&#93;网格中的每个胶囊彼此共享权重。由于具有区块非线性，可以把PrimaryCapsules视作一个符合式1的卷积层。最后一层（DigitCaps）有对每个数字类有一个16维的胶囊，所有低一层的胶囊都可以是这一层胶囊的输入。 作者们只在两个连续的胶囊层（比如PrimaryCapsules和DigitCaps）之间做路由。因为Conv1的输出是1维的，它所在的空间中不存在方向可以和高层的向量方向达成一致性。所以在Conv1和PrimaryCapsules之间没有路由。所有的路由逻辑值(bij)被初始化为0。因此，一开始一个胶囊的输出（ui）会以相同的概率（cij）传入到所有的母胶囊（v0，v1，…，v10）。作者们用TensorFlow实现了这个网络，选择了Adam优化器和TensorFlow的默认参数，包括指数衰减的学习率用来优化式4的边缘损失的总和。 4.1 为了正则化效果而做的重构工作 作者们使用了一个额外的重构损失，希望数字胶囊能对输入数字的实例化参数做编码。在训练过程中，作者们用掩蔽的方法只把正确的数字胶囊的激活向量保留下来。然后用这个激活向量来做重构。数字胶囊的输出会传入一个由3个全连接层组成的解码器，它的结构如图2，用来建模像素密度。 作者们极小化回归单元的输出和原来图片的像素亮度之间的平方误差，并把重构误差收缩到原来的0.0005倍，这样才不会在训练过程中盖过边缘误差的作用。如图3所示，CapsNet的16维输出的重构是鲁棒的，同时也只保留了重要的细节。 五、把 Capsule 用在MNIST上 使用 28×28 MNIST的图片集进行训练，训练前这些图片在每个方向不留白地平移了2个像素。除此之外，没有进行其他的数据增改或者转换。在MNIST数据库中，6万张图片用于训练，另外1万张用于测试。 图3： 利用3次路由迭代学习的CapsNet对MNIST中的测试照片进行重构。(l, p, r)分别代表真实标签、模型预测和重建结果。最右两列展示的是重建失败的例子，解释了模型是如何混淆了图片中的“5”和“3”。其他列属于被正确分类了的，展示了模型可以识别图像中的细节，同时降低噪声。 表1：CapsNet 分类MNIST数字测试准确度。结果包含了三次测试得到的平均数和标准差。 测试中作者使用的是单一模型，没有进行“综合”或者明显的数据扩增方法。（Wan等人在「Regularization of neural networks using dropconnect」中通过“综合”及数据扩增实现了0.21%的错误率，而未使用这两种方法时的错误率是0.57%）作者们通过3层神经网络实现了较低的错误率（0.25%），这一错误率以往只有更深的网络才能达到。表1展现的是不同设置的CasNet在NMIST数据库上的测试错误率，表明了路由以及正则器重构的重要性。其基线是一个标准的三层神经网络（CNN），分别具有256、256及128个通道。每个通道具有5×5的卷积核，卷积步长为1。接着有两个全连接层，大小分别为328、192。最后的全连接层通过dropout连接到带有交叉熵损失的10个分类输出的softmax层。 5.1 capsule的单个维度表示什么 由于模型中只向DigitCaps层的胶囊传递一个数字的编码并置零其他数字，所以这些胶囊应该学会了在这个类别已经具有一个实例的基础上拓展了变化空间。这些变化包括笔画粗细、倾斜和宽度。还包括不同数字中特定的变化，如数字2尾部的长度。通过使用解码器网络可以看到单个维度表示什么。在计算正确的数字胶囊的激活向量之后，可以将这个激活向量的扰动反馈给解码器网络，并观察扰动如何影响重建。这些扰动的例子如图4所示。可以看到，胶囊的一个维度（总数为16）几乎总是代表数字的宽度。有些维度表示了全局变化的组合，而有些维度表示数字的局部变化。例如，字母6上部分的长度和下部分圈的大小使用了不同的维度。 图4：维度扰动。每一行表示DigitCaps16个维度表示中的一个维度在&#91;-0.25, 0.25&#93;范围，步长0.05时的重构结果 5.2 仿射变换的鲁棒性 实验表明，每个DigitCaps层的胶囊都比传统卷积网络学到了每个类的更鲁棒的表示。由于手写数字的倾斜、旋转、风格等方面存在自然差异，训练好的CapsNet对训练数据小范围的仿射变换具有一定的鲁棒性。 为了测试CapsNet对仿真变换的鲁棒性，作者们首先基于MNIST训练集创造了一个新的训练集，其中每个样本都是随机放在40× 40像素的黑色背景上的MNIST数字。然后用这样的训练集训练了一个CapsNet和一个传统的卷积网络（包含MaxPooling和DropOut）。 然后，作者们在affNIST数据集上测试了这个网络，其中，每个样本都是一个具有随机小范围仿射变换的MNIST数字。模型并没有在任何放射变换，甚至标准MNIST自然变换的训练集合上训练过，但一个训练好的带有早期停止机制（early stop）的CapsNet，在拓展的MNIST测试集上实现了99.23％的准确度，在仿射测试集上实现了79％的准确性。具有类似参数数量的传统卷积模型在扩展的MNIST测试集上实现了类似的准确度（99.22％），在仿射测试集上却只达到了66％。 六、高度重叠数字的分割 动态路由可以视为平行的注意力机制，允许同层级的胶囊参与处理低层级的活动胶囊，并忽略其他胶囊。理论上允许模型识别图像中的多个对象，即使对象重叠。Hinton等人的目的是分割并识别高度重合数字对象（「 Learning to parse images，2000」中提出，其它人也在类似的领域实验过他们的网络，Goodfellow等人在「Multi-digit number recognition from street view imagery using deep convolutional neural networks，2013」中，Ba等人在「Multiple object recognition with visual attention，2014」中，Greff等人在「Tagger&#58; Deep unsupervised perceptual grouping，2016」中）。一致性路由使利用对象的形状的先验知识帮助进行分割成为了可能，并避免在像素领域进行更高级别的细分。 6.1 MultiMNIST数据集 作者们通过在数字上覆盖另一个来自相同集合（训练或测试）但不同类别的数字来生成MultiMNIST训练测试数据集。每个数字在每个方向上最多移动4个像素，产生3636像素的图像。考虑到2828像素图像中的数字是以20*20像素的范围作为边框，两个数字的边框内范围平均有80%的重合部分。MNIST数据集中的每个数字都会生成1K MultiMNIST示例。训练集的大小为60M，测试集的大小为10M。 6.2 MultiMNIST数据集上的结果 作者用MultiMNIST的训练数据中重新训练得到的3层CapsNet模型，比基线卷积模型获得了更高的分类测试准确率。相较于Ba等人在「Multiple object recognition with visual attention，2014」的序列注意力模型，他们执行的是更简单的、数字交叠远远更小的任务（本文的测试数据中，两个数字的外框交叠率达到80%，而Ba等人的只有4%），而本文的模型在高度交叠的数字对中获得了与他们同样的5%的错误率。测试图片由测试集中的成对的图片构成。作者们把两个最活跃的数字胶囊看作胶囊网络产生的分类结果。在重建过程中，作者们每次选择一个数字，用它对应的数字胶囊的激活向量来重建这个数字的图像（已经知道这个图像是什么，因为作者们预先用它来生成合成的图像）。与上文MNIST测试中模型的唯一不同在于，现在把将学习率的衰减步数提高到了原来的10倍，这是因为训练数据集更大。 图5：一个经3次路由迭代的CapsNet在MultiMNIST测试数据集上的样本重建结果 如图中靠下的图像所示，两个重建出的互相交叠的数字分别显示为绿色和红色的。靠上的图显示的是输入的图像。表示图像中两个数字的标签；表示用于重建的两个数字。最右边的两列显示了从标签和从预测重建的两个错误分类样例。在例子中，模型将8错判成7；在的例子中，模型将9错判成0。其他的列都分类正确并且显示了模型不仅仅考虑了所有的像素同时能够在非常困难的场景下将一个像素分配给两个数字（1-4列）。值得说明的是，在数据集产生的过程中，像素的值都会被剪裁到1以内。两个含“*”的列显示了重建的数字既不是标签值也不是预测值。这些列显示模型不仅仅找到了所有存在的数字的最佳匹配，甚至还考虑了图像中不存在的数字。所以在的例子中，模型并不能重建数字7，是因为模型知道数字对5和0是最佳匹配，而且也已经用到了所有的像素。的例子也是类似的，数字8的环并没有触发为0的判断，因为该数字已经被当做8了。因此，如果两个数字都没有其他额外的支持的话，模型并不会将一个像素分配给这两个数字。 图5中的重构表明，CapsNet 能够把图片分割成两个原来的数字。因为这一分割并非是直接的像素分割，所以可以观察到，模型可以准确处理重叠的部分(即一个像素同时出现在多个数字上)，同时也利用到所有像素。每个数字的位置和风格在DigitCaps中都得到了编码。给定一个被编码数字，解码器也学会了去重构这一数字。解码器能够无视重叠进行重构的特性表明，每个数字胶囊都能从PrimaryCapsules层接收到的不同激活向量来获取位置和风格。 表1 也着重表现了这一任务中胶囊之间路由的重要性。作为CapsNet分类器准确率的对比基线，作者们一开始先训练了带有两层卷积层和两层全连接层的卷积神经网络。 第一层有512个大小为99的卷积核，步长为1；第二层有256个大小为55的卷积核，步长为1。在每个卷积层后，模型都连接了一个2*2大小，步长2的池化层。 第三层是一个1024维的全连接层。 所有的这三层都有ReLU非线性处理。 最后10个单元的层也是全连接。 我们用TF默认的Adam优化器来训练最后输出层的Sigmoid交叉熵损失。 这一模型有24.56M参数，是CapsNet的11.36M参数的两倍多。作者们从一个小点的CNN(32和64个大小为5*5的卷积核，步幅为1，以及一个512维的全连接层)开始，然后逐渐增大网络的宽度，直到他们在MultiMNIST的10K子集上达到最好的测试精度。他们也在10K的验证集上搜索了正确的学习率衰减步数。 作者们一次解码了两个最活跃的DigitCaps胶囊，得到了两张图片。然后把所有非零的像素分配给不同的数字，就得到了每个数字的分割结果。 七、其它数据集 作者们在 CIFAR10 的数据及上测试了胶囊模型，在用了不同的超参和7个模型集成（其中每个模型都通过图像中24x24的小块进行三次路由迭代）后得到10.6%的错误率。这里的图片都是三个颜色通道的，作者们一共用了64种不同的 primary capsule，除此之外每个模型都和在 MNIST 数据集中用的一模一样。作者们还发现胶囊能够帮助路由softmax增加一个“以上皆非”的分类种类，因为不能指望10个 capsules 的最后一层就能够解释图片里的一切信息。在测试集上有 10.6% 的错误率差不多也是标准的卷积网络初次应用到 CIFAR10 上能达到的效果。 和生成模型一个一样的缺点是，Capsules 倾向于解释图片中的一切。所以当能够对杂乱的背景建模时，它比在动态路由中只用一个额外的类别来的效果好。在 CIFAR-10 中，背景对大小固定的模型来说变化太大，因此模型表现也不好。 作者们还用了和 MNIST 中一样的模型测试了 smallNORB 数据集，可以得到目前最好的 的 2.7% 的错误率。smallNORB 数据集由 96×96的双通道灰度图组成。作者们把图片缩放到 48×48 像素，并且在训练时从中随机裁剪 32×32 的大小。而在测试时，直接取中间 32×32 的部分。 作者们还在 SVHN 的 73257 张图片的小训练集上训练了一个小型网络。我们把第一个卷积层的通道数减少到 64个，primary capsule 层为 16 个 6维胶囊，最后一个胶囊层为8维的。最后测试集错误率为 4.3%. 八、讨论以及以往工作 30年来， 语音识别的最新进展使用了以高斯混合作为输出分布的隐马尔可夫模型。这些模型虽然易于在一些计算机上学习，但是存在一个致命的缺陷：他们使用的“n种中的某一种”的表示方法的效率是呈指数下降的，分布式递归神经网络的效率就比这种方法高得多。为了使隐马尔可夫模型能够记住的迄今它所生成字符的信息倍增，需要使用的隐藏节点数目需要增加到原来的平方。而对于循环神经网络来说，只需要两倍的隐藏神经元的数量即可。 现在卷积神经网络已经成为物体识别的主流方法，理所当然要问是其中是否也会有效率的指数下降，从而引发这种方法的式微。一个可能性是卷积网络在新类别上泛化能力的困难度。卷积网络中处理平移变换的能力是内置的，但对于仿射变换的其他维度就必须进行选择，要么在网格中复制特征检测器，网格的大小随着维度数目指数增长，要么同样以指数方式增加的标注训练集的大小。胶囊通过将像素强度转换为识别到的片段中的实例化参数向量，然后将变换矩阵应用于片段，以预测更大的片段的实例化参数，从而避免了效率的指数下降。学到了部分和整体之间固有的空间关系的转换矩阵构成了具有视角不变性的知识，从而可以自动泛化到的视角中。 胶囊使得我们可以做出一个非常具有表征意义的假设：在图像的每一个位置，至多只有一个胶囊所表征的实体的实例。这种假设是由一种称为“crowding”(Pelli等人「Crowding is unlike ordinary masking&#58; Distinguishing feature integration from detection，2004」) 的感知现象驱动的，它消除了绑定问题，并允许一个胶囊使用分布式表示(它的激活向量)来对给定位置的该类型实体的实例化参数进行编码。这种分布式表示比通过在高维网格上激活一个点来编码实例化参数的效率要高得多，并且通过正确的分布式表示，胶囊可以充分利用空间关系可以由矩阵乘法来建模的特点。 胶囊中采用的神经活动会随着视角的变化而变化，而不是试图消除神经活动中视角变化带来的影响。这使它们比“归一化”法(如Jaderberg等「Spatial transformer networks，2015」)更具有优势&#58;它们可以同时处理多个不同仿射变换或不同对象的不同部件。 胶囊同时也非常擅长处理图像分割这样的另一种视觉上最困难的问题之一，因为实例化参数的矢量允许它们使用在本文中演示的那样的一致性路由。对胶囊的研究目前正处于一个与本世纪初研究用于语音识别的递归神经网络类似的阶段。根据基础表征性的特点，已经有理由相信这是一种更好的方法，但它可能需要一些更多的在细节上的洞察力才能把它变成一种可以投入应用的高度发达的技术。一个简单的胶囊系统已经在分割数字图像上提供了无与伦比的表现，这表明了胶囊是一个值得探索的方向。 （完） 关注AI研习社，回复【论文】即可获取论文原文及翻译。 欢迎各界朋友加入字幕组，让雷锋字幕组翻译水平更上一层楼。组长微信：julylihuaijiang。 雷锋字幕组翻译 / 熊浪涛、小颖同学、sophie、Clay、李振、孟庆淳、Jackie、小耗子在南京、张小彬、Moonsea、陈智敏 审校 / 晓凡 统筹 / 囧囧、凡江 转载来源：Hinton的Capsule论文全公开！首发《胶囊间的动态路由》原文精译]]></content>
      <categories>
        <category>科学</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>机器学习</tag>
        <tag>人工智能</tag>
        <tag>CNN</tag>
        <tag>科学</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【重磅】Hinton大神Capsule论文首次公布，深度学习基石CNN或被取代]]></title>
    <url>%2F2017%2F2f547fb7%2F</url>
    <content type="text"><![CDATA[【重磅】Hinton大神Capsule论文首次公布，深度学习基石CNN或被取代 转载来源：【重磅】Hinton大神Capsule论文首次公布，深度学习基石CNN或被取代]]></content>
      <tags>
        <tag>新智元</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[吴恩达Deeplearning.ai 全部课程学习心得分享]]></title>
    <url>%2F2017%2F35812f7d%2F</url>
    <content type="text"><![CDATA[选自Medium 作者：Ryan Shrott 机器之心编辑部 本文作者，加拿大国家银行首席分析师 Ryan Shrott 完成了迄今为止（2017 年 10 月 25 日）吴恩达在 Coursera 上发布的所有深度学习课程，并为我们提供了课程解读。 目前 Coursera 上可用的课程中，有三门课非常值得关注： 神经网络与深度学习（Neural Networks and Deep Learning） 改进深度神经网络：调整超参数、正则化与优化（Improving Deep Neural Networks&#58; Hyperparamater tuning, Regularization and Optimization） 结构化机器学习项目（Structuring Machine Learning Projects） 我发现这三门课非常重要，在其中，我们可以从吴恩达教授那里获得很多有用的知识。吴恩达在教学语言上做得很好，解释概念清晰明了。例如，吴恩达明确指出监督学习并没有超出多维曲线拟合过程的范畴，而对于这种方法的其他理解方式，如对人类神经系统的模拟，实际上并不严谨。 学习这些课程的基础知识要求不多，只要求你事先掌握一些线性代数，以及 Python 基础编程知识。在我看来，你也需要了解向量计算来理解优化过程的内在知识。但如果你不关心内部运作方式，并只希望了解高级层面上的内容，尽管跳过微积分的部分。 第 1 课：为什么深度学习如此热门？ 现在人类产生的 90% 数据是在最近 2 年内被收集的。深度神经网络（DNN）能够利用体量巨大的数据。因此，DNN 超越了较小的网络和传统学习算法。 规模是如何推动 DNN 性能的 此外，算法上的创新也使得 DNN 的训练速度变得更快。例如，从 Sigmoid 激活函数改为 RELU 激活函数对梯度下降等任务的优化过程产生了巨大影响。这些算法的改进使得研究人员可以更快地遍历灵感→ 代码→ 经验的开发循环，从而带来更多的创新。 深度学习开发循环 第 2 课：深度学习中的向量化 在开始这门课之前，我并不知道神经网络可以在没有任何明确循环语句的情况下被实现（除了层之间的）。吴恩达点明了 Python 中向量化编程设计的重要性。课程附带的作业引导你进行向量化的编程，同时这些方法也可以很快迁移到你自己的项目中。 第 3 课：深入了解 DNN 前几门课实际上在引导你使用 NumPy 从头开始实现前向和反向传播。通过这种方法，我更加深入了解了高级深度学习框架（如 TensorFlow 和 Keras）的工作原理。吴恩达解释了计算图背后的想法，从而让我们了解了 TensorFlow 如何实现「神奇优化」的。 第 4 课：为什么需要深度？ 在这一节中，吴恩达深入解释了 DNN 的层概念。例如，对于面部识别系统，他向我们解释了先处理的层是用于处理面部边界的，其后的层用于将这些边界识别为面部组件（如鼻子、眼睛、嘴等），再其后的层会将这些组件整合到一起识别人的身份。他还解释了电路理论（circuit theory）的思想——存在一个函数，需要来自隐藏单元指数式的数字来适应浅网络的数据。可以通过添加有限数量的附加层来简化指数问题。 第 5 课：处理偏差和方差的工具 吴恩达解释了研究者识别和处理偏差方差相关问题的步骤。下图诠释了一种解决这些问题的系统性方法。 解决偏差和方差问题的方法 他还解决了偏差和方差之间的「权衡」（tradeoff）。他认为在现在这个深度学习的时代，我们拥有独立解决每个问题的工具，使权衡不再存在。 第 6 课：正则化 为什么向成本函数添加惩罚项会降低方差？在上这门课之前我的理解是它使权重矩阵接近于零，从而产生一个更「线性」的函数。吴恩达给出了另外一种和 tanh 激活函数相关的解释，即较小的权重矩阵生成较小的输出，使得输出围绕在 tanh 函数线性区域的中心。 tanh 激活函数 他还给出了 dropout 的有趣解释。之前我认为 dropout 在每次迭代中消灭随机神经元，就像越小的网络线性程度就越强一样。但是吴恩达的解释是从单个神经元的角度来看待生命（life）。 单个神经元的角度 由于 dropout 随机消灭连接，这促使神经元向父系神经元中更加均匀地扩展权重。通过扩展权重，它可以减少权重的 L2 范数（squared norm）。他还解释了 dropout 是 L2 正则化的自适应形式，两种方法效果相近。 第 7 课：归一化为何有效？ 吴恩达展示了为什么归一化可以通过绘制等高线图的方式加速优化步骤。他详细讲解了在归一化和非归一化等高线图上进行梯度下降所需要的迭代次数变化，即相同优化算法没经过归一化操作会需要更多的迭代数。 第 8 课：初始化的重要性 吴恩达表示不使用参数初始化可能导致梯度消失或爆炸。他展示了多个步骤来解决这些问题。基本原则是确保每一层的权重矩阵的方差都近似为 1。他还讨论了 tanh 激活函数的 Xavier 初始化。 第 9 课：为什么使用小批量梯度下降？ 吴恩达使用等高线图解释了使用小批量和大批量训练之间的权衡。基本原则是较大的批量每次迭代会变慢，较小的批量可以加快迭代过程，但是无法保证同样的收敛效果。最佳方法就是在二者之间进行权衡，使得训练过程比一次性处理整个数据集要快，又能利用向量化技术的优势。 第 10 课：高级优化技术的直观理解 吴恩达解释了合适使用动量（momentum）和 RMSprop 等技术限制梯度下降逼近极小值的路径。他还用球滚下山的例子生动地解释了这一过程。他把这些方法结合起来来解释著名的 Adam 优化。 第 11 课：基本的 TensorFlow 后端理解 吴恩达不仅解释了如何使用 TensorFlow 实现神经网络，同时还讲解了在优化过程中出现的后端进程。有一个家庭作业就是鼓励我们使用 TensorFlow 实现 dropout 和 L2 正则化，这加强了我对后端过程的理解。 第 12 课：正交化 吴恩达还讨论了机器学习策略中正则化的重要性。其基本思想是，我们希望实现并控制影响算法性能的因素，即一次只控制一个影响性能的因素。例如为了解决偏差问题，我们可以使用更大的网络或更鲁棒的优化技术，我们希望这些控制只影响偏差而不会影响其它如泛化等问题。缺少正交化控制的案例是过早停止了算法的最优化过程，因为这样会同时影响模型的偏差和方差。 第 13 课：单数值评估度量的重要性 吴恩达强调了选择单数值评估度量（single number evaluation metric）的重要性，它可以允许我们评估算法。如果目标改变，我们应该在模型开发过程中仅更改评估度量。吴恩达给我们讲解了一个使用猫分类应用识别色情图片的案例。 第 14 课：测试和开发集的分布 通常我们会假设测试集与开发集（dev sets）的分布相同，这就确保了我们在迭代过程中朝向正确的目标优化。这同样意味着如果你决定纠正测试集中错误的标注数据，那么你需要在开发集中纠正错误标注的数据。 第 15 课：处理不同的训练和测试/开发分布 吴恩达介绍了为什么我们对训练和测试/开发集没有相同的分布这一问题感兴趣。因为我们希望根据实际关心的样本来计算评估度量。例如我们可能希望使用和训练问题无关的的样本进行训练，但我们并不希望算法使用这些样本进行评估，这就令我们的算法可以在更多的数据上进行训练。经验上，这种方法可以在许多案例上产生非常好的效果。缺点是可能我们的训练和测试/开发集有不同的分布，这个问题的通常解决办法是，可以留出一小部分训练集，并确定训练集的泛化性能。然后我们可以比较这些误差率与实际的开发误差，并计算一个「数据误匹配」的度量标准。吴恩达还解释了解决这些数据误匹配问题的方法，例如人工数据合成。 第 16 课：训练集/开发集/测试集大小 在深度学习时代，训练集/开发集/测试集的分隔方法也发生了巨大的改变。之前，我只知道较普遍的 60/20/20 分隔。吴恩达强调，对于一个非常大的数据集，应该使用 98/1/1 甚至 99/0.5/0.5 的分隔。这是因为开发集合测试集只要足够大能保证模型处于团队设定的置信区间即可。如果你使用 1 千万个训练样本，那么 10 万样本（即数据集的 1%）就足够保证开发集和/或测试集的置信区间了。 第 17 课：近似贝叶斯最优误差 吴恩达解释了在某些应用中人类级别的性能如何作为贝叶斯误差的替代。例如，在视觉和听觉识别任务中，人类级别的误差通常很接近贝叶斯误差，可以用于量化模型中可避免的偏差。如果没有诸如贝叶斯误差这样的基准测试，理解网络中的方差和可避免的偏差问题是很困难的。 第 18 课：误差分析 吴恩达介绍了一种能显著提高算法性能的有效性的误差分析技术。基本想法是手工标注错误分类的样本，集中精力处理对错误分类数据影响最大的误差。 猫识别 App 误差分析 例如，在猫识别中吴恩达认为模糊的图像最容易导致误差。这种敏感性分析可以令人看到在降低总体误差的过程中，你花费的精力到底有多值得。还有一种可能是，修复模糊图像是很费力的任务，而其它的误差更容易理解和修复。敏感性分析和近似操作都将作为决策过程的因素。 第 19 课：什么时候使用迁移学习？ 迁移学习允许将一个模型的知识迁移到另一个。例如，你可以将一个猫识别 app 中的图像识别知识迁移到放射诊断中去。实现迁移学习需要用更多的数据重训练网络的最后几个层，以用于相似的应用领域。其思想基础是网络的低层的隐藏单元拥有更加广阔的应用范围，即对具体的任务类型不敏感。总之，当任务之间拥有相同的输入特征，并且需要学习的任务拥有比需要训练的任务多得多的数据的时候，迁移学习是可行的。 第 20 课：什么时候使用多任务学习？ 多任务学习迫使单个神经网络同时学习多个任务（和每一个任务都配置单独的神经网络相反）。吴恩达解释道，当任务集合通过共享低级特征获得学习增益，以及每一个任务的数据量规模相似的时候，这种方法能工作得很好。 第 21 课：什么时候用端到端的深度学习？ 端到端的深度学习需要多层处理并将它们组合到单个神经网络中，这使得数据能在没有人工设计步骤引进偏差的前提下自主进行优化过程。另一方面，这个方法需要非常多的数据，有可能排除潜在的手工设计成分。 结论 吴恩达的深度学习课程使我对深度学习模型的开发过程有了基本的直观理解，以上我解释过的课程只不过是这个课程中展示资料的一部分。即使完成了课程你也还不能称为深度学习专家，而我唯一的不满是课程的作业布置太简单了。顺便提一句，写这篇文章并没有得到 deeplearning.ai 的批准。 转载来源：吴恩达Deeplearning.ai 全部课程学习心得分享]]></content>
      <categories>
        <category>科技</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>机器学习</tag>
        <tag>吴恩达</tag>
        <tag>在线教育</tag>
        <tag>加拿大</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[学界 | 词嵌入2017年进展全面梳理：趋势和未来方向]]></title>
    <url>%2F2017%2F33fb6aa3%2F</url>
    <content type="text"><![CDATA[学界 | 词嵌入2017年进展全面梳理：趋势和未来方向 转载来源：学界 | 词嵌入2017年进展全面梳理：趋势和未来方向]]></content>
      <tags>
        <tag>机器之心</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[月入五千的国贸人，教你如何活的像年薪百万]]></title>
    <url>%2F2017%2F6f1463fd%2F</url>
    <content type="text"><![CDATA[月入五千的国贸人，教你如何活的像年薪百万 转载来源：月入五千的国贸人，教你如何活的像年薪百万]]></content>
      <tags>
        <tag>脉脉编辑部</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[「TensorFlow 谷歌神经机器翻译」从零开始打造属于你的翻译系统]]></title>
    <url>%2F2017%2F1bee3236%2F</url>
    <content type="text"><![CDATA[1 新智元编译 机器翻译——自动在两种语言之间进行翻译的任务——是机器学习中最活跃的研究领域之一。在多种机器翻译方法中，序列到序列（“seq2seq”）模型最近取得了巨大的成功，并已经成为大多数商业翻译系统的事实上的标准，例如谷歌翻译。这是由于 seq2seq 模型能够利用深度神经网络捕捉句子意义。但是，虽然 seq2seq 模型（例如 OpenNMT 或 tf-seq2seq）有大量的资料，但是缺少可以同时教知识和构建高质量翻译系统的技能的教程。 谷歌今天公布了一个用 TensorFlow 构建神经机器翻译（NMT）系统的教程，全面解释 seq2seq 模型，并演示如何从零开始构建 NMT 翻译模型。这个教程从 NMT 的背景知识讲起，并提供构建一个 NMT 系统的代码细节。接着，教程讲解注意力机制（attention mechanism），这是让 NMT 能够处理长句子的关键。最后，教程提供如何复制谷歌的 NMT 系统（GNMT）中的关键功能，在多个 GPU 上进行训练的详细信息。 这一教程还包括详细的基准测试结果，使用者可以自行复制。谷歌的模型提供了强大的开源基准，性能与 GNMT 的结果相当，在流行的 WMT’14 英语 - 德语翻译任务上实现了 BLEU 得分 24.4 的性能。 教程还包括其他基准测试结果（英语 - 越南语，德语 - 英语）。 此外，这个教程还提供了完全动态的 seq2seq API（与 TensorFlow 1.2 一起发布），旨在使构建 seq2seq 模型更加简洁： 使用tf.contrib.data中新的输入管道轻松读取和预处理动态大小的输入序列。- 使用padded batching和sequence length bucketing来提高训练和推理速度。- 使用流行的架构和训练schedule训练seq2seq模型，包括几种类型的attention和scheduled sampling。- 使用in-graph beam search在seq2seq模型中执行推理。- 为多GPU设置优化seq2seq模型。使用padded batching和sequence length bucketing来提高训练和推理速度。 使用in-graph beam search在seq2seq模型中执行推理。 希望这一教程有助于研究界创造更多新的NMT模型并进行实验。完整教程的GitHub地址：https&#58;//github.com/tensorflow/nmt，本文提供主要内容的翻译介绍。 神经机器翻译（seq2seq）教程 作者：Thang Luong, Eugene Brevdo, Rui Zhao 目录 导言- 基础神经机器翻译背景知识基础 安装教程 训练——如何构建你的第一个NMT系统 嵌入 编码器 解码器 损失 梯度计算和优化 实践——让我们开始训练一个NMT模型 推理——如何生成翻译 中级教程注意力机制的背景知识 Attention Wrapper API 实践——构建一个以注意力为基础的NMT模型 提示与技巧构建训练，评估和推理图 数据输入管道 更好的NMT模型的其他细节 双向RNN 束搜索（Beam Search） 超参数 多GPU训练 基准IWSLT英语 - 越南语 WMT德语 - 英语 WMT英语 - 德语（完全比较） 其他资源- 致谢- 参考文献致谢 导言 序列到序列（seq2seq）模型（Sutskever et al.，2014，Cho et al.，2014）在机器翻译、语音识别、文本概况等各种任务中取得了巨大的成功。本教程提供了对 seq2seq 模型的全面解释，并演示了如何从头开始构建一个具有竞争力的 seq2seq 模型。我们专注于神经机器翻译（NMT）任务，这是第一个大获成功的 seq2seq 模型的测试平台。教程中包含的代码是轻便，高质量，生产就绪，并结合了最新的研究观点的。我们通过以下方式实现这一目标： 使用最新的解码器/注意力包装 API，TensorFlow 1.2 数据迭代器- 结合我们在构建循环模型和 seq2seq 模型方面的专长- 提供构建最好的 NMT 模型以及复制谷歌的 NMT（GNMT）系统的提示和技巧。结合我们在构建循环模型和 seq2seq 模型方面的专长 我们认为，最重要的是提供可以让人轻松复制的基准。因此，我们提供了完整的实验结果，并在以下公开数据集对模型进行了预训练： 小规模：IWSLT Evaluation Campaign 提供的 TED 演讲（133K句子对）的英语 - 越南语平行语料库。- 大规模：WMT Evaluation Campaign 提供的德语 - 英语平行语料库（4.5M句子对）。大规模：WMT Evaluation Campaign 提供的德语 - 英语平行语料库（4.5M句子对）。 我们首先提供构建 NMT 的 seq2seq 模型的一些基本知识，说明如何构建和训练一个 NMT 模型。第二部分将详细介绍构建一个有竞争力的 NMT 模式的注意力机制。最后，我们将提供一些提示和技巧，以构建最佳性能的 NMT 模型（包括训练速度和翻译质量），例如 TensorFlow 的最佳实践（batching, bucketing），bidirectional RNN 和 beam search。 基础 神经机器翻译的背景知识 回到过去，传统的基于短语的翻译系统是通过将源语言的句子分解成多个部分，然后逐个短语地进行翻译。这导致机器翻译的结果与人类翻译的结果很不同。人类是通读整个源句子，理解它的含义，然后进行翻译。神经机器翻译（NMT）模拟了这样的过程！ 图1：编码器-解码器架构，NMT的一个通用方法的示例。编码器将源句子转换成一个“meaning”向量，这个向量通过解码器传递，产生翻译结果。 具体来说，NMT 系统首先使用编码器读取源语句来构建“meaning”向量，即表示句子意义的一个数字序列; 然后，解码器处理句子向量以输出翻译结果，如图1所示。这一架构同城被称为编码器-解码器架构（encoder-decoder architecture）。以这种方式，NMT 解决了传统的基于短语的方法中翻译局部性的问题：它可以捕获语言的远距离依赖性，例如性一致， 句法结构，等等，并产生更流畅的翻译，如谷歌的神经机器翻译系统所演示的。 NMT 模型的具体结构有所不同。序列数据的一般选择是大多数NMT模型使用的循环神经网络（RNN）。通常，RNN用于编码器和解码器。但是，RNN模型在以下方面不同：（a）方向性——单向或双向; （b）深度——单层或多层; 和（c）类型——通常是普通RNN，长短期记忆（LSTM）或循环门单位（Gated Recurrent Unit, GRU）。有兴趣的读者可以在这篇博客文章了解有关RNN和LSTM的更多信息：http&#58;//colah.github.io/posts/2015-08-Understanding-LSTMs/ 在本教程中，我们将一个单向的深度多层RNN作为示例，并将LSTM作为一个循环单元。图2是这样一个模型的例子。在这个示例中，我们构建一个模型来将源句子“I am a student”翻译成一个目标句子“Je suisétudiant”。在高层水平上，NMT模型由两个循环神经网络组成：编码器RNN简单地处理输入的源词汇，不进行任何预测; 另一方面，解码器RNN在预测下一个单词的同时处理目标句子。 更多信息请参阅Luong（2016）的教程（https&#58;//github.com/lmthang/thesis），本教程正是基于这个教程的扩充。 图2：神经机器翻译——将源句子“I am a student”翻译成目标句子“Je suisétudiant”，这是一个深度循环架构的例子。这里，“”表示解码处理的开始，“&lt;/ s&gt;”提示解码器停止。 安装教程 要安装本教程，你需要在系统上安装TensorFlow。本教程要求最新版本的TensorFlow（version 1.2.1）。要安装TensorFlow，请按照官方的安装说明进行操作（https&#58;//www.tensorflow.org/install）。 安装好TensorFlow之后，您可以通过运行下面的代码下载本教程的源代码： git clone https&#58;//github.com/tensorflow/nmt/ 训练——如何构建你的第一个NMT系统 我们先用一些具体的代码片段看看构建一个NMT模型的核心，详细解释一下图2。我们后面会提供数据准备和完整代码。这部分涉及model.py文件。 在底层，编码器RNN和解码器RNN作为输入接收以下内容：首先是源句子（source sentence），然后是一个边界标记“”，提示从编码模式到解码模式的切换，最后是目标句子（target sentence）。对于训练过程，我们将为系统提供以下张量，它们是time-major的格式，包含单词索引： encoder_inputs &#91;max_encoder_time, batch_size&#93;&#58; 源输入单词 decoder_inputs &#91;max_decoder_time, batch_size&#93;&#58; 目标输入单词 decoder_outputs &#91;max_decoder_time, batch_size&#93;&#58; 目标输出单词，即 decoder_inputs左移动一个时间步长，同时在右边附一个句末标记。 为了提高效率，我们一次训练多个句子（batch_size）。测试过程略有不同，我们会在后面讨论。 嵌入 给定词类属性，模型必须先查找源和目标嵌入以检索相应的词汇表示。为了使嵌入层工作，首先要为每种语言选择一个词汇表。通常，选择词汇大小V，并且只有最常用的V词汇被视为唯一的。其他所有词汇都转换成一个“unknown”字符（token），并且都得到相同的嵌入。通常在训练期间学习嵌入的权重，每种语言一套。 同样，我们可以构建 embedding_decoder 和 decode_emb_inp。请注意，可以选择使用预训练的单词表示（例如 word2vec 或 Glove vector）来初始化嵌入权重。一般来说，给定大量训练数据，我们可以从头开始学习这些嵌入。 编码器 一旦被检索到，那么嵌入词汇就作为输入被喂入主网络中，该主网络由两个多层RNN组成——用于源语言的编码器和用于目标语言的解码器。这两个RNN原则上可以共享相同的权重; 但是，在实践中，我们经常使用两种不同的RNN参数（这些模型在拟合大型训练数据集时做得更好）。编码器RNN使用零向量作为起始状态，构建如下： 请注意，句子具有不同的长度以避免计算上的浪费，我们通过source_seqence_length 告诉 dynamic_rnn 确切的源句子长度。由于我们的输入是 time major 的，因此设置 time_major = True。 在这里，我们只构建一个单层LSTM，encoder_cell。在后面的部分将介绍如何构建多层 LSTM，添加 dropout，以及使用 attention。 解码器 解码器也需要访问源信息，一个简单的方法就是用编码器的最后一个隐藏状态（encode_state）来初始化解码器。 在图2中，我们将源代码“student”的隐藏状态传递到解码器端。 这里，代码的核心部分是 BasicDecoder ，接收 decode_cell（类似于encoder_cell）的 decoder，一个 helper，以及作为输出的前一个 encoder_state。通过分开 decoders 和 helpers，我们可以重复利用不同的代码库，例如，可以用 reedyEmbeddingHelper 替代 TrainingHelper 进行 greedy decoding。更多信息请查看 helper.py。 最后，我们还没提到 projection_layer，它是一个密集矩阵（dense matrix），用于将顶部的隐藏状态转换为维度V的对数向量（logit vectors）。这个过程在图2的顶部说明了。 损失 有了上面的 logits，现在可以计算训练损失： 这里，target_weights 是与 decode_outputs 大小相同的0-1矩阵，它将目标序列长度之外的位置填充为值为0。 重要注意事项：我们用 batch_size 来分割损失，所以我们的超参数对 batch_size是“不变的”。有的人将损失以 batch_size * num_time_steps 进行分割，这可以减少短句子的翻译错误。更巧妙的是，我们的超参数（应用于前面的方法）不能用于后面的方法。例如，如果两种方法都使用学习律为1.0的SGD，那么后一种方法有效利用更小的学习率，即1 / num_time_steps。 梯度计算和优化 我们现在已经定义NMT模型的前向传播。计算反向传播只需要几行代码： 训练RNN的重要步骤之一是梯度剪切（gradient clipping）。这里，我们按照global norm警醒剪切。最大值max_gradient_norm通常设置为5或1。最后一步是选择优化器。Adam优化器是常见的选择。也需要选择学习率（learning rate）。learning_rate的值通常在0.0001到0.001之间; 也可以设置为随着训练的进行，学习率降低。 在我们自己的实验中，我们使用标准SGD（tf.train.GradientDescentOptimizer）以及可降低的学习率设置，从而产生更好的性能。具体见benchmark部分。 实践——训练一个NMT模型 让我们开始训练第一个NMT模型，将越南语翻译成英语！代码的入口点是 nmt.py 我们将使用一个小型的TED 演讲（133K训练样本）的平行语料库来进行这个实践。我们在这里使用的所有数据可以在下面网址找到：https：//nlp.stanford.edu/projects/nmt/。我们将使用tst2012作为dev数据集，tst2013作为测试数据集。 运行以下命令下载训练NMT模型的数据：nmt/scripts/download_iwslt15.sh /tmp/nmt_data 运行以下命令开始训练： 上面的命令训练一个具有128-dim的隐藏单元和12个epoch的嵌入的2层LSTM seq2seq模型。我们使用的dropout值为0.2（保持或然率为0.8）。如果不出现error，随着训练的困惑度值（perplexity value）降低，应该可以看到类似下面的logs： 详细信息请参阅train.py。 我们可以在训练期间启动Tensorboard来查看模型的概要： tensorboard –port 22222 –logdir /tmp/nmt_model/ 以上是从英语翻译成越南语的训练，通过下面的代码可以简单地变成从越南语翻译成英语： –src=en –tgt=vi 推理——如何生成翻译 在训练NMT模型时（以及已经训练完时），你可以得到之前模型没见过的源句子的翻译。这个过程称为推理（inference）。训练和推理（测试）之间有明确的区别：在推理时，我们只能访问源句子，即encoder_inputs。执行解码有很多种方法。解码方法包括greedy解码，采样解码和束搜索（beam-search）解码。这里，我们将讨论贪心解码策略。 它的想法是很简单的，如图3： 我们仍然以与训练期间相同的方式对源句子进行编码，以获得encoder_state，并使用该encoder_state来初始化解码器。- 一旦解码器接收到开始符号“&lt;s”（参见代码中的tgt_sos_id），就开始进行解码（转换）处理。- 对于解码器侧的每个时间步长，我们将RNN的输出视为一组logits。我们选择最有可能的单词，即与最大logit值相关联的id作为输出的单词（这就是“greedy”行为）。例如在图3中，在第一个解码步骤中，单词“moi”具有最高的翻译概率。然后，我们将这个词作为输入提供给下一个时间步长。- 这个过程继续进行，直到生成句尾标记“&lt;/ s&gt;”作为输出符号（在我们的代码中是tgt_eos_id）。一旦解码器接收到开始符号“&lt;s”（参见代码中的tgt_sos_id），就开始进行解码（转换）处理。 这个过程继续进行，直到生成句尾标记“&lt;/ s&gt;”作为输出符号（在我们的代码中是tgt_eos_id）。 图3：Greedy解码——训练好的NMT模型使用greedy搜索生成源句子“Je suisétudiant”的翻译。 令推理与训练不同的是步骤3。推理使用模型预测的单词，而不是总是正确的目标单词作为输入。以下是实现greedy解码的代码。它与解码器的训练代码非常相似。 在这里，我们使用GreedyEmbeddingHelper而不是TrainingHelper。由于我们预先不知道目标序列长度，所以使用maximum_iterations来限制翻译长度。 一个启发是解码最多两倍的源句子长度。 训练好一个模型后，现在可以创建一个推理文件并翻译一些句子： 注意，上述命令也可以在模型正在训练时运行，只要存在一个训练的检查点。 详细请参阅inference.py。 进阶版：注意力机制 说完了最基本的 seq2seq 模型后，下面是进阶版！ 注意力机制：背景 为了建立最先进的神经机器翻译系统，我们将需要更多的“特殊材料”：注意力机制，这是 Bahdanau 等人于 2015 年首次引入，然后由 Luong 等人在同年完善的。注意力机制的关键在于通过在翻译过程中，对相关来源内容进行“注意”，建立目标与来源之间的直接连接。注意力机制的一个很好的副产品，是源和目标句子之间的对齐矩阵（如图 4 所示）。 图4：注意力机制可视化：源和目标句子之间的比对的例子。图像来自论文 Bahdanau et al.，2015。 在简单的 seq2seq 模型中，开始解码时，我们将最后的源状态从编码器传递到解码器。这对比较短和中等长度的句子效果很好；然而，对于长句子，单个固定大小的隐藏状态就成了信息瓶颈。注意力机制并不是丢掉在源 RNN 中计算的所有隐藏状态，而是让解码器将它们视为源信息的动态存储器。通过这样做，注意力机制改善了较长句子的翻译质量。如今，注意力机制成为神经机器翻译的首选，而且也成功应用于许多其他任务（包括图说生成，语音识别和文本摘要）。 我们现在介绍注意力机制的一个实例，这个实例是 Luong 等人在 2015 年论文中提出的，已被用于 OpenNMT 开放源码工具包等多个最先进的系统，TF seq2seq API 教程中也使用了这个例子。 图5：注意力机制：Luong 等人 2015 年所述的基于注意力的 NMT 系统的例子。这里详细介绍了注意力计算的第一步。为了清楚起见，没有将图 2 中的嵌入和投射层绘制出来。 如图 5 所示，注意力计算在每个解码器时间步长都有发生，包括以下阶段： 比较当前目标隐藏状态与所有源状态，获得注意力权重“attention weight”（可以如图 4 所示）；1. 基于注意力权重，计算上下文矢量（context vector），作为源状态的加权平均值；1. 将上下文矢量与当前目标隐藏状态相结合，产生最终的注意力向量“attention vector”；1. 注意力向量作为输入，被传递到下一个时间步。基于注意力权重，计算上下文矢量（context vector），作为源状态的加权平均值； 注意力向量作为输入，被传递到下一个时间步。 注意力机制中最关键的是什么？ 根据 score 函数和 loss 函数的不同，存在很多不同的注意力变体。但在实践中，我们发现只有特定的一些选择很重要。首先是注意力的基本形式，也即目标和源之间的直接关系。 其次是将注意力向下馈送到下一个时间步长，这是告知网络过去的注意力做了什么决定（Luong 等人，2015）。最后，score 函数的选择往往会导致性能表现不同。 AttentionWrapper API 在部署 AttentionWrapper 时，我们借鉴了 Weston 等人 2015 年在 memory network 方面的一些术语。与可读写的 memory 不同，本教程中介绍的注意力机制是只读存储器。具体来说，源的一组隐藏状态被作为“记忆”（memory）。在每个时间步长中，使用当前目标隐藏状态作为“query”来决定要读取 memory 的哪个部分。通常，query 需要与对应于各个内存插槽的 key 进行比较。在我们的介绍中，恰好将源隐藏状态作为“key”。你可以受到记忆网络术语的启发，得出其他形式的注意力！ 由于有了 attention wrapper，用 attention 扩展普通 seq2seq 代码就十分简单了。这部分参考文件 attention_model.py 首先，我们需要定义注意机制，例如（Luong等人，2015）： 在以前的 Encoder 部分中，encoder_outputs 是顶层所有源隐藏状态的集合，其形状为 &#91;max_time，batch_size，num_units&#93;（因为我们将 dynamic_rnn 与 time_major 设置为 True）。对于注意力机制，我们需要确保传递的“记忆”是批处理的，所以需要转置 attention_states。 将 source_sequence_length 传递给注意力机制，以确保注意力权重正确归一化（仅在 non-padding 位置上发生）。 定义了注意力机制后，使用 AttentionWrapper 解码单元格： 代码的其余部分与 Decoder 那节是一样的！ 实践：构建基于注意力的 NMT 模型 为了实现注意力，我们需要使用 luong，scaled_luong，bahdanau 或 normed_bahdanau 中的一个，作为训练期间的注意力 flag 的值。这个 flag 指定了我们将要使用的注意力机制。 我们还需要为注意力模型创建一个新的目录，这样才不会重复使用以前训练过的基本 NMT 模型。 运行以下指令开始训练： 在训练完成后，使用同样的推理指令 model_dir 做推理： 玩转 NMT：窍门和技巧 构建训练图、评估图和推理图 在 TensorFlow 中构建机器学习模型时，最好建立 3 个独立的图： 首先是训练图，其中：1. 批次、bucket 和可能的子样本从一组文件/外部输入输入；1. 包括前向和后向 op；1. 构建优化器，并添加训练 op。包括前向和后向 op； 其次是评估图，其中：1. 批次和 bucket 从一组文件/外部输入数据；1. 包括 1 个训练前向 op 和不用于训练的其他评估 op包括 1 个训练前向 op 和不用于训练的其他评估 op 最后是推理图，其中：1. 可能不批量输入数据；1. 不会对输入数据进行子采样；1. 从占位符读取输入数据1. 包括模型前向 op 的一个子集，也可能含有用于存储 session.run 调用之间状态的其他特殊输入/输出。不会对输入数据进行子采样； 包括模型前向 op 的一个子集，也可能含有用于存储 session.run 调用之间状态的其他特殊输入/输出。 构建单独的图有几个好处： 推理图通常与其他两个不同，因此需要分开构建；- 这样评估图也更简单，因为没有了额外的反向 op；- 可以为每个图分别实现数据馈送；- 各种重用都更加简单。例如，在评估图中，不需要用 reuse = True 重新打开可变范围，因为训练模型已经创建了这些变量。不需要到处使用 reuse=；- 在分布式训练中，训练、评估和推断分开用不同的机器做很正常。反正都需要各自建图。因此，分开建图也有助于你构建分布式训练系统。这样评估图也更简单，因为没有了额外的反向 op； 各种重用都更加简单。例如，在评估图中，不需要用 reuse = True 重新打开可变范围，因为训练模型已经创建了这些变量。不需要到处使用 reuse=； 主要的问题是，在只有单机的情况下，如何在 3 个图中共享变量 Variables。这可以通过为每个图使用单独的 session 来解决。训练 session 定期保存检查点，评估和推理 session 定期从检查点恢复参数。 下面的例子显示了两种方法的主要区别。 统一建图：一个图里 3 个模型 分别建图：3 个 session 共享变量 注意，后一种方法很容易就能转换为分布式版本。 另一个区别在于，我们使用了有状态的迭代器对象，而不是使用 feed_dicts 来在每个 session.run 调用中提供数据。这些迭代器使输入管道在单机和分布式设置中都容易得多。 其他技巧：双向 RNN 编码器的双向性通常会带来更好的性能（但由于使用了更多层，速度会有一些降低）。在这里，我们给出一个简单的例子，说明如何用单个双向层构建编码器： 其他技巧：Beam Search 虽然贪婪解码得出的翻译质量不错，但是 beam search 解码器可以进一步提高性能。Beam search 在翻译时总是将一小部分顶级候选词留在身边，从而在搜索空间更好地探索所有可能的翻译。 Beam 的大小称为“宽度”width；大小为 10 的宽度基本就够了。以下是 Beam search 的示例： 其他技巧：超参数 有些超参数能带来性能的进一步提升。以下是根据我们的经验列出的一些超参数： 优化函数：虽然在“不太熟悉”的架构里，Adam 能带来不错的结果，但如果你能训练 SGD，SGD 通常会更好；- 注意力：Bahadnau 风格的注意力需要解码器双向性才好用；Luong 风格的注意力在不同设置下都挺好。在这份教程中，我们推荐两个变体： scaled_luong &amp; normed bahdanau其他技巧：多 GPU 训练 训练一个 NMT 模型需要好几天。将不同的 RNN 层放在不用的 GPU 上能提升训练速度。以下为一个例子： 你可能会发现，随着 GPU 数量的增长，基于注意力的 NMT 模型训练速度提升非常有限。这是因为标准注意力架构在每个时间步长使用顶层（最后一层）的输出做为 query 注意力。这意味着每一次解码都需要等前面的步骤完全结束了才行。因此，无法在多台 GPU 上并行解码 RNN。 谷歌提出的 GNMT 注意力架构使用底层（第一层）输出作为 query 注意力。因此，前一步刚刚结束就能实行注意力计算。我们实现了 GNMTAttentionMultiCell 中的架构，这是 tf.contrib.rnn.MultiRNNCell 的一个子类。 以下是使用 GNMTAttentionMultiCell 创建解码器单元的示例： 最后的基准部分请参考原文。 原文：https&#58;//github.com/tensorflow/nmt 点击阅读原文查看新智元招聘信息 转载来源：「TensorFlow 谷歌神经机器翻译」从零开始打造属于你的翻译系统]]></content>
      <categories>
        <category>科技</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>Google翻译</tag>
        <tag>英语</tag>
        <tag>语音识别</tag>
        <tag>TED</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度 | 理解深度学习中的卷积]]></title>
    <url>%2F2017%2F19726d11%2F</url>
    <content type="text"><![CDATA[深度 | 理解深度学习中的卷积 转载来源：深度 | 理解深度学习中的卷积]]></content>
      <tags>
        <tag>机器之心</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[手把手教你打造一个曲风分类机器人]]></title>
    <url>%2F2017%2F0d0d3374%2F</url>
    <content type="text"><![CDATA[手把手教你打造一个曲风分类机器人 转载来源：手把手教你打造一个曲风分类机器人]]></content>
  </entry>
  <entry>
    <title><![CDATA[万字「全文」详解谷歌神经网络机器翻译NMT，人人可利用TensorFlow快速建立翻译模型]]></title>
    <url>%2F2017%2Fa87e959c%2F</url>
    <content type="text"><![CDATA[图：pixabay 原文来源：Google Research Blog、GitHub 作者：Thang Luong、Eugene Brevdo、Rui Zhao 「机器人圈」编译：BaymaxZ、嗯~阿童木呀、多啦A亮 机器翻译作为自动翻译语言之间的任务，是机器学习社区中最活跃的研究领域之一。在机器翻译的众多方法中，序列到序列（“seq2seq”）模型最近取得了巨大的成功，并已成为大多数商业翻译系统中的标准。然而，虽然seq2seq模型（如OpenNMT或tf-seq2seq）上有大量的材料，但是缺乏教学人员知识和技能的材料，可以轻松构建高质量的翻译系统。 近日，TensorFlow在GitHub上宣布一个新的神经机器翻译（NMT）教程，让读者能够充分了解seq2seq模型，并展示如何从零开始构建翻译模型。 •简介 •基础 oNMT背景知识 o安装教程 o训练 – 如何构建首个NMT系统 降维 编码 解码 损失 梯度计算和优化 o实践——让我们来训练NMT模型l o推理——如何生成翻译 •中级 o注意力机制背景知识 o注意力包装器 API o实践——建立基于注意力的NMT模型 •提示和技巧 o建立训练、评估和推理图 o数据输入流水线 o其他细节 双向循环神经网络 集束搜索 超参数 多GPU训练 •测试基准 oIWSLT 英语到越南语 oWMT 德文到英文 oWMT 英文到德文 — 完全比较 •其他资源 •声明 •参考 简介 序列到序列（seq2seq）模型在诸如机器翻译、语音识别和文本概括等各项任务中，取得了巨大的成功。本教程为读者提供了对seq2seq模型的全面介绍，并展示了如何从头构建一个seq2seq模型。我们专注于神经机器翻译（NMT）的任务，这是第一个成功的seq2seq模型的测试平台。包含的代码是轻量级的、高质量的、生产就绪的，并与最新的研究思想结合在一起。我们通过以下方式实现此目标： 1.使用最新的解码器/注意力包装器API，TensorFlow 1.2数据迭代器； 2.结合了我们在建立循环和seq2seq模型方面的专长； 3.提供提示和技巧，以构建最好的NMT模型，并复制Google的NMT（GNMT）系统； 我们认为重要的是，提供人们可以轻松复制的基准。因此，我们提供了完整的实验结果，并对以下公开数据集的模型进行了预先训练： 1.小规模：由IWSLT评估组织提供的平行语料库，包含TED谈话中的英语到越南语的133000个句子对； 2.大规模：WMT评估组织提供的德语到英语的平行语料库（450万个句子对）； 我们首先介绍关于NMT的seq2seq模型的一些基本知识，说明如何构建并训练vanilla NMT模型。第二部分将详细介绍建立一个高效的NMT模式的注意力机制。然后，我们将讨论提示和技巧，以构建最佳的NMT模型（包括速度和翻译质量），例如TensorFlow最佳实践（批处理、降级），双向RNN和集束搜索。 基础 •NMT背景知识 回到过去，传统的基于短语的翻译系统将源语句分解成多个组，然后逐句翻译。这导致翻译产品不一致性，而且翻译的水平跟人类相比差异很大。人类通读整个源句，理解它的含义，然后再翻译。而神经机器翻译（NMT）正是这么模拟的！ 图1.编码器-解码器架构。NMT一般方法的示例。编码器将源句子转换成通过解码器传递以生成翻译的“含义”向量。 具体来说，首先，NMT系统使用编码器，读取源语句，以构建“思想”向量，表示句子意义的数字序列；然后，解码器处理句子向量，以发出翻译，如图1所示。这通常被称为编码器-解码器架构。以这种方式，NMT解决了传统的、基于短语的方法中的本地翻译问题：它可以捕获语言的长期依赖性，例如语法结构等等，并产生更流畅的翻译。 NMT模型因具体结构而有所不同。顺序数据的自然选择是大多数NMT模型使用的循环神经网络（RNN）。通常，RNN用于编码器和解码器。然而，RNN模型在以下方面会不同：（a）方向性——单向或双向; （b）深度——单层或多层；（c）类型——通常是vanilla RNN、长短期记忆网络（LSTM）或门控循环单元（GRU）。有兴趣的读者可以在这篇博文上找到有关RNN和LSTM的更多信息。 在本教程中，我们将一个深度多层RNN视为单向，并将LSTM作为循环单元。我们在图2中展示了一个模型的例子。在这个例子中，我们建立一个模型，将源句子“我是一个学生”翻译成一个目标句子“Je suisétudiant”。NMT模型由两个循环神经网络组成：编码器RNN简单地处理输入源句子，而不进行任何预测；另一方面，解码器在预测下一个单词的同时，处理目标句子。 有关更多信息，请参阅本教程的作者Luong于2016年撰写的文章。 图2.神经机器翻译将源语句“我是一名学生”翻译成一个目标句子“Je suisétudiant”，展示一个深度循环架构。这里，“”表示解码处理的开始，而“&lt;/ s&gt;”告诉解码器停止。 安装教程 要安装本教程，你需要在系统上安装TensorFlow。本教程需要安装最新版本的TensorFlow（版本1.2.1）。要安装TensorFlow，请按照安装说明进行操作。 一旦安装了TensorFlow，你便可以通过运行以下方式，下载本教程的源代码： 训练-如何构建你的第一个NMT系统 我们先来看看，构建一个具体代码片段的NMT模型的核心，我们将更详细的解释图2。数据准备和完整的代码将稍后介绍。这部分可参考文件model.py。 在底层，编码器和解码器RNN作为输入接收以下内容：首先，源语句，然后指示从编码到解码模式的转换的边界标记“”和目标句子。对于训练，我们将为系统提供以下张量，包含单词索引： encoder_inputs &#91;max_encoder_time，batch_size&#93;：源输入字。 decode_inputs &#91;max_decoder_time，batch_size&#93;：目标输入字。 decode_outputs &#91;max_decoder_time，batch_size&#93;：目标输出字，这些是decode_inputs向左移动一个时间段，右边附加一个句末尾标记。 为了提高效率，我们一次训练多个句子（batch_size）。测试略有不同，所以我们稍后再讨论一下。 降维 考虑到词语的分类性质，模型必须首先查找源和目标降维，以检索相应的词表示。为了使降维层可以工作，首先要为每种语言选择一个词汇表。通常，选择词汇大小V，并且只有最频繁的V被视为唯一的。所有其他单词都转换为“未知”令牌，并且都获得相同的降维。通常，降维权重在训练期间学习，每种语言一套。 同样，我们可以构建embedding_decoder和decode_emb_inp。请注意，可以选择使用预训练的单词表示（如word2vec或Glove向量）初始化降维权重。一般来说，考虑到大量的训练数据，我们可以用scratch来学习这些降维。 编码器 一旦检索到，则将词语降维作为输入馈送到主网络中，该主网络由两个多层RNN组成——源语言的编码器以及用于目标语言的解码器。这两个RNN原则上可以共享相同的权重; 然而，在实践中，我们经常使用两种不同的RNN参数（这些模型在拟合大型训练数据集时做得更好）。编码器RNN使用零向量作为其起始状态，建立过程如下： 请注意，句子具有不同的长度，以避免浪费计算，我们通过source_seqence_length告诉dynamic_rnn确切的源句长度。由于我们的输入是基于时间的，我们设置time_major = True。 在这里，我们只构建一个单层LSTM，encoder_cell。我们将介绍如何构建多层LSTM，添加dropout，并在后面的部分中使用注意力。 解码 解码器还需要访问源信息，一个简单的方法就是用编码器的最后一个隐藏状态（encode_state）来初始化它。在图2中，我们将源代码“学生”的隐藏状态传递到解码器端。 这里，这个代码的核心部分是BasicDecoder的对象，解码器，它接收decode_cell（类似于encoder_cell），一个帮助器，和作为输入的encoder_state。通过分离解码器和帮助器，我们可以重用不同的代码库，例如，可以用GreedyEmbeddingHelper替代TrainingHelper进行贪婪解码。了解更多请查阅helper.py。 最后，我们还没有提到projection_layer，它是一个密集矩阵，将顶部隐藏状态转换为维度V的对数向量。我们在图2的最上面说明这一过程。 损失 鉴于上述情况，我们现在可以计算我们的训练损失： 这里，target_weights是与decode_outputs相同大小的零矩阵。它掩盖了值为0的目标序列长度之外的填充位置。 重要的注意事项：值得指出的是，我们用batch_size来划分损失，所以我们的超参数对batch_size是“不变的”。有些人将损失除以（batch_size * num_time_steps），它可以减少短句所造成的错误。更巧妙的是，我们的超参数（以前的方式应用）不能用于后一种方式。例如，如果两种方法都使用学习1.0的SGD（随机梯度下降），则后一种方法有效地使用了更小的1 / num_time_steps的学习速率。 梯度计算和优化 我们现在已经定义了我们的NMT的前进模式。计算反向传播传递只是几行代码的问题： 训练RNN的重要步骤之一是梯度剪切。在这里，我们按照规范剪辑。最大值max_gradient_norm通常设置为5或1的值。最后一步是选择优化器。Adam优化器是常见的选择，我们也选择学习率，学习率的值通常在0.0001到0.001之间；随着训练的进行，可以减少。 在我们自己的实验中，我们使用标准SGD（tf.train.GradientDescentOptimizer），具有降低的学习率调度，从而产生更好的性能。 参见基准。 实践——让我们来训练NMT模型 我们训练我们的第一个NMT模型，从越南语翻译成英语！ 我们的代码的入口是nmt.py。 我们将使用一个小型平行语言的TED谈话（133000个训练示例）来进行此练习。我们在这里使用的所有数据都可以在以下网址找到：https&#58;//nlp.stanford.edu/projects/nmt/。我们将使用tst2012作为我们的dev数据集，tst2013作为测试数据集。 运行以下命令下载培训NMT模型的数据： nmt/scripts/download_iwslt15.sh /tmp/nmt_data 运行以下命令开始训练： 上述命令在12个周期内，训练了一个具有128个隐藏单元和降维的2层LSTM seq2seq模型。 我们使用的dropout值为0.2（保持概率为0.8）。如果没有错误的话，我们应该在我们训练时看到类似于下面的日志。 查看train.py获取更多信息 我们可以在训练期间启动Tensorboard查看对模型的总结： 从英语和越南语训练反向翻译可以简单地实现： –src=en –tgt=vi 推理——如何生成翻译 当你训练你的NMT模型（一旦你拥有训练模型），你可以获得以前不可见的源语句的翻译。而这个过程就称为推理。训练和推理（测试）之间有明确的区别：在推理时，我们只能访问源语句，即编码器输入，而执行解码的方法有很多种。解码方法包括贪婪算法、采样和波束搜索解码。在这里，我们将讨论贪婪解码策略。 这个想法很简单，我们将图3中进行阐述： 1．我们依然使用与训练期间相同的方式对源语句进行编码从而获得encoder_state，并使用该encoder_state来初始化解码器。 2．一旦解码器接收到起始符号”“ （参见我们代码中的tgt_sos_id），解码（翻译）过程就开始了。 3．对于解码器端的每个时间步长，我们将RNN的输出视为一组逻辑。我们选择最可能的单词，id与最大逻辑值相关联，将其作为发出的单词（这就是“贪婪”行为）。例如在图3中，在第一解码步骤中，单词“moi”具有最高的翻译概率。然后，我们将这个词作为输入提供给下一个时间步长。 4．该过程将一直继续，直到生成句尾标记”“ 作为输出符号产生（参见我们代码中的tgt_eos_id）。 图3.贪婪解码——训练好的NMT模型如何使用贪婪搜索生成源语句“Je suisétudiant”的翻译。 第3步，是什么使推理与训练如此不同。推理使用的是模型预测的单词，而并非总是将正确的目标单词作为输入，以下是实现贪婪解码的代码。它与训练解码器非常相似。 在这里，我们使用的是GreedyEmbeddingHelper而不是TrainingHelper。由于我们预先不知道目标序列长度，所以我们使用maximum_iterations来限制翻译的长度。思路是将源句长度的两倍作为解码最大长度。 训练了一个模型后，我们现在可以创建一个推理文件并翻译一些句子： 注意，即使模型仍在训练中，但只要存在训练检查点，就可以运行上述命令。有关详细信息，请参阅inference.py。 中级 既然已经知晓最基本的seq2seq模型，那就来进一步完善！为了建立最先进的神经机器翻译系统，我们将需要更多的“秘密武器”：注意力机制，这是Bahdanau等人于2015年首次提出的，然后由Luong等人于2015年完善。注意力机制的关键在于，在翻译过程中通过对相关源文件内容进行“注意”，从而建立起目标文件与源文件之间的直接连接。注意力机制的一个很好的附加作用就是源句和目标句之间有个可便捷查看的对齐矩阵（如图4所示）。 图4.注意力可视化——源句和目标句之间的对齐示例。图像摘自（Bahdanau 等人在2015年发表的论文）。 请记住，在vanilla seq2seq模型中，当开始解码过程时，我们将最后的源状态从编码器传递到解码器中。这对于较短和中等长度的句子来说效果很好；但是，对于长句，单个固定大小的隐藏状态就成了信息瓶颈。注意力机制不是放弃在源RNN中计算的所有隐藏状态，而是提供了允许解码器窥视它们的方法（将它们视为源信息的动态存储器）。通过这样做，注意力机制改进了较长句子的翻译效果。现如今，注意力机制是一种流行的标准，并已经成功应用于许多其他任务中（包括图像字幕生成，语音识别和文本自动摘要）。 注意力机制的背景 我们现在在描述Luong等人在2015提出的注意力机制的一个实例，它已被用于包括像OpenNMT这样的开源工具包在内的多个最先进的系统，以及本教程中的TF seq2seq API中。我们还将提供与注意力机制其他变体的连接。 图5.注意力机制——（Luong等人于2015所著论文）中描述的基于注意力的NMT系统的示例。我们详细介绍了注意力计算的第一步。为了清晰起见，我们不在图（2）中显示降维和投影层。 如图5所示，注意力计算发生在每个解码器的时间步长中。它包括以下阶段： 1.将当前目标隐藏状态与所有源状态进行比较，以获得注意力权重（如图4所示）。 2.基于注意力权重，我们计算上下午矢量作为源状态的加权平均值。 3.将上下文矢量与当前目标隐藏状态相结合，产生最终的注意力向量。 4.注意向量作为输入馈送到下一个时间步长（输入馈送）。前三个步骤可以通过以下等式来总结： 在这里，函数score用于将目标隐藏状态$$h_t$$ 与每个源隐藏状态$$\overline&amp;#123h&amp;#125_s$$进行比较，并将结果归一化以产生注意权重（源位置的分布）。评分函数有多种选择；通用的评分函数包括方程式中（4）给出的乘法和加法形式。一旦计算，注意力矢量 $$a_t$$ 用于导出softmax logit和loss。这类似于vanilla seq2seq模型中顶层的目标隐藏状态。函数f也可以采取其他形式。 注意力机制的各种实现可以在attention_wrapper.py中找到。 •注意力机制中有什么是重要的？ 如上述方程式所示，有许多不同的注意事项。这些变体取决于评分函数和注意力函数的形式，以及在评分函数中是否使用的是先前的状态$$h_&amp;#123t-1&amp;#125$$， 而不是Bahdanau等人在2015年提出的的 $$h_t$$。根据经验来说，我们发现只有某些选择是很重要的。首先，注意力的基本形式，即目标和来源之间的直接关系。其次，将注意力向量向下馈送到下一个时间步长，以便通知网络关于过去的注意决定，正如Luong等人在2015年论文中所演示的那样。最后，评分函数的选择往往会导致不同的表现。参见基准测试结果部分。 注意力包装器 API 在执行注意力包装器时，我们借鉴了（本论文）在内存网络方面的一些术语。本教程中介绍的注意力机制是只读内存，而不是具有可读写的内存。具体来说，引用一组源隐藏状态（或其转换版本，例如，Luong评分方式中的$$W\overline&amp;#123h&amp;#125_s$$或者Bahong评分方式中的 $$W_2\overline&amp;#123h&amp;#125_s$$）来作为“记忆”。在每个时间步长中，我们使用当前目标隐藏状态作为“查询”来决定要读取内存的哪个部分。通常，查询需要与对应于各个内存插槽的keys进行比较。在上述注意力机制的介绍中，我们恰好将源隐藏状态（或其转换版本，例如Bahdanau评分风格中的$$W_1h_t$$ ）用作“keys”。这是一个可以从这种记忆网络术语中得到启发，以获得其他形式的注意的机制！ 得益于注意力包装器，延长我们的vanilla seq2seq代码的注意力就变得微不足道了。这部分可参考文件attention_model.py 首先，我们需要定义注意力机制，例如（Luong等人在2015所著论文中那样）： 在以前的编码器部分中，encoder_outputs是顶层所有源隐藏状态的集合，其形状为&#91;max_time，batch_size，num_units&#93;（因为我们将dynamic_rnn与time_major设置为True以达到高效的效果）。对于注意力机制来说，我们需要确保传递的“记忆”是批处理的，所以我们需要转置attention_states。我们将source_sequence_length传递给注意力机制，以确保注意权重正确归一化（仅在非填充位置上）。 定义了注意力机制后，我们使用AttentionWrapper来包装解码单元格： 代码中的其余部分与Section Decoder中的代码几乎相同！ 动手实践——建立以注意力为基础的NMT模型 为了确保能够维持注意力，我们需要使用luong, scaled_luong, bahdanau或normed_bahdanau中的一个作为训练期间的attention标志的值。该标志指定了我们将要使用的注意力机制。此外，我们需要为注意力模型创建一个新的目录，所以我们不用重复使用以前训练过的基本NMT模型。 运行以下命令从而开始训练： 训练后，我们可以使用相同的具有新的model_dir的推理命令，进行推理： 提示与技巧 •建立训练、评估和推理图形 在TensorFlow中构建机器学习模型时，最好建立三个独立的图形： •训练图，其中： 批处理、降级操作和可能的子样本从一组文件/外部输入中输入数据。 包括正向和反向操作。 构建优化器，并添加训练操作。 •评估图，其中： 从一组文件/外部输入中输入数据进行批处理和降级操作。 包括正向训练操作和不用于训练的其他评估操作。 •推理图，其中： 可能不会批量输入数据。 不会对数据进行子采样或者降级处理。 从占位符读取输入数据（数据可以通过feed_dict或C ++ TensorFlow服务二进制文档直接提供给图表）。 包括模型正向操作的一个子集，以及可能的用于存储session.run调用之间状态的附加特殊输入/输出。 构建单独的图表有几个好处： •推理图通常与其他两个不同，因此分开构建是有意义的。 •评估图变得更简单，因为它不再具有所有额外的反向运算。 •可以为每个图分别实现数据馈送。 •可变重用简单得多。例如，在评估图中，不需要重新打开具有reuse = True的可变范围，因为训练模型已经创建了这些变量。因此，相同的代码可以重用，而无需在任何地方都使用reuse = arguments。 •在分布式训练中，工作人员分别进行训练，评估和推理是很平常的。这些都需要建立自己的图形。因此，以这种方式构建系统将为你进行分布式训练做好准备。 复杂性的主要来源在于如何在单个机器设置中的三个图表中共享变量。这通过在每个图形中使用单独的会话来解决。训练会话定期保存检查点，并且评估会话，并推断会话会从检查点中恢复参数。下面的例子显示了两种方法的主要区别。 之前：三个模型在单个图表中共享单个会话 之后：三个模型在三个图表中，三个会话共享相同的变量 请注意后一种方法如何“准备”转换为分布式版本。 新方法的另一个区别在于，我们使用有状态的迭代器对象，而不是使用feed_dicts来在每个session.run调用（从而执行我们自己的批处理、降级和操作数据）中提供数据。这些迭代器使输入流水线在单机和分布式设置中都容易得多。我们将在下一节中介绍新的输入数据流水线（在TensorFlow 1.2中介绍）。 数据输入流水线 在TensorFlow 1.2之前，用户有两种方式将数据提供给TensorFlow训练和评估流水线： 1.直接通过feed_dict使用每个训练session.run读取数据。 2.在tf.train（例如tf.train.batch）和tf.contrib.train中使用排队机制。 3.使用像tf.contrib.learn或tf.contrib.slim这样的更高级别的框架帮助器。 第一种方法对于不熟悉TensorFlow的用户来说更容易，或者只需要在Python中完成异步输入修改（即自己的小型排队）即可。第二和第三种方法更加标准，但灵活性稍差一些，他们还需要启动多个python线程（队列运行器）。此外，如果使用不正确的队列可能会导致死锁或不透明的错误消息。然而，队列比使用feed_dict更有效，并且是单机和分布式训练的标准。 从TensorFlow 1.2开始，有一个新的系统可用于将数据读入TensorFlow模型：数据集迭代器，如tf.contrib.data模块中所述。数据迭代器是灵活的，易于理解和操纵，并通过利用TensorFlow C ++运行时提高效率和多线程。 可以从批量数据Tensor，文件名或包含多个文件名的Tensor创建数据集。例如： 所有数据集可以通过输入过程进行相似的处理。这包括读取和清理数据、降级（在训练和评估的情况下）、过滤和批处理。 要将每个句子转换成字串的向量，例如，我们使用数据集映射转换： 我们可以将每个句子向量切换成一个包含向量和它的动态长度的元组： 最后，我们可以对每个句子执行词汇查找。给定一个查找表对象表，该映射将第一个元组元素从字符串向量转换为整数向量。 连接两个数据集也很容易。如果两个文件包含彼此的逐行翻译，并将每个文件读入其自身的数据集，则可以通过以下方式创建一个包含压缩行元组的新数据集： 可变长度句子的分批是很直接明确的。以下转换从source_target_dataset批处理batch_size元素，并将源和目标向量分别贴到每个批次中最长源和目标向量的长度。 从此数据集发出的值将是嵌套元组，其张量具有最大尺寸为batch_size的尺寸。 结构将是： 1.迭代器&#91;0&#93; &#91;0&#93;具有批量和填充的源句子矩阵。 2.迭代器&#91;0&#93; &#91;1&#93;具有批量的源大小向量。 3.迭代器&#91;1&#93; &#91;0&#93;具有批量和填充的目标句子矩阵。 4.迭代器&#91;1&#93; &#91;1&#93;具有批量的目标大小向量。 最后，也可以将类似大小的源句子批量化在一起。有关详细信息和完整实现，请参阅utils / iterator_utils.py文件。 从数据集读取数据需要三行代码：创建迭代器，获取其值并初始化它。 一旦迭代器被初始化，访问源或目标张量的每个session.run调用将从基础数据集请求下一个小型数据。 更好的NMT模型的其他细节 •双向RNN 编码器侧的双向性通常会提供更好的性能（随着使用更多的层，速度有一些降低）。 在这里，我们给出一个简单的例子，说明如何用单个双向层构建编码器： 变量encoder_outputs和encoder_state可以与编码器一样使用。 请注意，对于多个双向层，我们需要对编码器_state进行一些操作，有关详细信息，请参阅model.py，更多细节请运行_build_bidirectional_rnn()。 集束搜索 虽然贪婪解码可以给我们相当合理的翻译质量，但是集束搜索解码器可以进一步提高性能。集束搜索的想法是通过在我们翻译的同时，保留一小堆顶级候选来更好地探索所有可能的翻译的搜索空间。集束的大小称为波束宽度；大小为10的最小波束宽度通常是足够的。有关更多信息，请参阅Neubig的第7.2.3节（2017）。 以下是集束搜索如何完成的示例： 请注意，使用相同的dynamic_decode（）API调用，类似于Section解码器。一旦解码，我们可以访问如下的翻译： 有关详细信息，请参阅model.py，或_build_decoder（）。 超参数 有几个超参数可以导致额外的性能。在这里，我们根据自己的经验列出一些&#91;免责声明：其他人可能不会同意我们写的内容！&#93;。 优化器：虽然Adam可以导致“陌生”体系结构的合理化，但如果你可以使用SGD训练，一般会导致更好的性能。 注意力：Bahdanau方式的注意力往往要求编码器方面的双向性运作良好; 而Luong方式的注意力往往适用于不同的设置。对于本教程代码，我们建议使用Luong＆Bahdanau方式的注意力的两个改进的变体：scaling_luong和normed bahdanau。 多GPU训练 训练NMT模型可能需要几天时间。 在不同的GPU上放置不同的RNN层可以提高训练速度。 以下是在多个GPU上创建RNN图层的示例。 此外，我们需要在tf.gradients中启用colocate_gradients_with_ops选项来并行化梯度计算。 你可能会注意到，随着GPU数量的增加，基于NMT模型的注意力的速度提高非常小。标准注意力架构的一个主要缺点是在每次步骤中使用顶层（最终）层的输出来查询注意力。这意味着每个解码步骤必须等待其前一步骤完成；因此，我们无法通过简单地将RNN层放置在多个GPU上来并行化解码过程。 GNMT注意力架构通过使用底部（第一）层的输出来查询注意力来并行化解码器的计算。因此，一旦上一步的第一层和注意力计算完成，每个解码步骤就可以开始。我们在GNMTAttentionMultiCell中实现了该架构，这是tf.contrib.rnn.MultiRNNCell的子类。以下是使用GNMTAttentionMultiCell创建解码器单元的示例。 基准 IWSLT英语到越南语 练习：133000个案例, dev=tst2012, test=tst2013，下载脚本。 训练细节：我们用双向编码器（即编码器的1个双向层）训练512单位的2层LSTM，降维dim为512. LuongAttention（scale = True），dropout keep_prob=0.8。所有参数都是一致的。我们使用学习率1.0的SGD如下：训练12000步（〜12个周期）; 经过8000步，我们每1000步开始减半学习率。 结论：TODO（rzhao）：添加从英文到越南语训练模型的URL。 以下是2个模型（模型1、模型2）的平均结果。 我们用BLEU评分来衡量翻译质量（Papineni 等人于2002年提出）。 训练速度： TitanX上K40m的（0.37s步进时间，15.3K wps）（0.17s步进，32.2K wps）。 这里，步进时间是指运行一个小批量（大小128）所需的时间。 对于wps，我们计算源和目标上的单词。 WMT：从德语到英语 训练：450万个样例，dev = newstest2013，test = newstest2015 下载脚本 训练细节：我们训练超参数的过程与英语到越南语的实验类似，但以下细节除外。使用BPE https&#58;//github.com/rsennrich/subword-nmt（32000个操作）将数据拆分为子字单元。我们用双向编码器（即编码器的2个双向层）训练1024单位的4层LSTM，降维dim为1024。我们训练350000步（〜10个周期）；在170000步之后，我们每17000步开始减半学习率。 结论：TODO（rzhao）：添加德文到英文训练模型的URL。 前2行是2个模型（模型1、模型2）的平均结果。 第三行的结果是使用GNMT注意力（&#91;model&#93;（LINK））在4个GPU上运行结果。 这些结果表明我们的代码为NMT建立了强大的基准系统。（请注意，WMT系统通常会使用我们目前没有的大量单语数据。） 训练速度：Nvidia K40m（2.1s步进时间、3.4K wps），Nvidia TitanX（0.7s步进、8.7K wps）。 我们仅对K40m进行了基准测试： 这些结果表明，没有GNMT注意力，使用多个GPU的收益是最小的。 如果有GNMT注意力，我们从多个GPU获得了50％-100％的加速。 WMT：英语到德语——完全比较 前两行是我们的GNMT注意力模型：模型1（4层）、模型2（8层）。 上述结果表明，我们的模型在类似架构的模型中具有很强的竞争力。 &#91;注意，OpenNMT使用较小的模型，目前的最佳结果（截至本文中）为28.4，由Transformer network（Vaswani等人于2017年提出）获得，其具有明显不同的架构。 其他资源 为了深入了解神经机器翻译和序列模型，我们强烈推荐以下材料：Luong，Cho，Manning（2016）、luong（2016），以及Neubig（2017）。 很多工具可以构建seq2seq模型，所以我们可以选择其中一种语言： Stanford NMT https&#58;//nlp.stanford.edu/projects/nmt/ &#91;Matlab&#93; tf-seq2seq https&#58;//github.com/google/seq2seq &#91;TensorFlow&#93; Nemantus https&#58;//github.com/rsennrich/nematus &#91;Theano&#93; OpenNMT http&#58;//opennmt.net/ &#91;Torch&#93; 声明 我们要感谢Denny Britz、Anna Goldie、Derek Murray和Cinjon Resnick为TensorFlow和seq2seq库带来了全新功能。另外感谢Lukasz Kaiser对seq2seq代码库的初步帮助； Quoc Le为复制GNMT提出的建议; Yonghui W和Zhifeng Chen对于GNMT系统的细节贡献， 以及谷歌大脑团队的支持和反馈！ 转载来源：万字「全文」详解谷歌神经网络机器翻译NMT，人人可利用TensorFlow快速建立翻译模型]]></content>
      <categories>
        <category>科技</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>英语</tag>
        <tag>脚本语言</tag>
        <tag>WPS</tag>
        <tag>机器人</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Facebook AI研究院提出了创造性对抗网络CAN，AI的艺术造诣也要超过人类了]]></title>
    <url>%2F2017%2F718d2bae%2F</url>
    <content type="text"><![CDATA[雷锋网 AI 科技评论按：能够迭代进化、模仿指定数据特征的GAN（生成式对抗性网络）已经是公认的处理图像生成问题的好方法，自从提出以来相关的研究成果不少，在图像增强、超分辨率、风格转换任务中的效果可谓是惊人的。 （具体可以参见 Valse 2017 | 生成对抗网络（GAN）研究年度进展评述 - 雷锋网） 利用GAN达到图像超分辨率和风格转换示例今年也有利用GAN做的简笔画到图像转换模型pix2pix（代码地址 https&#58;//github.com/phillipi/pix2pix，demo地址https&#58;//affinelayer.com/pixsrv/）。除了下图转换猫的，还有建筑物的、鞋子的、包包的，模型非常有想象力，随便画也没关系，感兴趣的读者可以自己到demo地址里画画看。 demo中用把线条转换成猫的示例GAN能生成艺术作品吗？GAN既然已经有如此的图像生成能力了，我们能不能用GAN生成艺术作品呢，毕竟许多现代艺术作品看照片好像也并不怎么复杂，比如下面这幅；超写实主义的就更不用说了。 蒙德里安《红黄蓝的构成》然而，要创造出一副人类觉得有艺术价值的作品并没有那么简单。人类喜欢创新性的作品，人类不喜欢完全模仿的作品；《蒙娜丽莎》和《兰亭集序》只有原作者的原版才被认可是世界艺术瑰宝，后世的人就算基于它们创作，也要有自己的创新，才能带来新的艺术价值，才能被观赏者认可。 根据GAN的基本结构，鉴别器D要判断生成器G生成的图像是否和其它已经提供给鉴别器D的图像是同一个类别（特征相符），这就决定了最好的情况下输出的图像也只能是对现有作品的模仿，如果有创新，就会被鉴别器D识别出来，就达不成目标了。上面几个GAN的例子就能体现出鉴别器D带来的这个特点，用GAN生成的艺术作品也就注定缺乏实质性的创新，艺术价值有限。 那么，能不能让GAN具有一些创新的能力，让这些创新有艺术价值、带有这些创新的作品还能够被人类认可呢？罗格斯大学艺术与人工智能实验室、Facebook人工智能研究院（FAIR）、查尔斯顿学院艺术史系三方合作的这篇论文就通过CAN（Creative Adversarial Network，创造性对抗网络）给出了一种答案。神经网络库Keras的作者François Chollet也在Twitter上推荐了这篇文章。 先看看作品如何CAN模型生成的一些艺术作品可以看到，生成的艺术作品风格非常多样，从简单的抽象画到复杂的线条组合都有，内容层次也有区分。论文中也有对比测试结果，CAN生成的作品不仅比GAN生成的更讨人喜欢，甚至来自巴塞尔艺术展的人类艺术作品都比不上CAN。（具体数据看后文） 如何认识艺术创新刚才说到，艺术作品需要有创新性，CAN中的C就是Creative，创新性的意思。那么创新性要如何衡量呢、如何达到呢？ 以往基于GAN的图像生成方法研究中，人类可以把训练好的网络生成的图像和客观事实相对比（超分辨率、图像补全问题中）或者根据经验判断（风格转换问题中），用来衡量网络的效果；也有过一些更早期的算法，让人类作为训练反馈的一环，引导网络的训练过程。但是对于这次的课题需要设计一个能自动训练和生成、还要衡量作品的创新性的系统而言，以往的方法就起不到什么帮助。 同时，在作者们看来，为了能模仿人类艺术创作的过程，算法中很重要的一部分就是要把算法的创意过程和人类艺术家以往的艺术作品联系起来，像人类一样把对以往艺术的理解和创造新艺术形态的能力整合在一起。 为了能够想办法找到一个能够衡量创新性、参与迭代训练的创新性指标，作者们找来了一组艺术理论。 D.E.Berlyne认为，从生理心理学的角度讲，人类的状态中有一种叫做“唤醒水平”的指标，它可以衡量一个人有多警醒、多兴奋；唤醒水平可以从最低的睡觉、休息，一直到暴怒、激动。而一副作品具有“唤醒潜力”的总体特质，它可以提升或者降低观者的唤醒水平；它是作品新颖性、意外性、复杂性、多义性和疑惑性高低的综合体现，这几个属性越高，作品的唤醒潜力就越高。- Colin Martindale（1943-2008）提出过一个假说，他认为在任一时刻，创意艺术家们都会尝试增加他们作品的“唤醒潜力”，这就是一种拓宽创作习惯边界的方法。但是，这种增加动作必须使得观察者的负面反应尽可能小（尽量使观察者不付出额外的努力），否则过于激进的产品就会受到负面的评价。- Colin Martindale还提出过一个假说，他认为当艺术家探索艺术风格的更多作用的时候，转换艺术风格就会有提高“唤醒潜力”的作用。 这组理论只是解释艺术创新的理论中的寥寥几个，但是它们综合起来给出了两个具有计算性的、可以用于迭代训练的指标： 创新作品的创新程度不能过高，观者不认为作品是艺术作品的可能性应当尽可能小；1. 新的艺术风格就是创新的体现。新的艺术风格就是创新的体现。 CAN网络的构建根据提炼出的这两个指标，论文中基于GAN的原型构建了这样一种新型的对抗性网络CAN。 CAN模型的系统框图首先，对于“指标1：创新作品的创新程度不能过高，观者不认为作品是艺术作品的可能性应当尽可能小”，就可以转换为经典的对抗性网络，G生成图像，经过艺术作品训练过的D判断G生成图像的是不是艺术作品。这样的对抗性网络生成的图像就已经可以被人类看作是艺术作品。 然后，论文中的模型还根据“指标2：新的艺术风格就是创新的体现”增加了一部分新结构用来处理艺术风格。 论文中使用了25类不同的带标签艺术作品用于D的训练，包含了抽象印象派、立体派、现代派、巴洛克、文艺复兴早期等等风格的共7万5千多幅。然后经过训练的D除了要反馈一幅图像“是否是艺术作品”外，还要反馈“能否分辨图像是哪种艺术风格”。G然后就会利用D的反馈生成尽量难以分辨艺术风格的图像——难以归类到现有分类中的，就是创新了。 “是否是艺术作品”、“是否难以分辨艺术风格”是两种对立的信号，前一种信号会迫使生成器G生成能够被看作的艺术的图像，但是假如它在现有的艺术风格范畴中就达到了这个目标，鉴别器D就能够分辨出图像的艺术风格了，然后生成器就会受到惩罚。这样后一种信号就会让生成器生成难以分辨风格的作品。所以两种信号就可以共同作用，让生成器能够尽可能探索整个创意空间中艺术作品的范围边界，同时最大化生成的作品尽可能游离于现有的标准艺术风格之外。 这也就是论文标题「CAN&#58; Creative Adversarial Networks Generating “Art” by Learning About Styles and Deviating from Style Norms」的含义，创造性对抗网络可以学习艺术风格，然后背离这些现有的风格进行艺术创作。 还说艺术风格，现在是“不好分辨”，“好分辨”不行吗？相比GAN，CAN增加的反馈是“是否难以分辨艺术风格”，追求的是生成的图像艺术风格难以分辨。虽然根据艺术理论的推导，新的艺术风格是一种创新，但既然是多加了一个反馈，追求“生成的图像艺术风格容易分辨”可以吗？会不会也能生成不错的作品呢？ 从另一个角度看，假如追求“难以分辨”的CAN确实比追求“容易分辨”的CAN生成的图像更好，那这就是模型选取了合理的反馈的最佳体现。 说做就做。除了CAN之外，论文中还建立了三种模型用来对比。 DCGAN 64x64：经过艺术作品训练的DCGAN（深度卷积生成式对抗网络），输出分辨率为64x64- DCGAN 256x256：相比DCGAN 64x64，生成器多加了两层网络，输出分辨率为256x256- scCAN：style-classification-CAN，追求“生成的图像艺术风格容易分辨”的CANDCGAN 256x256：相比DCGAN 64x64，生成器多加了两层网络，输出分辨率为256x256 这三种模型生成的画面像下面这样 两种DCGAN和scCAN生成的画面scCAN生成的画面中确实有了可辨认的风格，比如人物特写、风景或者群像。但是直观看上去并不怎么讨人喜欢。 让我们再来看一组CAN生成的图像，上方是人类评价最高的、下方是人类评价最低的。应该说都比scCAN生成的图像精彩得多。 人类评价最高和最低的CAN生成的图像人类能给CAN的图像打几分？根据刚才的图像可以看到，CAN的效果当然不错，DCGAN 256x256的图像其实也挺好。那么CAN的图像对观画的人来说是不是真的已经难以分辨创作者了呢？跟真的艺术家创作的作品相比高下又如何呢？ 为了具体比较，论文中做了几个实验，让人类给不同组的作品打分。 实验1、2：来自抽象印象派艺术家的作品、选自巴塞尔艺术展的作品、CAN生成的图像、DCGAN生成的图像，一共4组作品，由普通人判断这些作品来自人还是电脑，并给作品打分。 结果：实验1里有53%的人认为CAN的图像是来自人类的，认为DCGAN 64x64的图像来自人的有35%； 实验2里认为CAN的图像来自人类的比例是75%，DCGAN 256x256则是65%。来自抽象印象派艺术家的作品无疑是比例最高的，但有意思的是，两个实验里认为巴塞尔艺术展的作品来自人的比例都还不如CAN高（实验1中41%，实验2中48%）。 实验2的结果数据，先让人类评价者从几个角度评价作品，再判断是否是人类创作的。认为图像是人类创作的评价者比例为Q6实验3：让人类评价者从用心程度、视觉结构、互动性、启发性几个角度给作品评分，结果CAN全部得分最高。这个结果可谓出人意料。 实验3结果数据实验4：为了确认CAN和scCAN之间新颖性和美学表现的高低，请了一群艺术史学生对随机选出的CAN和scCAN图像进行评价。认为CAN的图像更新颖的比例为59.47%，认为CAN的图像更加有美学吸引力的比例为60%，确实有显著区别。 结论论文中表示，虽然这样的模型还是不能对艺术风格概念有任何语义方面的理解，不过它确实展现出了从以往的艺术作品中学习的能力。至于为什么人类会在多个方面给CAN打出高分，作者们也希望和大家进行开放性的探讨。 论文原文地址： https&#58;//arxiv.org/abs/1706.07068，雷锋网 AI 科技评论编译 转载来源：Facebook AI研究院提出了创造性对抗网络CAN，AI的艺术造诣也要超过人类了]]></content>
      <categories>
        <category>文化</category>
      </categories>
      <tags>
        <tag>人工智能</tag>
        <tag>艺术</tag>
        <tag>Facebook</tag>
        <tag>蒙娜丽莎的微笑</tag>
        <tag>蒙德里安</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[无人机消费者报告]]></title>
    <url>%2F2017%2F4bb57de0%2F</url>
    <content type="text"><![CDATA[无人机消费者报告 转载来源：无人机消费者报告]]></content>
      <tags>
        <tag>爱否科技</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[市售主流 55 寸电视横评，教你如何选电视 | 消费者报告]]></title>
    <url>%2F2017%2F8c838c11%2F</url>
    <content type="text"><![CDATA[市售主流 55 寸电视横评，教你如何选电视 | 消费者报告 转载来源：市售主流 55 寸电视横评，教你如何选电视 | 消费者报告]]></content>
      <tags>
        <tag>爱否科技</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kaggle百万美元大赛优胜者：用CNN识别CT图像检测肺癌]]></title>
    <url>%2F2017%2F84d64692%2F</url>
    <content type="text"><![CDATA[王小新 编译自GitHub 量子位 出品 | 公众号 QbitAI 今年，Kaggle网站举办了一场用肺部CT图像进行肺癌检测的比赛Data Science Bowl 2017，提供百万美元奖金池。美国国家癌症研究所为比赛提供了高分辨率的肺部CT图像，在比赛中，参赛者根据给定的一组病人肺部CT三维图像，预测癌症风险。 Julian de Wit和Daniel Hammack合作完成的解决方案获得了比赛的第二名。Wit最近写了一篇博客来介绍他们的方案。他们通过3D卷积神经网络，来构建结节探测器，预测患癌可能性。Wit在64位的Windows10系统下，结合TensorFlow 0.12.0和Keras库实现该网络模型。 BTW，第2名的奖金是20万美元。 以下内容编译自Wit的文章： 初步了解和制定计划在决定参赛之前，我观看了一个Bram van Ginneken关于肺部CT图像的介绍视频，有了基本的了解。我试图直接观察一些CT扫描样本，发现这是一个很难的问题，难度与大海捞针相当。视频中提到，图像的信噪比大约为1&#58;1000。论坛中的一些讨论也提到，神经网络不能直接从这些原始图像中学习到有用信息。目前只有1300个训练样本及对应的癌症标签，这与网络提取出的图像实际特征相差甚远。 我们希望获得更高信噪比的训练集，或是找到标签和图像特征之间更直接的关系，来训练神经网络。幸运的是，比赛组织者指出，可以借鉴一个先前举办的比赛LUNA16。在LUNA16数据集中，医生为800多个病人CT图像中精心标记了1000多个肺结节。当然，LUNA16比赛也提供没有标记结节的数据集。 因此，你可以从整体CT图像中的标记周围裁剪出小型3D图像块，最终可以用更小的3D图像块与结节标记直接对应。结节大小是癌症的一个影响因素，数据集也说明了结节的大小，所以我认为这是一个有用的信息。 图1：方法网络示意图 我还注意到LUNA16数据集是由另一个公开数据集LIDC-IDRI转化过来的。在原始数据集中，医生不仅要检测结节，而且还评估了结节的恶性程度和其他指标。我们发现，恶性程度是评估患癌风险的最佳指标，也是神经网络可以学习的。 最终的计划方案是训练一个神经网络来检测结节，并评估结节的恶性程度。在预测时，网络通过滑动窗口来遍历整体CT图像，分别判断每个滑动窗口的区域包含恶性信息的可能性。最后基于这种信息和其他特征，估计该患者发展成癌症的可能性。 数据预处理和创建训练集在预处理中，要使扫描图像的尺度尽可能一致。我首先重新缩放了CT图像，使每个像素点只表示1x1x1毫米的体积。我们也尝试了一些其他尺度的实验，最终确定了采用1毫米的尺度，这能很好地平衡计算精度和计算负荷。对于CT图像，像素强度可以用Hounsfield来表示，一般叫做亨氏单位。论坛里提到，要尽量降低像素强度，即最大化hounsfield值，然后归一化处理。同时还要确保所有CT扫描都具有相同的方向，因为CT图像旋转超过45度，意味着在图像采集过程中出现错误。 极大部分关于结节检测的文献都是先从CT扫描图像中分离出肺组织。然而目前没有合适的分割方法，能够很好地处理隐藏在肺组织边缘周围的结节和肿块。在CT图像中，这些区域会直接被删除，更不用说使用结节探测器进行类型判定。我想要训练一个U-net网络，来更好地分割肺部。与传统的分割方法相比，U-net网络能够更有效地解决实际的图像分割问题，在同为Kaggle举办的卫星图像分割比赛中被广泛地使用。当我观察这些CT图像时，我认为可以通过肺组织的边缘，构建框架来找到肺结节。这么做可能是有用的，最后我决定对原始图像进行训练和预测。在调整训练数据后，该网络效果不错，似乎没有负面影响。 在这个竞赛中，给定了训练数据，可能没有很大的发挥空间。然而处理训练集是必须的，但不是最重要的那部分工作。我使用样本中的标签，自动生成训练集的标签，也采用主动学习方法，添加部分人工标记。以下是带有标记的不同数据集。 表1：标记后的训练集 LIDC数据集中被正面标记的数量是LUNA16数据集样本数的五倍。因为这些标记是4名医生的综合注释，所以一个结节可能被标记了4次。但LUNA16也忽略了不到3名医生标注的结节。我决定在训练集中保留这些被忽视的结节，因为他们也提供了宝贵的结节恶性信息。 LUNA16 v2数据集的标签是直接从LUNA16传来，一般是多个结节检测系统错误标出的假阳性结节。要注意的是，部分结节是上面提到的不到3名医生标记的结节。我保留了这些结节标记，是为了平衡那些可疑的假阳性结节。 为了得到肺部轮廓，我需要得到非肺组织的底片。我使用了论坛中提到的简单肺分割算法，在分割掩码边缘周围进行采样标注，从而分割得到肺部组织。 在进行第一轮训练之后，我在LUNA16数据集上进行结节预测，得到了所有假阳性结节，也并入LUNA16 v2数据集中。 随着比赛的进行，我想建立第二个模型。对于这个模型，我做了放射科医生的工作，在NDSB数据集上训练网络。我手动地从癌症样本中选择明显的阳性结节，并从非癌症样本中选择假阳性结节，用这些数据训练了第二个模型。我希望效果不错，但我是一个不合格的放射科医生，实际上第二个模型比无手动标注的模型要糟糕得多。但是结合这两个模型，这个得到的融合模型比单独的模型效果更好，所以我保留了第二个模型。 我简单地建立了一个结节观察器，来调试所有的标记。在观察时，我注意到医生忽略了一些大于3cm的大结节。在LIDC数据集的说明文档中，我发现医生被要求忽略掉大于3cm的结节。我担心这些被忽视的结节区域会迷惑分类器，故删除了相重叠的底片。下面是肺部CT图像的一些截图。 图2：图像中的标记。 左上：LUNA16 v2数据，右上：非肺组织的边缘， 左下：假阳性的区域，右下：被移除的无标注区域。 3D卷积神经网络的训练方法和网络结构在一个高质量的训练集下，我们仍需要多次调整来有效地训练神经网络。数据集的两类样本量严重不平衡，正反两类样本量的比为5000：500000，且正面例子的大小和形状有很大差异。我曾考虑使用U-net网络，但2D U-net不能完全利用结节本身的三维结构信息，3D U-net网络的训练过程非常缓慢而且不灵活。我不使用U-net的主要原因是不需要建立细粒度的概率图，而只是一个粗略的检测器。在CT图像的滑动窗口中，建立小型的3D Convnet，这更加轻便和灵活。 我的第一个目标是训练一个可作为基础的结节检测器。我先要对正面例子进行过采样，将正反两类的样本比上调到1&#58;20。为了提高模型的泛化能力，我尝试了一些图像增强操作，但是只有一些无损的操作是有用的。最后我用了大量的转化操作和所有3D翻转操作。 设计好分类器后，我想训练一个用于估计恶化程度的回归模型。将肿瘤恶化程度分为从1（很可能不是恶性）到5（很可能是恶性的）。为了强调肿瘤的恶性程度，我将标签平方，范围扩大为从1到25。最开始，我考虑了分阶段的一种方法，用第一个网络来分类节点，然后训练另一个网络估计结节的恶化程度。为了缩短计算时间，我尝试只用一个网络，以多任务学习的方法，同时进行训练这两个任务。当编程实现后，我发现这个方法简单快速，网络的效果也很好。 通常，神经网络的结构是比赛和案例研究中最重要的成果之一。对于这场比赛，我在神经网络的结构上花费的时间相对较少，因为已经有很多优秀网络可供参考。刚开始我使用了一些简单的VGG网络和Resnet网络的相似结构，但是它们的性能大致相同。然后我尝试用一个预训练好的C3D网络，原有的网络权重根本没有帮助，但直接初始化权重后，这种网络结构的效果很好。基于C3D网络进行若干次调整后，我得到最终的分类评估网络。 我首先调整了输入大小，设置为32x32x32 mm。这看起来可能太小，但是在后续的网络层中加入一些技巧，发现这种维度的实际效果很好。这个想法是保持一切轻量化，并在比赛结束后再建立一个更大输入维度的网络。但是由于Daniel的网络输入是64x64x64 mm，我决定保持目前的输入大小，使网络的输出互补。接下来我立即对z轴进行平均池化操作，使得每个体素表示2mm的区域。这进一步减少了网络的参数量，并没有影响到精度，因为在大多数扫描中，z轴会比x轴和y轴更粗糙。最后，我在网络顶部引入了64个节点的全连接层。这里，我们不是直接预测恶性肿瘤，而是通过训练图片的中级特征，输出结节的恶化程度。 表2：3D Convnet的网络结构示意图 有趣的插曲：“奇怪组织”检测器看着论坛的帖子，我发现所有的团队都在做类似的工作，我也在寻找一个能直接上手的方法。在观察CT扫描图像时，我发现了一些其他的事情。与LUNA16数据集一样，大部分的工作集中在识别肺结节上。然而，当癌症发展时，它们转变成肺肿块或更复杂的组织。并且我注意到，当扫描图像中有很多“奇怪组织”时，它发展为癌症的概率更大。此外，在很多CT图像中，我的结节探测器没有发现任何结节，这造成了一些很不好的假阴性现象。 在训练集中有10例存在上述现象，其中的5例为癌症病例。为了解决这些严重的假阴性，在扫描时，需要检测获得奇怪组织的数量。很幸运，在LUNA16数据集上包含了很多这样的样本，所以我很快对数据集进行标记并训练了一个U-net网络。加入奇怪组织检测器后，效果不错，我因此提高了本地CV值和LB上的排名。因为它对于不同的模型提升不同，很难评定实际效果，但我认为它大约提升了0.002-0.005。说实话，我认为这种改进是一个创新性的补充。以下是一些包含有奇怪组织的样本。 图3：带有奇怪组织的CT图像样本。 肺气肿基本上是由吸烟导致的，我也试图建立一个肺气肿检测器。论坛上的医生都说，当肺气肿存在时，患有癌症的概率升高。有一些简单的算法公布了如何评估CT扫描中肺气肿区域的数量，设置hounsfield单位为950，来扫描CT图像。然而，我应用这种方法后，发现效果并不好。然后我标记了一些例子来训练一个U-net，发现效果不错，但是我的本地CV值没有丝毫提升。我的猜测是，因为肺部出现问题，进行扫描得到数据集中的许多病例，因此很多肺气肿样本没有看做是肺结节和癌症病例。我不能确定这个想法的正确性。 最后一步：癌症预测训练好网络后，下一步是让神经网络检测结节并估计其恶化程度。我建立的CT结节观察器很容易查看网络结果。我觉得神经网络的效果很好：它检测到了许多我完全忽视的结节，而我只看到很少量的假阳性结节。但是还存在一个严重的问题：由于它错过了一些非常大的明显结节，所以影响了对于假阴性的得分，有时使LogLoss升高了3.00。作为尝试，我试图对CT图像进行了两次降采样，看看检测器是否会检测大结节。值得注意的是，它的效果非常好。因此，我调整了网络结构，让网络预测3个尺度，分别为1，1.5和2.0。我觉得值得花这么多的时候来改善这方面的性能。 图4：在缩放1x的左图中，没有很好地检测到大结节；但是在2x放大的右图时，效果较好。矩形的大小表示坚持到的恶性肿瘤。 鉴于这些数据和一些其他特征，我想训练一个以梯度推进的分类器来预测一年内癌症的发病率，这是比较容易实现的。但是问题在于，排行榜的得分是基于给定的200名患者，这里面意外地包含了大量异常患者。经过一些调整后，我通过交叉验证得到了本地的平均值为0.39-0.40，而排行榜得分在0.44和0.47之间变化。此时很难将排行榜得分与本地CV值相关联。提高了本地CV值可能导致LB评分的降低，反之亦然。 我花了很多时间来研究本地CV值和LB评分的关系。我没有成功，所以我只使用能同时改进CV值和LB排名的技巧和特征。这是一场两阶段的比赛，而且与实际的训练集相比，第二阶段的数据存在与LB数据集更相似的可能。在这个地方，很多队伍也只能碰运气，结果显示排行榜上的很多队伍模型处于过拟合状态。最后我只使用7个特征来训练梯度推进器，分别是3个尺度下的最大恶性结节及其Z轴的位置和样本中奇怪组织的数量。 我也融合了两个模型来提高效果。第一个模型是在完整的LUNA16数据集上训练的。第二次，我试图从NDSB训练集中选择明显的阳性样本和假阳性样本，应用主动学习来训练。由于我不是放射科医师，所以我为了保险起见，只选择癌症病例的阳性例子和非癌症病例的阴性例子。我做错了，因为第二个模型比没有额外标注的LUNA16模型更糟糕。通过平均两个模型的输出，对LB排名有了很好的推动作用，并且显著提高了本地CV值。 与Daniel合作在进行机器学习比赛时，将不同角度的解决方案组合在一起往往是个好主意。我和Daniel在以前的医疗比赛中一起合作过，知道他是一个非常聪明的人，且他的参赛思路一般和我不同。他是从研究的角度来看问题，而我一般以工程的角度来看问题。当我们合作时，我们确信结合两者互补的方法，能有一个很棒的解决方案。 我们这次组队，一开始就发现两个人对LIDC数据集中的恶性信息有完全相同的观察角度，解决方案也很相似，感到有点失望。不过幸运的是，剩余部分的设计方法完全不同，结合后显著改进了LB排名和本地CV值。下面列举出一些主要的区别。 表3：Julian和Daniel之间设计方法的差异 强强联合是一个很好的选择。虽然我因为LB得分感到担忧，但Daniel觉得应该主要关注本地CV值。所以最终我减少研究本地CV值和LB的匹配关系，并着重于改进本地CV值。在最后的排行榜上，证明这是一个很好的决定，因为在最后，第二阶段的排行榜与本地CV值相当匹配，我们获得了比赛的第二名。尽管有许多队伍，在第一阶段取得了很好的排行榜得分，后来被证明模型过拟合。 总结与感想我们在观察网络对CT图像的结节检测时，模型效果很好。第一阶段，logloss为0.43，公开排行榜的ROC准确率为0.85，第二阶段，logloss为0.40，私人数据集的ROC准确率更高。这让我很兴奋，因为在这个数据集上，我们的模型已经是一位训练有素的放射科医生了。 对于放射科医师来说，这个自动结节检测的模型可能很有帮助，因为在实际判断中，部分结节容易被忽视。模型对肿瘤恶化程度的评估效果也很好，但训练样本量只有1000个，所以应该有很大的改进空间。 在Daniel和我合作的解决方案中，应用了相当多的工程办法，许多步骤和决定是基于经验和直觉来确定的。我们没有足够的时间来准确地验证所有方法的效果。下面提出进一步研究的一些建议。 1. 建立放射科医师基准线。根据一个放射学家在这个数据集上的具体表现，建立一个具有参考意义的基准。 2. 对NDSB数据集的恶性肿瘤标注。在这场比赛中，训练样本只有约1000个结节。输入更多精确标记的例子，肯定进一步提升算法准确度。 3. 尝试更多不同的神经网络结构。我花了很少时间来选择效果最佳的网络结构，可能会错过一些效果更好的结构。 相关资源这次比赛的Kaggle地址（含说明、数据集等）：https&#58;//www.kaggle.com/c/data-science-bowl-2017/ 文中提到的另一个比赛LUNA16：https&#58;//luna16.grand-challenge.org/ 该项目的完整程序请查看GitHub链接：https&#58;//github.com/juliandewit/kaggle_ndsb2017 Dan Hammack也公布了他的代码：https&#58;//github.com/dhammack/DSB2017/ （完） 招聘 量子位正在招募编辑记者、运营、产品等岗位，工作地点在北京中关村。相关细节，请在公众号对话界面，回复：“招聘”。 One More Thing… 今天AI界还有哪些事值得关注？在量子位（QbitAI）公众号会话界面回复“今天”，看我们全网搜罗的AI行业和研究动态。笔芯~ 转载来源：Kaggle百万美元大赛优胜者：用CNN识别CT图像检测肺癌]]></content>
      <categories>
        <category>科技</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>肺癌</tag>
        <tag>癌症</tag>
        <tag>Kaggle</tag>
        <tag>肺气肿</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[和女当事人复原中金黄洁事件始末]]></title>
    <url>%2F2017%2F5ebd8403%2F</url>
    <content type="text"><![CDATA[和女当事人复原中金黄洁事件始末 转载来源：和女当事人复原中金黄洁事件始末]]></content>
      <tags>
        <tag>山石观市</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[干货|提高你设计审美的那些网站（一）]]></title>
    <url>%2F2017%2Fb2a8f283%2F</url>
    <content type="text"><![CDATA[干货|提高你设计审美的那些网站（一） 转载来源：干货|提高你设计审美的那些网站（一）]]></content>
      <tags>
        <tag>一个干货</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[中国金融圈里的 9 大派系（史上最全，推荐收藏）]]></title>
    <url>%2F2017%2F61a03c30%2F</url>
    <content type="text"><![CDATA[中国金融圈里的 9 大派系（史上最全，推荐收藏） 转载来源：中国金融圈里的 9 大派系（史上最全，推荐收藏）]]></content>
      <tags>
        <tag>华尔街见闻</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[个人的中小型项目前端架构浅谈]]></title>
    <url>%2F2017%2Fcbf020a5%2F</url>
    <content type="text"><![CDATA[一、为什么要有一个好的架构 首先明确一点，架构是为需求服务的。前端架构存在的目的，就我个人理解来说，有以下几点： 1. 提高代码的可读性 一个好的架构，代码的可读性一定是很强的。 简单来说，假如有一个新人加入团队，那么他接手这个项目，一定是容易上手的，能简单轻松的了解整个前端部分的相互关系，从而找到自己需要重点关注的点。而不是需要花很多时间去熟悉这个项目的很多细节，才能开始上手做东西。 就文件来说，可以从文件名上，分清哪些是页面、哪些是逻辑、哪些是样式、哪些是可以复用的组件、哪些是图标组、又有哪些是移动端或是PC端专享的样式/逻辑等。 就代码来说，包括统一的命名风格，封装在同一个文件里的代码的相关性足够强等。 2. 提高代码的可维护性 一个好的架构，一定是易于维护的，例如在新增需求、更改需求、修正bug，都不会造成意料之外的变化，比如说修改了一个页面组件的内容，却导致另外一个页面组件发生变化（这也太坑了）。因此，要低耦合，高内聚，以及输入和输出是可预期的。 3. 提高代码的可扩展性 一个好的架构，一定扩展性要强，不能写死。 需求变更太TM正常了，新增需求也太TM正常了。因此好的架构，必须要考虑到这些情况的发生，因为这些是一定会发生的。所以，一定要避免把代码写死。 比如页面组件A里需要有一个日历组件，而这个日历组件引用的是别人的（比如从github上找的）。那么尽量不要直接在页面组件A里面直接引用这个日历组件，而是将写一个日历组件B，在这个日历组件B里封装你引用的日历组件C，然后通过这个日历组件B来进行操作。 原因很简单，假如某天产品经理说，这个日历组件太丑了，我们换一个吧。如果你直接在页面组件A里内嵌这个引用的日历组件C，你很可能就要改很多代码（因为不同日历组件的使用方法和暴露的接口可能不同）。假如你还在其他多个地方引用了这个日历组件，那就更糟糕了！每个地方都要改。 而若是将引用的日历组件C封装到自己写的日历组件B之中，那么你只需要改日历组件B里的相应代码即可，而因为日历组件B暴露的接口是不变的，那么自然不用修改页面中的代码了。 附图，以日历组件为例，是否考虑到扩展性的结果 未考虑到扩展性：考虑到扩展性： 4. 便于协同 包括前端和后端的协同，前端和前端之间的协同。 具体来说，前后端的协同通常是以ajax为交互，那么应至少有一个用于专门封装了所有ajax请求的文件，所有ajax请求都封装在这里。在开发时，这里封装的方法应该可以模拟发送和接收约定好的交互内容，方便开发联调。 而前端和前端的协同，主要体现在同时在更改代码时，不会影响对方代码的正常运行。因此要求封装、解耦以及低干扰度是必须的。 5. 自动化 自动打包，压缩，混淆，如果有必要，再加上自动单元测试。 总结：总结来说，一个好的架构的目的是，让前端写代码写的舒服，让后端联调的舒服，让产品经理改需求改的舒服。 二、我如何设计架构 我不敢说自己的架构是好的架构（显然不是啦），只能分享自己最近做的一个项目，它的架构的如何做的。 首先，确定需求：1、一个中小型网站，同时面向移动端和PC端（单端大概15个页面，算上弹窗大约20个）； 2、预算有限（给的钱少），开发时间有限（一个月）； 3、可能存在一定程度上的需求变更（比如增加页面或修改某些页面内容）； 4、客户可能不太在乎优化（但是我自己在乎啊）； 5、要求兼容IE9以上。 其次开始决定：1、兼容IE9以上说明可以使用主流框架，而无需必须使用jQuery。因此我采用了vue，版本是2.0； 2、预算有限，时间有限，因此PC端和移动端共html和js，独立css； 3、页面有限，因此无需将架构层级划分的比较细，只需要按其类型划分即可； 4、根据原型图来看，页面复杂程度有限，复用部分不是很多，因此可以确定哪些东西需要封装复用，哪些比较复杂需要独立封装，哪些比较简单为了简化开发难度可以直接耦合； 5、自己比较熟练单页面网站，因此采用以单页面为主，异步加载其他页面的形式。 于是使用相关配套的东西，比如webpack，vue-router等，另外为了开发和生产的方便性，采用以下模式进行开发。 第三，划分功能：首先有一个根html，用户需要通过访问它来加载我们的js逻辑，因此js逻辑的代码被写在main.js之中。 在main.js之下，我们的前端代码可以被划分为三部分： 组件树- 功能模块- 各种资源功能模块 如下图： 功能划分好之后，相同功能的放在同一个文件夹下，命名风格应该类似。 具体来说，组件树相关的东西，通常是以.vue结尾，放置在components文件夹下；资源，有图片或者国际化资源等，以.png或者.js或.json结尾，放置在resources文件夹下；而功能插件、服务等，因为可能被多处引用，因此为了方便引用，放在src文件夹下，并且该文件夹是components文件夹和resources文件夹的上级文件夹。 第四，细化功能模块：功能、组件树以及资源，我们已经明确了有哪些东西，那么接下来，我们要明确这些东西该如何以文件的形式来划分。 如下图： 1. 项目构建相关 因为要使用vue.js，也要使用es6语法，因此babel是必须的；- 又因为要自动化混淆打包，因此webpack也是必须的；- 最后因为要方便多人协同，因此npm的package.json的配置，方便不同人可以快速自动化通过npm install来安装依赖，也是必须的。又因为要自动化混淆打包，因此webpack也是必须的； 2. CDN相关而又因为我们要采用外部字体（需求要求引入非常见字体），因此CDN加速是必须的，该字体文件放在html中来配置引用即可。 3. 配置和插件 我们需要直接引入一些插件和配置文件；- 为了使用vue，我们需要一个根组件，那么就是App.vue；- 使用vue-router，我们需要配置路由文件，因此router-config.js这个路由配置也是必须的；- 然后我们还需要以插件形式引入一些功能和服务，因此有了Plugin-开头的若干个vue插件，这些都是根据需要封装好的低耦合高内聚方法；为了使用vue，我们需要一个根组件，那么就是App.vue； 然后我们还需要以插件形式引入一些功能和服务，因此有了Plugin-开头的若干个vue插件，这些都是根据需要封装好的低耦合高内聚方法； 4. 需要的npm依赖当然，要使用vue肯定要引入vue.js，类似的还有vue-router.js和各种兼容性polyfill和全局插件。 5. 抽离出的功能模块 除了直接引用的这些插件，我们还有一些和项目高度耦合的功能服务，我认为不能作为插件，但依然需要抽离出来封装好，方便使用和修改；- 如封装ajax请求的ajax.js，所有的ajax请求都放置其中，只对外暴露接口，方便管理和使用；- 又如实时国际化功能的组件LanguageManager.js，他需要引入国际化资源和管理国际化资源的加载；- 又例如实现跨组件通信的event-bus.js；- 又比如管理用户信息的user.js。如封装ajax请求的ajax.js，所有的ajax请求都放置其中，只对外暴露接口，方便管理和使用； 又例如实现跨组件通信的event-bus.js； 总结：而这些划分，都体现在上图之中。这就是src目录下的功能模块文件，我们需要的绝大多数功能都可以包括在其中，我们只需要按照实际开发中的需要，将对应的功能写入在这些文件中并引用即可。 第五，组件树：之前谈了功能模块的划分，接下来是组件树。 因为是中小型页面，因此组件树的层级无需太深，但该抽离出来的依然还是要抽离，尽量保证抽离出来的组件解耦以及一个页面组件的逻辑不要太多。如下图： 0. 根组件所有组件最终往上找，都会找到共同的根组件App.vue，根组件只负责管理他的直接子组件。 每个组件都只负责管理自己的直接子组件，不跨级管理，并且不依赖于自己的子组件（否则可能因为子组件的未加载或错误而导致父组件错误），做到解耦和内聚。 1. 弹窗dialog和弹窗tips因为弹窗dialog和弹窗提示tips可能同时存在，因此将其划分为2个组件，方便管理。 2. 未登录页面和登录页面因为页面存在登录和未登录状态，而为了加载速度考虑，当未登录时，不加载已登录页面，因此需要划分出来，并进行异步加载处理。 3. 未登录页面未登录页面又分为三种情况： 初始页面：毫无疑问要直接加载- 登录弹窗：点击登录时加载（异步）- 注册弹窗：点击注册时加载（异步）登录弹窗：点击登录时加载（异步） 之所以分拆开，是因为根据需求，已登录用户刷新页面，可以直接进入登录后页面，因此无需登录和注册，这种处理可以减少流量消耗，提升加载页面加载速度（特别是注册弹窗需要加载的内容还比较多）。 4. 已登录页面已登录页面有较多页面，采用默认加载初始页，然后异步加载其他页面（访问时）。 5. 弹窗dialog由于逻辑较少，代码量不多，因此为了方便管理，统一将其合并在一个vue文件中，共同相同的打开逻辑，根据传递的key决定打开哪一个。这样在新增弹窗时，无需再去写弹窗的打开、关闭逻辑。 假如有较复杂的弹窗，可以以子组件的形式引入到当前vue文件中，如此也方便管理； 6. 国际化管理和页面高耦合，负责加载对应的国际化资源，并进行切换管理。 7. 页面组件可能有子页面和复用的组件，按照正常方式引用即可。 8. 样式文件可以独立写为.css文件，但因为我的公共样式文件比较少，因此我还是将其放在一个.vue文件中，并在App.vue里来引用。 9. 页面组件起名 通常以.vue为结尾，除了国际化LanguageManager.js因为高耦合度，因此以.js结尾并是一个单独的vue实例，表示他更像是一个功能模块，而不是一个vue的页面组件；- 基础页面，如登录和未登录页面，公共组件（并且是header和footer这种），以base-开头；- 弹窗统一以box-为开头；- 可复用的组件以extend-开头；- 引入的外部组件以import-开头；- 普通页面组件以page-开头（这些页面往往是一个独立的页面，并且挂靠在登录或未登录页面下）；- 注册弹窗因为逻辑比较复杂，并且同类较多，因此以register-为开头。基础页面，如登录和未登录页面，公共组件（并且是header和footer这种），以base-开头； 可复用的组件以extend-开头； 普通页面组件以page-开头（这些页面往往是一个独立的页面，并且挂靠在登录或未登录页面下）； 通过以文件名来划分，不同的页面组件之间的区分可以说是一目了然，同时也方便管理。 本文来自CSDN博客：http&#58;//blog.csdn.net/qq20004604/article/details/70480932 转载来源：个人的中小型项目前端架构浅谈]]></content>
      <categories>
        <category>科技</category>
      </categories>
      <tags>
        <tag>路由器</tag>
        <tag>CSS</tag>
        <tag>HTML</tag>
        <tag>DIALOG</tag>
        <tag>腾讯TM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[webpack+vuejs+element打造大型web项目]]></title>
    <url>%2F2017%2F3900e45f%2F</url>
    <content type="text"><![CDATA[webpack+vuejs+element打造大型web项目 转载来源：webpack+vuejs+element打造大型web项目]]></content>
  </entry>
  <entry>
    <title><![CDATA[用Python实现优先级队列的3种方法]]></title>
    <url>%2F2017%2Fac420b8c%2F</url>
    <content type="text"><![CDATA[你能想出几种方法来在Python中实现优先级队列？阅读下面文章并找出Python标准库提供了哪些方案？ 优先级队列是一种容器型数据结构，它能管理一队记录，并按照排序字段（例如一个数字类型的权重值）为其排序。由于是排序的，所以在优先级队列中你可以快速获取到最大的和最小的值。 你可以认为优先级队列是一种修改过的普通队列：普通队列依据记录插入的时间来获取下一个记录，优先级队列依据优先级来获取下一个记录，而优先级取决于排序字段的值。 优先级队列经常用来解决调度问题，比如给更紧急的任务更高的优先级。 我们以操作系统的任务调度为例：高优先级的任务（比如实时游戏）应该先于低优先级的任务（比如后台下载软件更新）执行。通过在优先级队列中依据任务的紧急程度排序，我们能让最紧急的任务优先得到执行。 我们下面讲解几种实现优先级队列的方法，有的使用的是Python的内置数据结构，有的使用的是Python标准库提供的数据结构。两种情况各有千秋，在我的心中其中有一种方法在绝大多数时候都是最优的方案，当然你也需要自己去判断哪个才是你最需要的。 手动维护排序列表 使用排序列表你可以快速地获取或删除最大的或者最小的元素，缺点是向列表中插入元素是一个很慢的操作，复杂度在O(n)。 不过标准库提供的bisect.insort方法让你能够在O(log n)的时间里找到需要插入的位置，这帮缓慢的插入操作改善了部分性能。 如果是把元素放到队列的最后，然后直接重新排序，那么复杂度是O(nlog n)。 所以排序列表的这种实现方法，只有在插入操作很少的时候才合适。 heapq模块 heapq是一个二叉堆的实现，它内部使用内置的list对象，它无论插入还是获取最小元素复杂度都在O(log n)。 这个模块是实现优先级队列的一个很好的选择。 由于heapq只是提供了一个最小的堆实现，所以为了让它成为实用的优先级队列，还需要添加很多额外的代码来保证顺序，和提供其他必不可少的功能。 queue.PriorityQueue类 这个优先级队列内部使用了heapq，所以它的时间复杂度和heapq是相同的。 不同的是PriorityQueue的操作是同步的，提供锁操作，支持并发的生产者和消费者。 依据使用场景，它可能很有用，也可能有点太大了。通常来说它的基于类接口要比heapq的基于函数的接口更友好。 一个比较好的默认选项 在你的程序中应该使用哪一个优先级队列的实现呢？上面三种实现都有自己的使用场景，但是在我心里queue.PriorityQueue是最好的选择。 确实有时候它显得有点过重了，但是我很看重它的面向对象的接口，以及一个清晰地表意的名字。 译者：诗书塞外英文原文：https&#58;//dbader.org/blog/priority-queues-in-python#. 转载来源：用Python实现优先级队列的3种方法]]></content>
      <categories>
        <category>科技</category>
      </categories>
      <tags>
        <tag>软件</tag>
        <tag>编程语言</tag>
        <tag>Python</tag>
        <tag>科技</tag>
        <tag>数据结构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[想赶上机器学习的火车，你的企业现在应该怎么做？]]></title>
    <url>%2F2017%2F2d69e787%2F</url>
    <content type="text"><![CDATA[雷锋网按：本文为「范式大学系列课程」第 2 篇文章：机器学习老司机：如何成为 ML-ready 的公司？ 机器学习已经在商业领域展示了巨大潜力，那么管理者如何将其纳入日常决策和长期规划？一个公司怎样才能 ML-ready？ 当你考虑在企业中应用机器学习技术时，很多问题就会出现。 我的业务是否适合机器学习模型？- 我可以从机器学习模型中获得什么收益？- 这是一个降低成本的问题，还是增加收入的问题？- 我现在的数据积累足够吗，如果不够的话该怎么办？- 我需要什么样的人才帮助我实现企业人工智能的升级？ - 换句话说，如果你的企业想赶上机器学习的火车，现在应该怎么做？我可以从机器学习模型中获得什么收益？ 我现在的数据积累足够吗，如果不够的话该怎么办？ 换句话说，如果你的企业想赶上机器学习的火车，现在应该怎么做？ 先给你一张信息表，然后我们会从 6 个步骤详细解析。 步骤一：定义问题 应用机器学习的公司一般有两种： 一种是以机器学习模型作为企业核心业务的公司，例如今日头条、News in Palm；- 另一种是通过机器学习增强现有业务流程的公司，例如抱抱通过机器学习优化主播推荐。另一种是通过机器学习增强现有业务流程的公司，例如抱抱通过机器学习优化主播推荐。 对于后一种公司，清楚的定义问题会是第一个挑战。无论是个性化推荐、增加活跃度还是降本增收，都应该收敛到一个点，即我们可以通过获得正确的数据把任务变成机器学习可解决的问题。 例如，如果你想通过数据发现“高流失风险”的客户，以此降低用户的流失率，这就是机器学习可以解决的问题。你会拥有已经流失的用户（这就是机器学习的标签），流失行为背后相关的数据（例如社交媒体的活动、使用频率等），那就可以通过机器学习算法找到用户流失和用户行为之间的隐藏关系。 当然，这里面更重要的问题是，当你知道这个用户将要流失时，你准备做些什么？机器学习可以告诉你使用什么样的挽留策略能拉回他。 另一个例子是提高用户满意度。用户满意度是一个主观的指标，不同的人、场合对用户满意度的衡量标准都不一样。如果要通过机器学习来预测用户满意度，最终的结果可能就会不理想。 定义机器学习的问题，最终可以落在两个点上： 1、从业务出发，机器学习往往致力于解决标准商业逻辑和系列规则不能解决的问题。所以在考虑是否需要机器学习的时候，不妨问问自己，当你做决策的时候，有多经常是基于经验假设而非清晰的分析论据？ 2、从技术出发，机器学习往往需要客观的预测指标，例如流失率、点击率、停留时长等。同时你也需要考虑数据反馈的周期，例如在信用卡反欺诈的任务中，盗刷后被用户发现并提交反馈的时间往往需要 1 周甚至 1 个月，那么系统就要考虑到负面反馈的时间。 通过机器学习强化业务流程是一个非常广泛的领域，我们可以在内容推荐、金融反欺诈、医疗健康等各行各业都看到它的身影。 步骤二：强化业务流程 当你建立了机器学习模型，下一步便是结合模型强化业务流程。一般来说会有三个层次： 1、描述 采集数据进行机器学习分析，通过图表和报告描述现状 2、预测 找到业务发展的模式，做出预测 3、行动 结合模型预测，给出不同的解决方案 麦肯锡曾经披露了一家国际银行的故事，他们通过机器学习改进违约客户相关的业务流程。通过机器学习模型，他们发现有一群平时白天使用信用卡的客户，在晚上也在大量使用信用卡。机器学习发现该行为模式和违约风险紧密相关，在进一步的问询后发现，这群人正在经历某些紧张的时刻。银行的解决方案是向这群高风险的人提供财务建议，并为他们建立新的信用额度。 步骤三：确保你的数据质量足够好 机器学习是关于数据的科学，它从数据中获得有价值的洞察。一般来说，使用机器学习辅助决策是避免偏见的好方法，但这比想象的更为棘手，因为它不能避免数据本身的偏见。例如 Google 最近陷入了一起争议，在对男人和女人的广告中，他们在男人的广告中展示了更多高级岗位。Google 的数据科学家并没有性别歧视，但算法背后的数据是有偏见的，因为它是从社交网络的互动中收集上来的。 确保数据质量足够好 基本可以说，你所拥有的数据质量，定义了算法的质量。数据可能是嘈杂的、冲突的、有偏见的和缺失的，这会对问题解决有非常不良的影响。为了优化模型开发，你需要让数据更匹配要解决的问题，所以在早期最好有熟悉业务的数据科学家支持，逐步开发和收集解决问题所需的数据。不过这里需要注意的是，尽管业务决策者寻求的是具体建议和结果预测，但数据科学家往往只能提供相关的数据特征。只有真正把数据投入到机器学习系统，才能知道最终的结果会怎么样。 确定最小预测准确度 我们需要定义最小的预测准确度。不同的业务会有不同的准确度要求，例如在涉及医疗的业务中，有些任务需要高达 95% 以上的预测准确度。而在一个预测飞机票价的算法中，预测准确度高于 75% 就足以支持客户的预定任务。 打破数据孤岛，匿名化并共享数据 数据科学家小组经常面临一个障碍，在项目的谈判阶段就需要获取数据。对于业务人员来说，了解成本是决定是否开展机器学习业务的关键因素，但在看不到实际数据的情况下，几乎不可能准确估计预测准确度水平和实施价格，这往往是谈判瘫痪的原因。企业高管不能将商业敏感数据交给技术公司，而技术公司在获得数据之前几乎无法给出明确的答案。 我们的解决方案是提供数据子集而不是整个数据库，并将其匿名化。对于拥有数据科学家的公司，在不同的部门之间共享数据也是共同的管理挑战。过度管制的数据策略，或者仅仅在各部门囤积数据，会大大减缓数据分析的进程。这就是为什么要在更高层面给数据科学家和技术公司权限的原因。 好消息：即便数据不够好，它可以修复 即便你的数据集是凌乱的而非结构化，也有办法获得好的结果。今天，数据科学家已经准备好在起步阶段应用一些方法，重组、清洗数据集，并进一步优化得到更好的建模效果。 但坏消息是，数据科学家可能需要相当长的时间完成数据清洗并进行到建模阶段。如果你没有专业知识，是否应该提前自己处理？一般来说是否定的，因为即便自己做了，最后的数据集也可能需要重新处理。 步骤四：弥合技术和商业愿景之间的差距 如果你问数据科学家最喜欢的算法，你可能会听到决策树、神经网络、逻辑回归、Kernel 方法、主成分分析等。但是这些算法如何和商业愿景结合起来？你会需要一个懂得业务和基本数据分析知识的人，他能够在业务流程中找到机器学习能够起作用的指标，领导数据科学计划，扩大机器学习应用场景的选择，调整业务和技术的愿景。 一般来说有四种方法： 1、建立机器学习团队 机器学习科学家的价格要比普通程序员高很多。当你打算建立一个机器学习的团队时，一定要给他足够的支持，因为他需要创造性的工作才能发挥作用，而这往往会和很多组织的结构发生冲突。 2、公司内专家 + 机器学习平台 你可以使用公司已有的业务专家，在 1-2 个数据科学家的帮助下，就可以通过机器学习平台解决问题。这些平台往往拥有友好的界面，公司内部的业务专家可以通过短时间的培训学习如何使用，这样你就可以把数据计划扩展到更大的专家组，解决更多的公司业务问题。利益相关，我们推荐自家的产品：第四范式先知平台。 3、机器学习解决方案公司 现在市面上已经有一些机器学习解决方案公司了，但机器学习和传统的编程不同，因为它需要克服信任的门槛。机器学习解决方案的任务面临的挑战是共享数据。根据拥有的数据类型，也许你需要以某种方法匿名化，隐藏敏感信息，例如客户联系人和他们的位置。当然，当你匿名化的时候，你也要接受解决方案公司会难以使用外部数据来丰富数据集以得到更好的建模结果。 4、和大学院校、研究机构合作 大学院校、研究机构已经有很多数据科学的研究生和博士，他们大多拥有建立机器学习模型的能力。不过和高校研究机构合作的费用一般会比较贵。 步骤五：模型过时了，需要更新 大多数的机器学习模型是在静态数据子集上开发的。一旦部署了模型，它们将会随着时间的推移而变得过时，预测也会变得不准确。根据业务环境的变化，你应该在一段时间后更换模型，或者重新培训，一般来说会有两种基本方法： A/B测试：一个新的模型会被引入和旧的模型竞争。当新的模型超过了旧的模型，旧的模型就会被替代。这个过程将会一直重复。 在线更新：模型的参数会随着连续性的新数据流而变化。 因此，如果你希望机器学习的分析保持在稳定的水平，一定要及时更新机器学习的模型。 步骤六：是否需要定制的算法 定制的算法会有一些好处，例如它能够更匹配你的数据集和要解决的问题，训练的速度也会更快。但相对应的，它的开发和进一步迭代都价格不菲。所以如果你是一个大型企业，你可以考虑采用定制算法；如果你是中小型的企业，定制算法会带来严重的财务和管理负担。 实际上，如果是常见的预测任务，那么现成的算法模型是可行的。通过一些成熟的算法，集成好的机器学习软件，你可以很轻松的部署机器学习系统，快速解决业务流程中的问题。 无论你最终是否决定定制算法，我们都建议你先用成熟的算法试一试。 参考文章： Developing Machine Learning Strategy for Business in 7 Steps,altexsoft. How to Make Your Company Machine Learning Ready,hbr. 「范式大学」由第四范式发起，致力于成为“数据科学家”的黄埔军校。「范式大学系列课程」会和大家推荐戴文渊、杨强、陈雨强等机器学习领域顶尖从业人士的最新分享，以及由第四范式产品团队推荐和整理的机器学习材料。 转载来源：想赶上机器学习的火车，你的企业现在应该怎么做？]]></content>
      <categories>
        <category>科技</category>
      </categories>
      <tags>
        <tag>软件</tag>
        <tag>机器学习</tag>
        <tag>Google</tag>
        <tag>大学</tag>
        <tag>杨强</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[零基础写python爬虫之使用Scrapy框架编写爬虫 - 新客网]]></title>
    <url>%2F2017%2F84a01932%2F</url>
    <content type="text"><![CDATA[零基础写python爬虫之使用Scrapy框架编写爬虫 - 新客网 转载来源：零基础写python爬虫之使用Scrapy框架编写爬虫 - 新客网]]></content>
  </entry>
  <entry>
    <title><![CDATA[机器学习和深度学习引用量最高的20篇论文：2014-2017]]></title>
    <url>%2F2017%2F491d612b%2F</url>
    <content type="text"><![CDATA[选自Kdnuggets 作者：Thuy T. Pham 机器之心编译 参与：邵明、黄小天 机器学习和深度学习的研究进展正深刻变革着人类的技术，本文列出了自 2014 年以来这两个领域发表的最重要（被引用次数最多）的 20 篇科学论文，以飨读者。 机器学习，尤其是其子领域深度学习，在近些年来取得了许多惊人的进展。重要的研究论文可能带来使全球数十亿人受益的技术突破。这一领域的研究目前发展非常快，为了帮助你了解进展状况，我们列出了自 2014 年以来最重要的 20 篇科学论文。 我们筛选论文的标准是来自三大学术搜索引擎谷歌学术（scholar.google.com）、微软学术（academic.microsoft.com）和 semanticscholar.org 的引用量。由于不同搜索引擎的引用量数据各不相同，所以我们在这里仅列出了微软学术的数据，其数据比其它两家稍低一点。 我们还给出了每篇论文的发表时间、高度有影响力的引用数量（HIC）和引用速度（CV），以上数据由 semanticscholar.org 提供。HIC 表示了以此为基础的论文情况和与其它论文的关系，代表了有意义的引用。CV 是最近 3 年每年引用数量的加权平均。有些引用的 CV 是 0，那是因为 semanticscholar.org 上没有给出数据。这 20 篇论文中大多数（包括前 8 篇）都是关于深度学习的，但同时也很多样性，仅有一位作者（Yoshua Bengio）有 2 篇论文，而且这些论文发表在很多不同的地方：CoRR (3)、ECCV (3)、IEEE CVPR (3)、NIPS (2)、ACM Comp Surveys、ICML、IEEE PAMI、IEEE TKDE、Information Fusion、Int. J. on Computers &amp; EE、JMLR、KDD 和 Neural Networks。前 2 篇论文的引用量目前远远高于其它论文。注意第 2 篇论文去年才发表！要了解机器学习和深度学习的最新进展，这些论文一定不能错过。 1. 论文：Dropout：一种防止神经网络过拟合的简单方法（Dropout&#58; a simple way to prevent neural networks from overfitting） 链接：http&#58;//suo.im/3o6l4B- 作者：Hinton, G.E., Krizhevsky, A., Srivastava, N., Sutskever, I., &amp; Salakhutdinov, R. (2014). Journal of Machine Learning Research, 15, 1929-1958.- 数据：引用：2084、HIC：142、CV：536- 摘要：其关键思想是在神经网络的训练过程中随机丢弃单元（连同它们的连接点）。这能防止单元适应过度，显著减少过拟合，并相对于其它正则化方法有重大改进。作者：Hinton, G.E., Krizhevsky, A., Srivastava, N., Sutskever, I., &amp; Salakhutdinov, R. (2014). Journal of Machine Learning Research, 15, 1929-1958. 摘要：其关键思想是在神经网络的训练过程中随机丢弃单元（连同它们的连接点）。这能防止单元适应过度，显著减少过拟合，并相对于其它正则化方法有重大改进。 2. 论文：用于图像识别的深度残差学习（Deep Residual Learning for Image Recognition） 链接：http&#58;//suo.im/1JrYXX- 作者：He, K., Ren, S., Sun, J., &amp; Zhang, X. (2016). CoRR- 数据：引用：1436、HIC：137、CV：582- 摘要：目前的深度学习网络层数越来越多，越来越难以训练，因此我们提出了一种减缓训练压力的残差学习框架。我们明确地将这些层重新定义为与输入层有关的学习残差函数，而不是学习未被引用的函数。与此同时，我们提供了全面的经验证据以表明残差网络更容易优化，并可通过增加其层数来提升精确度。作者：He, K., Ren, S., Sun, J., &amp; Zhang, X. (2016). CoRR 摘要：目前的深度学习网络层数越来越多，越来越难以训练，因此我们提出了一种减缓训练压力的残差学习框架。我们明确地将这些层重新定义为与输入层有关的学习残差函数，而不是学习未被引用的函数。与此同时，我们提供了全面的经验证据以表明残差网络更容易优化，并可通过增加其层数来提升精确度。 3. 论文：批标准化：通过减少内部协移加速深度神经网络训练（Batch Normalization&#58; Accelerating Deep Network Training by Reducing Internal Covariate Shift） 链接：http&#58;//suo.im/3sJtk1- 作者：Sergey Ioffe, Christian Szegedy (2015) ICML.- 数据：引用：946、HIC：56、CV：0- 摘要：训练深度神经网络的过程很复杂，原因在于每层的输入分布随着训练过程中引起的前面层的参数变化而变化。我们把这种现象称为内部协变量转移（internal covariate shift），并可利用归一化层输入来解决此问题。通过将此方法应用到最先进的图像分类模型，批标准化在训练次数减少了 14 倍的条件下达到了与原始模型相同的精度，这表明批标准化具有明显的优势。作者：Sergey Ioffe, Christian Szegedy (2015) ICML. 摘要：训练深度神经网络的过程很复杂，原因在于每层的输入分布随着训练过程中引起的前面层的参数变化而变化。我们把这种现象称为内部协变量转移（internal covariate shift），并可利用归一化层输入来解决此问题。通过将此方法应用到最先进的图像分类模型，批标准化在训练次数减少了 14 倍的条件下达到了与原始模型相同的精度，这表明批标准化具有明显的优势。 4. 论文：利用卷积神经网络进行大规模视频分类（Large-Scale Video Classification with Convolutional Neural Networks） 链接：http&#58;//suo.im/25lfXF- 作者：Fei-Fei, L., Karpathy, A., Leung, T., Shetty, S., Sukthankar, R., &amp; Toderici, G. (2014). IEEE Conference on Computer Vision and Pattern Recognition- 数据：引用：865、HIC：24、CV：239- 摘要：针对图像识别问题，卷积神经网络（CNN）被认为是一类强大的模型。受到这些结果的激励，我们使用了一个包含 487 个类别、100 万 YouTube 视频的大型数据集，对利用 CNN 进行大规模视频分类作了一次广泛的实证评估。作者：Fei-Fei, L., Karpathy, A., Leung, T., Shetty, S., Sukthankar, R., &amp; Toderici, G. (2014). IEEE Conference on Computer Vision and Pattern Recognition 摘要：针对图像识别问题，卷积神经网络（CNN）被认为是一类强大的模型。受到这些结果的激励，我们使用了一个包含 487 个类别、100 万 YouTube 视频的大型数据集，对利用 CNN 进行大规模视频分类作了一次广泛的实证评估。 5. 论文：Microsoft COCO：语境中的通用对象（Microsoft COCO&#58; Common Objects in Context） 链接：http&#58;//suo.im/DAXwA- 作者：Belongie, S.J., Dollár, P., Hays, J., Lin, T., Maire, M., Perona, P., Ramanan, D., &amp; Zitnick, C.L. (2014). ECCV.- 数据：引用：830、HIC：78、CV：279- 摘要：我们展示了一个新的数据集，通过将对象识别问题放入更广泛的场景理解问题的语境中，以推进当前对象识别领域中最先进的技术。我们的数据集包含了 91 种对象类型的照片，这些图片对于一个 4 岁大的孩子而言，很容易识别。最后，我们利用可变形部件模型（DPM）为边界框和分割检测结果提供了一个基线性能分析。作者：Belongie, S.J., Dollár, P., Hays, J., Lin, T., Maire, M., Perona, P., Ramanan, D., &amp; Zitnick, C.L. (2014). ECCV. 摘要：我们展示了一个新的数据集，通过将对象识别问题放入更广泛的场景理解问题的语境中，以推进当前对象识别领域中最先进的技术。我们的数据集包含了 91 种对象类型的照片，这些图片对于一个 4 岁大的孩子而言，很容易识别。最后，我们利用可变形部件模型（DPM）为边界框和分割检测结果提供了一个基线性能分析。 6. 论文：使用场景数据库学习场景识别中的深层特征（Learning deep features for scene recognition using places database） 链接：http&#58;//suo.im/2EOBTa- 作者：Lapedriza, À., Oliva, A., Torralba, A., Xiao, J., &amp; Zhou, B. (2014). NIPS.- 数据：引用：644、HIC：65、CV：0- 摘要：我们引入了一个以场景为中心的新数据库，这个数据库称为「Places」，里面包含了超过 700 万个标注好了的场景。我们提议使用新方法去比较图像数据集的密度和多样性，以表明 Places 与其它场景数据库一样密集并更具多样性。作者：Lapedriza, À., Oliva, A., Torralba, A., Xiao, J., &amp; Zhou, B. (2014). NIPS. 摘要：我们引入了一个以场景为中心的新数据库，这个数据库称为「Places」，里面包含了超过 700 万个标注好了的场景。我们提议使用新方法去比较图像数据集的密度和多样性，以表明 Places 与其它场景数据库一样密集并更具多样性。 7. 论文：生成对抗网络（Generative adversarial nets） 链接：http&#58;//suo.im/3YS5F6- 作者：Bengio, Y., Courville, A.C., Goodfellow, I.J., Mirza, M., Ozair, S., Pouget-Abadie, J., Warde-Farley, D., &amp; Xu, B. (2014) NIPS.- 数据：引用：463、HIC：55、CV：0- 摘要：通过对抗过程，我们提出了一个评估生成模型的新框架。在此框架中，我们同时训练两个模型：生成模型 G 捕获数据分布；判别模型 D 评估样本示来自训练数据集（而不是来自 G 中）的概率。作者：Bengio, Y., Courville, A.C., Goodfellow, I.J., Mirza, M., Ozair, S., Pouget-Abadie, J., Warde-Farley, D., &amp; Xu, B. (2014) NIPS. 摘要：通过对抗过程，我们提出了一个评估生成模型的新框架。在此框架中，我们同时训练两个模型：生成模型 G 捕获数据分布；判别模型 D 评估样本示来自训练数据集（而不是来自 G 中）的概率。 8. 论文：通过内核相关滤波器实现高速跟踪（High-Speed Tracking with Kernelized Correlation Filters） 链接：http&#58;//suo.im/2BBOea- 作者：Batista, J., Caseiro, R., Henriques, J.F., &amp; Martins, P. (2015). CoRR- 数据：引用：439、HIC：43、CV：0- 摘要：大多数的现代追踪器，为应对自然图像中的变化，典型的方法是采用翻译和缩放样本补丁训练分类器。我们针对包含成千上万个翻译补丁数据集提出了一个分析模型。结果表明结果数据矩阵是循环的，我们可以利用离散傅立叶变换对角化已有的循环矩阵，将存储和计算量降低了几个数量级。作者：Batista, J., Caseiro, R., Henriques, J.F., &amp; Martins, P. (2015). CoRR 摘要：大多数的现代追踪器，为应对自然图像中的变化，典型的方法是采用翻译和缩放样本补丁训练分类器。我们针对包含成千上万个翻译补丁数据集提出了一个分析模型。结果表明结果数据矩阵是循环的，我们可以利用离散傅立叶变换对角化已有的循环矩阵，将存储和计算量降低了几个数量级。 9. 论文：多标签学习算法综述（A Review on Multi-Label Learning Algorithms） 链接：http&#58;//suo.im/3LgpGf- 作者：Zhang, M., &amp; Zhou, Z. (2014). IEEE TKDE- 数据：引用：436、HIC：7、CV：91- 摘要：本论文的主要目的是对多标签学习问题进行及时回顾。在多标签学习问题中，一个实例代表一个样本，同时，一个样本与一组标签相关联。作者：Zhang, M., &amp; Zhou, Z. (2014). IEEE TKDE 摘要：本论文的主要目的是对多标签学习问题进行及时回顾。在多标签学习问题中，一个实例代表一个样本，同时，一个样本与一组标签相关联。 10. 论文：深层神经网络特征的可传递性（How transferable are features in deep neural networks） 链接：http&#58;//suo.im/aDLgu- 作者：Bengio, Y., Clune, J., Lipson, H., &amp; Yosinski, J. (2014) CoRR- 数据：引用：402、HIC：14、CV：0- 摘要：我们用实验量化了深层卷积神经网络中每层神经元的一般性与特异性，并报告了一些令人惊讶的结果。可传递性受到两个不同问题的不利影响：（1）以牺牲目标任务的性能为代价，实现更高层神经元对原始人物的专业化，这是预料之中的；（2）与分裂共同适应神经元（co-adapted neuron）之间的网络有关的优化困难，这是预料之外的。作者：Bengio, Y., Clune, J., Lipson, H., &amp; Yosinski, J. (2014) CoRR 摘要：我们用实验量化了深层卷积神经网络中每层神经元的一般性与特异性，并报告了一些令人惊讶的结果。可传递性受到两个不同问题的不利影响：（1）以牺牲目标任务的性能为代价，实现更高层神经元对原始人物的专业化，这是预料之中的；（2）与分裂共同适应神经元（co-adapted neuron）之间的网络有关的优化困难，这是预料之外的。 11. 论文：我们需要数百种分类器来解决真实世界的分类问题吗？（Do we need hundreds of classifiers to solve real world classification problems） 链接：http&#58;//suo.im/2w14RK- 作者：Amorim, D.G., Barro, S., Cernadas, E., &amp; Delgado, M.F. (2014). Journal of Machine Learning Research- 数据：引用：387、HIC：3、CV：0- 摘要：我们评估了来自 17 个「家族」（判别分析、贝叶斯、神经网络、支持向量机、决策树、基于规则的分类器、提升、装袋、堆叠、随机森林、集成方法、广义线性模型、最近邻、部分最小二乘和主成分回归、逻辑和多项回归、多元自适应回归样条法等）的 179 个分类器。我们使用了来自 UCI 数据库中的 121 个数据集来研究分类器行为，这些行为不依赖于所选取的数据集。最终胜出的是使用 R 语言实现的随机森林方法和 C 中使用 LibSVM 实现的带有高斯内核的 SVM。作者：Amorim, D.G., Barro, S., Cernadas, E., &amp; Delgado, M.F. (2014). Journal of Machine Learning Research 摘要：我们评估了来自 17 个「家族」（判别分析、贝叶斯、神经网络、支持向量机、决策树、基于规则的分类器、提升、装袋、堆叠、随机森林、集成方法、广义线性模型、最近邻、部分最小二乘和主成分回归、逻辑和多项回归、多元自适应回归样条法等）的 179 个分类器。我们使用了来自 UCI 数据库中的 121 个数据集来研究分类器行为，这些行为不依赖于所选取的数据集。最终胜出的是使用 R 语言实现的随机森林方法和 C 中使用 LibSVM 实现的带有高斯内核的 SVM。 12. 论文：知识库：一种概率知识融合的网络规模方法（Knowledge vault&#58; a web-scale approach to probabilistic knowledge fusion） 链接：http&#58;//suo.im/3qCSs6- 作者：Dong, X., Gabrilovich, E., Heitz, G., Horn, W., Lao, N., Murphy, K., … &amp; Zhang, W.(2014, August). In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining ACM- 数据：引用：334、HIC：7、CV：107- 摘要：我们引入了一个网络规模的概率知识库，它将网页内容提取（通过文本分析、表格数据、页面结构和人工注释获得）与来自现存知识库中的先验知识相结合，以构建新知识库。我们部署监督学习方法去融合不同的信息源。该知识库比先前发布的任何结构化知识库大得多，并且具有概率推理系统，该概率推理系统能计算事实准确性的校准概率。作者：Dong, X., Gabrilovich, E., Heitz, G., Horn, W., Lao, N., Murphy, K., … &amp; Zhang, W.(2014, August). In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining ACM 摘要：我们引入了一个网络规模的概率知识库，它将网页内容提取（通过文本分析、表格数据、页面结构和人工注释获得）与来自现存知识库中的先验知识相结合，以构建新知识库。我们部署监督学习方法去融合不同的信息源。该知识库比先前发布的任何结构化知识库大得多，并且具有概率推理系统，该概率推理系统能计算事实准确性的校准概率。 13. 论文：用于高维数据的可扩展最近邻算法（Scalable Nearest Neighbor Algorithms for High Dimensional Data） 链接：http&#58;//suo.im/hjTa4- 作者：Lowe, D.G., &amp; Muja, M. (2014). IEEE Trans. Pattern Anal. Mach. Intell.- 数据：引用：324、HIC：11、CV：69- 摘要：我们提出了用于近似最近邻匹配的新算法，并将其与以前的算法进行比较。为了将其扩展到大型数据集（不适合单机的存储处理）上，我们提出了一种分布式最近邻匹配框架，该框架可以与论文中描述的任何算法一起使用。作者：Lowe, D.G., &amp; Muja, M. (2014). IEEE Trans. Pattern Anal. Mach. Intell. 摘要：我们提出了用于近似最近邻匹配的新算法，并将其与以前的算法进行比较。为了将其扩展到大型数据集（不适合单机的存储处理）上，我们提出了一种分布式最近邻匹配框架，该框架可以与论文中描述的任何算法一起使用。 14. 论文：回顾超限学习机的发展趋势（Trends in extreme learning machines&#58; a review） 链接：http&#58;//suo.im/3WSEQi- 作者：Huang, G., Huang, G., Song, S., &amp; You, K. (2015). Neural Networks- 数据：引用：323、HIC：0、CV：0- 摘要：我们的目标是报告超限学习机（ELM）的理论研究和实践进展所处的现状。除了分类和回归，ELM 最近已经被扩展到集群、特征选择、代表性学习和许多其他学习任务。由于其惊人的高效性、简单性和令人印象深刻的泛化能力，ELM 已经被广泛用于各种领域，如生物医学工程、计算机视觉、系统识别、控制和机器人。作者：Huang, G., Huang, G., Song, S., &amp; You, K. (2015). Neural Networks 摘要：我们的目标是报告超限学习机（ELM）的理论研究和实践进展所处的现状。除了分类和回归，ELM 最近已经被扩展到集群、特征选择、代表性学习和许多其他学习任务。由于其惊人的高效性、简单性和令人印象深刻的泛化能力，ELM 已经被广泛用于各种领域，如生物医学工程、计算机视觉、系统识别、控制和机器人。 15. 论文：一份关于概念漂移适应的调查（A survey on concept drift adaptation） 链接：http&#58;//suo.im/3bQkiz- 作者：Bifet, A., Bouchachia, A., Gama, J., Pechenizkiy, M., &amp; Zliobaite, I. ACM Comput. Surv., 2014- 数据：引用：314、HIC：4、CV：23- 摘要：该文全面介绍了概念漂移适应。它指的是当输入数据与目标变量之间的关系随时间变化之时的在线监督学习场景。作者：Bifet, A., Bouchachia, A., Gama, J., Pechenizkiy, M., &amp; Zliobaite, I. ACM Comput. Surv., 2014 摘要：该文全面介绍了概念漂移适应。它指的是当输入数据与目标变量之间的关系随时间变化之时的在线监督学习场景。 16. 论文：深度卷积激活特征的多尺度无序池化（Multi-scale Orderless Pooling of Deep Convolutional Activation Features） 链接：http&#58;//suo.im/3gNw8e- 作者：Gong, Y., Guo, R., Lazebnik, S., &amp; Wang, L. (2014). ECCV- 数据：引用：293、HIC：23、CV：95- 摘要：为了在不降低其辨别力的同时改善卷积神经网络激活特征的不变性，本文提出了一种简单但有效的方案：多尺度无序池化（MOP-CNN）。作者：Gong, Y., Guo, R., Lazebnik, S., &amp; Wang, L. (2014). ECCV 摘要：为了在不降低其辨别力的同时改善卷积神经网络激活特征的不变性，本文提出了一种简单但有效的方案：多尺度无序池化（MOP-CNN）。 17. 论文：同时检测和分割（Simultaneous Detection and Segmentation） 链接：http&#58;//suo.im/4b0ye0- 作者：Arbeláez, P.A., Girshick, R.B., Hariharan, B., &amp; Malik, J. (2014) ECCV- 数据：引用：286、HIC：23、CV：94- 摘要：本文的目标是检测图像中一个类别的所有实例，并为每个实例标记属于它的像素。我们称将此任务称为同时检测和分割（SDS）。作者：Arbeláez, P.A., Girshick, R.B., Hariharan, B., &amp; Malik, J. (2014) ECCV 摘要：本文的目标是检测图像中一个类别的所有实例，并为每个实例标记属于它的像素。我们称将此任务称为同时检测和分割（SDS）。 18. 论文：一份关于特征选择方法的调查（A survey on feature selection methods） 链接：http&#58;//suo.im/4BDdKA- 作者：Chandrashekar, G., &amp; Sahin, F. Int. J. on Computers &amp; Electrical Engineering- 数据：引用：279、HIC：1、CV：58- 摘要：在文献中，有许多特征选择方法可用，由于某些数据集具有数百个可用的特征，这会导致数据具有非常高的维度。作者：Chandrashekar, G., &amp; Sahin, F. Int. J. on Computers &amp; Electrical Engineering 摘要：在文献中，有许多特征选择方法可用，由于某些数据集具有数百个可用的特征，这会导致数据具有非常高的维度。 19. 论文：用回归树集成方法在一毫秒内实现人脸校准（One Millisecond Face Alignment with an Ensemble of Regression Trees） 链接：http&#58;//suo.im/1iFyub- 作者：Kazemi, Vahid, and Josephine Sullivan, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition 2014- 数据：引用：277、HIC：15、CV：0- 摘要：本文解决了单个图像的人脸校准问题。我们展示了怎样使用回归树集成来直接从像素强度的稀疏子集估计面部的地标位置，并通过高质量的预测实现了超实时性能。作者：Kazemi, Vahid, and Josephine Sullivan, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition 2014 摘要：本文解决了单个图像的人脸校准问题。我们展示了怎样使用回归树集成来直接从像素强度的稀疏子集估计面部的地标位置，并通过高质量的预测实现了超实时性能。 20. 论文：关于作为混合系统的多分类器系统的调查（A survey of multiple classifier systems as hybrid systems） 链接：http&#58;//suo.im/3c9EFD- 作者：Corchado, E., Graña, M., &amp; Wozniak, M. (2014). Information Fusion, 16, 3-17.- 数据：引用：269、HIC：1、CV：22- 摘要：模式分类领域目前关注的焦点是几种分类器系统的组合，构建这些分类器系统可以使用相同或者不同的模型和／或数据集构建。作者：Corchado, E., Graña, M., &amp; Wozniak, M. (2014). Information Fusion, 16, 3-17. 摘要：模式分类领域目前关注的焦点是几种分类器系统的组合，构建这些分类器系统可以使用相同或者不同的模型和／或数据集构建。 转载来源：机器学习和深度学习引用量最高的20篇论文：2014-2017]]></content>
      <categories>
        <category>科技</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>机器学习</tag>
        <tag>微软</tag>
        <tag>科技</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[3000多年的欧洲历史，BBC用10部纪录片讲完了]]></title>
    <url>%2F2017%2F162236a5%2F</url>
    <content type="text"><![CDATA[文 | 年牧（历史研习社社员） BBC的纪录片对于纪录片的爱好者而言是必追的，自然类、科技类还有文史类，款款都是小编的最爱，每天都煲个一两集，这不就有十部经典的历史类BBC纪录片在等着你吗！ 1十字军东征 &gt;&gt;&gt;&gt;内容简介 BBC的这部纪录片以三集的长度为我们介绍了关于这场十字军东征的历史，并且也详细地介绍了为争夺基督教世界最神圣土地–圣城耶路撒冷的控制权，教皇对伊斯兰世界发动的新的圣战，。而这场关乎基督教和伊斯兰教的斗争持续了两个世纪，直至今日仍引发争论。 &gt;&gt;&gt;&gt;网址链接 http&#58;//www.bilibili.com/video/av3051095/ 2文明的轨迹 &gt;&gt;&gt;&gt;内容简介 BBC的这部纪录片又名：西方艺术史话。其两年才拍摄完毕，剧组跨越13个国家，曾走访百多个城市拍摄。纪录片通过13集的长度为我们展现了西方文明艺术的历史，虽然由于拍摄时间较早，有些观点可能已经有所发展，但是这种通过艺术、音乐来呈现历史观点的做法，成为日后无数纪录片看齐并努力超越的基准。 &gt;&gt;&gt;&gt;网址链接 http&#58;//www.bilibili.com/video/av1990120/ 3英国司法史 &gt;&gt;&gt;&gt;内容简介 BBC这部纪录片系统地介绍了英国司法的三部曲，共分三集：法律的诞生、追求自由、无罪推定。虽然主题是法律（司法史），但跟随主持人的脚步，你将了解到英国法的历史不过是英国人民的历史。 &gt;&gt;&gt;&gt;网址链接 http&#58;//www.bilibili.com/video/av4096167/ 4中世纪思潮-系列 &gt;&gt;&gt;&gt;内容简介 4集长度的BBC纪录片，在不同层面为我们展现出中世纪的历史，其包括了中世纪欧洲的启蒙、基督教信仰、中世纪阶级等方面的概述，讲故事的叙述方式总让人觉得非常神秘。 &gt;&gt;&gt;&gt;网址链接 http&#58;//www.bilibili.com/video/av2940699/ 5维多利亚女王和她的子女们 &gt;&gt;&gt;&gt;内容简介 BBC这部纪录片既可以说是历史方面的纪录片，又可以说是一部有关于皇室家庭的纪录片，这三集纪录片通过维多利亚女王与她丈夫和九个孩子的私人关系来探索她的统治。三集分别包括：最佳的方案，屋内的暴君，终归是王子。 &gt;&gt;&gt;&gt;网址链接 http&#58;//www.bilibili.com/video/av5252177/ 6英国史 &gt;&gt;&gt;&gt;内容简介 BBC的这部纪录片对于想要了解与学习英国历史是很好的途径，通过介绍英国文明，让我们感受到了从新石器时代到伊丽莎白时代，从17世纪暴乱的国内战争到我们所知道的日不落大不列颠帝国的历史情节，再现了这一个伟大而又波澜壮阔的国家。 &gt;&gt;&gt;&gt;网址链接 http&#58;//www.bilibili.com/video/av4755161/ 7日耳曼部落 &gt;&gt;&gt;&gt;内容简介 BBC纪录片通过4集为一系列为我们展现了日耳曼部落的历史发展，这4集由四个章节组成，分别为野蛮人对阵罗马、条顿堡森林战役、帝国的和平以及在基督十字的引领下，为我们展现出这一段神秘的历史。 &gt;&gt;&gt;&gt;网址链接 http&#58;//www.bilibili.com/video/av3414258/ 8母老虎：英国的那些女王们 &gt;&gt;&gt;&gt;内容简介 这部BBC的纪录片很是特别，从其名字可以看出主要讲的是英国的女王们，就如网友评论所说的“……这部纪录片用一种更加悲悯的态度描述了性别歧视加诸于她们的不公以及她们的挣扎。中世纪女性只能牺牲原有性别的一些天性才能换得与男权主义对抗的筹码”。“探索她们的历史也让我们了解到，漫漫长路至今，世界似乎从未改变”。 &gt;&gt;&gt;&gt;网址链接 http&#58;//www.bilibili.com/video/av3100743/ 9古代埃及人 &gt;&gt;&gt;&gt;内容简介 这部BBC的纪录片很是特别，它不像是一般的纪录片，而是试图用叙事的方式来重现古埃及社会，在四集关于古代埃及的历史故事中，演员们都一丝不苟的用古埃及的语言来交流着，并且运用了CGI技术重现已经不复存在的历史场景。对于重新展现历史，了解历史有着特别的作用。 &gt;&gt;&gt;&gt;网址链接 http&#58;//www.bilibili.com/video/av5215173/ 10伦敦：双城记 &gt;&gt;&gt;&gt;内容简介 该部BBC纪录片以英国历史上最为引人注目的时期之一——17时期为开始时间，并且以这一时期的两份出色的调查行记为主题。而前后两份行记中所体现出的差异为我们揭示了伦敦的起源。而通过对这两份行记加以比对，可以帮助我们更好地了解伦敦如何能在这个不寻常的世纪中发生巨大变革。 &gt;&gt;&gt;&gt;网址链接 http&#58;//www.bilibili.com/video/av3147771/ 当然这十部历史记录片只是众多优秀历史纪录片的其中一小部分，只要大家多去留意，相信会有更多优秀历史纪录片在等待大家，同时也欢迎大家给予补充！（ps&#58;观看时请记得关弹幕哦~） 看10万历史学人逆时针“解毒”世界，请关注公众号“历史研习社”（ID mingqinghistory）。 转载来源：3000多年的欧洲历史，BBC用10部纪录片讲完了]]></content>
      <categories>
        <category>旅游</category>
      </categories>
      <tags>
        <tag>BBC</tag>
        <tag>欧洲</tag>
        <tag>英国</tag>
        <tag>基督教</tag>
        <tag>维多利亚女王</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习 Python 库 Top 20]]></title>
    <url>%2F2017%2F37b60b28%2F</url>
    <content type="text"><![CDATA[机器学习 Python 库 Top 20 转载来源：机器学习 Python 库 Top 20]]></content>
      <tags>
        <tag>Python开发者</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[五大顶级CSS性能优化工具，值得程序员一试！]]></title>
    <url>%2F2017%2F234b915b%2F</url>
    <content type="text"><![CDATA[为什么Web页面的加载速度如此重要？在这个信息化的时代，如果一个网站的加载时间过长，大部分用户会极其不耐烦地选择“关掉”！这让辛辛苦苦熬夜敲代码的程序员们情何以堪，不管网站功能如何强大，用户根本都没来得及看一眼，结果网站访问量越来越少，粉丝转化率越来越低，最后，程序员可能就要下岗了。 了解前端开发的程序员都知道，影响网站性能的因素有很多，例如，HTTP请求数量，臃肿的代码，繁重的媒体文件等。如何编写CSS以及如何在浏览器中加载样式表都会对加载时间造成重大影响，接下来推荐五款针对CSS的性能提升工具，以帮助广大前端开发程序员创建一流的网站。 TestMyCSS TestMyCSS是一款免费的在线优化工具，具有很多功能。它可用来检查代码冗余，验证错误，未使用的CSS和寻求最佳做法。程序员只需将网址输入网站的CSS文件，就可以立即开始使用，TestMyCSS可以发现需要改进的所有项目。不仅如此，程序员还可以看到一些有用的提示： 如何简化复杂的选择器- 需要去掉的重复的CSS属性和选择器- 代码中存在的重要声明的数量- 不必要的类特异性- 不必要的IE修复- 不需要供应商前缀的CSS属性- 具有标签名的类或ID规则，例如a.primary-link- 通用选择器使用不当需要去掉的重复的CSS属性和选择器 不必要的类特异性 不需要供应商前缀的CSS属性 通用选择器使用不当 Stylelint StyleLint是一款相当强大的CSS linter，它与PostCSS（一种开发工具）一起编写了最先进的CSS，linter是一个可通过代码捕获潜在错误的程序。 StyleLint可以用来： 检查拼写错误，如打字错误，十六进制颜色无效，重复选择器等。- 寻求最佳做法实现- 统一编码风格，如每个CSS规则中的一致间距等。- 支持新的顶级CSS语法- 使用stylefmt自动修复一些警告，一种格式化CSS规则的工具- ……寻求最佳做法实现 支持新的顶级CSS语法 …… StyleLint非常强大，程序员可以使用其具备的： StyleLint CLI（命令行界面）- 构建工具的插件，例如webpack，gulp等。- 文本编辑器的插件，例如Atom，Sublime Text等。- StyleLint Node API- StyleLint PostCSS插件构建工具的插件，例如webpack，gulp等。 StyleLint Node API CSS Triggers CSS Triggers提供在线的页面解析参考，程序员可通过此参考了解哪些CSS属性触发了重绘和合成，但不引发布局，这些是浏览器在渲染网页时的执行过程。 Layout：浏览器生成每个元素的几何形状和位置- Paint：浏览器将每个元素的像素解析为图层- Composite：浏览器在屏幕上绘制图层。Paint：浏览器将每个元素的像素解析为图层 合成操作是浏览器执行的最廉价操作，如果你的CSS动画的代码反复触发合成和重绘操作的属性，则很难将60fps（每秒帧数）作为平滑网页动画的关键数字。 cssnano 当使用CSS呈现页面的关键路径时，使用精简的、结构良好的样式表文档就变得很重要。 换句话说，默认的浏览器网页渲染过程，直到样式表被加载，解析和执行完成后才停止。因此，如果CSS文档大而且杂乱，网站的加载时间就会很久。 cssnano是PostCSS的CSS优化和分解插件。cssnano采用格式很好的CSS，并通过许多优化，以确保最终的生产环境尽可能小。 Critical Critical是处理上一节提到的CSS关键路径问题的另一个工具。为了获得最佳性能，程序员可能需要考虑将关键CSS直接插入到HTML文档中，因为这消除了关键路径的额外往返行程…… 该想法的具体实践是查找关键的CSS规则，并将这些规则放在HTML文档的部分。Critical生成并内联关键路径CSS，程序员可同时使用Grunt和Gulp。有关使用此工具内联关键CSS的详细教程，可访问（https&#58;//www.sitepoint.com/how-and-why-you-should-inline-your-critical-css/） 以上五大工具可帮助前端开发程序员搭建一个可快速加载的网站，同时让样式表更精简，减少错误，进而方便浏览器的加载和解析。其实相关的性能优化工具有很多，但各有优劣，你会选择哪一款呢？ 转载来源：五大顶级CSS性能优化工具，值得程序员一试！]]></content>
      <categories>
        <category>科技</category>
      </categories>
      <tags>
        <tag>CSS</tag>
        <tag>程序员</tag>
        <tag>HTML</tag>
        <tag>Sublime Text</tag>
        <tag>文本编辑器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python里的黄金库，学会了你的工资至少翻一倍]]></title>
    <url>%2F2017%2F254d249b%2F</url>
    <content type="text"><![CDATA[Python里的黄金库，学会了你的工资至少翻一倍 转载来源：Python里的黄金库，学会了你的工资至少翻一倍]]></content>
  </entry>
  <entry>
    <title><![CDATA[机器学习如何入门？Kaggle CTO刚刚写了份详细的指南]]></title>
    <url>%2F2017%2F3cbde866%2F</url>
    <content type="text"><![CDATA[李林 编译整理量子位 出品 | 公众号 QbitAI 上周，Ben Hamner很忙。 作为全球最大数据科学和机器学习竞赛平台Kaggle的联合创始人&amp;CTO，他在Quora上参加了一场AMA，还参加了一场机器学习会议。会议我们最后再说，先说AMA：它是ask me anything的首字母缩写，美国reddit、Quora等社区经常邀请名人参加这种在线问答活动，比如说盖茨就每年都会在reddit上搞AMA。 AMA中得票最高的答案是“研究机器学习和人工智能最好的资源是什么？”Hamner在答案中把机器学习的入门过程分成8步，写了一份详细的指南。量子位将要点编译如下： 你很幸运，要开始研究机器学习和人工智能，现在是比任何时候都好的时机。这个领域近年来在快速发展，专业人士发布并改进着高质量的开源软件工具和库，每天都有新的线上公开课和博客文章出现。机器学习在各个领域每天贡献着数十亿美元的营收，带来了无与伦比的资源和大量工作机会。 这也意味着，你在刚接触这个领域时会有被淹没的感觉。下面是我的入门方法，如果你在研究过程中卡住了，去Kaggle上搜索一下，很可能有人遇到过和你类似的问题，如果没有，可以在我们的论坛上发帖提问，这是个获得指引的好办法。 1. 找一个你感兴趣的问题从一个你想解决的问题入手，会更容易集中精力，也更有学习的动力，这种方法比照着一份长得吓人的散乱知识点清单来学习要好很多。和被动地阅读相比，解决问题也能驱使你深入到机器学习之中。 好的入门问题有以下几个标准： 涉及你个人感兴趣的领域；- 有现成的数据适合用来解决这个问题，否则你需要花大把的时间来找数据；- 你能够在一台机器上流畅地处理这些数据，或者它的子集。有现成的数据适合用来解决这个问题，否则你需要花大把的时间来找数据； 想不出来要解决的问题？上Kaggle嘛……Kaggle有个入门系列竞赛，提供了适用于新手的机器学习问题。推荐从泰坦尼克号乘客的生还概率预测（https&#58;//www.kaggle.com/c/titanic）开始。 2. 做一个快速、脏乱、黑客范儿的端到端解决方案初学者很容易陷入一个实现细节之中，或者为错误的机器学习算法仔细调试，你需要避免这种错误。你的目标，是尽可能快地把端到端的基本解决方法做出来：读入数据、把它处理成适用于机器学习的格式、训练一个基本的模型、得出结果、评估它的性能。 3. 改进你的解决方案现在，基本功能已经实现，发挥创造性的时候到了。你可以尝试对最初解决方案中的每个组件进行优化，然后测试修改带来的作用，搞清楚该在哪个部件上花时间。通常来说，获取更多的数据或者请洗数据之类的预处理步骤，比优化机器学习模型有着更高的投入产出比。 这些步骤可能需要你亲自上手处理数据，比如说通过检查特定的某一行、通过可视化方法来查看数据分布等方式，来更好地理解数据的结构和怪癖。 4. 写出来你的解决方案&amp;分享想要获得别人对你的解决方案的反馈，最好的方法就是写出来并分享。写出你的解决方案意味着你会以新的方式去看它，并加深理解，也能让别人理解你的工作并做出反馈、帮你学习进步。写作也有助于开始建立机器学习作品集，来展示你的能力，对找工作很有帮助。 我们以Kaggle数据集和Kaggle Kernels为例，它们分别可以用来分享数据和解决方案，从而获得反馈，看其他人如何对你的问题进行扩展。这也是丰富你的Kaggle资料的办法。 5. 在更多问题上重复1-4步现在，你已经完成了一个自己喜欢的问题，接下来应该在不同领域的问题上多试几次。 你在入门的时候是不是用了表格式的数据？选一个要用到非结构化文本的问题，再试试解决图像相关的问题。 你是不是先解决了一个结构化的机器学习问题？很多有价值的创造性工作，一开始都有赖于从宽泛的商业或研究对象找到一个定义清晰的机器学习问题。 Kaggle竞赛和数据集为机器学习的两个方面：定义清晰的机器学习问题和原始数据来源提供了一个良好的起步点。 6. 认真地参加Kaggle竞赛和上千人比赛着去解决同一个问题，尽力做到最好，是一个很好的学习机会，这能够驱使你在这个问题上不断迭代，找到解决问题的有效途径。 关于其他人是怎样解决问题排除bug的，针对某个竞赛的论坛上有着丰富的资源，kernels体现了其他人对数据的洞察，并且给你提供了一个轻易的上手途径，获胜者的博客文章则展示了什么样的方法效果最好。 Kaggle竞赛提供了和别人组队的机会，我们的社区成员有着不同的背景和技能，每个人都能从其他人身上学到东西。 7. 在专业领域应用机器学习这让你在大部分时间中都能接触到机器学习，有助于自我提升。决定你想要成为什么样的角色、建立和这个角色相关的个人项目列表，是一个很好的开端。 如果你还没准备好应聘机器学习相关职位，也可以在你现在的岗位上开辟新项目、寻找提供咨询的机会、参与黑客马拉松和数据相关的社区服务机会、这些都能帮你在机器学习领域立足。专业领域的工作通常需要比较强的编程能力。 在专业领域应用机器学习，有这些价值的机会： 将机器学习用于生产系统；- 专注于机器学习研究，将技术发展的最高水平向前推进；- 用机器学习进行探查、分析，来提升你的产品和商业决策。专注于机器学习研究，将技术发展的最高水平向前推进； 8. 帮助别人研究机器学习教人学习能帮你巩固对基础概念的掌握。教别人有很多不同的方法，你可以根据自己的风格选一个： 写论文；- 做演讲；- 写博客文章和教程；- 在Kaggle、Quora等网站上回答问题；- 亲自指导；- 在Kaggle Kernels和GitHub上分享代码；- 讲课；- 写书。做演讲； 在Kaggle、Quora等网站上回答问题； 在Kaggle Kernels和GitHub上分享代码； 写书。 One More Thing… 这次AMA，其实Hamner最想谈的是Kaggle的未来，他在资料里列出了自己愿意回答的话题： Kaggle的未来- 开放数据- Kaggle竞赛- 机器学习和AI- 数据科学工作流程- 产品和工程- Kaggle为何加入Google开放数据 机器学习和AI 产品和工程 可惜Quora上的群众对Kaggle的未来似乎并不关心，反正竞赛照常举行，数据集照常提供，量子位也不知道这个未来该从何问起。 不过，吃瓜群众不关心Kaggle的未来也没关系。周五，Hamner还去纽约的机器学习大会MLConf上做了以《Kaggle的未来：我们从何处来，到何处去》的演讲，也就是我们开头说的那场会议。 在量子位（公众号：QbitAI）对话界面回复“Kaggle”，我们会把Hamner这次演讲的PPT发给你。 今天AI界还有哪些事值得关注？在量子位（QbitAI）公众号会话界面回复“今天”，看我们全网搜罗的AI行业和研究动态。笔芯❤~另外，我们建了一个机器学习入门群，希望和大家互相帮助、共同进步。欢迎加量子位小助手的微信：qbitbot，介绍一下你自己，符合要求的，我们会拉你进群。 转载来源：机器学习如何入门？Kaggle CTO刚刚写了份详细的指南]]></content>
      <categories>
        <category>科技</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>人工智能</tag>
        <tag>Kaggle</tag>
        <tag>Quora</tag>
        <tag>Reddit</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Vue2.0 实践，顺手撸了一个小项目]]></title>
    <url>%2F2017%2F08d53a1d%2F</url>
    <content type="text"><![CDATA[传送门：https&#58;//github.com/icepy/index-oa-template，或阅读原文。 这个项目的背景还要从“移动企业门户”说起，这是我厂的一个小项目，现也开源在Github上，其目的是为了给企业开发自己的企业门户提供参考和模板，可以快速的用起来，或者参考一下我们是如何来实现移动企业门户的。 Demo 效果如图： 这个项目，主要使用了Vue2.0和Vue-router，其实Vue-router不是必须的，因为这只有一个页面，但是也应用到了Vue的方方面面。在创建项目时，用了vue-cil来初始化这个项目，不过我也为它修改了一些自己想要的东西，没错就是weex相关的构建与入口，具体如何用同一个Vue2.0项目既可以跑Web也可以跑Native，你可以参考一下我这个项目中的weex分支。 既然用到了vue-router，那还是简单的贴一下代码，非常简单： importVuefrom’vue’ importRouterfrom’vue-router’ importHomefrom’pages/home/index.vue’ Vue.use(Router); constroutes= &#91; &amp;#123 path&#58;’/‘, name&#58;’home’, component&#58; Home &amp;#125 &#93;; exportdefaultnewRouter(&amp;#123 routes&#58; routes &amp;#125); importVuefrom’vue’; importAppfrom’./App’; importrouterfrom’./router’; newVue(&amp;#123 el&#58;’#app’, router, template&#58;’‘, components&#58; &amp;#123 App &amp;#125 &amp;#125); router的配置可以看的出来，非常的简单，一个对象用来描述一个path的所有，也是如此，你可以顺手的描述其它的规则，有path，component，等等。 整体的组件也是非常简单的，因为只需要引用即可。实际上稍微复杂一点的地方，主要在page/home/index.vue文件中，因为在这个文件里做了一些其它的事情，比如获取userid实现免登，那么只有当获取到userid之后才能去获取用户信息，也就是界面 下午好，管理员，icepy的变更。我用了$watch来处理这个问题，比较简单，： this.$watch(‘userId’,function()&amp;#123 this.getUserInfo(); &amp;#125); getUserInfo定义在methods中： getUserInfo&#58;function()&amp;#123 // 根据userid获取用户详细信息 constself=this; constgetUserInfoRequest= &amp;#123 url&#58;OPENAPIHOST+’/api/get’, params&#58; &amp;#123 userid&#58;this.userId &amp;#125 &#125; dingWISDK.getUserInfo(getUserInfoRequest).then(function(response)&amp;#123 constdata=response.data; self.meta.userInfo= data; &amp;#125).catch(function(error)&amp;#123 alert(‘获取用户信息 error：’+JSON.stringify(error)); &amp;#125); &amp;#125 当userid有变化时，立即调用getUserInfo来更新用户界面。 其实从源代码中可以看见，界面都是一个个单独的组件，通过数据的传递来渲染，单组件文件系统，没什么好说的，大家有兴趣可以多看一看官网的文档。当你不需要加入vuex时，对于驱动界面还是比较简单的，书写下来，只是有一些地方需要注意，特别是React开发者转过来的： props传递，需要用v-bind&#58;，并且在子组件中用props&#58;&#91;&#93;来声明- 监听事件时不需要bind，v-on&#58;click=”microAppsOpenLink(item,$event)”- 建议很多东西都写全，不要写简写，比如&#58;xxx这种，如果不是长期应用vue的开发者，看起来还要思考很久- 因为dom变成了模板，模板有编译的过程，处理类似比如自动触发一个事件这样的需求时需要想一想，ref的处理，看起来没那么直观- 引用组件的时候，需要显示的声明，比如：监听事件时不需要bind，v-on&#58;click=”microAppsOpenLink(item,$event)” 因为dom变成了模板，模板有编译的过程，处理类似比如自动触发一个事件这样的需求时需要想一想，ref的处理，看起来没那么直观 components&#58; &amp;#123 banner&#58; banner, applist&#58; applist, item&#58; item, admin&#58; admin, userlist&#58; userlist, appmanager&#58; appmanager &amp;#125, 也许还有很多需要注意的细微之处，待你慢慢挖掘了。 比较好的消息是WebStorm开始原生支持Vue了，可见其火热的趋势，回过头来可以看到我们做事情时的一些反思：贵在坚持。 你身边如果有朋友对混合领域（跨技术栈）或全栈，编程感悟感兴趣，可以转发给他们看哦，^_^先谢过啦。 更多精彩内容可关注我的个人微信公众号：搜索fed-talk或者扫描下列二维码，也欢迎您将它分享给自己的朋友。 转载来源：Vue2.0 实践，顺手撸了一个小项目]]></content>
      <categories>
        <category>科技</category>
      </categories>
      <tags>
        <tag>软件</tag>
        <tag>GitHub</tag>
        <tag>Pages</tag>
        <tag>多看阅读</tag>
        <tag>路由器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[剖析Elasticsearch集群系列第一篇 Elasticsearch的存储模型和读写操作]]></title>
    <url>%2F2017%2F1f2b3fca%2F</url>
    <content type="text"><![CDATA[剖析Elasticsearch集群系列第一篇 Elasticsearch的存储模型和读写操作 转载来源：剖析Elasticsearch集群系列第一篇 Elasticsearch的存储模型和读写操作]]></content>
  </entry>
  <entry>
    <title><![CDATA[阳光宽频网]]></title>
    <url>%2F2017%2Ff77435e6%2F</url>
    <content type="text"><![CDATA[阳光宽频网 转载来源：阳光宽频网]]></content>
  </entry>
  <entry>
    <title><![CDATA[讲真，女人会穿衣服到底有多重要？]]></title>
    <url>%2F2017%2F0bb72fde%2F</url>
    <content type="text"><![CDATA[讲真，女人会穿衣服到底有多重要？ 转载来源：讲真，女人会穿衣服到底有多重要？]]></content>
      <tags>
        <tag>你喵姐</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2017 春节 九州美食温泉之旅,九州旅游攻略 - 马蜂窝]]></title>
    <url>%2F2017%2Fe1f6a3d6%2F</url>
    <content type="text"><![CDATA[2017 春节 九州美食温泉之旅,九州旅游攻略 - 马蜂窝 转载来源：2017 春节 九州美食温泉之旅,九州旅游攻略 - 马蜂窝]]></content>
  </entry>
  <entry>
    <title><![CDATA[是男人就要多看这几本书，本本都是情商的提速器，为情商提速]]></title>
    <url>%2F2017%2Fb99ccb3d%2F</url>
    <content type="text"><![CDATA[是男人就要多看这几本书，本本都是情商的提速器，为情商提速 转载来源：是男人就要多看这几本书，本本都是情商的提速器，为情商提速]]></content>
  </entry>
  <entry>
    <title><![CDATA[武汉全市各开发区、功能区、行政区 对表报告明确定位狠抓落实]]></title>
    <url>%2F2017%2F901c5be3%2F</url>
    <content type="text"><![CDATA[万涓归流，汇为大海。拼搏赶超，加快复兴大武汉，全市各区域各领域都能从党代会报告中找到行动方向。坐不住、等不起、慢不得，紧跟党代会召开的脚步，全市各开发区、功能区和行政区闻鸡起舞，踏上新征程，精确对表党代会报告，狠抓贯彻落实。 市委常委︑东湖高新区党工委书记胡立山： 建设“四个先行区”走在前列 我们将围绕“建设天下谷——全球有影响力的创新创业中心”目标，以统筹、创新、开放和绿色发展为主线，建设好“四个先行区”，并走在前列。 统筹“双自联动”战略，做改革创新的先行区。紧密结合东湖国家自主创新示范区和自贸试验区建设，坚持“双自联动”，重点推进政务服务、科技体制机制等关键领域的改革创新，加速发展四新经济，加快形成以创新为主要引领和支撑的新兴产业体系和发展模式。 全面推进自主创新示范区建设，做战略性新兴产业发展的先行区。依托长江存储、武汉华星光电、武汉天马等，做大做强光电子产业；培育壮大人福医药等，大力发展生物健康产业；紧盯未来产业，超前加快布局智能化新能源汽车、AR/VR、人工智能等产业。 全面启动自贸区建设，做国际化能力提升的先行区。加快构建与国际规则相一致的营商环境，加快引进一批国际化企业，推动企业走出去，完善国际化城市功能，大力发展国际化教育、医疗、体育、文化和社区。 推动科技新城建设，做产城融合发展的先行区。落实“绿色发展”理念，坚持高水平规划建设和管理，建设生态宜居智慧科技新城。 （记者肖娟） 市委常委︑武汉临空港经开区管委会主任︑东西湖区委书记曹裕江： 制定实施临空产业发展行动计划 在奋力拼搏赶超的进程中，武汉临空港开发区要以只争朝夕的精神状态，以快干、实干、精干的工作作风，在培育新产业结构、培育发展新动能、扩大有效投资、建设美丽乡村等方面当好“四个示范”。 一是以通用航空产业为重点，着力培育新产业结构。在巩固壮大食品、机电、物流产业的同时，加快制定实施临空产业发展三年行动计划，推进汉北通航机场前期审批工作，适时启动通航产业园总体规划的编制。 二是以国家网络安全人才与创新基地为重点，着力培育发展新动能。加快确定产、学、研协同建设发展的总体规划与实施方案，及时制定出台政策措施，组建支持基地建设的产业基金，及时签约一批项目，启动建设一批项目。 三是以提高签约率、供地率、开工率为重点，着力扩大有效投资。建立招商引资“一号工程”领导机制，建立健全区级领导“一挂两包”工作制度，党政“一把手”亲自抓、直接抓。 四是以“一产三产化”为方向，着力建设美丽乡村。大力推进“四水共治”“湿地绿道”、农村新社区建设工程，深度发展乡村休闲游。 （记者罗京） 市委常委︑武汉开发区工委书记︑汉南区委书记胡亚波： 办世界飞行者大会发展通航产业 市第十三次党代会提出“发展先进制造业”，武汉开发区（汉南区）是全市重要的制造业区域，将以国家级开发区的使命感，具体到年度计划，具体到项目，用一个个小目标来推动产业转型升级。 2017年，开发区（汉南区）将继续做大汽车产业集群，促进东风雷诺、新兴重工全面量产，加快推进东本三厂建设，2017年整车产量力争达到140万辆，并大力发展智能制造和新兴产业，培育发展机器人、新材料、通用航空等新兴产业集群，构建迭代产业体系。 今年，首届“国际航联世界飞行者大会（WFE）”将在开发区（汉南区）举行，预计将有60多个国家、3000多名运动员、国内外约1000架飞行器齐聚武汉，300余家航材及装备生产商参展，60余万观众观赛。作为飞行者大会活动的主要场地，目前，汉南通用航空机场正在建设中。 以此为契机，开发区（汉南区）推动中航通飞、太航星河、珠海易航等项目落地建设，努力把我区打造成为国内最知名的通航产业发展区。 （记者康鹏 通讯员周明德） 东湖风景区管委会工委书记︑管委会主任陈光菊： 规划建设东湖城市生态绿心 东湖风景区作为我市唯一以生态保护为主的功能区，必须要在“建设生态化大武汉”中走在全市前列。2017年，我们的工作将紧紧围绕“规划建设东湖城市生态绿心，传承楚风汉韵，打造世界级城中湖典范”这一目标展开。 第一，加快推进东湖绿道二期项目建设。我们将按照市委市政府的统一部署，与地产集团密切配合，积极参与东湖绿道二期规划建设，全力做好征地拆迁、环境整治等服务保障工作。 第二，加快推进东湖水质提档升级。我们将按照“增二类、扩三类、转四类、灭五类”的全市治水总目标，继续完善《东湖水环境综合治理规划》，通过实施引江济湖、六湖连通、底泥疏浚等工程实施湖泊综合整治。 第三，加快推进东湖景中村改造。我们将遵循“景村交融、和谐共生”的核心理念，深入研究东湖景中村特点，切实找准适合不同片区的村庄改造模式。（见习记者晋晓慧） 硚口区委书记景新华： 加快汉正街核心区块建设 硚口区将以打造中部地区现代服务业标杆为目标，着力做好以下几方面工作： 全力推进汉正街建设改造，加快征收拆迁、转型升级、功能再造和文化保护，规划打造武汉金融中心、国际总部中心、高端商务中心，把汉正街·武汉中央服务区建设成为长江主轴核心区块。 做强现代商贸业，深化国家电子商务示范基地建设，推动实体商业和电子商务融合发展，支持中百、工贸、宜家、恒隆、凯德等重点企业发展，提升重点功能区的影响力和辐射力。 做大工业服务业，支持标致雪铁龙、远大等工业总部壮大发展，加快引进一批工业总部、研发、设计和售后服务企业，打造工业服务业集聚区。 做优健康服务业，依托同济医院、同济医学院等优质资源，重点发展高端医疗、精准医疗、智慧医疗、第三方检测、健康大数据平台，打造环同济健康城。 （记者董晓勋） 汉阳区委书记徐洪兰： 做强汉阳文化创意产业 汉阳区针对产业结构不优、城区功能不强这个最大短板，聚力改革创新，做强文化品牌，努力将汉阳打造成武汉建设国家级文化创意中心的核心区。 汉阳的优势在于文化资源丰富，我们将依托琴台大剧院、晴川创意谷和大归元改造项目，统筹推进琴台中央文化艺术区、归元太古里、五里城市综合体建设，努力打造琴台国际金融文化中心。发挥路桥工程设计建设企业相对集聚的优势，努力建设国家级路桥工程设计文化创意中心。 我们还将发挥武汉国际博览中心平台优势，对标国际一流会展功能区，加大招商引资力度，完善会展产业链，引进国际知名展会，努力将四新地区打造成会展文化产业聚集地。 “汉阳造”是工业文化的一个代表，我们计划依托黄金口智能制造园，引导企业弘扬“汉阳造”精神，提高产品质量和文化内涵，争当产品标准和行业规范制定者，努力将黄金口都市工业园建成都市工业文化试点示范园区。（记者蔡爽） 武昌区委书记张幸平： 建设好滨江高端商务区 武昌滨江文化商务区地处主城区内长江南岸核心区域，我们将勇于担当，将其打造成武汉长江中轴亮点区块。我们将遵循“一年打基础，两年求突破，三年见实效”的原则，对滨江文化商务区的建设时序进行控制。 2017年启动项目全面开工，启动滨江核心区地铁产业中心、福星惠誉、万吨冷库、华电总部、交职院、昙华林、斗级营等地块建设。2018年全面启动建设，集中启动武昌滨江核心片、337片地块、得胜桥沿线建设。2019-2020年初具规模，启动二桥南片及万吨冷库片周边地块建设。2021年核心区全面建成，推动月亮湾长江论坛、武车宿舍片等亮点项目建设。 未来几年，武昌滨江文化商务区将以高端商务为龙头，国际金融为主导，文化创意、信息咨询产业集群、国际社区、商业配套为支撑，人文生态发展为基底，建成代表武汉现代服务业聚集最高水平之一，人文底蕴最深厚的城市亮点区域。 （记者李咏） 青山区委书记苏霓斌： 从源头上减少排放 建设生态化大武汉，青山必须勇于担当、奋发作为。通过政企联动建设良好生态环境，为武汉成为国际知名宜居城市作出应有贡献。 一是针对青山地区工业“三废”排放占全市60%左右的实际，完成上级督办的突出环境问题整改等，切实从源头上减少排放。 二是启动30平方公里北湖生态试验区建设，强力推进已经策划的19个、投资590亿元的重点项目；对地区17个公园进行提档升级，进一步缩小空气质量优良天数与全市的差距。全力以赴抓好118项、投资112亿多元的海绵城市试点，坚决改变地下排水管网等基础设施建设滞后现状。 三是要大力弘扬干事创业精神，引导干部队伍提振“精气神”，始终保持干事创业的蓬勃朝气，克难攻坚的昂扬锐气，用实际行动投入全面复兴大武汉新征程中。 （记者李锐） 洪山区委书记黎东辉： “刻意人为”留住大学生 洪山将在推进“留住大学生”这一效益最高的发展战略上主动作为，在做好“学、城、人”结合这一最大文章上“刻意人为”，在将“大学之城”这一最大资源转化为经济发展的最大优势上“走在前列、做出样板”。 一是以融合发展为主攻方向，打造学城联动的大学之城。借鉴国际成功经验，打破大学与城市之间有形、无形的“围墙”，实施校区、社区、园区“三区联动”，以校带城、以城促校。 二是以凝聚青年为核心目标，打造育才聚才的青年之城。开展“百万大学生看武汉”等活动，增进大学生对武汉的热爱；实施“青桐计划”等项目，吸引更多的青年英才在洪山成家、立业、扎根。 三是以创新创业为关键抓手，打造筑梦圆梦的梦想之城。以“创谷”计划为引领，建设一批“三生融合”的创新平台；以高校、科研院所为依托，打造高端研发机构、创新创业平台、风险投资机构和高新技术企业集聚区。 （记者刘元聪） 武汉化工区党工委书记︑管委会主任黄家喜： 打造绿色石化产业集群 化工区将坚持以建设“一流生态化工园区”为目标，推进产业集聚发展、绿色发展和创新发展，努力打造国家重要石化产业基地、长江中游重要化工物流基地和国家新型工业化产业示范基地。 全区将强化一个目标。把招商引资作为经济工作主抓手，制定硬目标，出台硬举措，强化硬考核，努力实现招商引资提质增效。 我们将从五类项目入手，着力抓好项目引进，大力推进产业链招商。一是乙烯扩能项目，推动乙烯扩能改造，能够增加原料供应，进一步拉动下游产业链发展，提升规模效应和龙头效应；二是下游产业项目，包括奥克二期、百杰瑞、江苏大力士等6个项目；三是化工新材料项目；四是重点产业配套项目，拟定重点产业项目的责任分解表，明确时间节点、形象进度、责任单位和责任人，以铁的手腕推动项目落地；五是世界500强、化工50强项目，化工行业龙头项目，实现突破。 （记者汪文汉 通讯员王莉） 武汉新港管理委员会主任张林： 出台武汉航运中心实施方案 做足“水、港、人”三篇文章，对武汉新港来说，核心就是做足“港”文章，我们必须立足于“干”，在现代化、国际化、生态化建设各方面作出应有贡献。 一要抓紧完成阳逻国际港核心功能区系列规划，完成武汉长江中游航运中心总体规划的审批，出台实施方案。 二要稳定开行“江海直达”等品牌航线，完善上游全中转、下游全分流运输模式，提升“中三角省际集装箱公共班轮”航线效能，筹划近洋集装箱直航、汽车滚装江海直达运输的船型及运输组织工作，争取早日开通。 三要加快铁水联运设施建设，实现水运枢纽与铁路枢纽的“强强联合”，加快推进疏港公路改造和建设。 四要实现武汉新港空港综保区封关运营，首年进出口总货值达到4亿美元；实现武汉航交所年交易额突破20亿元，推出汽车滚装运价指数，形成定价机制；建成武汉电子口岸·国际贸易“单一窗口”，推动贸易便利化。 五要依托航运服务中心，引进大企业，发展国际结算，加快武汉航运产业总部区建设，提升供应链管理能力。 六要推进大招商，发展临港经济，打造长江经济带新的增长点。 （记者汪文汉） 江岸区委书记王炜： 勾画长江主轴上最美画卷 党代会提出“规划优化武汉长江主轴，打造世界级城市中轴文明景观带”，江岸作为首善之区，有条件、有责任为武汉打造长江主轴多做贡献，努力勾画出长江主轴上最美画卷。 突出功能布局，高水平建设亮点区块。推进历史文化风貌街区保护利用，通过景观提升、功能置换、以文聚气，形成演绎“老汉口”独特魅力的城市文化地标。加快汉口滨江国际商务区建设，发展总部经济、金融服务、高端商务，打造一流国际企业总部聚集区、滨江高端商务区。做足水文章，坚持“四水共治”，加强长江岸线自然生态环境保护，打造环抱江岸、开放舒适的城市绿色廊道。 我们还将总结推广“百步亭经验”，加快形成一批先进基层党组织示范群。实施“红色引擎工程”，以党组织的坚强有力引领带动各类群团组织和社会组织，以党员队伍的生机活力组织发动广大志愿者，密切联系服务群众，促进基层治理体系和治理能力现代化。 （记者李婷 通讯员祝丽芳） 江汉区委书记张俊勇： 建设国家现代服务业示范区 对照市第十三次党代会报告提出“发展现代服务业”和“建设世界一流的城市亮点区块”的要求，江汉区将高举现代服务业大旗，坚持产城融合，聚力高端高效，建设国家现代服务业示范区，构筑富裕活力美丽幸福新江汉。 在做强优势产业上积极作为。以国家服务业综合改革示范区建设为抓手，推动生产性服务业向价值链高端延伸，做强金融、商贸流通、商务服务、通信信息、文化创意、科技服务等“六中心”。推动生活性服务业向高品质升级，建设一批一流的国际学校、国际医院、国际社区。 在提升城区品质上积极作为。以《江汉区城市土地集约利用评价与发展规划》为引导，提高土地复合利用率。以“三旧改造”为抓手，改善群众居住环境，拓宽产业发展空间。围绕打造世界级城市中轴景观，把武汉中央商务区等建设成为城区转型和形象品质提升的亮点区块。（记者刘元聪） 蔡甸区委书记刘子清︓ 突出“法”字创新招商 蔡甸区要高标准建设中法武汉生态示范城，做好“法”字文章，创新招商方式。 蔡甸全区上下正牢牢抓住这一历史上最大的发展机遇，推动大保护、大开放、大开发，以中法科技谷为核心，以产兴城，打造中法国际合作创新区和湖北对外开放新高地；实施经济强镇改革，超常规推进常福、奓山、沌口一体化发展，按国家级开发区标准，做大做强蔡甸经济开发区；抢占战略性新兴产业高地，大力发展通航产业，开展低空旅游；坚持绿色发展不动摇，深挖蔡甸绿色宝藏，加快建设泛沉湖湿地国际旅游度假区。 蔡甸区已启动招商“一号工程”，党政主要负责人带头落实周末赴外地招商制度，以项目为中心组织经济活动，强化对干部抓经济发展能力的考核和评价，选拔了15名招商专员。目前，蔡甸已在法国、北京、上海、深圳等地设立招商联络处，建立了招商网络开展工作。 （记者朱波） 江夏区委书记王清华： 今年经济增速不低于12% 江夏将坚定“生态立区、工业兴区、创新强区”战略，紧抓发展第一要务，在前两年GDP增速分别以13.1%、12.8%领跑全市各区的基础上，今年也要实现不低于12%的增速。 实施“创新强区”战略。以阳光创谷为主要载体，以腾讯武汉研发中心为主要龙头，以区科投公司为主要平台，着力打造创新创业生态系统；加强政府、企业与高校有效衔接，通过“以校带城、以城促校”持续推动大学与城市融合发展。 实施“生态立区”战略。按照“四水共治”理念，加快组建湖泊管理局，持续巩固“三非”整治、“三网”拆除、“三退”还湖等水体治理成果；充分利用湖泊面积占全市近一半的独特水资源优势，规划建设一批环湖创新创业生态区。 实施“工业兴区”战略。聚焦汽车及零部件、智能装备制造、战略性新兴产业等产业，创新招商引资方式方法，持续扩大有效投资。 （记者林敏） 黄陂区委书记吴祖云： 推进大临空大临港国际化 黄陂是我市拥有大临空大临港优势的第一大区，我们要打好“水、港、人”三张王牌，促进大临空大临港发展向国际化迈进。 把招商引资、招商引智作为赶超发展的“一号工程”精心实施，运用多种方式大招商、招大商，全力培育临空制造与服务、智慧物流、全域旅游等骨干产业，并向规模化、集群化、高端化发展。 加快建设“铁水公空”多式联运综合交通枢纽区，充分发挥黄陂作为天河机场所在地、亚洲最大铁路编组站场所在地和武汉新港规划总部聚集地的优势，按照产城融合思路，高标准建设100平方公里的临空经济示范园、汉口北配套产业园和武湖、前川新城（罗汉）工业园等重点园区，特别在航空货运、飞机维修保养、通用航空、航空物流、临空商贸、仓储服务等方面要有大手笔、新突破。 坚定不移提升现代服务业，形成金融、商贸、物流、旅游、会展、运动、康养和高端酒店等优势功能版块，加快建设航交所、物流交易所、公共物流信息平台和行业总部。 （记者彭仲） 新洲区委书记陈新垓： 加快建设多式联运核心区 市第十三次党代会报告提出，“在阳逻地区规划建设多式联运核心区”。作为武汉“大临港”的主战场，我们将加快多式联运集疏运体系建设。 一是强力推动招商引资。围绕港口经济和先进制造业发展，努力引进港口物流、仓储服务、航空航天、节能环保等一批产业大项目、大企业。 二是做好阳逻新城“港、产、城”一体化发展“多规合一”工作。合理划分阳逻新城边界，在此区域内进一步完善城市总体规划、土地利用规划、产业发展规划和综合交通规划，科学划分“港在哪里、产在哪里、城在哪里”，明确阳逻新城各功能定位。 三是超前谋划建设阳逻新城的对外交通通道。提前完工轨道交通21号线，力争轨道交通10号线、21号线邾城延伸线建设项目纳入武汉市第四期轨道交通建设规划；全力推动江北铁路建设，力争提速外环东扩，打通阳逻港与鄂州顺丰机场连接通道，谋划建设临港大道，打通阳逻板块和光谷板块的连接通道。（记者谭德磊） 长江日报（官方微信ID：whcjrb)全媒体讯 转载来源：武汉全市各开发区、功能区、行政区 对表报告明确定位狠抓落实]]></content>
      <categories>
        <category>时政</category>
      </categories>
      <tags>
        <tag>投资</tag>
        <tag>经济</tag>
        <tag>武汉东湖</tag>
        <tag>长江</tag>
        <tag>新能源汽车</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[适合3月龄宝宝早教的6个小方法_宝宝树孕育知识_宝宝树]]></title>
    <url>%2F2017%2F0c260037%2F</url>
    <content type="text"><![CDATA[适合3月龄宝宝早教的6个小方法宝宝树孕育知识宝宝树 转载来源：适合3月龄宝宝早教的6个小方法宝宝树孕育知识宝宝树]]></content>
  </entry>
  <entry>
    <title><![CDATA[数据挖掘 知识重点（整理版）]]></title>
    <url>%2F2017%2Fa0803723%2F</url>
    <content type="text"><![CDATA[数据挖掘 知识重点（整理版） 转载来源：数据挖掘 知识重点（整理版）]]></content>
      <tags>
        <tag>大数据挖掘DT数据分析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[每日一笑，大王叫我来巡山，鸡宝宝们排队来拜年啦]]></title>
    <url>%2F2017%2F882ce809%2F</url>
    <content type="text"><![CDATA[每日一笑，大王叫我来巡山，鸡宝宝们排队来拜年啦 转载来源：每日一笑，大王叫我来巡山，鸡宝宝们排队来拜年啦]]></content>
  </entry>
  <entry>
    <title><![CDATA[下一代Web应用模型——Progressive Web App]]></title>
    <url>%2F2017%2Fc48cafa1%2F</url>
    <content type="text"><![CDATA[下一代Web应用模型——Progressive Web App 转载来源：下一代Web应用模型——Progressive Web App]]></content>
      <tags>
        <tag>CSDN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[嵌入式视觉的概念及关键因素解析]]></title>
    <url>%2F2017%2F8f539df5%2F</url>
    <content type="text"><![CDATA[嵌入式视觉的概念及关键因素解析 转载来源：嵌入式视觉的概念及关键因素解析]]></content>
      <tags>
        <tag>CSDN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[双语视频 | BBC拍了一部《中国春节》纪录片，这才是春节的正确打开方式]]></title>
    <url>%2F2017%2F2fad95e1%2F</url>
    <content type="text"><![CDATA[双语视频 | BBC拍了一部《中国春节》纪录片，这才是春节的正确打开方式 转载来源：双语视频 | BBC拍了一部《中国春节》纪录片，这才是春节的正确打开方式]]></content>
      <tags>
        <tag>留学家长圈</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[史上最强100种PPT标题做法！]]></title>
    <url>%2F2017%2F0906cc27%2F</url>
    <content type="text"><![CDATA[史上最强100种PPT标题做法！ 转载来源：史上最强100种PPT标题做法！]]></content>
      <tags>
        <tag>PPT研究院</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[据说这是柯基的标准睡姿]]></title>
    <url>%2F2017%2Fc9068afd%2F</url>
    <content type="text"><![CDATA[柯老板标准睡姿，好像用锅铲翻个面~肿么办 柯老板标准睡姿，好像用锅铲翻个面~肿么办 柯老板标准睡姿，好像用锅铲翻个面~肿么办 柯老板标准睡姿，好像用锅铲翻个面~肿么办 柯老板标准睡姿，好像用锅铲翻个面~肿么办 柯老板标准睡姿，好像用锅铲翻个面~肿么办 柯老板标准睡姿，好像用锅铲翻个面~肿么办 柯老板标准睡姿，好像用锅铲翻个面~肿么办 关注微信公众号：有宠资讯（yczx-pet），参与更多公益活动，和韩红一起关爱小动物。 转载来源：据说这是柯基的标准睡姿]]></content>
      <categories>
        <category>宠物</category>
      </categories>
      <tags>
        <tag>柯基犬</tag>
        <tag>宠物</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[天上装逼哪家强？一日千里坐俄航！]]></title>
    <url>%2F2017%2F25b644dd%2F</url>
    <content type="text"><![CDATA[天上装逼哪家强？一日千里坐俄航！ 转载来源：天上装逼哪家强？一日千里坐俄航！]]></content>
      <tags>
        <tag>视觉志</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[微信小程序1万字实操指南]]></title>
    <url>%2F2017%2Fa072b859%2F</url>
    <content type="text"><![CDATA[微信小程序1万字实操指南 转载来源：微信小程序1万字实操指南]]></content>
      <tags>
        <tag>虎嗅网</tag>
      </tags>
  </entry>
</search>
